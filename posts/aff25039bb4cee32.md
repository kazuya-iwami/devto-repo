---
title: 'AWS re:Invent 2025 - Reinventing Pharma Document Workflows to Unlock Value at Scale with Agentic AI'
published: true
description: 'In this video, Pranava Goundan and Raghav Sharma from ZS present their agentic AI solution, Scriptiva, for transforming pharmaceutical document workflows. They explain how clinical trials require 90+ documents over 5-8 years, consuming 10 person-years of effort. Their accelerator addresses fragmented data sources, non-standardized templates, and manual review processes through a layered architecture on AWS featuring digitization services, a knowledge layer connecting documents and data, agent orchestration frameworks, and template management. The solution can reduce clinical development timelines by three months, accelerating drug delivery to patients.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Reinventing Pharma Document Workflows to Unlock Value at Scale with Agentic AI**

> In this video, Pranava Goundan and Raghav Sharma from ZS present their agentic AI solution, Scriptiva, for transforming pharmaceutical document workflows. They explain how clinical trials require 90+ documents over 5-8 years, consuming 10 person-years of effort. Their accelerator addresses fragmented data sources, non-standardized templates, and manual review processes through a layered architecture on AWS featuring digitization services, a knowledge layer connecting documents and data, agent orchestration frameworks, and template management. The solution can reduce clinical development timelines by three months, accelerating drug delivery to patients.

{% youtube https://www.youtube.com/watch?v=JkZu0zlWloU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/0.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=0)

### The Challenge: Transforming Pharma Document Workflows in Clinical Trial Development

 Hello everyone, and thank you for joining us for the talk today. My name is Pranava Goundan, and I'm a partner at ZS. I lead our life sciences R&D AI capability. Hello everyone, my name is Raghav Sharma, partner with ZS based in our New Delhi office. I lead the technology for our drug development practice where my focus is to build next generation assets and accelerators to help our clients accelerate their drug development life cycle journey. I'm excited to be here and I truly appreciate all of you taking the time to attend this session. With this, I'll let Pranava kick us off today.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/60.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=60)

Yeah, absolutely. So for some of you in the audience, this might already seem like a very important problem. For some of you, you're wondering why this is an important problem to tackle, to improve pharma document workflows with agentic AI. So to get us started, let's just take the clinical trial development process. As many of you know,  in order to bring treatments to market, we run very extensive clinical trials that regulators monitor and provide oversight for.

Well, today in these processes when you're running a clinical trial, there are over 90 plus documents for a single clinical trial that need to be submitted to regulators over the course of, let's say, the five to eight year period that the clinical study is being run. That's initially to declare the protocol of a clinical trial all the way through to submitting the findings from your clinical study, submitting it to regulators for approval, and so on. Now this is a very time intensive process today. Almost 10 person years' worth of effort goes into authoring these 90 plus documents that are submitted. So this is the work of medical writers that are creating the clinical trial protocols, that are creating the informed consent forms and a whole host of these documents.

And as you can imagine, a lot of that authoring work is effort that is repetitive and in fact actually involves a number of low value tasks. So where do these low value tasks come from? If you start thinking about, okay, I'm going to write a clinical trial protocol document, you start to realize a lot of your input documents that you might refer to are other documents that you have to review, but there are also data sources that you might have to consume, structured data sources. Unfortunately today we know the data ecosystem in life sciences in many spaces is very fragmented, so there is effort that goes into finding the right information and being able to source the right nugget that you might plug into your document.

And obviously if you're dealing with a diversity of different data sources, that leads to chances of error. Oftentimes, even though the regulators require these documents to follow certain templates, what we find when we work with organization after organization is that these templates, even within a single organization, are not standardized. There are variants of templates that exist either because templates have evolved over time or different submissions of drugs across different therapeutic areas use different templates or different geographies use different templates.

Of course all of this effort is highly manual if you think about bringing all of this information together as you're authoring your document and submitting it, so obviously it's highly manual intensive. And lastly, these documents need to be submitted to regulators, so they go through extensive and careful reviews. But review processes are again human driven processes, and the handoff from one expert to another leads to delays, inefficiencies throughout that process, and indeed leads to possibilities of risk.

So that's the process as it stands today just in the context of clinical document authoring. And what I'd like to remind you is clinical document authoring is not the only kind of document generation in pharma workflows. They have to generate documents in the medical process to engage with physicians. They have to generate documents to commercialize their therapies, develop documents to engage with regulators on an ongoing basis. So the scope of the impact you can drive through something like this is quite dramatic.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/260.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=260)

How might we solve this with agentic AI? So the opportunity for us is the following. Can we develop the right agentic AI  workflows that not only support the document generation itself? We know LLMs and the technology evolutions that have happened over the last couple of years, yes, they're very good at generating content. They're also very good at actually processing data. So if you think about all the input data processing that needs to happen that eventually does get translated into the document, a lot of that data generation can also be powered by agentic AI workflows.

However, that's not the only piece. We need to make sure that in these regulatory submissions we are deeply embedding the domain that is needed. All of the knowledge, the scientific knowledge, the regulatory knowledge has to be embedded through this process, and that requires embedding the intelligence into your workflow itself.

In a way that is digitalized and can drive reusability, not just for a single document but for the entire ecosystem of documents. Lastly, the opportunity is, can we do this in a way that is enterprise scale? We don't want to change all of the systems that exist today. We want to be able to seamlessly plug into the systems of record that exist today, both from an input standpoint and from an output consumption standpoint. If we can do all of these things right, we can drive, for a single clinical trial, almost three months of timeline reduction. So the potential is significant just in the clinical development process.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/370.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=370)

### A Process-Centric Approach: Introducing Scriptiva for Document Authoring

If we can reduce the clinical development process by about three months cumulatively through driving this process transformation with agentic AI, that means we can bring medicines to patients faster, and that's a big impact statement. So with that, I wanted to share a little bit about how we are thinking about document authoring.  I think for us, the big insight is we should stop thinking about authoring documents as individual documents. Today we think about authoring the clinical trial protocol document or the informed consent form, or further downstream the clinical study report. Rather than focusing on individual document creation, we really should be thinking about the document as just an output.

Actually, the process of creating a document is an extensive process that involves transforming data, synthesizing data, deriving knowledge, and then embedding that knowledge into a document. So how can we build the right end-to-end agentic-enabled workflow and optimize those workflows? A lot of the optimizations are actually people and process transformations. And how do we do this in a way that we power content reuse? So rather than thinking document-centric, our advice is that you should be thinking about this entire process as a process-centric transformation itself. And so that's what we have been doing.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/440.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=440)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/460.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=460)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/480.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=480)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/490.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/500.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=500)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/510.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=510)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/520.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=520)

Now many of you are wondering, so what exactly have you all created?  And I think that leads into a short demo to show you the core technology that we've created. It's an asset base that we call Scriptiva. The short demo will introduce it to you before I hand it over to my colleague Rav.      

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/530.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=530)

### Under the Hood: Technical Architecture and Knowledge Layer Integration

All right, so this was the demo of our accelerator Scriptiva, and we have used this across clients  to help them accelerate their journey of document authoring. As you saw in the demo, Scriptiva provides all the foundational capabilities like the template management, the prompt management, the mapping rule engine, and a Microsoft Word plug-in-based authoring experience, which aligns very well with the way medical writers do the authoring today. And one important clarification I would like to provide here is that Scriptiva is not a product. Scriptiva is not a SaaS offering. It's an accelerator-based model which you can tailor and embed within the client ecosystems.

We do understand that every client has invested in different technology and different platforms. They have their own SOPs and workflows. This way, the accelerator model provides that level of flexibility where you can pick and choose the selective components like the agents or the prompts or the templates, customize them, and embed them within the client ecosystem. So with this, let's get to see what's under the hood and how the Scriptiva solution is powered. I'll start with a very high-level solution overview, and our solution approach is anchored in what we call a cake and a candle strategy. The cake here represents the enterprise-grade foundational platform.

The candles here are the different documents that are to be enabled by this platform today. If I would have to touch upon some of the foundational capabilities that this platform needs to enable, some of those are digitization services, the agent development frameworks, the orchestration engines, and the knowledge layer as well. Once you have established this horizontal platform, the cake, while there is an initial investment to set up this platform, as you scale across the document and the data landscape, there is so much efficiency when you have to onboard new documents or the data use cases as well. When I say data, the same platform will enable the capabilities for you to automate your STDM generations, ADaM generations, TLG generations all the way until the CSR authoring as well.

So with this, I'll move to the information architecture of the solution. We have developed our solution on AWS technology stack. This is a layered architecture. I'll not touch upon all the components, but I'll speak about some of the foundational components of the solution. If we read this page from the bottom towards the top, the very first foundational layer is the digitization layer. Why is this layer important? Because all the source data has to be made available in a machine-readable format that agents can consume. These source systems represent your upstream documents, the historic documents like protocols, ICF forms, CSRs, and this also includes the digitization of metadata specifications, primarily everything that has to come together to enable the agents to run the authoring process.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/660.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=660)



Once this digitization layer has been established, the next important layer is the knowledge layer, and I would say this is the brain of the overall solution. This is the layer that connects all the dots, linking the documents, data, mapping rules, relationships, everything together. If I would have to explain the significance of the knowledge layer with an example, let's say one of the sections of an ICF document has been authored. This knowledge layer holds the information: what was the upstream document, what was the section of the upstream document that was used to author the specific section of the downstream document, what were the prompts that were used to author this section, what were the agents involved in the authoring of this section?

Fundamentally, this layer is enabling you to make your solution audit-ready, and this also helps you to handle some of the very fundamental operational use cases. For example, protocols often do go through amendments. If a protocol amendment has happened, this knowledge layer will surface the information because a section of the protocol has changed. These are the downstream documents that need to be updated. In a way, this knowledge layer is converting the pile of documents into a well-connected, traceable, and intelligent network of documents and thereby enabling the agents to generate accurate, auditable, and regulatory-ready content.

Once these two foundational layers have been established, the next layer is the template management and the prompt management where we enable capabilities for the user to manage the different versions of the templates and the associated prompts as well. On top of this is the agent orchestration framework where all the different agents which are needed for authoring, for example, the authoring agent, planning agent, critique agent, etc., are deployed and set up. On the top we have the consumption layer. An example of the consumption layer is Scriptiva. We could have live assistants or co-pilot assistants that can be enabled for the medical writers to ease their experience of writing today.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/840.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=840)



So with this, we'll move to the last section of the solution overview. This is a slightly more detailed view which explains how the construct of the knowledge layer, the agents, the tools, and the memory, all of these come together to enable the end-to-end authoring experience. Again, I'll explain this with an example. Let's say as a medical writer, I have selected a document and maybe one section or more than one section to be authored. As soon as this information is made available to the planner agent, the planner agent hits the knowledge layer: get me the information of what are the upstream documents, what are the upstream data sources that I need to author the specific section. It also hits the knowledge layer to get the transformation rules that are needed to author the section. Is this the direct copy-paste of the content? Is this the summarization of the content, or are there certain calculations that need to be done here as well?

Once the planner agent has all this information, it generates a plan of action and then starts orchestrating different agents. An example of this is the planner agent may call the authoring agent, the tone refinement agent, and the critique agent to generate the right information.

Inherently, all these agents are powered by a suite of different tools. An example of the tool layer is the data copy tool, the table extraction tool, or the image extraction tool. So all these tools are enabling the agents to perform their job. And while all these interactions are happening across the different agents, all this information is stored in a memory layer.

This memory layer also stores the user feedback, meaning if an agent has generated a specific section and a medical writer has made some edits to it, that information is stored in the memory so that for the future iteration, it serves as a feedback loop to the authoring agent. So this way, the construct of the knowledge layer, the agents, the tools, and the memory, they gel together to complete the end-to-end authoring experience. So with this, this concludes our overall solution approach and how under the hood the different agents and knowledge layer are coming into the picture to enable the end-to-end authoring experience. And with this, I'll hand it back to Pranava to take us to closure.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/980.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/990.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=990)

 So I hope you got a little bit of a glimpse of how ZS is transforming and building the technology underpinnings for transforming the document generation workflow.  But that's only the technology aspect of it. A big part of this, if I were to even reflect on the title of our presentation, it is about the process transformation itself, and so workflow transformation is a core component of this aspect. In today's presentation, we didn't touch on that, but we have the benefit of having worked with at least four of the top ten life sciences companies in building this and in transforming this document authoring ecosystem with agentic workflows.

We believe that our ability to not only understand the data and the technology that is needed to build all of this that Raghav walked you through, but also coupling that with the deep domain experience and the deep domain knowledge of the entire workflow that happens today for clinical document generation, indeed manufacturing document generation, and several other document generations is a critical ingredient. And the most important thing is how do you bring these things together to actually drive the impact, the promised impact that we might have of reducing timelines in bringing these drugs and these treatments faster to patients. And so we think this is really what differentiates us, and we're very happy to talk to you.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/aff25039bb4cee32/1070.jpg)](https://www.youtube.com/watch?v=JkZu0zlWloU&t=1070)

I'm sure today we only gave you a glimpse. We're very happy to follow up with you after this talk if you have questions and engage with you and share a little bit more about what we've been doing. Thank you all so much. Thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
