---
title: 'AWS re:Invent 2025 - Agentic AI Meets responsible AI: Strategy and best practices (AIM422)'
published: true
description: 'In this video, Michael Kearns and Peter Hallinan from AWS explore Agentic AI and Responsible AI practices. Kearns discusses the spectrum of agentic systems from simple LLM interactions to autonomous planning agents, sharing his experience using coding agents to write a 50-page mathematical paper. He examines scientific challenges including embeddings as agents'' native language, privacy implications of data warping in embedding spaces, agent negotiation behaviors mirroring human patterns like the Ultimatum Game, and the difficulty of encoding subjective common sense. Hallinan addresses practical evolution from today''s single-vendor agents to future multi-agent ecosystems, highlighting infrastructure needs for long-running tasks, proactive behaviors, and third-party workflow integration. He emphasizes two critical mindset shifts: understanding AI as statistical inference rather than rule-based logic, requiring separate evaluation stacks, and recognizing trust as fundamental for agent-to-agent interactions. The session concludes with AWS''s Well-Architected framework for building responsible AI applications, covering benefit-risk analysis, release criteria, simulation environments, and data flow management.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/0.jpg'
series: ''
canonical_url: null
id: 3093249
date: '2025-12-08T21:46:28Z'
---

**ü¶Ñ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


üìñ **AWS re:Invent 2025 - Agentic AI Meets responsible AI: Strategy and best practices (AIM422)**

> In this video, Michael Kearns and Peter Hallinan from AWS explore Agentic AI and Responsible AI practices. Kearns discusses the spectrum of agentic systems from simple LLM interactions to autonomous planning agents, sharing his experience using coding agents to write a 50-page mathematical paper. He examines scientific challenges including embeddings as agents' native language, privacy implications of data warping in embedding spaces, agent negotiation behaviors mirroring human patterns like the Ultimatum Game, and the difficulty of encoding subjective common sense. Hallinan addresses practical evolution from today's single-vendor agents to future multi-agent ecosystems, highlighting infrastructure needs for long-running tasks, proactive behaviors, and third-party workflow integration. He emphasizes two critical mindset shifts: understanding AI as statistical inference rather than rule-based logic, requiring separate evaluation stacks, and recognizing trust as fundamental for agent-to-agent interactions. The session concludes with AWS's Well-Architected framework for building responsible AI applications, covering benefit-risk analysis, release criteria, simulation environments, and data flow management.

{% youtube https://www.youtube.com/watch?v=OGvXA1dAh1U %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/0.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=0)

### Introduction to Agentic AI: From Simple Interactions to Autonomous Systems

 Good afternoon, everyone, and welcome to the session on Agentic AI Meets Responsible AI: Strategy and Best Practices. Continuing the theme of re:Invent, where it seems that all things agentic are front and center, today, along with my colleague Peter Hallinan, who's sitting over here and will take over shortly, I'll outline things in a second. We don't just tell you some basics about Agentic AI. I'm going to talk a little bit about the scientific challenges behind it and possibly anticipated solutions to those, and then Peter will talk more about industrial evolution and practicalities.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/50.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=50)

Just to cover the agenda, I'm going to start and kind of level set us all and talk about  what agents are and what Agentic AI means. Then I'll spend most of my time talking about scientific frontiers, or more accurately, challenges. Then Peter will take over and talk about more practical topics.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/70.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=70)

Just to give some kind of spectrum along which we can arrange Agentic AI,  I think the word agentic and Agentic AI is probably terminology that's a little bit too crude for the vast spectrum of things that are covered. I'm sure many of you are familiar with and are perhaps regular users of large language models. We're all used to the phenomenon of interacting with a large language model in a dialogue and a turn-taking basis. I think the logical extreme of Agentic AI is an LLM-based system that has unfettered access to external resources and information and can take consequential, possibly irreversible actions in the real world. Think things like engaging in financial transactions on your behalf, signing contracts on your behalf, or even more consequentially, taking military actions or medical decision-making actions autonomously.

I don't know of any agentic system right now that touches that logical extreme or comes even close to it, which is why I think we need language for the stuff that's in between. But in this chart, what we're doing is arranging systems along the particular dimension of what level of autonomy the agent has. At one end, we have just turn-taking interactions with an LLM, possibly in service of completing a task or getting information. In the middle, I think, is where most of the action will be in the near term, and that is around what I'm going to shortly call planning agents. These are agents that you don't need to hold their hand on a turn-by-turn dialogue-making basis. You can give them a high-level goal and ask them to plan and execute the steps to it and expect that it will make forward progress, possibly get stuck and revise its plan and continue.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/180.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=180)

 What are some examples of agents that I kind of have in mind? Well, I think one example that people often think of is an agentic executive assistant. What do I mean by an agentic executive assistant? This would be somebody that has access to and knows everything about your schedule, your calendar, your commitments, your travel preferences, the conditions under which you might be willing to pay for an upgrade to first class on a long international flight, and the like, and actually has the ability to engage in transactions for you, to go out and find tickets that you might have bought yourself and purchase them for you.

### Advanced Planning Agents: A Mathematical Paper Written with AI Assistance

By advanced planning agents, I mean agents that can, as I said, just take a high-level specification and plan the steps to execute that task. Let me digress for a second and tell you about a little experience I had over the summer. As you might have noticed on the title slide, I am what's known as an Amazon Scholar, which means that I divide my time between Amazon and the computer science faculty at the University of Pennsylvania. I'm a career-long machine learning researcher. I started working in the field in the late 1980s, so suffice to say I've seen an alarming amount of change in the last four decades or so.

This summer, a faculty colleague and I and a visiting graduate student actually wrote from scratch a roughly 50-page mathematical paper from scratch with the help of a coding-based LLM tool, and I'm going to tell you a little bit more about that experience. Many of you, I'm sure, are quite familiar with and perhaps are regular users again of coding agents where you can basically say, well, I want to write some Python code that does the following, and you interact with it in natural language, and it actually writes the code for you and can run experiments for you and the like. I think really just in the last year these systems have also become quite proficient at proving theorems.

So we wrote this paper describing what the high-level results we thought would look like, what the theorem statements would be, and we didn't know all the details because you don't know all the details until you actually prove the theorem. So we didn't know things like what the exact convergence rate of our proposed algorithm would be, but we wanted to basically prove that it would converge to something good and then quantify the rate. And so we did this in a turn-taking way. We would say, well, here's the high-level statement of the theorem, here are what we think the steps of the proof are, we think you'll need this tool from probability theory in this step, and why don't you try. And then we would interact with it and correct it and go back and forth.

Somewhere midway through this project, my colleague noticed that there was a little button at the bottom of the interface that said turbo mode, and it turns out that in turbo mode you could just say, here's the theorem I want you to prove, I'm going away for a few hours, why don't you try to prove it. And as you may know, if you've used these systems, they talk to themselves, and so suddenly scrolling starts happening in grayed-out text and you see the model talking to itself and saying, oh, that didn't work, I see what I did wrong, I should go back and fix this. Sometimes when we did this, we would come back and it would still be churning away nonsensically, stuck in some very bad state. Other times we would come back and see that actually 45 minutes after we left, it had done exactly what we asked it to.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/410.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=410)

I think we'll see a lot of this kind of agent in the near future where you just specify the high-level goal, maybe with as much detail as you're able to specify, and then you just let the model run and plan its own steps and backtrack as needed and go forward. So let me now turn to what I consider to be some of the  interesting scientific challenges of agentic AI. And as a scientist, by the way, I'm usually most comfortable talking about known science, about what's out there on a particular topic. In this case, I can't do that because despite the buzz around agentic AI, not just at re:Invent but in the scientific community, there's not a lot of literature specifically on the topic of agentic AI. And that's obviously going to change very soon.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/440.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=440)

### The Native Language of AI Agents: Understanding Embeddings and Their Implications

 But let me start off by talking about the native language of AI agents, or embeddings. So when we interact with a large language model in a natural language like English, the LLMs don't actually directly work on that English language content. They actually map, anytime you give an LLM input, whether it's in the form of an English prompt or an image or you point it at a code base or you point it at all of the documents so far in a legal case, the first thing it does is transform that data into a still high-dimensional but much lower-dimensional space. And the primary purpose of that transformation is to preserve similarities between content.

So for instance, in an image embedding model, two photos of different families would be placed nearer to each other in embedding space than either one of those family photos would be to, say, a landscape. Two romance novels would be put closer to each other in a language embedding space than either would be to a car user manual, for instance. And so the high-level idea of embeddings is to re-represent data in a way that captures similarity between data rather than the raw data itself. And by the way, if you've ever had the sort of disturbing experience of an LLM seeming to completely lose its place in the dialogue that it's having with you or suddenly return to some topic that you were discussing hours or days ago completely out of the blue, it's because actually there's no memorization of the dialogue so far by the LLM. There is just the embedding of the dialogue so far, and this leads to these kind of cognitive tics that you sometimes see, less as time goes on, but they're still there.

And so we should anticipate that agentic AI, since it's based on LLMs, will also basically reason in embedding space rather than in the sort of superficial input space that we give it, whether it be language or images or code or what have you. And so I think there are some interesting implications to this. I don't think we, you know, one of the things that's hard about AI and LLMs and deep learning in general is that these re-representations of the data are very powerful, but we still have very poor understanding of what's really going on and what they mean. So in particular, in the same way if we opened up your cortex and pointed at a particular neuron in your cortex, for the most part except in very simple cases like low-level visual processing, we don't actually know what the particular purpose of that neuron is.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/620.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=620)

In the same way we really don't know what the meaning of embedding space is. In the cartoon here on the right, the idea is that several different users have eaten at different  restaurants and maybe given them different ratings and preferences. You have that raw data and then you want to map that to embedding space. In this particular cartoon, one thing that's inaccurate about it just for the purposes of simplification is that the axes are labeled by particular properties we know, like what is the quality of the food, what is the ambiance of the restaurant, and so on.

But embedding space, the axes would basically be entirely unlabeled. They would have no direct semantics that we could interpret. One of the near-term implications of this is that when you think about agents going out on our behalf and consuming or finding content and interacting with other agents, even though they're quite proficient at conversing with us in English, they won't converse with each other in English, and they don't need English in order to understand content.

For instance, I predict that at some point in the not too distant future, every website on the Internet will have a little place that basically says if you're an agent, here is the pre-computed embedding of all of the content on this website. It makes no sense for every single agent that goes to that website to have to recompute the embedding itself. You may as well just pre-compute it and put it there. This can't happen today because these embedding models are proprietary and so they're not standardized, but I predict that there will be some amount of standardization that will make this possible because it's just one of these things that makes practical sense.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/720.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=720)

### Privacy and Security Challenges in Embedding Space

 Another area where embeddings are important and that we don't understand quite well yet is when it comes to things like privacy and security. If you think about embeddings taking the original data and mapping it to some abstract space whose primary goal is to preserve distances, the data is going to get warped through that transformation, and we don't understand the shape or properties of that warping.

In particular, we have all these divisions in our mind about different types of data. For instance, when I think about my data, I think of my financial history as being generally quite separate from my medical history. These are different types of data for different types of functions. There's no guarantee when my data gets mapped into embedding spaces that that sort of clear separation that I have in my mind will still exist, and so this has privacy implications.

In particular, when your agentic executive system assistant is shopping around for plane tickets for you and deciding whether you would want to pay for an upgrade to first class, if a human executive assistant was doing that, they wouldn't take into account how your stock portfolio had been performing recently, even though in principle that is financial information that might have bearing on your willingness to pay for a first class ticket. I think without safeguards put in place, there's no particular guarantee that your agentic executive assistant would realize that that same boundary should be obeyed because of this blurring of different data types and the blurring of boundaries in data space. Obviously this has privacy and security implications as well.

There's a very small literature, the last time I checked a couple of papers, on the very good question of if I have some data that's privacy sensitive and I compute an embedding of it into some lower dimensional space, how much can you reverse engineer about the original raw data just from the embedding? The reason that there's a paper on this is that the answer is you can do some reverse engineering. We'll need to start rethinking, I think, basic tenets and models around things like privacy, security, encryption, and the like in an agentic AI world.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/860.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=860)

### Agent Negotiations and the Ultimatum Game: When LLMs Mirror Human Behavior

 I've used a couple of examples when your agent was going out and performing transactions for you, and we might also anticipate a future where your agent is going out and negotiating things for you, essentially not just going out and engaging in transactions. You might expect that in the same way for many years now we've had websites that basically try to crawl and find the best prices for you on flights, there will be agents that try to do that for you. You can imagine your agentic AI assistant in service of making travel arrangements for you, kind of bartering or negotiating with another agent. What might that look like?

Well, there's also a small but rapidly growing literature, not just in the AI and machine learning community, but in the social sciences as well, in which there are studies that try to replicate or study the analog of human behavior in agents and LLMs. Let me tell you about the first example of this that I know from about four years ago from a colleague and friend of mine.

In behavioral game theory, there's something known as the Ultimatum Game. What is the Ultimatum Game? An Ultimatum Game is, first of all, a very simple kind of math problem that I'll describe to you in a second, but it's also, more interestingly, a behavioral experiment that economists have been running for many, many decades now in many, many cultures, varying certain conditions of the setup and so on. But the basic setup is the following. You bring two players into a lab, let's call them Alice and Bob. You say, "Alice, you get to move first and you can propose any split of $10 that you want with Bob." Then Bob responds, and Bob can either accept the split that Alice has proposed, in which case you actually pay the two participants their respective pieces, or Bob can reject, and then both players get nothing.

What does game theory or microeconomics predict about this? It's very, very simple. Alice and Bob being perfectly rational beings, Alice should reason as follows: since Bob is perfectly rational, I should offer the smallest non-zero amount to Bob, say a penny. So Alice would propose $9.99 and a penny. Bob, being rational, would rather have a penny than not have a penny, and so Bob accepts. This isn't that interesting.

What is interesting is that when you run this game in the lab, nothing even remotely close to this happens, but what does happen is extremely consistent across many conditions and cultures. What happens is almost all of the time, let's say close to 90% of the time, Alice will offer to Bob something between $3 and $5, very rarely more than $5. There's sort of a priming for both players already that Alice, being in a position of power by moving first, might deserve more of the pie than half. Conditioned on Alice's proposal to Bob being within that $3 to $5 range, Bob almost always accepts. The acceptance rate is extremely high. In the 10% or so of cases where Alice gets more aggressive and offers Bob something less than $3, the rejection rate of Bob skyrockets to close to 100%.

You can go look this up. This experiment has been run hundreds, possibly thousands of times. One of the things that's interesting about it is that, first of all, it deviates from the mathematical prediction by a lot. The other thing is that it's very, very consistent across different subjects and cultures, and so it's as if we all neurologically have a hardwired sense, a shared hardwired sense, of what's fair in this game, even though probably most of you have never heard of the Ultimatum Game before right now.

My colleague basically went out and replicated this experiment using LLMs instead of Alice and Bob. He would say to one LLM, and he would give them personas also to sort of match the variability of human subjects that you would get in a laboratory experiment. So he would give a prompt to one LLM and say, "Your name is Melissa. You're a 37-year-old medical technician living in Lexington, Kentucky. You get to propose any split." Then he would give a prompt to the other LLM, and guess what happens? The LLMs actually statistically almost perfectly match the behavior of human subjects that I mentioned.

This is becoming quite an area within not just AI and machine learning but the social sciences more generally, and so people are starting to study what happens when agents negotiate with each other, when they bargain with each other, and interesting phenomena are starting to arise. In general, the tracking of human behavior is quite good, and also the tracking of human foibles as well. For instance, there was a recent paper about a year ago that showed that LLMs engaged in a price negotiation game essentially arrived at outcomes that would be considered collusion and price fixing from a regulatory standpoint. This is both amusing, but I think it's also worth thinking about that when we think about agentic AI.

We don't just need to think about what happens with individual agents in the same way that we think in game theory or finance about collective behavior or emergent phenomena. We need to think about that with agentic AI as well. You might wonder, if we soon arrive at a world where a lot of activity, including financial and strategic activity, is mediated by agents, how will we think about things like herding behavior in stock markets or the systemic risk that led to the 2008 financial crash? These are examples of cases where the behavior of any individual agent might seem perfectly natural or rational, but you arrive at some collective outcome that is quite undesirable.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1260.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1260)

### The Challenge of Subjective Common Sense in Agentic AI

The last topic I want to mention is maybe the one that's fuzziest and the one on which I really don't know any  current science around, which is the challenge of what I would call subjective common sense. Let me first talk about common sense rather than subjective common sense. I will use the fact that I've been around forever in this field to tell you that for the longest time, AI and machine learning were confounded by the problem of imbuing models with common everyday common sense. By common sense, I mean facts that we would all agree on even if we'd never thought about them before.

For instance, in a series of pictures, if I show you a glass of water sitting on a surface, on a table, and then I shift it in one direction, say I shift it to the right, we would all still agree that it's still a glass of water on that surface. The fact that I've moved it somewhere on the table doesn't change the fact that it's a glass of water sitting on the table. We would also all know that if I tip it on its side, it's still a glass on a table, but it's no longer a glass of water anymore.

For the longest time in AI, dating back to the beginning of the field, people struggled with the question of how to imbue models and AI systems not just with the ability to perform extremely targeted tasks very well like face recognition, but to have this sort of ability to do common sense everyday reasoning that any human being could do. Somehow, in the last 15 years or so, that problem has largely been solved. I have a colleague at Duke who, as far as I can tell, spends all of his time posting on social media examples of LLMs failing basic common sense things, but for the most part this has gotten much better. I think this is just due to the fact that if a model sees a fact or images of glasses on tables sitting upright and tipped on their side enough, it'll learn this.

By subjective common sense, I mean heuristics that you have around your own behavior that are perfectly sensible, that you have good reasons for, but are personal to you and are not shared like our knowledge about glasses of water on tables. As an example, I will just use my own behavior around whether I leave doors open or closed or locked or unlocked, and I invite you to think about your own behaviors in this regard. It's actually quite complicated. In my office, when I leave for the day, I lock my office behind me. When I'm there during the day, sometimes I leave the door open and I'm in my office. Sometimes I'm in my office and I close the door, but I don't lock it because I'm taking a meeting or I want to be able to focus.

Sometimes when I'm just going to be nearby on the floor or at a whiteboard out in the hall, I will leave my door open and unlocked to signal that I'm around but just not in there at the moment. You probably all have similar heuristics and they will probably all differ slightly from mine. I think that imbuing agentic AI with this sort of personalized subjective common sense is going to be a real challenge because, unlike things like facts about glasses of water on tables, it can't be learned from data. There's not a massive trove of data out there on my particular habits regarding doors and locking.

Lest this example seems too far removed or abstract, I invite you to just swap doors and locking with accounts and passwords. Perhaps many of you choose to share the passwords to your Spotify or Netflix accounts with other family members or people that you're close to, but you wouldn't do the same thing with your bank account probably.

And so how agentic AI is going to somehow, I think we're kind of returning to the 1970s where nobody had any idea how we were ever going to get general common sense into models, and then machine learning on just sufficiently massive troves of data turned out to be a good solution to that. I don't see that solution working anytime soon for your agentic AI.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1570.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1570)

### The Evolution of Agents: From Close Human Interaction to Complex Infrastructure

And so I think I'm going to turn it over to Peter now to come talk about more practical topics and sort of the evolution of agentic AI, in particular the ways in which it's different than just LLM-based generative AI.  So I'm Peter Hallinan and I'm the Director of Responsible AI for AWS, and I lead a team of scientists and engineers and various folks who are interested in tackling the open science problems and then translating solutions into best practices for our internal teams and for customers. So let's talk a little bit about the science challenges. I hope are becoming clearer. Let's talk about some of the other challenges that we face.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1600.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1600)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1640.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1640)

All right, so where are we going in the nearer term with agents?  Let's take a look. There's lots of effective agents today, or things that are called agents that are effective. Here's a few from Amazon. Kiro, the SageMaker Model Customization Agent, SageMaker Data Agent. There's one we just wrote a blog about, which is a Compliance Screening Agent. So across our businesses we have to be very careful about the transactions, so now we have an internal agent that takes care of that. But what is sort of the primary characteristic of these agents?  Is that they have very close human interaction, and you have single vendor accountability. So the world that we were just talking about, where we have lots of critters running around all doing useful work for us, is a bit different than this world here. Now this world here is yielding some value, quite a bit of value, but we have a ways to go.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1670.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1670)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1730.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1730)

All right, so how might things evolve  over the next year or two? The first thing is obviously sort of more task types. So consider an example where you want to use your miles to get business class tickets to Paris, say if you're living in the US for December. You can do it, but the airlines release those tickets sort of slowly over time according to their own schedules. This is a task where you have to sort of continuously go back and forth. It's a bit of drudgery and something you can forget to do, and then you go, oh darn, because you missed it or what have you. So this is the kind of thing where the agent should be released to do something over a period of time. It's not a fast response. Another one might be, hey look, there's interesting social media conversations, but you want the comments to accumulate in some way before you spend your time. You don't want to go back to it repeatedly.  So this is a trend I would call sort of fast to slow. You could call it short term to long term if you want to, but it's a different type of task.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1740.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1740)

 Here's a second trend, and by the way, the three trends I'm going to talk about right here are correlated, they're not independent. But the second type is that not every task needs to be explicitly specified by the user. So you could trigger a log analysis from a ticket. We have an agent, a DevOps agent, that's doing things like that. Or you know, you have an agent that's listening to traffic reports, and in this sort of ideal personal assistant view, it updates your restaurant reservation because it knows you're not going to get there on time. So these are inferred tasks, and there's sort of an implicit assumption here that they're accessing information and tracking it so that you don't have to. And so this I would think of as sort of reactive to proactive. A third one is more connectivity to third party workflows.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1820.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1820)

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1830.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1830)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1840.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1840)

All right, so here we have a nice little remote restaurant off on a lovely beach. They do have a telephone, a landline, old school, right? And then you have your state-of-the-art travel agent that's helping you out, and you ring them up.  And then someone else has their state-of-the-art travel agent, and then a third person has, and then a whole bunch of other people have, and you've kind of DDoSed the poor  restaurant, right? They just can't cope with this. And so this connectivity to third-party workflows is an implicit part of all the  sort of magic and excitement of agents, right? When we talk about all the drudgery that we could eliminate, we need to understand how it impacts those other systems that we're talking to.

So here, what happens to the restaurant if Travel Agent One cancels? It really impacts their business. They don't have that many tables, and so you begin to wonder if that offline restaurant really has to go get its own Ma√Ætre D' agent or go out of business or how it handles this. But it has a business decision that it needs to confront, right? And this applies to online workflows. Going back to airlines for a minute, some airlines have a policy where you buy the ticket and then you can change your mind within 24 hours. Well, that policy is crafted to deliver the flyer benefit. It's a nice policy, but it's very carefully driven by data and what's the likelihood of rebooking, and now they're holding a seat. Who do they risk not selling it to, et cetera, et cetera?

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1920.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1920)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1940.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1940)

All of a sudden you put agents in it, the math changes. It assuredly changes, and so how do these policies adapt? So we talk about agents, but then we have to also talk about all the things, whether or not they are agents, that the agents are interacting with and how that change happens, right? And so,  from today, where we're really sort of nailing it in a lot of domains on well-understood workflows, to the future where we have these sophisticated goals, we see sort of this general trend, right? And on the X-axis,  you have kind of the complexity of the task from simple to complex, or the complexity of the user request. On the vertical axis, you have kind of the robustness, the resiliency, the strength of the infrastructure that you support the agents with.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/1980.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=1980)

So if you think about that, the fastest slow, well, that thing had to be up and running continuously for a long period of time. It's not quite the same as just interacting with an LLM. If it goes down the next day, maybe that's fine or what have you, but this really takes infrastructure to support. So anyway,  that's kind of the general trend that we see happening now. How do we realize the benefits while minimizing the risks as this rolls out?

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2000.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2000)

### Mindset Shift One: Statistical Inference vs. Rule-Based Logic in AI Systems

Okay, so a couple of things. The first observation I want to make is that when Generative AI hit  the world by storm a few years ago, it really broadened the number of folks who are engaging with AI and turned it from a sort of very small community to a very large community with, shall I say, a lack of understanding of the difference between statistical inference, right, and rule-based logic. And so there's a major mindset shift which the industry as a whole is still in the process of making, right, where people realize that AI is not just smart software. It is actually a different kind of beast, all right?

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2050.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2050)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2060.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2060)

And to highlight some of the changes here,  obviously traditional software is rule-based logic. If you have a statistical model, it's based on statistical inference. We'll get to a deeper implication of that in a second.  Your specking changes, right? With the traditional model, you can write it down in English. With AI, you're using data sets to spec. So to Michael's earlier point about where are the data sets that encode the full range of your personal preferences, and even do you want to have such data sets lying around? I mean, that's also a question, right? But the data sets are key here.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2090.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2090)

A third point, right, is that with  traditional software, you expect it to work out of the box. The developer is responsible for the quality of that product. But with the statistical model, it's different.

The difference is significant. The developer anticipates the kinds of input distributions that they'll get, but they can't know for sure due to privacy constraints. We don't look at the prompts that come into our Bedrock models, right? What you see as a result of this is that deployers and in some cases end users have to also test because they're the ones that understand their particular data.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2140.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2140)

A fourth point is this:  when you release a piece of software, version N+1, you expect it to work better than version N on each and every input. But for a statistical model, you're going to expect it to work on average better, and that's a big difference. It might not be on average, it could be your metric of choice, but you can release an improved model and some users of it may see worse performance.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2180.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2180)

A final point I want to make about this kind of mindset shift is that  with rule-based software, traditional software, there is a single explanation for how you got a particular output from a particular input. The code can be implemented in different ways, yes, but the chain of logic is the same, whereas on the statistical side, it's different. In fact, you release a model, and that model does have a logic inside it, but there's also tens of thousands or millions of models that had the exact same performance that you did not release that had other chains of logic inside them, maybe a little bit different, maybe very different. So the question of explainability becomes more complicated.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2230.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2230)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2260.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2270.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2270)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2290.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2290)

A couple of implications of this. First of all, on  the AI side, there are two stacks that you have to worry about. If we think about an AI system and you're improving it version by version on the horizontal axis and then you're looking at its performance along a particular metric on the vertical axis, right, and you test it on some dataset, maybe you get this kind of performance.  But you test it on a different dataset, you get a different performance trajectory, right? You test it on a third dataset and  maybe it's altogether going down. Performance is a function of the system and the evaluation dataset. It is not a function of the system by itself, and I know if some of you have heard us talk before about this, we're pounding this point again and again, but we still need to  get this clear.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2320.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2320)

Because the result of this is that if you're a developer of an AI solution, generative, traditional, whatever, you've got your system stack and then you've got your evaluation stack. Your evaluation stack as the developer represents what you anticipate downstream folks to use, but then you have downstream folks who are going to have to have their own stack with  their data to check what's going on. As a whole, we are moving in this direction as an industry, but this is not a well-established design pattern at this point in time.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2340.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2340)

What's another implication? And here we get into what is our traditional responsible AI slide.  The fact is, statistical models have a variety of technical properties. It's not just overall performance. It includes issues around AI privacy, AI security, safety, fairness, veracity, and so on. There's a bunch of these things, and the models released have these properties whether or not you as a developer made a decision about the properties. That's a very important point. So if we go back here to the stack downstream, they can measure for properties that you may not have made an explicit design decision about.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2380.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2380)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2390.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2390)

 And so when we think of responsible AI, what we're really thinking about is getting folks to make explicit  decisions about balancing the desired benefits and the potential risks of a service. That's the goal. You want to make these decisions explicitly, not implicitly.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2420.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2420)

And I think the last point is probably very self-evident, but from a product management point of view, life just got a little more complicated. You had all the stuff that you had to worry about  before with traditional software, and now you have these other properties. Now it doesn't necessarily mean you have to optimize every one of these properties. It's perfectly fine to make a decision that this one we'll deal with later or what have you, as long as you disclose that. But these are still additional considerations.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2450.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2450)

### Mindset Shift Two: Building Trust and Infrastructure for Agent Ecosystems

So that was mindshift one, and we haven't yet completed that mindshift as an industry. Now come agents, because why wait?  Life is exciting. What is the second mindshift that's provoked by agents? Well, there is one, and let's go back to Alice and Bob, our canonical science friends, as we work through some of these issues. There are things that Agent Alice would like to know about Agent Bob before Alice interacts with Bob. Who are you? Who owns you? How can you help me? The reason for this is because Alice is there to do actual work, and there are decisions that she needs to make. How much information do I share with you? That does matter. How do I negotiate a fair resource exchange with you?

So for example, if you're going to have a zero trust posture, is it like walking up to a counter at 7-Eleven and just buying it and walking off? Maybe not at all. How do you enforce the terms of the exchange? How much time or cost or information does the agent Alice want to invest in this particular transaction? What happens if it gets detailed and they're sort of going back and forth on a lot of things and the agent budget is sitting there ticking down? The transaction will cost money at the end of the day, so trust becomes rather critical here.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2560.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2560)

Now, this isn't the only situation in which issues of trust come up, and there's a variety of  mechanisms to address these. We have norms. An example of a norm, for example, like with the Internet, is a robots.txt file. There's not a law around robots.txt. It's a norm and it has some issues, but it has worked relatively well. There's also standards. MCP is an emerging standard. TCP/IP in the early days of the Internet, I think some people maybe regret how long TCP/IP persisted because it has the doubling thing. And then there can be existing AI regulations.

But the mindset shift that I'm talking about here is that these are not necessarily core technical problems. These are problems about privacy, about the exchange of information, about what do I do. There can be research that supports the optimization of these decision-making processes, but people themselves make these kinds of decisions all the time. They may not be perfectly optimal, but they are functional. We have a civilization. So this is the kind of thing that we have to work on, but we're not going to succeed on it if we tackle everything with a zero trust model and hunker down. You'll never get that vision of interactions that are productive.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2670.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2670)

So what does this mean in practice? As we think about a very simple agent ecosystem here, we've got four agents and two humans. The first thing that you're going to need is the agent infrastructure that we talked about from the earlier diagram.  You need real support for a variety of either short-running or long-running interactions. This is the kind of thing that Agent Cop is shooting towards. But you also need infrastructure to help people steward this emerging ecosystem, and I use the word stewardship instead of governance because this is not a pure regulatory challenge. It's not a challenge that has one person who's accountable for it.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2720.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2720)

It's a community working together to make it happen. Let me give you one specific example of what I mean. Today, if you go to an Amazon Nova model,  the images are watermarked, and there's a service out there where you can submit the image and it'll decide whether or not that image actually has the Nova watermark. We have that piece of infrastructure colored in blue up there on the screen.

Now, what happens if every agent starts checking for the provenance of any image that it accepts, either from a human or another agent, because it needs to know for IP reasons where that image came from? Right now, that infrastructure for the watermark checker is designed for humans. It has an API, but it's not designed for billions of agents to all of a sudden start talking to it. It's going to go down. So how is that going to be supported, developed, and funded? You can imagine other kinds of checking infrastructure where it's engaged in each transaction that involves novel content or maybe even non-novel content, and there can be other pieces of infrastructure as well that may turn out to be relevant. But all this has yet to be invented.

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/2810.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=2810)

So what do we do today? Let's get really down to brass tacks.  Because we're still working through Mindshift one and because agents at their core are based on foundation models, a lot of the work of building an individual agent is similar to the work of building any other AI solution. We have at this re:Invent released what I would call a framework for best practices to help non-ML experts develop AI applications, and it's available online via our Well-Architected tool. Anyone can go to that QR code URL and see it. The notion is that we want to provide people with a set of questions to help them think about the decisions that they need to make, and then for each question we offer some best practices that will help answer those questions.

Let's go through a couple of examples on the agent side. I should say that the design of the framework is such that you start with a narrowly defined use case, because if you want to actually evaluate and make a data-driven decision about how well a particular system is serving your use case, the narrower the use case, the more effective your analysis. For example, building a face recognition service, that's not a use case. That doesn't necessarily help you tune how well that system is going to work. Building a system or specifying a use case where you're recognizing the face of a missing child, or you're trying to look up an actor in a library of video, those are more specific use cases. You can tune precision and recall, you can adapt your data sets, and so on.

You want to go narrow, and then you want to assess the benefits and risks of that particular use case with as little dependency on the specific AI solution as possible. Then you want to translate the benefits and the potential risks into release criteria, which are typically statistical measures of a property. You have to worry about confidence levels, confidence intervals, thresholds, things like that, and these properties will be in tension. But if you have that set of release criteria, then you can think about what data sets do I need to support assessing those criteria, what data sets do I need to build the system, and you can think about the architecture of the system more effectively. Then you proceed to evaluation and through it.

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3000.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3000)

 So, benefit and risk analysis.

All right, I'll try and cover this quickly so we can handle more questions. An example of a best practice in the framework is asking the builders to identify potential harmful events that might impact privacy. If we skip down to the agentic AI situation, you might want to wonder, well, let's suppose that Alice has an exchange of information with Bob and appropriately shares some information with Bob. It's a credit card number, and then Alice has an exchange of information with Chris, another agent, and appropriately shares information related to that transaction, maybe a Social Security number. But then it turns out that Bob and Chris somehow later shared the same memory store owned by the same person and the information got aggregated. What is the risk of that happening? Could that happen? How would you drive that risk to zero? These are the kinds of questions you want to ask early on.

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3080.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3080)

 In this case, when you're thinking about an agent, and hopefully this is pretty obvious, you're going to want to invest more time as a builder thinking about issues of adversaries and resource constraints, like what happens if I do run out of money while running my agent, how do I recover from that, and shared components. All right, let's pick another focus area, the release criteria.

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3110.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3110)

 This is one of the really challenging exercises in building an agent because what it's asking you to do is think about all the possible properties and come up with metrics for those properties that you are most concerned about. The science literature for many properties is just chock full of options. You have the inverse problem from the one that Michael was talking about where a lot of these deeper issues don't have much research. Some of this has too much, and what do you do as a builder? But you have to do the work. You have to sort of go through and think about, okay, if my agent is going to be narrowly focused, how effective are my safeguards at actually rejecting requests that ask, for example, an IT-based agent to go answer something about a legal issue. You just don't want that happening, so you want to measure that particular safeguard. So you need metrics for it.

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3180.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3180)

 And you basically want to map every expected benefit and potential risk to one or more release criteria, so that's very challenging, but if you do that work, everything goes way more smoothly. All right, another area just to cherry pick a little bit, data set planning here with agents.

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3200.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3200)

 Because again, the idea we're thinking about the future where the agents are interacting with different information sources, other agents, what have you, you need a simulation system. You've got to simulate the environment. With generative AI, with traditional AI you can have a more static approach to your data sets, but here you can't, or at least in many cases you cannot. And so there are simulation environments that are beginning to pop up. The UK Safety Institute has

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3240.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3240)

 one, Meta has recently written by one, but I would expect to see a lot more of these kinds of evaluation frameworks begin to crop up. And then on the system planning one, I mean, the major message here is that you want to think

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5def117737788cd/3260.jpg)](https://www.youtube.com/watch?v=OGvXA1dAh1U&t=3260)

 through the flow of every single type of data through your system, like the flow through interactions with different agents, different humans, the flow through components and the flow through logs. And this is something that is sort of part of best engineering practices in many situations, so it shouldn't be that much of a surprise or a challenge, but it is very important in this case where privacy is such a central issue. Anyway, I'm going to pause right here and if we have questions, let's take them. Michael, do you want to come up?


----

; This article is entirely auto-generated using Amazon Bedrock.
