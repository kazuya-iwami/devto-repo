---
title: 'AWS re:Invent 2025 - Symbolic AI in the age of LLMs (DAT443)'
published: true
description: 'In this video, Ora, co-author of the original RDF specification, explores symbolic AI''s relevance in the generative AI era. She covers the 80-year history of AI, explaining knowledge graphs, ontologies, and semantic reasoning through practical examples like RDF, OWL, and SHACL. The presentation demonstrates how ontologies capture domain semantics, enable data integration, and support symbolic reasoning. Ora discusses ontological commitments, world assumptions (open vs. closed), and reasoning techniques for uncovering implicit information. She advocates for hybrid neurosymbolic AI systems that combine LLMs with knowledge graphs to address hallucinations, improve explainability, and enhance energy efficiency, proposing architectures inspired by Kahneman''s dual process cognition model.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/0.jpg'
series: ''
canonical_url: null
id: 3093222
date: '2025-12-08T21:24:55Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Symbolic AI in the age of LLMs (DAT443)**

> In this video, Ora, co-author of the original RDF specification, explores symbolic AI's relevance in the generative AI era. She covers the 80-year history of AI, explaining knowledge graphs, ontologies, and semantic reasoning through practical examples like RDF, OWL, and SHACL. The presentation demonstrates how ontologies capture domain semantics, enable data integration, and support symbolic reasoning. Ora discusses ontological commitments, world assumptions (open vs. closed), and reasoning techniques for uncovering implicit information. She advocates for hybrid neurosymbolic AI systems that combine LLMs with knowledge graphs to address hallucinations, improve explainability, and enhance energy efficiency, proposing architectures inspired by Kahneman's dual process cognition model.

{% youtube https://www.youtube.com/watch?v=Atf4DVKGuMg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/0.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=0)

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/30.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=30)

### Introduction: The Enduring Relevance of Symbolic AI

 Okay, so I'm Ora, and I'm here to talk about symbolic AI. This may be a strange topic now that everybody wants to do generative AI, but I promise you it's something that everybody needs to understand and learn about if you don't already. So, a few words about me.  My scenes of my misspent youth. I have a PhD in AI and CS. I was involved in creating the original vision for the so-called Semantic Web. I was co-author of the original RDF specification. If you don't know what RDF is, I will tell you. I'm also author of this book, and fun fact, my software flew to the asteroid belt at some point. It was symbolic AI, and I will mention those technologies in a minute.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/70.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=70)

 So this is kind of my game plan today. I'll give a very brief history of AI, the part of AI that spans more than three years like everybody thinks. I'll talk about what are the elements of symbolic AI, and then I'll talk about what is symbolic AI today and why is it relevant and what are the useful technologies that you can apply today. And then I'll conclude with a little bit of discussion on what this means from the standpoint of non-symbolic AI, generative AI, all that new stuff, so to speak.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/120.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=120)

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/130.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=130)

All right, so when I originally wrote this talk,  I had a much bigger section on history, but the overlords didn't want that. So I'll just give you a very brief introduction.  So this is basically the history of AI. What's important to understand is that it spans more than 80 years, and it's sort of a pattern of excitement and disappointment. We are currently in the middle of the third AI summer. We call these summers and winters. And I have been sort of a little bit of a skeptic maybe of the new, new, new AI because I don't want to see a third AI winter. I lived through the second one, and I continue to work on AI, and people were downright hostile saying, why are you doing this? AI is the stuff that doesn't work. But here we are.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/190.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=190)

 Symbolic AI, or as it was originally called just AI, is a collection of all kinds of technologies, and this is my incomplete list. But I think it's a representative list in the sense that these are the important core technologies in symbolic AI. Now, of course, it's important to understand that the history of symbolic AI also includes successful deployment of systems that made use of these technologies, so it wasn't all just research or anything. And I won't talk about all of these. I simply don't have time. We would be here all day. But you can come and talk to me afterwards if you're particularly curious about something, and I'm happy to talk at any length about any of this.

What's another thing that's really important to understand about the history of AI is that we have this pattern where some technology comes about, we think it's AI, then later on we learn what it's actually about. We understand it, and then we go like, well, that can't possibly be AI because I understand how it works. And so many of these constituent technologies from the past have sort of stopped being AI. They are just mainstream. I mean, a rule-based system, nobody thinks of that as AI anymore. Heuristic search, no, certainly nobody thinks of that as AI, but they very much were AI around the time when they were conceived.

At the bottom here, what came from the long span of AI research were two new programming languages. Now you might think, ho hum, programming languages come about all the time. But this is not just languages. These were two completely new programming paradigms: functional programming and logic programming that originated within AI research, and now we don't really think of them as AI anymore.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/350.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=350)

### Understanding Symbolic Reasoning Through Logic and Set Theory

Throughout this talk, I will have to talk about things like symbolic reasoning and logical reasoning. So I'll give you just a very quick introduction to that. Here's Wally.  Wally's a good boy. In symbolic reasoning, we use mathematical logic and set theory as a sort of representational language. We can represent all dogs as a set, let's call it dogs. And then we can say that Wally is a member of this set, which is one way of saying that Wally is a dog.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/390.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=390)

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/400.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=400)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/410.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=410)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/420.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=420)

Now let me demonstrate a simple kind of inference rule, how we can do symbolic reasoning. So first, let's say  we want something where X belongs to a set, and let's call that set C. And then we also  say that C is a subset of D. And if those things hold, then we can conclude that X is actually  also a member of this larger set, set D. And this is an inference  rule. So now if we say dogs are a subset of mammals, and we know that Wally belongs to dogs, then we can conclude that Wally also belongs to the set mammals. Wally is a mammal, so to speak.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/460.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=460)

Now, this of course sounds incredibly trivial, right? But symbolic reasoning is like that, and real-world applications of symbolic reasoning are far more complex than this, but the principle still is the same. So here's just a visual representation. There's a set of dogs and Wally is there, and then there's the larger set of mammals.  So now everybody understands what symbolic reasoning is, yes? Nothing? Excellent. Good. So now we can move on.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/480.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=480)

### Knowledge Graphs: Modern Applications of Graph Technology

So let me talk about knowledge graphs and ontologies.  Because those are really the prominent manifestations of symbolic AI today. And in a way, they are that because about 25 years ago, there was in some way a sort of resurgence of symbolic AI techniques that brought about this. So a couple of terms first. A graph, when we talk about graphs, we mean the mathematical notion of a graph. So it's a structure of nodes and edges. Typically if you want to draw this on your whiteboard, it would be circles and arrows between them. But we don't mean a graph in a graphical sense.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/560.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=560)

And a knowledge graph is an application of a graph where basically the nodes represent or denote some real-world entities. And the edges then represent the various kinds of relationships between these entities. So we can use knowledge graphs to link information and organize information, and this thing that came about 25 years ago, the Semantic Web, knowledge graphs today are really the modern manifestation of that idea.  The primary purpose for knowledge graphs is really to capture knowledge, and oftentimes in a sort of collaborative organizational context. But there are other uses, very good uses of the same technology.

So something that I've seen many times and also built systems myself. So just make knowledge accessible to people and more recently to LLMs also. A sort of low-level feature of knowledge graphs, it almost comes like a sort of bonus benefit is that knowledge graphs are very good for data integration.

You can take whatever data you have, which we'll call legacy data, and you can map it to a knowledge graph. If you do this right, the knowledge graph will represent the integration of these different data sources. And then finally, there's something called digital twins. People build these as complex representations of physical systems in a digital form. So we could imagine a digital twin of an electrical system, and then you could use that to ask questions like, "Okay, if I throw this switch, what happens?" or "If this circuit breaker blows, who's affected?"

To build a knowledge graph, you have to have a model of what the information is in your knowledge graph, and this model is called an ontology. I will come back to what that means. Now, when you finally build this knowledge graph, you have to store it somewhere. You have to be able to manipulate it. And for that, you need a graph database. Just as a purely random example, Amazon Neptune is such a graph database, and a highly scalable one at that.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/700.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=700)

### Ontologies and Semantics: Defining Meaning in Data

So let's talk a little bit about what an ontology is. There is a common  definition of what an ontology is, and if you look it up, you always get the same definition and it's completely useless. So I won't even repeat it here. This is my definition of what an ontology is. An ontology consists first of all of concepts that you have identified. You can think of them like classes. So, to follow our example, a dog would be one of these concepts.

And then you define properties for these concepts. So in the case of a dog, that might be fur color, for example. You also define relationships between instances of these concepts. So a dog typically has an owner, and that owner typically is a human. You may specify logical constraints that have to hold, so my example here is that dog and cat, the intersection of dog and cat is empty, which means that if you think of these as sets, these sets don't have an intersection. They are disjointed.

And then finally, and this is not every ontology, but ontologies sometimes may also contain actual individuals, as we call them. So Wally would be an individual. Wally would be an instance of the class dog, if you want to think about it that way. To define an ontology, you need an ontology language. And what we didn't have back in the nineties, what we do now, is we have standardized ontology languages, and I'll come back to talk a little more about what these standardized ontology languages are. But it has really made all use of this stuff a lot easier because we have standards. That means people can build tooling and all that.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/820.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=820)

 So I mentioned that ontology is something that captures the semantics of a domain. So what is semantics, really? I mean, has anybody heard anyone use the word semantics? Yeah, I think people use it a lot. Most people don't know what it means. I find that just a tad ironic because semantics is really about meaning. And I think of semantics in a sort of slightly informal way that it's really definitions of how some piece of software can interpret your data.

And so, given that, semantics comes from the relationship that your data has with whatever definitions you have in your ontology. Ontologies are the means to kind of capture the semantics and convey those. But it can also come from the relationship between two pieces of data, that they have some kind of relationship and that gives them meaning. So, for example, to humans, when we say that this person is the child of that, we might be able to conclude that that other person is a parent.

And then things that you can't express this way, then of course, you have to hardwire the semantics in code. And historically, of course, everything, all semantics was hardwired.

The great aspiration is that we can move towards a situation where things are more definition and data-driven and not so much buried in some black box that cannot be really inspected so easily.

### The JSON Illusion: Why Machines Don't Understand Like Humans

An interesting thing about semantics is that people typically have a very hard time making the distinction between formal semantics and their own interpretation of some data, for example. This matters because it creates unrealistic expectations of what software systems can actually do. As a result, I often hear people say JSON is easy to understand. Who here thinks JSON is easy to understand? Some people, okay. Not everybody, that's good.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/970.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=970)

So I'll give an example of what I mean.  Here's a little snippet of JSON. I'll give you a moment to look at it. When you read it, you think, yep, okay, I understand what this is. I understand the meaning of this data. This is a very sort of human reaction.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1000.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1000)

I'll give you another snippet of JSON that ostensibly has exactly the same meaning, and you tell me if you still understand  what it means. Maybe somebody here actually speaks Finnish, and therefore you can interpret that the same way you interpreted the previous one. But I think of Finnish as sort of a language that's like a weak crypto mode. The key is known, but very few people have it.

Anyway, this is the same thing. I just translated the keys and some of the values to Finnish. In the English version you basically performed a grounding of these symbols. Grounding of symbols like degree or hobbies. You interpreted what that is based on your experience and your understanding of the English language, and now you think that that's the meaning.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1080.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1080)

Of course, I could say that this data means something completely different and has no relation to the English words that are there. But what you see here is how data looks like to all machines, all the time. And so I don't think JSON is easy to  understand.

### Working with Ontologies: Assessment, Design, and Practical Applications

All right. So that's kind of still background. I'll talk a little bit about what it means to work with ontologies. So let's say you decided you need a knowledge graph and therefore you need to have an ontology. The first question is really the sort of build or buy. And when I say buy, I don't really mean literally buy because ontologies tend not to be for sale, they tend to be shared, they tend to be public, but same difference.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1120.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1120)

So your choices really are, first of all, you could just  find some ontology and go like, okay, this is perfect for my use case, I will just use it as it is. You have to assess the suitability very carefully. And I'll talk a little bit about what this assessment actually is. It will be helpful when you start choosing ontologies.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1150.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1150)

You could take that existing ontology and go like, yeah, it's almost what I need,  but not quite, so I'm going to be extending it. Ontology languages have been designed in such a way that it's possible to take an existing ontology and extend that for your own purposes. There are also a thing called upper ontologies, which can be a good starting point for this kind of an exercise.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1190.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1190)

What's important to understand is that every ontology, or the designer of that ontology, has made some decisions about how to model the world, and you have to understand what these decisions are. Otherwise bad things could happen.  Now the third option is that you'll start from scratch, you know, go like, okay, I don't need what others have done. I'll just define my own. That's perfectly fine.

Just be prepared to do some work, because lately I've heard a lot of people say, oh, you know, I'll just let the LLM write me an ontology. An LLM can write you an initial draft of an ontology, but after that, it really requires human eyeballs.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1230.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1230)

All right, so let's now say you found an ontology, you're thinking maybe I'll extend this,  but I have to first assess the suitability. So what is that? First of all, consider your use cases. Not necessarily one use case. Pick a couple of use cases and be prepared to find more use cases in the future. Knowledge graphs have this interesting characteristic that you build something, and you think, okay, this covers my use cases, and then you realize, oh goodness, this knowledge graph can answer questions that I didn't anticipate, and then you may have more use cases and uses for that knowledge graph.

When we design an ontology, we usually define what is called competency questions. So these are really questions you anticipate that if you do this right, your resulting knowledge graph should be able to answer. So in some ways the competency question should be translatable to graph queries at some point. And then coverage means that you have something you want to model, you look at your enterprise or your organization and go like, okay, we deal with these kinds of concepts, make sure those concepts exist in the ontology that you're going to use. And if they don't, extend the ontology to cover everything.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1320.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1320)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1350.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1350)

Then there's this question that I mentioned that people have already made decisions  about how to model the world. I call these ontological commitments. And this is where what I like to call the law of unintended consequences comes into play. So it's like, in other words, this seemed like a good idea at the time. So you take this ontology and later on you realize, oh goodness, I can't do certain things because some decisions that were made way earlier.  That happens all the time.

And then finally, there is a question of expressive power. So expressive power is really a measure of what are the things you can say using your ontology. And for that you need something called a reasoner typically. You will always have this sort of a trade-off between expressive power and the computational burden for using a particular ontology that requires that particular level of expressive power. The more expressive, also more difficult to compute, generally speaking, as a sort of a gross generalization.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1400.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1400)

 So what can you use these things for then? A very typical use for ontologies is that you can operationalize or automate the processing of your data. You basically have data and have sort of the meaning, the semantics of that data, follow that data. And the aspirational end goal is that you can have software that can process that data, can interpret that data in some ways without actually that having been coded in there. You just give it the ontology and it interprets the ontology and can do meaningful things for that data. And that's a real thing that can be done.

Ontologies can also be used to uncover implicit information. So that whole example where Wally is a dog and now we find out, oh, Wally is also a mammal. We never said that Wally was a mammal. We uncovered that that was implicit in the data. Ontologies can serve as documentation. And having said all these things and there's a lot of sharing here, at least to me, the implication is that I really would like to stay away from closed proprietary ontologies. So they can certainly solve certain problems, but they don't give you the full benefit of what ontologies can do.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1490.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1490)

### Building an Ontology: From Public Resources to RDF Implementation

 All right, so some examples of shared public ontologies that you can use. So first of all, there are these upper ontologies that you can start from. Basic Formal Ontology is very, very abstract, and I always feel like you really need like a graduate degree in philosophy to use it. GIST is a much more practical one. Both are popular.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1520.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1520)

Then we have domain-specific ontologies. FIBO is about the financial  industry and all the different concepts in the financial industry. SNOMED is a medical thing, and CIDOC is an ontology for people who want to capture cultural heritage, museums, archives, you know, what have you.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1540.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1540)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1570.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1570)

Now, in addition to these two categories, we also have these kind of narrow  ontologies for single purpose that you can mix into your own definitions. These are four of my favorites. Dublin Core has been around for a long time. PROV-O is an excellent model for capturing provenance, and I will later have an example of what SKOS does when you build taxonomies and thesauri. 

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1610.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1610)

So let's build a quick ontology and see how that works. This uses RDF and OWL as the ontology languages, and it uses a particular syntax of RDF called Turtle. RDF has multiple syntaxes depending on your preferences. So we define a class called dog. We say it's a subclass of, first of all, mammal, and also a subclass of pet, and then we say it's disjoint with cat. So this is in line with what we talked about earlier. Then we define a few more classes: cat, human, mammal, pet, pretty clear stuff. 

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1650.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1650)

And then let's define a couple of properties. Let's define one property, the fur color property, and we say fur color property is something that belongs to dogs, and the value is some string. We don't say what that string can contain. An owner is a relationship that links pets and humans. So this is our simple ontology, and in this representation we would call this a T-box or terminology box. These are the definitions of your data. Then we also typically have another thing called an A-box or assertions box,  which is really the instantiations of those definitions.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1680.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1680)

So we say Wally is a dog, white and black fur, has an owner. Coco is another dog. Dora is a human. Things like that. Those are individuals, and those don't have to be part of your ontology, but they will go into your knowledge graph. And because we're using RDF,  we have a graph representation of this, so here it is. Wally is an instance of the class dog. Dora is an instance of a class human, and Wally has an owner called Dora. So simple graph structure, but RDF has this kind of self-similar structure.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1710.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1720.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1720)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1750.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1750)

So we can actually include the definitions themselves in the graph as well. So in this case, dog and human  would both be instances of a class called class, which now would be sort of a meta-class, obviously.  And here's an example of me querying the data that we saw earlier using Neptune. We have extended the Jupyter notebook so that you can run queries directly from cells and get the results, and those results can be tabular or they can be visualized as a graph. So all this I'm talking about is absolutely real, and when you want to start building a knowledge graph, we can help you. 

### Ontology Languages and Modeling Choices: RDF, OWL, SHACL, and Concept Schemes

All right, so I mentioned that there are different ontology languages. So first of all, RDF is a very simple ontology language. Here are some examples. I define pet, and I define the owner relationship. And then we can use that to infer new data. So if I say that owner is a relationship that links pets and humans, and then I say that Wally, who is a dog, and we know dogs are pets, has an owner called Dora, but I haven't said anything else about Dora, we can infer that Dora must be a human.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1800.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1800)

Now OWL is a more expressive ontology language,  and we can express restrictions using the subclass mechanism.

So here we say that a pet must have exactly one owner, and that owner must be a human. And if we then take a reasoner and we take your graph, if there are unsatisfied restrictions, two possible things can happen. Either the reasoner reports that there are inconsistencies, or it will add new information to your graph to make it so that those restrictions are satisfied. So similarly, it could infer that the owner, if you introduce an owner but it's not a human, the reasoner could actually infer that the owner actually is a human.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1850.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1850)

 SHACL is an RDF-based language that was originally designed for validating your RDF data. But you can also use it to express constraints. Those constraints in SHACL are called shapes. But unlike RDF and OWL, SHACL never infers new data. So if there are unsatisfied constraints, your SHACL engine will simply tell you that your data did not validate.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1890.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1890)

 So why is this? Why do RDF and OWL work differently from SHACL? So we have to talk about something called world assumptions. So first of all, closed world assumption is this thing where we assume that what's in your database is all that there is, there is nothing else. And so whatever reasoning you do can be based on the fact that you have complete knowledge. And that means that you can use something called negation by failure. So if you can't prove something to be true, you have to conclude that it's false. Traditional databases tend to use the closed world assumption.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1940.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1940)

 Open world assumption, on the other hand, is the idea that we don't know everything. There could be more information out there, and we can't possibly assume that we have it all. So that means you can't use negation by failure as an inference rule. We adopted this for the semantic web because we thought, well, you know, the web, maybe you haven't reached that particular server, you hadn't collected that particular piece of data. There could be more stuff, so it made perfect sense.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/1970.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=1970)

 And now, just a quick word about those ontological commitments or modeling commitments. So we defined that pet can have only one owner. But what if there's like joint ownership of some pet? How do you do that? So this is a very common occurrence. You pick an ontology, you do modeling, later you find out oops, can't do that. So possible solutions, if you control those upper definitions, you can just go and refactor your code a little bit, the code of the ontology that is, and make it so that there can be multiple owners, or you can introduce a new relationship, co-owner or something like that. There are ways to fix this.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2020.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2020)

 And then, when you define your ontology and you have different kinds of concepts, sometimes you have concepts that are really variations of one another, and you can capture this using a class hierarchy. The alternative is to capture these variations using something we call concept schemes, and I'll explain this. So let's say we want to represent dogs in such a way that we also capture the breed of a dog.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2050.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2050)

 Okay, so here's some definitions. A dog is a class, toy dog is a subclass of dog, Shih Tzu is a subclass of toy dog. And these are the, I don't know, American Kennel Club classifications of things. We introduced all these definitions and now we say that Wally is a Shih Tzu and a poodle. Yeah, these are called designer breeds nowadays. I would call them mutts, but anyway, here we use the class hierarchy to do that, and it sort of looks like this.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2090.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2090)

 Note that we can have instances that are instances of multiple classes. So multiple inheritance in RDF is not limited to classes alone.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2110.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2110)

If you don't like that, you can introduce that designer breed, which in this case is actually called ShihPoo.  It's a real, real thing. And then you can just say ShihPoo is a subclass of both Shih Tzu and Poodle, and then Wally is an instance of this class. All good.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2130.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2130)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2150.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2150)

Now, the alternative is to say all dogs are really much alike. I don't want multiple classes.  I don't want to clutter my ontology with a large number of classes. So let's put a property on dogs called breed and then decide what kind of values that property can take. And for that we need that SKOS  vocabulary that I mentioned earlier. And we build something that's called a concept scheme. And we define all these things that we earlier defined as classes. We now define them as SKOS concepts. And then we can just say that Wally is a dog and Wally's breed is Shih Tzu and Poodle. But basically you represented the same stuff. It's just that you only have one class now.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2190.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2190)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2200.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2200)

### Reasoning in Practice: Uncovering Implicit Information and Managing Entailments

And again here, if you only want a breed to have one value, you can again introduce the designer breed as a separate concept and then do this.  So this is how it works. This is how you define ontologies and work with the classes and the definitions. So, a couple of quick words  about how to actually work with reasoning, and this does not require a PhD in math. You can do this. I mean, maybe some of you do have a PhD in math, that's fine, but it's not a requisite here.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2220.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2220)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2240.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2240)

So there are many use cases for reasoning.  One is what we already talked about, uncover implicit information. We say Wally is a ShihPoo. We can conclude that Wally is a dog. We can also conclude that Wally is a mammal. Those can be handy things to know. We can discover inconsistencies if I say Wally is a cat and a dog.  The reasoner will tell me that you have a fundamental inconsistency, one that cannot be fixed by adding more data to the graph.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2250.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2250)

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2280.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2280)

 Reasoning can also be used to make query authoring easier because you can operate, let's say, just using the base classes, for example, and your ontology can evolve. But the reasoning basically insulates your queries from the changes in the ontology. You can add more dog breeds, and this query will still work.  RDF basically more or less just gives you taxonomical reasoning. But in addition to class hierarchies, also properties can have hierarchies, which is a very handy feature when you are extending somebody else's ontology. You can introduce your own properties, but you can say, well, these are really extensions of those properties that came from that other ontology.

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2310.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2310)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2340.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2340)

 So here I have defined an OWL class called black poodle, and I have defined it in a way where I say breed must have the value poodle, and fur color must have the value black. Now earlier I said that we use subclassing to introduce restrictions, but here I've used something called equivalent class, and the difference is this.  If you use subclass, that introduces a necessary condition. All black poodles must satisfy those constraints. But if you use equivalent class, it's a necessary and sufficient condition, which means that if you find something or the reasoner finds something where breed is poodle and fur color is black, it can actually conclude that this is an instance of the class black poodle, which means that the reasoner can be used for classifying your data.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2380.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2380)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2400.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2400)

 Well, now, of course, there's the question of how and when to do this. Some triple stores or graph databases for RDF, they have a built-in reasoner. If not, there are options. You can even use SPARQL, the query language, as sort of a reasoning  engine.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2430.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2430)

Here's an example of figuring out what the class of an instance is. Without reasoning, you can rewrite your query to basically climb the hierarchies in the ontology and do the same thing that a real reasoner would do. This happens at query time. If you want to do this before query, you can materialize these  results of the reasoning beforehand. Results of reasoning, by the way, are called entailments. You will see this word pop up every now and then. If I say entailment, think about results of reasoning.

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2460.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2460)

So this would be a way to materialize the entailments in your graph, and there is a choice between these two: do it at query time or do it beforehand.  If your data changes a lot, it may be more useful to do it at query time. But if your data is more or less static, you can do it up front and then you don't have to do anything when you're querying.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2480.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2480)

So when you're materializing, you will encounter this problem that we call  truth maintenance. I used to think that truth maintenance could be like something an authoritarian government does, but it turns out that truth maintenance is the question of, okay, I've materialized some entailments, and now I've changed the data. Which entailments do I have to retract now because they're no longer supported by the original data? And this can be a very costly operation.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2530.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2530)

If you use SPARQL, the query language, as your reasoning engine, you can do this what I think of as poor man's truth maintenance. You materialize, you put them in a separate named graph. RDF has this feature called named graphs. And then you can drop that graph and recompute when things have  changed. All right, so that's that.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2550.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2550)

### Bridging Old and New AI: How Symbolic Techniques Address LLM Limitations

Now a few words about the new AI and its relationship to the old AI, so to speak. So can the older AI help the new AI is really the question.  All of you must have seen this quote. What's important about this thing is that Arthur C. Clarke said, "Any sufficiently advanced technology is indistinguishable from magic." He didn't say it is magic. And there are a lot of people who think of the new AI as sort of magic, but it isn't magic, and it's really important to keep that in mind.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2580.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2580)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2600.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2610.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2610)

So we have some challenges.  We all know that these are challenges, so hallucinations, obviously. LLMs will make stuff up, and that's a feature. They're supposed to do that. But it makes it difficult to build systems. Anthropomorphism. People start thinking of their chatbot as an actual human.  And the answers sound convincing. People will believe them. They won't check them. This is a problem.  And then the computational efficiency is definitely a problem in the sense that if you already know how to compute something, then just compute it. Don't ask the LLM because it's highly inefficient. And our energy supply is finite.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2630.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2630)

 Related to this are some interesting historical criticisms of AI. Joseph Weizenbaum in the 1960s built what is the first chatbot, and it was very crude, obviously compared to what we have today. You sort of use these rules and pattern matching. But he noticed that people started having conversations with this chatbot as if it was a real person, and it turned Weizenbaum into a vocal critic of AI. He didn't like the idea of anthropomorphizing computers.

The Chinese room experiment is an interesting sort of philosophical question that really asks what is consciousness and what is understanding. Can machines understand something? Can an artificial system be conscious? I used to think it's only a philosophical question, but lately, maybe it's a more practical question. Even these are highly relevant today.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2690.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2690)

 The history of AI is about scarcity. We always had scarcity of something. We didn't have the right algorithms. We didn't have enough memory. We didn't have a fast enough processor. Those are more or less solved in many ways. We have enough of those, but we have other scarcities today.

This is my list of the scarcities we encounter today. So let's talk about this a little bit.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2720.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2720)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2740.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2740)

 We want correct answers, right? We don't want the system to hallucinate some bullshit. So you could pull the answer from a knowledge graph rather than have the LLM produce the answer for you. Accountability, trustworthiness, same thing.  You want to know where the answers are coming from. If they come from your knowledge graph, that knowledge graph can be curated and audited, and you can make sure that it contains provably factual information.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2760.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2760)

You want to be able to explain the answers.  An insurance company will tell a policyholder that their claim was denied. I'm sure the first question that the policyholder has is why was it denied. You want an explanation, and explainability is a prominent feature of symbolic reasoning. When you do symbolic reasoning, you can always trace back why you got the answer you have.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2790.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2790)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2810.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2810)

 And then there's this energy question. So getting an answer from a knowledge graph for a question is literally several orders of magnitude more efficient than getting it from an LLM. So there's a growing understanding that the future AI systems really are going to be  hybrid. They will combine different kinds of technologies, symbolic and these newer non-symbolic techniques.

But in cognitive science, there's been this idea of what they call dual process models of cognition. This is an idea that really was popularized by Daniel Kahneman's book Thinking Fast and Slow. It wasn't his idea originally, but he made it popular, and the idea is that there's a fast process that gives you an answer right away, but that answer may be incorrect or biased or something, or the answer can also be deliberated a little more and you get a more trustworthy answer.

And there was also in the 70s a lot of interesting work where people really started thinking about intelligence and consciousness and started drawing what we now would think are like architecture diagrams, software architecture diagrams. Aaron Sloman, Daniel Dennett did a lot of work like that. So here's my sort of very trivialized idea of how to use an LLM with a knowledge graph.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2880.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2880)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2910.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2910)

 Ask a question, the LLM translates the question into a query. The query runs, answer comes from your knowledge graph, and of course, not just basic query processing, it could include symbolic reasoning as well. Now how does this fast and slow thing work with this? So it could be something like this. LLM gives you a quick answer,  or you go the other route, you get a more sort of effortful answer. Other architectures, of course, are possible and make perfect sense, but I like this as sort of a simple explanation of what these things might look like.

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2920.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2920)

 So now you're going to ask, well, this is graph RAG, right? This is not graph RAG. So graph RAG uses a graph, not necessarily a knowledge graph, to basically improve the LLM performance, accuracy, whatever. But I'd like to kind of flip this and use an LLM to improve the performance and usability of knowledge graphs. And now with LLMs we finally have a decent natural language capability. And so, taking questions, translating them into something, but having the answers come from a knowledge graph, to me, sounds like a great idea.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2970.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2970)

### Neurosymbolic AI and the Future of Hybrid Systems

And then finally, people talk about neurosymbolic AI, so let me explain what that is.  So, symbolic AI is often seen as kind of unrealistic and too idealistic. Everything has to be perfect for things to work. Now, on the other hand, on the positive side, the representations are completely transparent and results can be explained.

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/2990.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=2990)

 Non-symbolic AI, on the other hand, while it can deal with the sort of messy real world data, the representations are opaque. You can't really look at them, and the results cannot be explained. So what we would really like is to get all the green stuff, right?

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/3010.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=3010)

 So here are some examples of what neurosymbolic AI could be. It is not a single architecture or methodology. There are many different ways to combine symbolic and non-symbolic processing. So combining the two, it could be that you have used a heuristic search, but your evaluation function in the search actually does some neural processing, for example, and obviously this System One and System Two architecture is an example of that too.

You could take non-symbolic input like a picture or photograph, convert that to some symbolic representation, and then use symbolic techniques to reason from that symbolic representation. Or you could take symbolic data but use that with non-symbolic machine learning algorithms, for example. People have done work on showing how mathematical equations can be simplified and then fed those as training data to a machine learning system and come up with a system where they claim that now they have a machine learning system that can do mathematical simplification. Now, that's still guessing, but results have been remarkably good.

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/3090.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=3090)

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/3100.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=3100)

 All right, so I suppose I should summarize a little bit. I hope you're going to take some of this with you.  So AI has been around for a long time, longer than three years, okay? And there have been many useful techniques and technologies developed over this period of 80 years or something. And these technologies, once they're well understood, they really stop being AI. Now in the early 2000s we saw this dramatic shift from symbolic to non-symbolic AI, but the symbolic AI techniques are still relevant.

[![Thumbnail 3130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/3130.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=3130)

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b399a9f5d0e6426/3150.jpg)](https://www.youtube.com/watch?v=Atf4DVKGuMg&t=3150)

 Ontologies, you can do this. You can use ontologies, you can build knowledge graphs, you can operationalize your processing of your data, you can document your data better using an ontology, and you can even use reasoning. This is all quite real.  And then finally, we can mitigate some of the problems that we have with LLMs using symbolic techniques. Not only ontologies and knowledge graphs, mind you, but planning and autonomous agents that come from symbolic AI. Those techniques can be very useful. We didn't have time to talk about that, but I'll hang out here. You can come and ask me if you're really interested.

The whole idea of planning particularly is something that the modern manifestation of agents tend not to do, and I always think that agents have to be able to do planning in order to exhibit autonomy. We'll see many more of these hybrid systems going forward. We have a little bit of time. I'm not going to answer questions now, but come talk to me afterwards. It's much easier to answer questions that way.

So thank you very much. And if you later on have questions, you can always email me. Remind me of this lecture. My email, so I'm Ora, and my email is super difficult to remember. It's Ora@Amazon.com. And thank you so much.


----

; This article is entirely auto-generated using Amazon Bedrock.
