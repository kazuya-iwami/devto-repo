---
title: 'AWS re:Invent 2025 - High-performance NLP & geospatial analysis with Amazon Redshift (ANT334)'
published: true
description: 'In this video, Ryan McMahon from Cambridge Mobile Telematics and Ravi from AWS discuss how they built StreetVision, a road safety platform that predicts crash locations by analyzing over a trillion daily data points. Using Amazon Redshift''s spatial capabilities and H3 hexagonal indexing, they aggregate driving behaviors like hard braking to identify hazardous road features such as occluded stop signs. The H3 implementation delivers 98% compute savings and up to 85% cost reduction compared to traditional polygon-based queries. Ravi demonstrates Redshift''s 100+ spatial functions, GEOMETRY and GEOGRAPHY data types, and integration with Amazon Q for natural language SQL queries, enabling DOTs to proactively address road risks before crashes occur.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - High-performance NLP & geospatial analysis with Amazon Redshift (ANT334)**

> In this video, Ryan McMahon from Cambridge Mobile Telematics and Ravi from AWS discuss how they built StreetVision, a road safety platform that predicts crash locations by analyzing over a trillion daily data points. Using Amazon Redshift's spatial capabilities and H3 hexagonal indexing, they aggregate driving behaviors like hard braking to identify hazardous road features such as occluded stop signs. The H3 implementation delivers 98% compute savings and up to 85% cost reduction compared to traditional polygon-based queries. Ravi demonstrates Redshift's 100+ spatial functions, GEOMETRY and GEOGRAPHY data types, and integration with Amazon Q for natural language SQL queries, enabling DOTs to proactively address road risks before crashes occur.

{% youtube https://www.youtube.com/watch?v=kTT3vnC3qyg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/0.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=0)

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/10.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=10)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/20.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=20)

### From Individual Driver Safety to Geospatial Road Risk Analysis: Cambridge Mobile Telematics' Evolution

Thank you very much. I'm Ryan McMahon from Cambridge Mobile Telematics.  We're a technology company that measures driving behavior,  and our mission is to make the world's roads and drivers safer. We do that by identifying and understanding  the risks that create crashes and understanding what risks contribute to those crashes. Our core mission is to help identify how to prevent them. We do that by understanding and identifying a vast array of sensors and processing that information on AWS today.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/60.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=60)

For most of the company's history, our work has been used on an individual one-to-one basis, meaning that individual drivers are measured and we process that information and provide it back to them. We also detect crashes and respond to them. Over time, the data has become increasingly complex  because we're gathering data from sensors that have come online throughout the company's history. We started with feature phones and have moved to smartphones. We use proprietary tags that we manufacture ourselves, and we're also expanding into other sensor platforms. All that information today is processed on AWS, but it has been processed and used for individual drivers.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/90.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=90)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/110.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=110)

The reason we do this is to identify and understand those basic risks so that we don't reach the top of the pyramid, which represents roadway fatalities.  Unfortunately, over 40,000 people die on the roads in the US every year. Even more than that, we're talking about over a million people globally, and we're a global company.  Over 15 years, our focus has been on individual drivers.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/120.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=120)

This session is about geospatial, and you may be asking yourself how these two things connect.  What happened was we started to do research on this topic and identify more than just individual driver risk. We started to identify a host of features that go beyond the individual driver, because when it comes to crash risk, it's not just the individual driver that leads to that negative outcome and those injuries and fatalities. In fact, the vehicle has a large role to play, and the road does as well. We've expanded significantly into understanding road risk.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/170.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=170)

The goal is to predict and identify where crashes are going to happen.  If we could simply identify where serious injury crashes happen and address the issues there, we can remove those fatal crashes. But the reality is that individual crashes, as sad as each one is that leads to injury or fatality, are not predictive in themselves to identify where the next fatal or injury crash will happen. So we have to identify the features that underlie that. What leads to the risk that gets us to the point of identifying where a crash is going to happen?

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/190.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=190)

That's where AWS came into play in a huge way for us.  We're processing an enormous amount of data every dayâ€”over a trillion data points are coming in. We had a problem when we wanted to move to road risk, because for individual drivers, the processing that goes into play is important, but it's not at the same scale from a geospatial perspective. So we had to take two steps further to get to the point of identifying those issues. This is what we built.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/220.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=220)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/240.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=240)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/260.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=260)

 This is an example from a road in Arkansas. I recently spoke at the Arkansas Safety Summit, and the director of the DOT, Jared Wiley, invited me to come and present. This is one of the data points I showed. It's an aggregation of hard braking events that occurred over a three-month period of time.  When you look at the aggregation of those events, it becomes clear when you actually start to dig deeper. We didn't identify this issue from an image; we identified it from human behavior. But if you look at the image, it becomes very clear that the stop sign is occluded.  There are many branches and trees that are blocking stop signs all over the world. But how many can you get to the point of identifying where the human behavior is concentrated and aggregated to identify that risk?

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/280.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=280)

The challenge is that if you get to this point where you have all this information to identify the behaviors that lead to crash risk and you can aggregate it together, how do you display it on a map that's useful to the DOTs, to the highway safety offices, to EMS, to get to the point of reducing those risks as they occur?  The question really came down to whether we should use road segments. We could take all this information and load it into pre-existing mapping coordinates. We decided we're going to use H3 hexes. But the problem was that at that point, AWS didn't support taking that data and moving into H3 hexes.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/320.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=320)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/330.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=330)

So this is really what AWS has been able to do: take all of that information today and give us the information we need to identify at a massive level.  We're now loading over 5 billion records every single day into a system that opens up and can do something like this. 

This is a stop sign in Washington D.C. I went there in person and took this picture. It's the same issue, the same exact issue. You can see that there's a stop sign that's secluded by a tree. What's happening is people are getting near-miss crashes. Today, the way these problems are solved, if you're a DOT or highway safety office, is we wait for the bad thing to happen, because the currency of risk today is unfortunately the crash itself. We're moving two steps beyond that.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/390.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=390)

AWS, if AWS didn't have those capabilities, we wouldn't be able to take this massive amount of research that we'd already done in identifying risk and be able to bring it to the solution to make it easy to use. Effectively, what I try to term it as is Zillow for road safety: looking at, from a mapping perspective, how do you identify and understand those risks and make it translatable across the globe. This is a huge scale of undertaking to identify those behaviors, to ultimately get to the point  where we can move further down the pyramid. From a technical perspective, the partnership that we had with AWS, 18 months ago, we couldn't have done this. It was that partnership that led to this development. I'm so impressed with what we've accomplished.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/420.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=420)

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/440.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=440)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/480.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=480)

### Amazon Redshift Spatial and H3: Enabling Petabyte-Scale Geospatial Processing

To talk more about that work of getting further down the pyramid and how technically we got to that, I'm going to hand it over to Ravi from AWS.  Redshift went down the path of building spatial capabilities into a cloud data warehouse. We went down this path some seven or eight years ago, and we're pretty far along now. The key to what Cambridge Mobile Telematics has built  into their StreetVision platform is using the spatial data together with the spatial query functionality with Redshift. You don't need to learn anything new; it's all done in SQL. You have all the spatial capabilities, and I will talk about some of the newer capabilities we've built, which specifically include the H3 functions, the hierarchical hexagonal indexing, which is an open source initiative that started at Uber. It is a huge leap in terms of spatial performance. 

So the question on top of all of your minds is: Why Redshift Spatial? All of you have probably heard of Amazon Redshift, one of the first cloud-native data warehouses to be released almost 14 years ago. The reason is very simple. Redshift has the right architecture to do massively parallel processing with MPP architecture from day one. It was the first cloud data warehouse to be built on MPP architecture, and it can handle petabyte-scale data while operating on columnar data. You get all the benefits of columnar data: your vectorized scans for really efficient query processing on read queries. That lends very well to petabytes of spatial data.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/580.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=580)

The volume of data that Cambridge Mobile Telematics has to deal with is massive; it's in the petabyte scale. Couple that with the highly precise, numerically accurate robustness of our spatial implementations, whether it's WGS84, the SRS standard for implementing spatial projections on an ellipsoid earth model, and leveraging all the functions that we're continuously adding to our spatial portfolio. The most recent of which is the H3 functions.  Those who are familiar with PostGIS libraries are probably already familiar with some of the spatial capabilities in Postgres-like databases. We follow the same syntax and the same sort of functional equivalence. When we first embarked on this, we launched GEOMETRY as a first-class data type in Redshift. Very soon, a couple years later, we added GEOGRAPHY as well.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/650.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=650)

Geometries are designed for two-dimensional, non-geoid-based coordinates, such as non-latitude-longitude-based systems for short distances where you don't need the projections and numerical accuracy of geography data types. Together with GEOGRAPHY and GEOMETRY as native first-class data types in Redshift, you have the building blocks to handle all spatial data in Redshift. We now have over a hundred spatial functions implemented natively within Redshift,  with the most recent being H3, which I'll discuss shortly.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/700.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=700)

Here's what it looks like when you run a sample query in Redshift. Just like any other cloud data warehouse, you create a table, load some spatial data, and use SQL to query it. You use all the built-in spatial functions, those hundred-plus functions I mentioned. With that, you have your spatial analytics capabilities. You can mix and match spatial data types with non-spatial data types like anything else. This is now standard across many cloud data warehouses, but Redshift, with its architecture for very high performance, really excels in processing spatial data. 

What's special about these spatial functions is that you can create visualizations like this. What you're seeing here is a map of Berlin. Let's say you have a table called "Accommodations" with all the coordinates of hotels in that polygon. Those little clouds you see are the parameters of the polygons. The query here shows you "Find me all the hotels in that polygon." You submit a shape file, extract the latitude and longitude, and use a regular SQL query. That's itâ€”you get all the hotels listed. This is a simple query, but think of it as a building block for processing at scale, millions of records per minute or per second in some cases.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/790.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=790)

We support various spatial formats including shape files, WKT (well-known text), WKB (well-known binary), and the extended versions of those formats, as well as GeoJSON and many other types. We have significant flexibility in reading and writing spatial data formats.  This is a sample of the hundred-plus functions we have, and it's continuously growing. What you see highlighted in yellow or orange is what we've recently added. We started with the five functions most needed by the Street Vision application, and there are several more H3 functions we plan to introduce.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/820.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=820)

So what is H3?  H3 is a hexagonal hierarchical indexing system. The problem with the old way of querying is that you have a polygon, you define a parameter, and you want to find points within that polygon or some other variations. For the SQL engine, this boils down to a Cartesian join, which is complex, expensive, and exponentially huge, so it can get bogged down. With H3 indexing, you have pre-computed indexes for your locations. Each cell's index is the location itself, and you can choose the resolution that interests you. There are 16 levels from zero to 15, and you can select the resolution you want. A lookup is lightning fast.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/890.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=890)

To see how fast it is, here's a comparison.  You can save 98 percent of compute with the new way of doing lookups compared to the old way. This shows you other resources about memory, CPU time, and query response times. H3 is a game changer for spatial lookups at scale.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/920.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=920)

You can save up to 85% on your Amazon Redshift charges just by using H3. 

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/940.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=940)

### Generative SQL and Natural Language Processing Integration in Amazon Redshift Query Editor

Some of you may be wondering how this fits into the NLP capabilities that Ryan talked about with the Street Vision platform and CMT. The way it all fits in is through Query Editor.  Amazon Redshift has the Query Editor, which is our SQL interface. It has been growing in features, and we now support Jupyter-style notebooks for SQL. It has full integration with Amazon Q for SQL, which has integration with all the models that Amazon has access to, both Amazon-developed and third-party models. This gives you the ability for non-SQL experts to use a SQL data warehouse like Amazon Redshift using plain English and natural language processing.

The biggest advantage is that Amazon Query Editor v2 is fully integrated with the Amazon Redshift cloud data warehouse. You have full query history, full metadata, and full context of the schema. It does the schema crawling, understands what you are trying to query, what tables and columns you need, and understands from your query history exactly what you are looking for. That is a huge difference. Some of the other query tools within the AWS portfolio, like QuickSight, are also building this integration with Redshift.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/1040.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=1040)

Finally, there is generative SQL.  Translating NLP in plain English, the intent of your query is where the generative part comes in. It looks at your history, but it also divines your intent. This is where it really is a game changer because it can do predictive queries. You have the ability to edit it in your notebooks. It generates the query, you can execute it as is, or you can edit it and submit it. That is one of the biggest advantages of generative SQL integration.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/59551d6023c480b2/1090.jpg)](https://www.youtube.com/watch?v=kTT3vnC3qyg&t=1090)

The biggest advantage is that it understands  the context of your schema, the tables and the layout, and the history. Taking those two together is really the big difference. If you use a third-party BI tool, you will be missing some of that context because they do not have the level of integration that we do to our own metadata and Redshift metadata. That is the real differentiator.

With that, I would like to thank you all for taking the time to attend today. We have a few minutes left, and I will open it up for questions. We will take questions for Adrian from Cambridge Mobile Telematics, Ryan, or myself from the Amazon Analytics team. I am more than happy to take questions. Thank you again for attending today.


----

; This article is entirely auto-generated using Amazon Bedrock.
