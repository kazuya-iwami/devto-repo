---
title: 'AWS re:Invent 2025 - Harnessing analytics for humans and AI (INV201)'
published: true
description: 'In this video, Mai-Lan Tomsen Bukovec, VP of Technology, Data and Analytics at AWS, explores three emerging analytics trends: Agentic AI integration across data workflows, open analytics architectures using Apache Iceberg and Parquet formats, and vectors for semantic understanding. Key announcements include the optimized Spark 3.5.6 engine across EMR, Glue, and Athena, AI-powered Spark upgrade agents, and the new serverless SageMaker notebook with built-in AI assistance. Intuit''s Tristan Baker demonstrates how they reduced data discovery time from 20 days to 9.4 hours using semantic layers and metadata management across 180 petabytes. AWS announces Iceberg write support in Redshift, S3 Vectors general availability supporting 20 trillion vectors, and OpenSearch GPU acceleration for 10x faster vector indexing at 25% lower cost. Supabase showcases their Open Warehouse Architecture integrating PostgreSQL with S3 Tables and Iceberg.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Harnessing analytics for humans and AI (INV201)**

> In this video, Mai-Lan Tomsen Bukovec, VP of Technology, Data and Analytics at AWS, explores three emerging analytics trends: Agentic AI integration across data workflows, open analytics architectures using Apache Iceberg and Parquet formats, and vectors for semantic understanding. Key announcements include the optimized Spark 3.5.6 engine across EMR, Glue, and Athena, AI-powered Spark upgrade agents, and the new serverless SageMaker notebook with built-in AI assistance. Intuit's Tristan Baker demonstrates how they reduced data discovery time from 20 days to 9.4 hours using semantic layers and metadata management across 180 petabytes. AWS announces Iceberg write support in Redshift, S3 Vectors general availability supporting 20 trillion vectors, and OpenSearch GPU acceleration for 10x faster vector indexing at 25% lower cost. Supabase showcases their Open Warehouse Architecture integrating PostgreSQL with S3 Tables and Iceberg.

{% youtube https://www.youtube.com/watch?v=L_Q7LPB5HcA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/0.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=0)

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/10.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=10)

### Introduction: Three Emerging Trends in Analytics and the Rise of Agentic AI

 Please welcome to the stage Vice President of Technology, Data and Analytics at AWS, Mai-Lan Tomsen Bukovec. Welcome to the analytics leadership talk. My name is Mai-Lan Tomsen Bukovec, and I run data and analytics services for AWS. Today, we're going to walk through three emerging trends in the world of analytics  and how our customers are steering into them with their data strategies and their AWS services.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/40.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=40)

Let's start first with how Agentic AI is delivering assistance in every step of your data journey, whether it is writing code, processing data in pipelines, or using data products, just like a human would, except much faster.  It's incredible how far agents have come in the last year. So many of our customers have put AI agents to work on real production workloads, and we have been doing the same. We have been working hard to integrate AI into our analytic services for you to use for the jobs to be done.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/50.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=50)

 As data lakes have grown, and if you saw in the keynote this morning, you saw that data lakes were represented as a data ocean in the Sony presentation, analytics have also been integrated in every aspect of business. Agentic AI is starting to become a natural part of how humans and applications work. We know that data and AI are increasingly intertwined, so we're building AI smarts into how you work with data on AWS.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/70.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=70)

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/90.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=90)

### AWS Optimized Spark 3.5.6 Engine and AI-Powered Upgrade Assistance

 We just launched this week a highly optimized Spark 3.5.6 engine as part of the latest versions of EMR, Glue, Athena Spark, and our new notebook for SageMaker.  Now you have one AWS optimized Spark engine that's powering these different AWS services and experiences. It's a super exciting launch, especially for the many customers out there using Iceberg as their open data format. Our optimized Spark engine speeds up both read and write performance on Iceberg data as well as any of the interactive queries that you're doing in Athena.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/120.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=120)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/150.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=150)

 Spark major version upgrades often require at least some refactoring of the applications that are sitting on top. We've incorporated almost a decade of helping customers migrate to new versions of Spark into AI assistance, so you can now use AWS AI to more easily migrate your applications to our new optimized Spark engine in EMR and Glue.  In order to take advantage of this AI assistance, you have different ways of doing it. You can do it in local VS Code or through Tools and SageMaker Unified Studio, or you can use our remote MCP servers from Kiro or any of your favorite coding assistance environments.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/190.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=190)

 The Spark upgrade agent is going to go all the way back to helping you with upgrades from versions as early as Glue 2.0, EMR on EC2 5.2, and any of your previous EMR serverless versions. We're excited about this. In AWS, we have a saying that there's no compression algorithm for experience, and our new Spark upgrade agent runs off of a knowledge base that's based on thousands of Spark upgrades, some that have gone well and some that have not. It plans the upgrade, it works through the error messages, it remediates failures automatically, and more.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/240.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=240)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/260.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=260)

 Now you can finish the work of upgrading your app to the latest Spark in EMR and Glue, and you can do it in weeks instead of months. This combination of a highly optimized Spark 3.5.6 engine and AI assistance for upgrades is a real game changer for our Spark customers. It lets you modernize quickly with the help of AI from AWS.  FINRA, which safeguards the integrity of the US capital markets, tried using our upgrade AI agent earlier this year, and now they plan to anchor their Spark version management on it entirely in 2026.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/290.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/300.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=300)

### The New SageMaker Notebook: Serverless, Polyglot, and AI-Integrated

 Agents are a great fit for Spark upgrades because it automates repetitive workflows, but AI is also really good at helping you write and fix code. 

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/340.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=340)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/350.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=350)

We've built AI into our new notebook in SageMaker, and we're also using AI under the hood inside our AWS services, like how Redshift query engine uses ML and AI to optimize queries and manage serverless clusters. We launched the new SageMaker notebook a few weeks ago, and when we launched this capability, we also integrated AI capabilities directly into the new notebook experience.  You have one place to process and analyze using SQL, Python, or natural language with AI assistance built in. 

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/360.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=360)

We built this new notebook to make working with data easier, and the goal is to take away the operational burden of managing the analytic services that the notebook uses behind the scenes.  Whether you're exploring data analysis, building an ETL pipeline, or training a machine learning model, the notebook provides you a streamlined programming environment for your end-to-end workflows, and it doesn't require you to manage any infrastructure. It connects with your data wherever it resides, such as Iceberg tables on S3 or your Amazon Redshift data warehouse.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/400.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=400)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/420.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=420)

As part of this notebook experience, we've built in AI assistance to support you every step of the way. It can answer your questions, write scripts or SQL queries, generate pipelines,  and create visualizations. The experience of working with data with a notebook is more intuitive, more efficient, and frankly, more fun to do data exploration and insight generation. Unlike traditional notebooks like JupyterLab IDE,  which SageMaker also supports, this new notebook is entirely serverless, so you don't need to tune, manage, or pre-provision query processing infrastructure.

What's really interesting about this SageMaker notebook is that it's polyglot, which means you can express your code in Python or SQL cells that interoperate with each other in the same notebook. You can write PySpark using Spark Connect in Python cells, and in SQL cells, you can write SQL queries against Redshift, Athena, or a third-party analytics engine like Snowflake, and you can reuse your results in Python. This notebook is an all-in-one data notebook that you can use with any open analytics architecture now and in the future.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/490.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=490)

The AI assistance that we've built into the data notebook helps you with data querying, exploratory data analysis, and model development. The most interesting thing is that you can describe your objectives in natural language right in notebook cells.  The agent that's under the hood for our AI assistance uses context from the data catalogs and metadata, and the rest of the work that you do in your notebook to suggest execution plans, generate code, and help you build. You can use the notebook in SageMaker by clicking into it from the AWS management console, or if you want to use the notebook capabilities, you can use a remote MCP server from your favorite coding environment.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/570.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=570)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/580.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=580)

### Live Demo: Building Data Products with AI Assistance in SageMaker Notebook

Here to show you how we've integrated our new SageMaker notebooks and our AI agent together in an integrated experience is Diane, the principal engineer from SageMaker Unified Studio. This is our notebook interface. It's a familiar interface, but it's more modernized.  It has a collection of cells where you can express your code in Python, SQL, Markdown, or natural language. You see the data explorer here with the New York City taxi data and an Iceberg table on S3,  and I have my chat agent open on the right.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/590.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=590)

Let's get into some of the basics. I can create a Python cell, read some sample data into a pandas data frame,  and see the output of the data in rich data tables. As I explore my data, I might notice a unit price column here, and there might be another column of interest, such as the category column, and then I can use these pieces of information to write a SQL query on my Python data frame.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/610.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/620.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=620)

I see the output in the same rich data table.  If I really wanted to step up my data exploration, I could use the notebooks' native visualization capabilities and render the data in interactive charts. 

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/630.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/640.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/650.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=650)

So with the basics out of the way, let's use the New York City taxi dataset to get some real-world jobs done. I can ask the agent  to read my New York City taxi dataset into a Spark data frame and give me the shape of the data. The agent immediately generates a plan for how it's going to find and read the data into Spark  and what kind of exploratory analysis it will do. What we can see is that Spark is just available in the notebook with no infrastructure set up, no clusters,  and no complicated configuration. It's just there, ready to work on your data.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/660.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=660)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/670.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/680.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=680)

What's powerful here is that the system understands where the data lives,  automatically chooses the best way to query the data, and it shows me the exploration steps. Upon completion, it summarizes what the steps have been able to accomplish.  This gives me a sense of the schema of the dataset. It produces some statistical summaries on factors such as trip distance, fare, and tip. 

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/690.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=690)

Every analysis comes with generated visualizations, and it really takes exploratory data analysis to the next level. As a user, I can just sit back and watch while the notebook does all the work.  As we go through this data exploration journey, the dataset might be large or small, but sometimes we might run into some issues because not everything goes according to plan. When that happens, it's very difficult to sift through all the large exceptions or stack traces, and for developers such as myself and many of you here, it's very difficult to figure out what the actual problem is when visualizations don't render.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/720.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/730.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=730)

What I can do on the notebook is use the fix with AI capability, and all I need to do is click the button.  When I click fix with AI,  the agent will analyze the exception, recommend the code that will fix the issue, and all I have to do is take a look at the generated code and hit accept and run. Just like that, my problem is solved and my visualizations are rendered. This gives me a sense that my data might not be of the best quality.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/750.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=750)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/760.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=760)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/780.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=780)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/790.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=790)

Given that,  I can ask the agent to clean my dataset and give it some factors it should consider as it performs the cleaning steps. What's happening here is that the agent understands  both the intent and the context of the data. It's doing in seconds what would usually take multiple scripts, queries, and validation steps. The notebook automatically generates Spark code, and every step of the way it shows the transformations that it applies to the cleaning process and a summary of the outcome. It's really powerful to also see  the outcome of the data and the preparation steps because I can really get a sense of how the quality of the data is improving based on the visualizations that I see on screen. 

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/800.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=800)

Now that I have some high-quality data, I can ask the agent to train a model that predicts trip prices. The agent recommends a regression approach.  It writes the code to train the model and returns a summary of the results, including things like feature importance and accuracy metrics. It even generated code so that I can visualize the predictions that the model would make. Once again, I can just sit back and relax and watch the notebook as it trains the model, shows the model performance, gives me a sense of the feature importance, and shows the visualizations of the predictions and successes and errors.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/830.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=830)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/840.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=840)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/850.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=850)

What's really powerful here is we're not switching tools.  We've gone from raw data to model insights all in one notebook, and AI is doing all the heavy lifting. I've done a lot of work and there's  a lot of rich context that's built into the notebook. Finally, I can ask the agent some general questions about the data. For example, I ask it to explain the key drivers of price, which is a general question. 

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/860.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=860)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/870.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/880.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=880)

Now the agent has access to all the rich context. What it gives me is some key findings not only in a text summary but also some code.  When I accept and run the code,  what the notebook will do is translate those insights into meaningful visualizations, and I can use this on a dashboard or in a report. 

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/890.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=890)

Now I've cleaned data from which I can generate insights. It's often useful to write this data back into my data lake so that I can do business intelligence or other things. I asked the agent to write the data back into my data lake. It gives me Spark code  which will write the clean dataset as an Iceberg table back into my data lake. It runs the code, I can see the schema, and it gives me some statistics as it does the write.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/910.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=910)

Now, if I dive back into my data explorer and expand my default database, I see the New York City taxi database here  cleaned and ready to go for further analytics. What we've just seen in the last few minutes is that we've taken some data that was on Iceberg on S3, we've cleaned it, we've transformed it, we've trained a predictive model, and we've produced business-ready insights all within a single serverless agentic notebook. This is really what building data products with AI looks like. It's less about managing infrastructure and more about accelerating the path from raw data to intelligent outcomes. Your data is the product, your notebook is the vehicle, and AI is the driver.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1000.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1000)

### Making AI a Natural Part of Analytics Workflows

I'm really excited to see what you all build with the new notebooks. Thank you for having me. It's awesome. Thank you. Great demo. It's a lot of fun to use, and you can get started with this right after this presentation and try it yourself. As I shared, when we built SageMaker Unified Studio, our goal was to make it easy to work with your data. We also have MCP servers that are both local and remote, so you can use any of these new capabilities for whatever your favorite AI or traditional coding tool is. No matter what your point of access,  our whole goal is to build AI into analytics so it becomes just a natural way for how you work.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1010.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1010)

### Open Analytics Architectures: Flexibility Through Open Data Formats and Cross-Service Building Blocks

The second  emerging trend in analytics that I want to talk about is open architectures. In today's modern analytics world, customers know that not one size fits all in an analytic solution. You can't just put every capability behind a single API. You want the flexibility to pick a solution that meets a specific need. At today's scale, you also know that you don't want to pay for what you don't need. You want the choice to be there now and in the future, whether it's a line of business making a decision on what analytic solution to use or for you at whatever stage you are in your data journey.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1060.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1060)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1100.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1100)

The thing that makes these open analytics architectures possible is open data formats like Parquet and Iceberg.  Open data formats are the key because it means that you can have an open data format in your data and then you can just swap out these different analytic solutions that you use with your shared dataset. If you use these open data formats, you can make those changes for analytics without getting locked into an expensive data migration. In S3, we started to see the shift to these open data formats about five years ago. Today, S3 stores exabytes of Apache Parquet data, and we average 25 million requests per second just for Parquet. 

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1110.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1110)

These open data formats have really taken off because customers don't want to deploy monolithic analytics architectures.  They want to let different organizations pick the right analytics solution for them, whether that's Redshift bringing the data warehouse smarts to a data lake, ad hoc querying using the new Athena for Spark, using our managed Kafka service MSK, managed Flink MSF, or any other analytic solution. An open analytics architecture helps you evolve your data strategy without having to do that data migration when you make a decision to go with a different analytics provider. Nobody does composable building blocks better than AWS.

The composability also means that it's cost effective and economical because you're not paying for a bundle of things that you're just not going to need. Because we have SageMaker Unified Studio, which you just saw in action, you don't sacrifice on the simplicity of experience. You now have an easy-to-use analytics development environment with SageMaker Unified Studio, and I'm sure many of you are going to want to live in that notebook. With AWS, we have this choice. We have the choice of services, or you can use the integrated IDE of SageMaker Unified Studio. We're also creating something new that we call analytics building blocks. An analytics building block is a capability that can be used across all of our analytics services, and it's only available in AWS analytics.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1250.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1250)

These analytics building blocks disaggregate a core analytics primitive from a specific service, allowing you to use them across all of our different services. Notebook is a great example with its polyglot cells. Quick dashboards is another example of this. This week, we introduced a new cross-service building block: a fully managed materialized view for Iceberg.  It's an incredibly powerful concept for builders of any type of application. You can create these materialized views using the Apache Spark 3.5.6 engine that we launched this week, and these views are then automatically stored in an S3 table with full Iceberg support, and they automatically appear as part of your AWS Glue data catalog as just another table.

The materialized view automatically detects changes and updates your views as new data arrives. It's fully managed with a whole infrastructure and microservice behind it, so there's no manual orchestration needed on your behalf. It runs on our fully managed infrastructure, so you don't have to worry about provisioning compute. These materialized views are a building block in themselves, and when you have them in your Glue catalog as a table, they can then be queried by Athena or SageMaker unified tools like the notebook you just saw, editor for VS Code, Jupyter Lab IDE, or Redshift can query the Iceberg table directly or any other third-party analytics solution as long as it's Iceberg compliant.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1340.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1340)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1360.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1370.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1370)

In addition, our Spark engines will automatically rewrite queries to use these views, giving you performance improvements of up to eight times in performance improvement without any code changes on your behalf. Another example of a cross-service building block is an S3 table.  Like our materialized view, S3 tables can be queried by any Iceberg-compliant analytic solution, and that could be an AWS solution like Athena, EMR, Redshift, or it can be a third-party solution like Snowflake. We iterate pretty fast across these cross-service building blocks. Since the launch of S3 tables at re:Invent a year ago,  we've added over fifteen new features, including this week, intelligent-tiering storage support for tables and cross-region, cross-account replication. 

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1400.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1400)

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1430.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1430)

### Intuit's Challenge: Reducing Time to Discover and Access Data from 20 Days

Customers all over the world in every industry are moving to these open analytics architectures because they want to work with data at scale, and many of our customers who are living on the frontier of data are leading the way. To tell you more about how they've evolved their architectures for humans and AI, I'd like to invite Tristan Baker, distinguished engineer for Intuit, to the stage.  Hello, I'm so excited to be here to talk a little bit about how Intuit uses data and AI to power prosperity for its customers. If you're not familiar, Intuit is built on what we call an AI-driven expert platform.  That platform drives our four core products, which are TurboTax, Credit Karma, QuickBooks, and Mailchimp. Collectively, those products serve our consumers and small and mid-market businesses.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1450.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1450)

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1470.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1470)

I'm going to talk a little bit about how we pull that off, the scale that it operates at, and the data strategy that sits behind all of it. The scale is eighty-six million consumers collectively receiving one hundred five billion dollars in refunds from our tax product every year, managing eleven point four trillion dollars worth of debt with products like Credit Karma.  We also have ten million small and mid-market businesses that manage two trillion dollars worth of invoices and manage the payroll for eighteen million US workers. It's a lot of stuff, and a lot of stuff means a lot of data. A lot of data means you need really good data and data services. You need artificial and human intelligence built on top. 

What that delivers for Intuit is something that we call a system of intelligence. You can contrast that with what Intuit products were a few years ago, which were systems of record. Our customers relied on us to manage data, manage their compliance workflows, file their taxes, close their books. They didn't necessarily rely on us for intelligence. But if we have this trove of data and we add intelligence on top of that, then what you end up having is a system of intelligence that can actually power the prosperity that we envision powering for our customers on our platform.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1530.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1530)

So what do all those customers mean in terms of what's happening under the covers and the scale that's occurring in the data layers? We have 70,000 tax and financial attributes per customer. We're managing 188 million natural language conversations through some of our more recent agentic experiences, and that makes up just a fraction of the 180 petabytes of data that we manage in our data lake. The AWS services that power all this are highlighted here at the bottom.  I'm going to go into more detail and more precision about how exactly those things are arranged in a moment, so stick with me for a few more slides.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1540.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1550.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1550)

First, I want to motivate our strategy with the problem that we were facing maybe 2.5 years ago.  How do you manage a data lake of 320,000 tables? We established a key metric that we like to call time to discover and access data, and when we measured that metric 2.5 years ago, it was something like 20 days, which is like eons in the speed and scale of the businesses that we are running.  So how do you bring that metric down? Well, first, let's understand it. Why is it 20 days?

If you look at this screen here, this is a snapshot of a typical data discovery screen that is part of our internal data discovery experience, and what you see is that a lot of important information is missing. We don't really know what the table contains. We don't know who owns it. We don't know where it came from or where it's going. It's not that this information is not knowable; it's just buried in people's heads or in non-standard wiki pages and tribal knowledge that takes tons of time to unearth. That time is measured in days because it takes about that long for the average team to finally get access to the data that they need and to be productive.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1630.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1630)

### Intuit's Four-Part Data Strategy: Semantics, Infrastructure, Metadata, and Governance

This is what motivates our data strategy. It's a bellwether metric for whether or not things are generally going right or wrong. We've tracked this over time, and I'm going to tell a story about how that has improved. We have a four-part data strategy, and I'm going to go through each part here one at a time. The first part of that four-part strategy is what we call standardizing data semantics. Yes, we have four or five different product lines, but more and more of these product lines need to be able to communicate with each other. Whenever something needs to communicate with something else, you have to have a common language that everyone understands. 

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1650.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1650)

It wasn't until we put a concerted effort into it that we could develop something like a common semantic layer. I'll give you a preview of what that looks like at a very high level. The semantic layer is divided into what we call domains and subdomains. One very popular and important domain is customer.  All of our customers have identity profiles and account profiles that help us identify who they are and where they're from. Some of our customers are consumers, so consumers then have credit, income, and tax profiles. These profiles are the lingua franca of how our TurboTax and Credit Karma products communicate with each other.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1670.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1670)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1690.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1690)

We also have another domain of information that we call business.  Our businesses manage profiles that describe commerce, their accounting, their money, customer relationship management behaviors, as well as lending. Collectively, this produces what we call the semantic layer, and it's what allows our products to communicate with each other. It creates the data glue that allows our products to communicate with each other. It's not just good enough to have this information available; you actually need it available in the right places.  When I say places, I mean data stores optimized for particular query patterns because the transactional query pattern is very different than an analytical query pattern. The query might be about the same semantic information, but the questions being asked are vastly different.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1710.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1710)

This brings us to the second part of the four-part strategy, which is to standardize common data infrastructure and pathways.  It starts with an observation that we want to make the typical thing easy to build and ask people to build typical things. If we can pull that off, then what we can do is create one-click tooling and one-click architectures that allow our development teams to very quickly and easily get not just an application layer that includes your typical app, service, and database layer, but to get it in a way that allows it to produce data that is available to the next typical thing in the picture.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1770.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1770)

We have the typical architecture of an Intuit product. We also have acquisition pipelines because a lot of our customer data is managed by financial institutions that live outside of the Intuit rails, but we still need that information in our environment in order to do useful things. We make it extremely easy for any development team to both source data from an external company or to produce their own application or service. Collectively, these two things produce what we call either product events or third-party data.  Where do those things go? They go to the next typical thing, which is the typical architecture of a data processing system. Those generally come in two different flavors: either streaming for real-time or near real-time processing, or batch pipelines for longer latency, higher workload pipelines.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1800.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1800)

That kind of processing is ultimately what takes the raw data in to produce the insights that we need out to begin to deliver on our promise of being a system of intelligence. One direction of the arrow out of this box is what we call data and insights about customers, and this arrow feeds another typical architecture of what we call our business intelligence systems.  This data flows into another section of our data lake.

Reports and dashboards are easily built on top of this section. Development and data exploration tools, like the screenshot I showed earlier, are also available right on top. This is what feeds our business leaders with the information they need to make strategic decisions about changing product strategy, changing product behavior, and going after new portions of a market.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1840.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1840)

The other thing that comes out of this typical architecture of a data processing system is what we call data and insights for customers. If it's data and insight for a customer, then it has to have a path that gets back  in front of the customer's eyeballs, and that goes up and back to the left. The data insights for customers go up to what we call our customer data cloud, which is where some of those 70,000 attributes I mentioned earlier are managed.

On top of that, we sit into its GenOS, which is a little bit beyond the scope of the conversation today, but suffice to say it's the core engine that drives a lot of our generic and intelligent experiences. When that data flows up through customer data cloud into GenOS, it essentially becomes the data and advice that we promise to deliver to our customers that helps them power their financial prosperity. That's the second part of the strategy.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1880.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1880)

I'll highlight a couple of the important bubbles here, which are the databases, the customer data cloud,  the event bus, and the data lake that essentially make up the three or four different typical data stores where the data and information needs to be available in different ways for different kinds of query patterns. Essentially this makes Intuit a zero ETL architecture, and it's not zero ETL in that data is not moving. Data is definitely moving, but from the perspective of the teams that use our typical architectures, it appears to work like magic.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1920.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1920)

### Intuit's Results: From 20 Days to 9.4 Hours and 80% Data Quality

If you're writing an application or service, the data is immediately available to your downstream analytics team. If your analytics team is producing new insights that we need to feed back into the product, then that's just magically and immediately available back into those products. I also said that I would highlight more  precisely where Intuit services are being leveraged, and this is more or less how they're laid out.

Many of Intuit's managed database systems like Aurora and DynamoDB are driving our transactional workloads for our products. The streaming and batch processing is heavily reliant on EMR, MSK, as well as Lake Formation, Glue data catalog, and S3, using many of those open table formats that were mentioned previously. Our BI tools and development tools were built on the backs of SageMaker, Athena, and QuickSight.

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1960.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1960)

Moving on to the third leg of our strategy is the metadata around the data itself.  Often that metadata is what is needed in order to describe this rich context. If all you knew about a data asset were the three boxes drawn on the screen, you would be left scratching your head with a lot of questions. What does it mean? How did it get there? Who owns it? Those kinds of things are only answerable if you decorate this kind of information with more metadata.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/1980.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=1980)

One of the most important and critical pieces of metadata that we had to collect very early on because we didn't have it, or at least not in the way that we needed, was ownership information.  Who are the teams? What are the business purposes under which they are operating? What projects? What are the roles of the different team members, be they data stewards or data developers? It wasn't until we had that information that we could then say, thank you for telling us you're an owner of something. Now I have 15 more things that I need you to tell me.

Some of those things become how are you organizing your data assets into data products? What is the semantic meaning of these data products, and how is it related to other data products that your other team members might be creating? You start stitching all of this information together and you begin to get the picture of the metadata and the context that Intuit has available.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2020.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2020)

Now that we have, especially the ownership information,  we can get to the last part of our strategy, which is to start coming up with rules and requirements and a way of judging people and the work that they are doing. There's nothing that makes me happier than clear rules and time-bound goals that our organization can chase after. We've developed a clear set of requirements broken up across five important dimensions: stewardship, documentation, data model, data observability, and operational stability.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2030.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2030)

We have clear requirements that allow us to measure the degree to which everybody's data products are meeting these criteria.  If you're doing a very good job, then you end up somewhere on the right-hand side of the spectrum. If you're doing a less than good job, then you're somewhere on the left-hand side of the spectrum. Then I come knocking and asking, what are you going to do for me by the end of this fiscal year?

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2070.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2070)

Now that we have all of this set up, what does that get us?  Well, that gets us a screenshot that looks a little bit more like this one than it did in the previous slide. Instead of seeing no context and no descriptions, we see rich context and rich descriptions. Even though it's blurred out here, there are names of people that own these things, and we can follow up with them if we have more questions.

We can see data quality measures measured over time, giving a sense of the reliability of the data. We also see self-serve access at the top, so we can automate the permissions and the management that allows us to grant teams the ability to touch this data, given that they have the right justification to do so.

2.5 years ago our data was less than 1% clean. We can now claim that it is at least 80% clean with goals of getting even higher by the end of this fiscal year. We took that metric over the course of 2 years ago from 20 years to 20 days to 14.2 hours, and then in the most recent year, we dropped that from 14.2 hours to 9.4 hours. My team has goals to get that even lower this year.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2180.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2180)

### Demo: Building Better Together Products with TurboTax and Credit Karma Integration

That's what this looks like in slides and screenshots, but I wanted to give you a sense of how this comes to life in a demo. So how does Intuit use this to build a better, better together product? There's recently been an effort to bring together more of the customer bases that use both TurboTax and Credit Karma, so that there's more of a seamless experience between these two offerings. I'll pull back up some of the diagrams we saw earlier, and what this looks like in 3 reasonably simple steps is that first, as most people do, you analyze a business question. 

What table and query will show the number of Credit Karma members that are also TurboTax users? Assuming you see a total addressable market and a market opportunity, you then ask your team to go build something. What API will that team probably have a question about? What API will return a Credit Karma member's unified profile, including their tax filing history and refund status? If you can answer those two questions reasonably simply and quickly, then you can reasonably quickly justify the effort to build something and then reasonably quickly build it, and that allows you to just ship something.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2200.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2200)

In this demo, what you'll see is some of the same application screens that we saw earlier,  but now they're in action. So somebody like a business analyst might ask how many Credit Karma members are also TurboTax users. All that rich metadata allows us to run a semantic search that levels that table right to the top of the search result. You'll see the star ratings as I described them earlier for this data set laid out here. You get a sense of where the data is coming from and where it's going because lineage metadata is rich and available.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2240.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2240)

The data quality gives you a sense of how reliable this data has been over time, and all of this rich metadata is powering what we call Intuit Assist, which is Intuit's generative AI analytics assistant.  The same question can be asked of this assistant, and it will simply spit out the answer for you, which you can then take into your notebook experience. I'm going to cut away from the screen before you see the answer because that's proprietary Intuit information. But suffice it to say there was an opportunity there.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2270.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2270)

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2280.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2280)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2290.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2290)

Now you can tell your development team, all right, let's go build something. We take them to the development portal. The development portal has a different API surface area to the data, but the same semantic meaning. So if you ask a slightly different but semantically similar question of our development portal, like what is a Credit Karma member's unified profile,  including their tax filing history and refund status, you get a different answer, an answer delivered in the form of an API rather than a SQL query.  But the developer is able to see of the 70,000 attributes, which are the 5 or 6, or maybe there's a dozen here, which are the most relevant to my question, and what is the GraphQL query that I can then build my application around. 

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2300.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2300)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2310.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2310)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2320.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2320)

That developer is then able to build that application again using some of those application rails and typical architectures that we discussed previously, and now you see what an experience might look like.  A customer of TurboTax finishes their tax return. They're then taken immediately into Credit Karma. We've been able to negotiate many of the compliance and security rules that come  into play when you're trying to transition information between two different products like this, and you're able to see things like federal refund and tax refund, California state refund status in a slightly different application,  but have confidence that it means semantically the same thing that you thought it did when you developed the idea.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2360.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2360)

I'm really proud and excited of the work that we've done, and I see some of my team here in the audience. I'm representing the work of hundreds of people over many years, so I'm really proud of what we've done. If you've learned anything today, I hope you can take some of these ideas back to your own teams and develop them for yourselves. If you've learned nothing else today, metadata is super important when building agentic experiences for your developers and just building more productive development teams. With that, I will finish up and hand it back to Milan. Thank you very much. 

### AWS Investment in Iceberg and Redshift Performance, Plus Snapchat's Use of Supabase

It's pretty amazing what Tristan and the team have done. Let's hear it for Intuit. It's just exciting to see what Intuit and so many customers have been doing with their data strategy, and this level of semantic understanding is going to be increasingly important as people's data lakes get bigger and bigger, and AWS is with you every step of the way. Now, as part of our commitment to these open architectures, we have been investing heavily in Iceberg support across all of our analytic services from the data layer in Amazon S3 Tables to our analytic services like Athena, EMR, and Amazon Redshift. We focus a lot on performance across our services.

I talked a little bit about the performance we achieved with the optimized Spark 3.5.6 engine that's now powering Glue and Athena and EMR, but we've been investing very heavily in Redshift as well. In fact, this year alone, Redshift has launched 37 new features and capabilities, and a couple of them are related to Iceberg performance.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2450.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2450)

We recently improved the Redshift read performance for queries on Iceberg by more than 2 times, and we did that by supporting distributed bloom filters, metadata caching, and optimized query planning. Right here at re:Invent, we are announcing  the capability of Iceberg table append write support in Redshift. It's a big shift for Redshift to be able to write as well as read into these Iceberg tables, and I know many of our Redshift customers are very excited about that.

This move to open formats is something that one of our AWS data partners, Supabase, knows a lot about. Let's hear from our joint customer Snapchat. My name is Derek. I'm a product manager working on Spectacles. Lens developers were constrained to client-site development only, and SnapCloud, powered by Supabase, now allows them to build more sophisticated and complex lenses.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2520.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2520)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2530.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2530)

Over the past five years, lenses have gone through an incredible transformation from their origin, which was lightweight face filters, to what they are on Spectacles today, something much closer to a full application. We were quite attracted to the developer experience of Supabase. It was amazing to see how fast you could get started.  We have come to appreciate what I would call the unique attributes of the PostgreSQL community. When you think about the geo extensions that are possible within PostgreSQL, how do you store data that is tied to physical locations and things like that, the more we learned, the more excited we got about this direction. 

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2540.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2550.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2560.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2560)

The hardest part about building a platform that provides backend as a service is the isolation, being able to support multi-tenants, for example,  the scaling and backing up of data and all these things that you need to build a resilient infrastructure that can grow with your product. All of that is basically handled by the Supabase platform.  One of the features that we released I found to be one of the most exciting ones was support for WebSockets, so that actually allowed developers to connect.  The glasses to other remote destinations in real time, which unlocks a lot of really cool use cases.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2570.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2570)

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2580.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2590.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2590)

Supabase built a PostgreSQL extension that makes it possible to easily query data from Iceberg files.  Since Iceberg is an open data format, we can easily take those Iceberg files and use them with our existing tooling, and all of that runs on AWS. With a medium like the glasses,  you think about the type of product it is. It is an incredibly intimate product that has access to camera and microphones able to sense the world around you. The camera is  the most fundamental building block of an AI experience if you think about it. Having that infrastructure isolated in an AWS instance that provides you all the flexibility was very critical.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2610.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2610)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2630.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2630)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2660.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2660)

### Supabase and the Open Warehouse Architecture: PostgreSQL, Iceberg, and Amazon S3 Tables

We are building the future of connected AR together using AWS scale, speed, and creativity.  Now to tell you more about Supabase and what Supabase is doing with AWS analytics and Iceberg and open formats, let's welcome Paul Copplestone, the CEO of Supabase and co-founder to the stage.  Thank you. One of the most critical decisions that any business can make when they're getting started is choosing the right database. For companies that choose the wrong database, one that can't scale, they face a pretty daunting exercise of migrating to a new database,  and this always happens at the worst possible time when their business is scaling fast.

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2700.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2700)

So choosing a database isn't about what you need today. It's a very forward-thinking decision. You have to think about what you might need in two, three, or even five years down the line. A lot of companies recognize how high stakes this decision is, and they turn to Supabase to help them get it right. I'm Paul, the CEO of Supabase. We are the world's fastest growing post-growth company. We've launched over 10 million databases for all different types of companies, including AI platforms like Lovable, v0 by Vercel,  and Sigma Make. They choose us because we're the most developer-friendly way to get a production-grade database.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2730.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2730)

We've built everything on AWS. After working with over five million developers, I've come to see that there are several common data challenges. The most frequent is choosing which technology to pair with their PostgreSQL database. Almost every developer today chooses PostgreSQL when they're getting started. 

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2760.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2760)

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2770.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2770)

It's the world's most popular database because it's both versatile and robust, but the versatility also has a downside. Developers often fall into something that I call the convenience trap. They start storing analytical data inside their PostgreSQL database because it's the one that they already have. This actually works pretty well for a while. But as we all know,  analytical data grows very fast and PostgreSQL isn't designed for analytical data. I'm sure many of you have faced this situation, so you know how to solve it. We introduced an analytics engine. 

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2810.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2810)

This is absolutely the right thing to do, but it also introduces another set of challenges. We now have two sets of everything: two schemas, two sets of data, two libraries, and often two sets of expertise to manage the different databases. Even harder, the data is now fragmented between two different databases, so getting critical business insights can become a bit of a nuisance. So how do we all solve this in the industry? A third database, of course, and this time in the form of a data warehouse. Then as the data starts to pile up, we have our final challenge, which is costs. 

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2820.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2820)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2850.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2850)

So to solve costs, we use S3, which is good for long-term and affordable data storage.  The craziest thing about this architecture is that every one of these services has their own proprietary data format. So the data ecosystem has redefined redundancy. At Supabase, we don't like complexity, so we set out to reimagine the best data architecture using Amazon S3 Tables. Amazon S3 Tables stores data in a format called Apache Iceberg. 

Apache Iceberg is an industry-standard open table format specifically for analytics and data warehouses. Amazon S3 Tables will become our unified storage layer for data. To build applications, we still need a database that can provide low-latency queries and no surprises. That's PostgreSQL, which delivers millisecond-level response times and is scalable, powering some of the world's largest applications. Supabase keeps these perfectly in sync. When you create a table in PostgreSQL, it's automatically created in Iceberg. When you insert data into PostgreSQL, it appears in Iceberg. When you no longer need the data, you simply delete it out of PostgreSQL, and it's already stored in S3, which is incredibly low cost and resilient.

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2920.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2920)

Amazon S3 Tables is not only low cost, it's extremely accessible, and Apache Iceberg increases this accessibility. Every major data warehouse already works with Iceberg  and all major analytical engines also work with Iceberg. It's become the universal language for analytics and data warehouses. For any CIOs in the audience, this is important for two key reasons. One, it gives freedom of choice to your developers and data engineers. They can choose their favorite data analytics engine to work with their data. Two, it reduces costs because there's no more waste. Analytical data is stored once inside Amazon S3 Tables, and instead of moving data to every warehouse, you can simply connect the engine and query directly from S3.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/2970.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=2970)

This is an emerging pattern that we're seeing across the industry, and  we call it the Open Warehouse Architecture. We believe it's the most efficient and cost-effective way to manage your data. We're launching this today on the Supabase platform, and we're not stopping there. We think data warehousing needs to go beyond traditional analytics. Today at re:Invent, AWS is announcing Amazon S3 Vectors, which adds support for embeddings. We can use embeddings to build AI-native features for our applications, but they also allow us to extract deeper insights from unstructured data.

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3020.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3020)

Vector buckets enable new capabilities beyond this. From today, you can use vector buckets on the Supabase platform, and we plan to make them an integral piece of the open warehouse architecture. 

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3040.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3040)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3050.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3050)

The old architecture is expensive and extremely siloed, with every engine isolated. Apache Iceberg is moving the data world from siloed to synced. With the open warehouse architecture,  everything is connected and there is no more waste. I believe that the future of data is open. 

PostgreSQL is open source and it is the world's most popular database. Apache Iceberg is an open format and has become the universal language for analytics. Amazon S3 has become the fundamental substrate for data and AI. Within six years, Supabase has become the most popular platform for PostgreSQL, and today we are starting our next journey with Iceberg, again on AWS, bringing the same developer friendliness to this new open format.

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3090.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3090)

### S3 Vectors: Enabling Semantic Understanding and Agent Memory at Massive Scale

Supabase is pretty amazing. I have enjoyed the many years that I have been working with them, and they have always been on the frontier of data. It is exciting to see what they are doing with both Iceberg and vectors, and that is in fact where the world is going. 

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3110.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3110)

Vectors give semantic meaning to your data. They mean that you can easily search and use your data no matter how much you have. What I am excited about for vectors is that vectors are the tool that let you understand your data without having to understand what is in your data. It is a tool that we can use now because of the power of these AI embedding models. 

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3150.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3150)

Vectors are emerging as a building block for both data semantic understanding and AI in the form of extended agent memory. Vectors are emerging across both of those domains, and it is equally useful in each. A year or so ago in S3, we saw that vectors were becoming one of our fastest growing storage types, so we launched our S3 Vectors preview this past summer. 

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3180.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3180)

In only a few months of preview availability, customers have run over one billion queries.  I have worked on S3 for a while, and I have to say this type of super rapid adoption of a new feature in preview is incredibly fast, even for S3. Here is what we are doing with vectors. Just like we introduced S3 at a cost and economics that made it possible to add any type of data into a data lake, we are doing the same thing with vectors. We are making vectors cost effective.

When we do that, customers of data and AI, the application builders, are going to use vectors in every aspect of their application. We are going to do with vectors what we did with data and data lakes. Customers can now treat this new dataset of AI and data understanding, semantic understanding, like any other data in their S3 data lake. They do not have to worry about security, durability, and availability, and you can let your vector dataset grow at S3 scale because of the cost and economics of vectors in S3.

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3270.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3270)

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3290.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3290)

Vector APIs are now just part of S3, and what that means is that it is simple to use vectors. Like objects and tables in S3, S3 Vectors are usable at even the smallest scale indexes  all the way up to billions of vectors, meaning that you can focus on building your applications and not thinking about how to size and configure a vector database. This morning, Matt Garman announced the general availability of S3 Vectors, and with it some pretty impressive changes since our July preview. 

We have increased the max capacity of vectors stored per index to two billion. That is forty times the preview capability. You get ten thousand indexes per bucket, which means that one vector bucket can store up to twenty trillion vectors.

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3350.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3350)

As S3 users know, S3 is excellent at massive throughput, and you get the same with vectors. You can stream 1,000 vectors per second, and all of this is delivered at 100 milliseconds or less latency for warm queries. This is a game changer and will fundamentally change how people think about semantic understanding of their data and other things like agent memory. We're already seeing tremendous diversity in how customers take advantage of our new vector storage. 

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3370.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3370)

The use cases we've observed combine two key things: semantic understanding of metadata and semantic understanding of media, as well as the ability to extend agent memory.  When you extend agent memory, you add more context and vectors. As you add more context and vectors, chatbots become smarter and more human because you can vectorize context about the human interacting with the chatbot. You can vectorize context about the account information of the human that the chatbot is interacting with, and all of that goes into vector storage to add, create, and personalize responses. You can see where this can really take off when you have virtually unlimited vector storage.

[![Thumbnail 3440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3440.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3440)

One of the fastest growing new types is hybrid search, and vector storage in S3 has integration with both Amazon Bedrock Knowledge Bases and OpenSearch. Hybrid search has really taken off with OpenSearch. This year we focused on OpenSearch and added many AI capabilities, such as agentic search that automatically converts natural language to precise queries. We've also added automatic semantic enrichment and, as of this year, GPU acceleration and auto-optimization for vector indexing. 

[![Thumbnail 3460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3460.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3460)

[![Thumbnail 3500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3500.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3500)

With GPU acceleration, you can now build billion-scale vector databases much faster. You can do it in under one hour and index vectors up to 10 times faster at 25 percent of the cost.  This is incredibly helpful for customers building multi-billion scale vector indices. GPU acceleration is a game changer for dynamic AI applications that have heavy writes. Because OpenSearch Serverless gives you GPU acceleration that's completely serverless, you don't have to manage any GPU instances and you only pay for what you use. 

[![Thumbnail 3520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3520.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3520)

When write volume drops, the GPUs automatically scale down and return to the pool. This means you're only paying for acceleration time, which is monitored through CloudWatch. It's fully managed, fully automated, and paid as you use. We also worked on simplifying how you find the optimal configuration for your specific use case so you can balance between cost and performance.  The difference between an optimal configuration for a vector database and one that just works can be 10 percent in search quality, hundreds of milliseconds in latency, or actually 3 times the cost. Getting the right configuration matters a lot.

[![Thumbnail 3560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3560.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3560)

We built auto-optimization to address this. You simply specify what you think your acceptable search latency and quality requirements are, and there's no need for expertise in tuning algorithms or quantization.  It's also integrated with vector ingestion pipelines, so you can build optimized indexes directly from Amazon S3 data sources. You pay a flat rate per job, and it's simple, fast, and fully automated. This world of vectors is evolving rapidly as a building block, and with AWS Vectors and OpenSearch combined, we're here to help you steer into this rapidly emerging trend for semantic understanding.

[![Thumbnail 3640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/729eb5fa5da43171/3640.jpg)](https://www.youtube.com/watch?v=L_Q7LPB5HcA&t=3640)

I believe that in the upcoming 12 to 18 months, anybody who works with data will want to build a semantic layer on top of their dataset so they can understand what's in their data. This combination of vectors and AI embedding models, along with vector databases like OpenSearch, will make it very fast for your high-scale, high-speed workloads. These capabilities are going to be critical for you to steer into this rapidly emerging trend of semantic understanding. It's a super interesting time to be working at this intersection of data and AI.  Here at AWS, we are deeply committed to helping you build not just what you need today, but a data foundation that you can evolve easily into all of these emerging trends.

Speaking on behalf of all of our teams in data and analytics, I want to say a big thank you to everyone in this room who uses our services to make data the backbone of your business. We are inspired by what you do and your commitment to your customers, and we're here to build what you need now and in the future. Thank you, everybody, and have a great re:Invent.


----

; This article is entirely auto-generated using Amazon Bedrock.
