---
title: 'AWS re:Invent 2025 - Build production AI agents with the Strands Agents SDK for TypeScript (AIM3331)'
published: true
description: 'In this video, Ryan and Nick from Amazon''s Strands Agents team introduce the TypeScript preview release of Strands and demonstrate their model-driven approach to building agents. They explain how Strands enables developers to create functional agents in just a few lines of code by letting powerful reasoning models determine their own workflows, rather than requiring developers to define deterministic steps. The presentation covers key features including MCP server integration, agent SOPs (Standard Operating Procedures) for steering model behavior, hooks for lifecycle control, and the new Steering feature for modular prompting. Nick shares how the team built the TypeScript SDK using Strands agents integrated with GitHub Actions, achieving remarkable efficiency where their agent became the largest code contributor with over 50,000 lines. They demonstrate the Refiner and Implementer agents working through GitHub issues and pull requests, following test-driven development to autonomously write, debug, and refine code based on team feedback.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/0.jpg'
series: ''
canonical_url: null
id: 3093245
date: '2025-12-08T21:45:25Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Build production AI agents with the Strands Agents SDK for TypeScript (AIM3331)**

> In this video, Ryan and Nick from Amazon's Strands Agents team introduce the TypeScript preview release of Strands and demonstrate their model-driven approach to building agents. They explain how Strands enables developers to create functional agents in just a few lines of code by letting powerful reasoning models determine their own workflows, rather than requiring developers to define deterministic steps. The presentation covers key features including MCP server integration, agent SOPs (Standard Operating Procedures) for steering model behavior, hooks for lifecycle control, and the new Steering feature for modular prompting. Nick shares how the team built the TypeScript SDK using Strands agents integrated with GitHub Actions, achieving remarkable efficiency where their agent became the largest code contributor with over 50,000 lines. They demonstrate the Refiner and Implementer agents working through GitHub issues and pull requests, following test-driven development to autonomously write, debug, and refine code based on team feedback.

{% youtube https://www.youtube.com/watch?v=NzZOm-kaO94 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/0.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=0)

### Introduction to Strands Agents and the TypeScript Release

 Good morning everybody, and the last thing between you and lunch, I suspect. I'm Ryan, I'm a Principal Product Manager at Amazon. I'm joined by my colleague Nick, who's going to come up here about halfway through. I've been really looking forward to talking to you all week. I lost my voice yesterday though, so please bear with me if you hear my voice break or I stop for some water.

Nick and I are both from the Strands Agents team. We've been really looking forward to talking to you about Strands at this conference. Many of you maybe have seen our booth, and today we're here to talk about our newest release, TypeScript, some of the other releases around it. I'm going to spend some time talking about this simple interface for building an agent, and I'm going to talk about what's behind that. What does it mean to build an agent in a few lines of code?

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/50.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=50)

 This code sample actually will run if you have installed the Strands and the Strands Tools packages from PyPI. If you have the default model provider of Bedrock, you can run this and run the agent loop. So I'm going to walk you through how that works, why it works, and then of course, we're going to talk a bit about the TypeScript release.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/70.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=70)

 Nick's going to come up and talk about how we've built it. So we'll follow through here, Strands, the TypeScript release, the model-driven approach to an agent loop, and so on. First, I just want to get this out of the way since many of you are coming here to join us now that we're releasing as of yesterday into TypeScript.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/90.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=90)

 Very excited about this. It's a preview release, meaning 0.1 on GitHub and NPM. Same style of Strands though. We've worked really hard to bring everything we've had since May when we released the Python SDK into TypeScript. That's the same simple interface, a few lines of code, run an agent loop. There are some limitations though in this preview release as we get to a 1.0 that is on parity with Python. So just note, this is about building single agent systems today. We haven't added in our multi-agent patterns. Some other functionality might be missing. We're going to fast follow here in a few weeks with things like OpenTelemetry, but today you can use our basic features like Bedrock and OpenAI models.

You can use the async and non-async with streaming. You can do full agent state and conversation management for multi-turn agents, of course, and hooks to inspect and change that lifecycle if you've started to play with Strands. Hooks are a powerful feature for both observing and changing behavior at different points of the lifecycle.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/160.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=160)

### Understanding Model-Driven Agents and the Agentic Loop

 So let me step back and talk about for both of these languages, for Python and for TypeScript, what does it mean to have a model-driven agent? I'm sure some of you are building agents right now. This is old news to you, but for those of you who haven't yet gotten into this, it's worth pausing on because I think a lot of folks think about agents as what if you just had LLM calls and you've given them internal external tools like with MCP. That's not quite agentic. It's a very broad term. I think the industry is still defining it.

Our definition here is that agents run tools in a loop to solve a goal. The design behind Strands that I'll talk through as we go is all about enabling that loop. So you can think about Strands as governing that agentic loop up top, where the input that you give that loop is the prompt. That's both system prompt and of course what your users are prompting your application with. What you're getting back is the end result.

In the loop, you have a powerful reasoning model that's equipped with tools and the goal you've assigned it through the system prompt. It's going through its own reasoning to figure out how to use those tools, whether the information it's gotten back is sufficient for solving that goal. It might run more tools if it needs to, and it completes that cycle as many times as necessary to return a satisfying response to the user. Throughout this lifecycle, we're going to talk through how you can control it, how you can add in ways to steer the model, ways to constrain the model. But that in a nutshell is it. You'll get really far, really fast by just embracing this architecture where you let the model come up with its own workflow. I'm going to share more about that.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/270.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=270)

 But first to kind of contextualize what's inside that agent loop a bit more. First of all, it's a model. That's kind of the number one thing that you need, and specifically a reasoning model that can use tools. These days, that's many, many choices. It hasn't always been that case. So think, of course, the big AI labs, you can use Gemini, OpenAI, Anthropic models. You can use the latest reasoning model from Nova. Many of these models are available on Bedrock. But Strands is designed to work with any model provider. So you can go directly to OpenAI or Anthropic's first party APIs as an example. Many more providers have that same interface.

You can also bring your own custom gateway. For instance, there's LiteLLM support integrated with Strands in the Python library today. We'll be adding that over time into the TypeScript interface. Many customers will bring their own gateway as well. It doesn't have to be LiteLLM. There's a whole abstraction that we call the custom model provider, where in Python or TypeScript, you can just define how to interact with that gateway product or another model provider that comes on the market. Many open source contributions to Strands have been model providers by users of those models or the vendors themselves, adding support to use Strands with those models.

Finally, you could think of that top layer as the reasoning model choice, but it doesn't have to just be a reasoning model. For example, a very common pattern is, let's say you pick Claude 3.5 Sonnet as your reasoning model for the agent loop. You can assign as a tool, say, a Stability image generating model. That's one of the tools available in our community package contributed by the Stability team. That reasoning model can then use other models that aren't reasoning models but are designed for other purposes like generating images. You can mix and match within the same agent that way. You can call out to many models within the agent.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/400.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=400)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/410.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=410)

### Equipping Agents with Tools: MCP, Functions, and Agent-as-Tools Pattern

For equipping the agent outside, we just embrace open standards. I'll tell you a little bit more about our design principles in a moment. But in a nutshell, you could bring any MCP server, you can use A2A for agent communication. All  the traces and trajectories can go off to your favorite Open Telemetry provider through OTEL. Because tools are so important to the agent loop, I just want to pause on what are the choices. 

Obviously there's MCP. You can use any MCP server with Strands as an MCP client, all the different authentication types. We continue to evolve our client implementation as the MCP spec does as well. In both Python and in TypeScript, you can bring your own function and use that as a tool. So if you've already got software or you just want to pull in some library that you want to use and expose that as a tool to your agent, you can do that really simply.

We're providing in the Python package a whole bunch of built-in tools. TypeScript has just a few of them, and we'll be expanding those over time as well. But those are a similar deal. It's as if you've built your own tool as a function, and now you can just install it via a package and import it. I pause on that agent-as-tools pattern because I think it's often overlooked as a way to build an agent. You may have heard of this as the supervisor pattern in orchestration contexts.

Think of this as you're building Strands with this big powerful reasoning model, and you may say, okay, I want to build an agent that's really good at interacting with some backend system. For example, I've had a customer who uses the MCP server from Atlassian for Jira. They wanted to specialize a whole agent on understanding how to interact with their Jira system, and they'll use features in Strands like hooks to parse out the tool calls, because a Jira story might be so large that it overwhelms the context window. That agent's whole job is to understand what's interesting about this story that I want to return. They've encapsulated that all as this sub-agent. They exposed that to that higher level orchestrator, the supervisor agent, as a tool. You get some modularity in that approach, and then you don't have to always use a deterministic tool, is my point. An agent itself can be a tool.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/530.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=530)

### Design Tenets: Building Strands with Simplicity and Extensibility

I'm going to start telling you a little bit of a story of why we built Strands and the history behind that,  but to set the stage for that, these are just the snapshot of contributing guidelines you can find in our repos, specifically the tenets that go along with how to contribute to Strands. I've summarized them here on the slide. They're all available in the GitHub repo, but if you're unfamiliar with Amazon's processes, we're really big on tenets. These are ways of defining how we make decisions. They're meant to add tension with one another so that when we as a team or as an open source community contributing to Strands are making decisions, we can use them as a guide to help us decide about trade-offs. They also, I think, tell the story of how we tried to build this software for you.

One is just simple, and really importantly, simple for the same abstractions from your laptop prototyping all the way to running it in production. In other words, getting that simple agent loop running should be very simple. Integrating telemetry and logging later should also be just as simple. We've talked a little bit about extensions already. Model providers are a good example of extensions, tools are another good example of extensions, but this applies to everything. Hooks are a mechanism to basically take over control at a specific lifecycle point, before and after tool calls, before and after model calls, that kind of stuff.

And then you can plug in your own hook. We provide some as well. For instance, there's a built-in hook to help you capture retries from Bedrock models for transient failures. So we take on those kinds of extensibilities and apply that everywhere we can. And then they're of course all meant to work together. They're all meant to lead you down the right path. As you start layering on more functionality, the defaults built into our abstractions are meant to give you a good time, and you can customize a lot of this.

And accessible to humans and agents is kind of an emerging one. A good tangible example of this, if you go to StrandsAgents.com, you can actually add slash latest slash llms.txt and get this format of markdown table of contents. Basically this llms.txt or llmstxt.org, I think is the URL for this standard. And it's a way of exposing to coding assistance or model training documentation that is an HTML to markdown twin. So as a human, you're reading an HTML document on our website, we expose markdown for models, and we make that available over a standard protocol that models know or systems around models know how to find. We've talked about common standards already. That's just as things emerge, we'll be adding those in. We already support the major ones today.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/690.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=690)



### The Origin Story: From Amazon Q to Open Source Strands

So back to the storytelling then, I mentioned that Nick and I are from the Strands team. Before that we were part of the Q and what is now known as Quiro teams at Amazon, and specifically we were working on a project to help enable other Amazon teams to extend Q. You can imagine that it's not just one team building these super agents at Amazon. It's many teams collaborating together, sharing their disparate expertise. And this is, what is it Nick, over one year ago now, almost two. It's a long time ago we were doing this, so this is at the very early days of agentic.

And we were observing many teams either building their own solutions around the Converse API or other APIs. They're just doing LLM calls in a loop around some workflow they built, or they were using off-the-shelf frameworks that were simpler than that but still weren't really agentic. They certainly didn't have the agentic loop that we're talking about today. They really required you to spend a lot of time in deterministic workflow. That meant that any engineer who had a good idea, any product team that wanted to build something, they were spending months getting something off the ground, so from idea to prototype and longer getting it into production.

And so what we focused on is, of course, because our demand was to onboard dozens of teams to bring agentic functionality into one sort of customer experience, we started working on another way. And that's what led to these results, for example, from the Quiro team for these experiences where they could go from concept to production in weeks, and that was our driving goal. We kept iterating on this internal framework to achieve those goals, adding the right functionality, figuring out the right developer experience, and resulted in helping these teams and other teams across Amazon move really, really fast.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/800.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=800)

In May of this year, we brought it into the public.  That was the initial preview release of Strands. We've had notable names giving us very generous mentions on social media. We've been very lucky to have over five million downloads, as Swami mentioned yesterday, five million downloads of the Python SDK on PyPI. Obviously we're just getting started with TypeScript and we're hoping it helps all of you as well. And I've been meeting with a lot of you. Part of why I lost my voice, talking about Strands and the agents you're building. And so I'm so thrilled to start seeing the signal from our initial release in May through now that you all are building much faster because we made Strands available to you and seeing the gains that we were able to build for customers inside the building.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/850.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=850)

### Model-Driven vs. Workflow-Driven Approaches: A Paradigm Shift

So let me come back then to the model-driven approach in this architecture.  So just to summarize, the point here is that you as a developer are thinking about very few things. You're thinking about the goal you want the agent to do, system prompt, and we'll talk about some techniques in a moment for being more fine-grained there. You're thinking about the tools to equip that agent with, which can themselves be agents and give you this modularity where you can break down complexity that way. And you're plugging this into an application, prompting it, getting a result. That's the agent loop.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/890.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=890)

And I wanted to differentiate that from this sort of  very big spectrum in the market today on choices either from do-it-yourself or from frameworks that you can get from any open source provider. It's the shift between workflow-driven and model-driven, is how I coined it.

The difference here is who is defining the steps that need to be taken to break down that goal into a plan, to make tool calls, analyze results of those tool calls, decide whether more are necessary, and iterate on that loop until a result is given back to the customer. Who does that work? What we grew up with on the Q team was all these development teams doing that work, describing these workflows using the abstractions given to them by these open source frameworks or that they were building as their own homegrown systems. They were doing all that work, and what that leads to is a pretty brittle system that also took a lot of time to build. Developers had to anticipate new scenarios, maybe a new category of customer question in a chat experience. More workflow needed to be authored and maintained to meet that use case, and if it broke, you had to troubleshoot these different steps.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/980.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=980)

The model-driven approach, and it's not just Strands' approach to be fair, there's a lot of frameworks that came out this year alongside Strands that help you with that approach, pushes all that work into the LLM and then shifts where you spend your energy steering that LLM into different functions.  So let me walk you through a practical example in case this isn't clicking. Think of early ChatGPT. You come into a chat experience and you had a pretty powerful thing. If I was in this scenario, I'm trying to debug why I can't connect to a remote instance in EC2, let's say. I had a pretty good experience going into ChatGPT and asking a question about how to do this, and I got a natural language set of questions or basically a tutorial. It might have been better than reading through different docs or reading through different blogs. I got a much nicer response, but it wasn't personalized at all. I think this is all old hat, you're all kind of getting there. This was the pre-agentic era. I asked the question, I got steps back, maybe those helped me, maybe they didn't.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1030.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1030)

So when agentic, and again defining agentic here is using tools with an agent,  letting it figure out its own work, you could still go down this path where, let's say in a Python framework, you're describing the steps of the LLM calls and you're breaking your agent down into specific calls. You might then anticipate, okay, the first step, let's check network connectivity, and you're making a direct call to the tool that handles VPC APIs or security groups or other things like that, and you're iterating down that workflow until you get a response. That works great. No harm done if you have an agent that works like this. The challenge is you had to do a lot of upfront work to describe those steps. You have to then parse the tool results and figure out, do I need to move on or not? Maybe you found yourself in a hybrid mode where you're letting the model figure out whether to move on or not, but you're still describing what the next step is.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1100.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1100)

If you decided, oh, I want this experience that I'm building here to support container connectivity or maybe an on-premise compute solution, not easy to do. You have to then not just add new tools, you have to add new workflow steps and you have to incorporate it into the flow you've built. That's the sort of gist of a workflow-driven  agent. Conceptually, the model-driven agent is much simpler. Just give it the goal, give it the tools. That's it. I saw that from my initial example of the International Space Station. We had the example of weather in Las Vegas. Those are examples of very simple agents, mind you, but they work and they work because all you're doing is setting this up using an orchestration framework like Strands.

Now, it does sound too simple to be true, and to some extent it is. You can get a prototype going really fast, but the number one question I'll get when I meet with you in a customer meeting is, okay, but I want to give it better instructions. I want more predictability out of the workflow. Maybe I want it to go faster, maybe be more cost efficient in terms of token usage. Maybe there are some dangerous outcomes that the model might take. I really want to make sure it can't create those dangerous outcomes. How do I steer it? How do I constrain it?

So my story here is think about this as a spectrum and think about where you're shifting your work and how big are those chunks of work. If you're thinking in a workflow-driven agent, you're doing a lot of work upfront to define the workflow, then you're iterating on that workflow, and your controls are intermingled with the workflow. Your guidance and steering and prompting, your tool use, your parsing of tool results, that's all intermingled and it's all monolithic. A model-driven approach says forget all of that, start with just giving the model free reign to figure out its own workflow, and then as situations require it, layer on other controls. You can do that at finer granularity for specific situations. So let me try to break that down.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1220.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1220)

### The Spectrum of Control: From Pure Autonomy to Constraints

The left side of the spectrum is pure autonomy. You give the model its goal, you give it tools, and it does its job. Your tools in this situation are really your system prompt. Try to make it really good.  My advice is always make it goal-oriented. Models love achieving a goal. Tell it what success looks like, and it'll probably do a pretty good job of finding success for you.

Research assistants are a good example of this, right? Models have all the knowledge they need. You can give them external sources, you can tell them what format you want, and they'll do a pretty good job. And then you can layer in things in Strands like structured output. There's a mechanism where you say, hey, I want to force maybe a markdown format you want, or maybe you want JSON to come out so you can fold it into a front-end application. That's something you can layer in at the left end of the spectrum that doesn't require you to think about the workflow and where that output is coming from. You're just saying, hey, at the end, my abstraction framework Strands is forcing structured output, and it's working with the model to regenerate until it matches my schema. That's all taken care of for me, and now I have a model-driven approach that returns the output I want.

Okay, but what if you need a little more guidance? So when we talk about steering and SOPs, maybe you've seen last week we launched a new repository on Strands called SOPs. If you haven't, check that out. I'll have a link at the end of the session. This is really just a technique for prompting, for system prompting in particular, that we've developed inside Amazon. There's a very large community outside the Strands team that have been working on this technique, and then we've decided to deliver it through Strands, and it's very helpful in this use case. Nick's going to walk you through it in detail, but that's an approach of just saying, okay, make your prompt more structured, give it a specific set of steps in the prompt that you suggest the agent take, and you can use keywords like they must do this, they should do that, and the model does a great job both following your guidance and adapting, right?

And that's a big part because it may sound like you're back in the workflow-driven approach again, but what if a remote API failed? Your workflow would have to account for that if you're just making LLM calls, or the model could figure out how to do a retry or another data source it can call upon while still achieving the steps you described in your prompt. Then on the far side, you're thinking about how to apply constraints to a model, and that's where you're really trying to protect around sensitive outcomes. Hooks are an example of this in Strands itself. So you can inspect after a tool call, make sure the results maybe passed through a PII guardrail, maybe you're parsing for certain information. That can all happen as part of Strands hooks where you get to basically eject from the agent loop, do some work, and hand back to the agent loop.

Another example of this is external to Strands, the policy engine feature that's shipped as part of AgentCore's gateway product. So this is like a hosted MCP server situation where you can have an MCP endpoint for remote targets. Those targets can be APIs, they can be Lambda functions. You can now assert deterministic policies to those gateways and, in other words, protect the data sources before a tool is called and also externalize that logic from the agent, where the agent can be treated as untrustworthy. And the gateway is providing you deterministic controls for, let's say, can an agent call this particular API for refunds before the customer was deemed eligible and you have a token that specifies they are, right? Like, don't let the model try to figure out how to game the system. You can actually block the model from making any tool calls until it's providing the proof that you need it to provide.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1460.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1460)

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1470.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1470)

### New Features: SOPs, Steering, and Evaluations Library

So think along that spectrum. That's the long story short here. There's controls along the way. We'll walk you through that on our website if you start building agents on StrandsAgents.com. And I'm going to transition to telling you just a couple of those examples that we launched in the last few weeks. So I mentioned SOPs. Nick's going to go into depth on these, but just to reiterate, it's natural language instructions.  They fit in, whether it's via Python import in this case or an MCP server, which you can use with TypeScript today, and the format looks like this.  Upper half, this is not one file, upper half is an example of an SOP, bottom half is an example in the Python SDK of how you load the SOP.

Look specifically to these steps. Obviously they are truncated, but you can give it these headings of step one is a setup step, step two is this, and then within that these keywords, these RFC 2119 keywords. These are just standard keywords that of course models are exposed to in training data. The SOP mechanism is really kind of exploiting that knowledge that a model has, and it follows these keywords quite well.

This has been something we've been experimenting with for a long time inside Amazon, and you really just follow this. You give it the steps, you give it the constraint keywords within those steps, and the model's going to do a good job.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1520.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1520)

We're also trying to figure out, okay,  if that works, you're off to a good path, but you're going to end up with a large prompt. So what we're starting to experiment with, and this shipped this week as an experimental feature in the Python SDK, is what I think of as modular prompting. We call it Steering, which is the name of the feature. It's a way of breaking down the critical bits of instructions and then injecting them in those lifecycle hooks I keep talking about. The mechanism is really one of those extensible engines in Strands where you can say, use an LLM as judge or a deterministic Python library. You could bring in Cedar if you wanted to and use those external to the agent to judge the trajectories that the agent's taking.

So it looks at their traces, and you can pass in basically the whole agent state. Examine it against, let's say your LLM as judge prompt is, is the tone and voice of this response from the model appropriate for these brand guidelines? And then you give it a response of like yes, no, and the feedback. So Strands will basically pause at the agent loop, interpret those instructions, and then if the agent is compliant with those instructions, it just returns control of the agent. And if it's not compliant, it returns feedback to the agent as part of return of control, and the agent absorbs that feedback, tries again, and goes back through the hook.

So that's the way to think of Steering, and I think it's going to be a pretty interesting place for us to go. If it works out, it's going to give you fewer instructions to give the model upfront, so bigger context windows to work with, less chances of the model getting too much steering data upfront, and more token efficient over time. And then we're also experimenting with plugging this into episodic memory systems so that that feedback loop can happen once for a particular agent's lifecycle, and it just gets better over time at following those instructions without needing that external hook.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1650.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1650)

 Another launch that we made this week is an evaluations library, and this is a little tangentially related to my talk track of controlling your agents, but I think it's useful because you need to evaluate your agents at some point in their lifecycle once you get past the prototyping stage. And we've had this gap in Strands for a while, not giving you the libraries you need to quickly take your agent and build evaluators against them. We're also adding in features with a Strands-based agent that will help you generate synthetic datasets that are relevant to your use case and then build your evaluators around that dataset.

You can come in with custom evaluators. There's some built-ins, and this is designed to work with an online evaluation system of your choice, including AgentCore evaluation system, which was launched this week, or others that you might be using. And really just pair that build time story on your laptop, something that's really native to the SDK experience, with those online systems where you're doing large scale evaluations.

### Building TypeScript with Agents: Agent SOPs for Better Code

Okay, so we're finally done talking to me. I'm teeing up the Strands TypeScript. We launched that this week, available in preview, and it was a bit non-traditional in a couple of ways that I'm excited for Nick to talk to you about. First, we built it with Strands, but it wasn't vibe coded. It was actually quite nuanced, and it was a collaborative experience as well between the Strands team and agents. And so for that, I want to bring Nick up and talk about how we built it and how some of these features helped.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1760.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1760)

Thanks, Ryan. Hi everybody. Thanks for coming today. My name is Nick. I'm a Senior Software Engineer at AWS, and I was the technical lead for building and developing the TypeScript SDK. Like Ryan said,  as a part of this presentation, I want to talk about how we actually went about building out this new feature, this new language for Strands. When we decided to support a new language in Strands, we wanted to be intentional about how we went about building out the language. We wanted to reflect on our experience building out the Python SDK and specifically not necessarily just focus on the easy developer experience that Ryan has explained, writing an agent in a few lines of code, but we really wanted to focus on the developer's experience writing that code. We spent a lot of time developing and writing code for Python, so we wanted to reflect on that experience and see how we could improve it.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1810.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1810)

Strands for Python was originally written by a group of dedicated engineers in Amazon,  who built the Python SDK as a prototype originally. They used a lot of these early Gen AI tools to bootstrap their development, to help them write code faster to get this up off the ground and have a working agent loop.

We spent a lot of time with these early AI-generated coding tools to write all of the code for us, but the problem was a lot of it was vibe coding. It wasn't very high-quality code that came out of using these tools. I spent a lot of time, we spent a lot of time as engineers reviewing, understanding, guiding, and steering these agents to get them to write code that we were comfortable shipping and bringing out to production for everybody else to use. I recently went back and analyzed the initial release of Strands for Python, and it was about 15,000 lines of code.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1880.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1880)

Coming into TypeScript, we were going to be building a codebase of a similar size. So we wanted to be intentional about how we actually helped our developers write all of this code. Specifically, we wanted to figure out how we can use agents to write code better and faster.  And I break it into these two pieces because we came up with interesting solutions on how to do each of those.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1890.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1890)

For writing code better,  we looked at Agent SOPs. Ryan introduced this earlier, and to give a high-level overview, it's a specification for writing agent instructions. It's literally a standard operating procedure that an agent follows in order to accomplish some tasks. The great thing that we found using Agent SOPs is that they're great at steering agents for somewhat repeatable behavior.

SOPs were initially thought up by one of our principal engineers, James Hood, who was similarly frustrated with these early AI-generated coding tools. He would spend a lot of time trying to get good code written out of them and spend a lot of time debugging and trying to get good results. Out of that frustration, he came up with this idea of Agent SOPs, and he shared it with the internal Amazon builder community. It was super, super successful. The last time that I checked, we had over 5,000 of these authored internally, used all over the place. Internal Amazonians love this idea. Like Ryan said, we wanted to release it as a part of Strands because we wanted to share it. It has been so successful to help engineers guide and steer agents.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/1970.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=1970)

Here's an example of the SOP again. You saw it earlier with Ryan, but I want to go a little bit more in depth in how it works.  You have the overview, you have the parameters, and you have the list of steps that the agent is going to follow in order to complete its task. Like I said, it's really easy to understand why this gives an agent repeatable behavior. It's really obvious what the agent is doing when it's following these steps to accomplish some goal.

As a developer who's trying to write these agents to accomplish some tasks, this means that this SOP is debuggable to me. If an agent is going through these steps, and at step three, it goes off and does something unexpected, as a developer, I know that I need to go in at step three and give it more instruction. We've worked on a couple of teams at Amazon where we were writing the system prompts for these agents, and developers were worried or afraid that if they messed with the authoring or the writing or the wording of the system prompt, it would make the agent do unexpected things.

But using SOPs, it makes it obvious and clear. It makes it much easier for the developer to go in and update this and have confidence that their changes won't break the system. I'm going to be showing a video a little bit later where we've actually done this in our development of our agents for TypeScript.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2050.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2050)

Like I said, this idea of Agent SOPs was very successful in the internal Amazon builder community. Some of the ones that we released are prompt-driven development and codesist.  These were two of the most popular SOPs because they figured out ways to guide agents in really impressive ways. Prompt-driven development is an SOP that has an agent guide you through a question and answer process to refine an idea into an implementable state.

The agent will ask you questions to disambiguate a problem. If it doesn't understand a certain aspect of what you're asking it, it'll do research using its tools, or if it's in a codebase, it'll explore the codebase, and it'll take the progress as it goes along, so you know what it's thought and done along the way. The really good thing about this is what we found in the early Gen AI coding tools is the agent likes to take the shortest path possible to solving a problem.

The shortest path possible isn't necessarily the one that I want it to take. Sometimes I want to guide it to write code in a way that my team is familiar with or use some functions that I've written that it may not be aware of. So having this back and forth question and answer process with an agent helps me disambiguate the problem.

It makes it very clear to the agent what it needs to do and aligns with what I want it to do. The output of this SOP is an implementable task. It's a markdown file for how to implement a feature in a codebase.

We developed another agent SOP which is Code Assist, which takes this task and actually implements it. So it follows the test-driven development methodology for writing code where you write the unit tests first, and then after that, you write the application code. We found that guiding agents to write code like this writes much better code. After it's written the features, I can go in, take a look at all of the code that it's written, and then give it feedback on how to update and fix it to better align with what I want. After a couple of iterations, when I say it's done, the agent can go ahead and commit this code.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2170.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2170)

### GitHub as a Team Member: The Refiner and Implementer Agents in Action

So using these two agent SOPs together, we've essentially solved that problem of how we can get agents to write code better. The second part of the problem, which is how we can get agents to write code faster, we solved in another interesting way.  Developers on my team typically develop and write software in a way like this, where on one side, you have the team of developers who are communicating with the individual developer through GitHub, and the individual developer might be creating pull requests or issues to track the progress of the code that they're writing. They're going to create and update pull requests and they're going to coordinate the feedback that they get from their team to their local IDE, maybe they're running Cursor, CLI, Claude Code, and they're doing that communication.

They're a middleman in this process for informing the agent on what feedback needs to be done in order to incorporate and get this change approved by the team. But we're spending a lot of time doing that, staging the local repository or staging your local codebase, understanding, reflecting and responding to the feedback, writing all of the test cases for the code as well. So instead, we wanted to shift this and have GitHub become a member of the team.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2230.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2230)

 We've taken advantage of GitHub's action feature, a GitHub workflow, which is triggered off of certain events on pull requests and issues to trigger a Strands agent. Like Ryan said, we have Strands writing Strands code. We have our GitHub workflows triggering a Python Strands agent to contribute code to our TypeScript repository. The Python code is now writing code for us for TypeScript. Using this system, we're essentially adding a new member to our team through GitHub.

The agent is acting as another contributor on our team, and I no longer have to be the middleman, understanding my team's feedback and passing it to the agent. Instead, the agent is doing that itself. It's reading through the GitHub issues, GitHub pull requests to do that coordination, so I don't need to. And this is how we solved the writing code faster. I'm not being the middleman anymore. The agent is doing that for me.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2290.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2290)

So using this new paradigm of an agent in GitHub, we had to rework the SOPs that I mentioned earlier, Prompt-Driven Development and Code Assist.  What we came up with are these two agents, which I'm calling the Refiner and the Implementer. And there are slight variations of the Prompt-Driven Development and Code Assist SOPs tuned towards GitHub. So the way that the Refiner works is given an issue with a general feature that I want to implement in the codebase, it will read that issue, it'll have the repository checked out and it can explore that repository to understand the implications of implementing that feature in the codebase.

It'll then come up with a list of questions and then post that as a comment on the issue for me to go in, review, update, and give it instruction on what to do next. We repeat that process of question and answer through comments on the issue until the agent determines that it's ready to implement, and then we can give that issue over to the Implementer agent. The Implementer agent, just like Code Assist, takes that implementable task, which is an issue, creates a branch in GitHub. It has the codebase checked out, and then it goes about implementing that code, that feature using test-driven development, and it makes a commit to the branch and creates a pull request.

On that pull request, again, just like any other member of the team, I can leave the feedback on the pull request, and it'll go ahead and read that, and then update the code for me. And again, now I'm just iterating. The really great efficiency here is that I don't have to sit down, write and debug code for a half hour. I can let the agent do that and I can go do something else that an agent can't do. And again, that's where we're really getting the efficiency of writing code faster with agents.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2390.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2390)

So I want to walk through a couple of examples of how we've actually used this to write code in our TypeScript repo.  Here, you can see the issue of an overview of a task that I want to implement in the TypeScript codebase. And as a developer, I know how I might want to go about doing this. But if I were to give this to an agent, it would take the shortest path possible to implementing this feature, and that's essentially what we're calling vibe coding.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2420.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2430.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2430)

To avoid this, I type in a slash strands command to kick off a GitHub action to trigger the refiner agent. You'll see in a second a label will be added to the issue to indicate that this agent is running. And the agent will be dropped in a codebase with this issue as instructions to go  about and explore and understand how that issue should be implemented. It leaves a comment here with clarifying questions on how it should go about implementing that feature. 

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2440.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2450.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2450)

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2460.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2460)

Now, while it's done that, I've gone off and done something else, and I can come back and answer its clarifying questions. So here I go, one, two, three, four, five, answering all of the questions.  And when I'm done, I can go ahead and type in slash strands again to trigger the refiner agent, and we keep doing this until the task is ready to be implemented.  This takes a couple of minutes. And once it's done, it'll trigger the agent again. And the agent again is dropped in the codebase. It's able to read my updated comment,  and it ultimately decides that the feature is ready to be implemented. And it leaves a comment for that, which you'll see in just a second. There we go.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2480.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2480)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2500.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2500)

So the agent has responded with a comment on the pull request, and then it's also updated the issue description  for the implementable task. And you'll see I'll refresh the page, and the agent has actually made the edit to the description here. So using this refiner agent, I've essentially disambiguated the task for an agent to go ahead and start the implementation here. Now, in this next video,  I'm going to go ahead and trigger the implementation agent. And it's really simple. I type in slash strands implement, and we're using the GitHub action again to trigger a new agent, which is the implementer, to kick off the implementation process.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2520.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2520)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2530.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2530)

Again, we add the label, we create a branch, we drop the agent in there, and it's going to follow this test-driven development methodology  to go ahead and start writing the code in our codebase. I think this one takes about 30 minutes. And again, this is where the efficiency comes in. I can go off and do something else while the agent is doing that.  I don't need to sit there and code and debug. I'm just doing the review and bar raising for the code that comes into our codebase.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2550.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2550)

So here it's created the issue, but something interesting I want to point out, which is about that steering and refining the agent SOP, is the agent wasn't actually able to create a pull request. We found that when designing these agents, in some cases the agent didn't have permission to do it and needed the developer  GitHub token to do it. So we added an extra instruction to the SOP to tell it to give me a link so I can do it on your behalf. And that's how we've essentially debugged these SOPs. So here I've gone ahead and clicked into the issue or the pull request, I've created it. And you can see this is the code that the agent has created on my behalf. I haven't done any of this.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2580.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2590.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2590)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2600.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2600)

So I'm going to skip ahead here a little bit. Now I've instructed the agent to update the description. Again, I'm not doing any of this. I'm telling the agent to do it.  You'll see the comment just below. And I've also spent a little bit of time leaving a couple of pieces of feedback on the pull request  for how I want the agent to update the code. There were a couple of things I didn't quite like. I wanted to change the naming of a discriminated union. And I'm going to go ahead again and trigger the agent. 

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2610.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2620.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2620)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2630.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2630)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2640.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2650.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2650)

And after a couple more minutes, it's going to go off, write the code, update the pull request, and leave a comment. We've refined this SOP a bit as well to  not just leave comments. It's also going to respond to the pull request comments that I've given and give insightful responses on how to do it. If it's confused about how to go ahead  and implement a feature, it'll ask me for clarification before it actually goes and does that. That's again that refinement of ideas. And you can see here it's responded to the comments that I've given it, saying that done, I've implemented this feature, done,  I've done that one. And I'm going to scroll up and take a look at the code as well to show that it's actually implemented this feature. So  using the system, the team has been really excited and really motivated to write code, and we found a lot of efficiencies out of this. It's not just me using it, it's our whole team,  and a lot of us are getting really good results.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2660.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2660)

### Results and Closing: Efficiency Gains and Community Resources

I showed the system to my team's principal engineer, Aaron, and he gave me feedback of this. I spent one hour of my time over three short  20-minute sessions total, instead of perhaps half a day writing code. Put it another way, I saved my principal engineer about four hours of time, and instead he was able to implement the feature in about one hour. And I've essentially four-x'd the efficiency of my principal engineer. It's not always quite this efficient or quite so time-saving, but we are finding more and more efficiencies on how to use the system.

Like I said, the real benefit here is the asynchronous nature. I can go ahead and kick off an agent or multiple agents to implement multiple features while I go off and do something else in the meantime, things that agents aren't quite yet able to do. And we typically find that low to medium complexity tasks are the best ones for agents to pick up. As a developer, I'm going to focus on the high complexity things and break them down into medium and lower complexity tasks, so I can delegate them to agents.

A graph that I really love is this next one. We've actually kept track of the contributions that the agent has made to our code base, and you can see that our agent is the largest contributor of code.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2720.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2720)

 My username is on the top right. I have 43 commits in about 14,000 lines. The agent is just a bit ahead of me and is the biggest contributor. All of our engineers are using the system to contribute code. But I also want to highlight that as engineers, we're still writing code. The agents aren't writing everything for us because there are still complex tasks that the agents aren't necessarily able to do, but we're finding efficiencies and really good ways to utilize these agents and help us speed up the development of TypeScript.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2760.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2760)

 Some closing notes that I wanted to give here. We've learned to spend less time writing code as engineers and offloading that to agents. If I were to pick up a feature for a code base, I have to check out the repository, do some context switching to understand what the feature is that I need to implement, go through the code base, write the code, and debug it. Once I'm done debugging it, I have to write the tests for it. Instead of spending all of that time doing that, I can offload that to the agent.

My time is much more valuable spent reviewing, refining, and bar raising that code, and giving high-level guidance on the direction and design that these agents should implement the code in. We're figuring out how to parallelize our time, offloading that coding and debugging process to agents, and I can go do things that agents aren't able to do. Using the system, we've essentially learned how to treat these agents as members of our team.

If you remember back to that diagram that I showed of the GitHub flow of us contributing to GitHub, the agent is just another member of the team. For TypeScript, it's the highest contributor of code in our team right now, and I think that's really exciting. We're going to keep on working and developing the system, and you can track our progress on our TypeScript SDK repo. This is how we built TypeScript. I'm really excited to explain how we built it, and I'm really excited for all of you to give it a shot and try it out. I'm going to hand it back to Ryan for some closing notes.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/16eb68a01003a90b/2850.jpg)](https://www.youtube.com/watch?v=NzZOm-kaO94&t=2850)

Thanks, Nick. Yeah, well, I hope you learned a lot from us today about what Strands is and how it can help you.  Obviously our team's been using it to get efficiencies on code-based agents. We've been working with customers who are building chat experiences, data processing pipelines, and research assistance. Strands is very flexible for all these use cases in TypeScript. Now you can get even closer to applications running in your browser and running in your client applications.

One of the things we didn't mention today is that this week we worked with the fine folks at Copilot Kit. They implemented an AGI integration for Strands that's available on our website as well. The QR code takes you to StrandsAgents.com. You can also find all the repos that we talked about today, including SOPs, evals, and of course the SDKs for Python and TypeScript at GitHub.com/Strands-Agents.

I'm Nick and I will be around for it looks like another 10 minutes if you'd like to ask any questions. We also have a few Strands stickers. If any of you are going back to the Venetian Expo Hall today, there is a Strands booth. You'll find it in the big AWS square in the middle of the expo hall. We've got a hot red orange excavator running around that has an agent running on it, so Strands running on local compute managing sensors and then connected to a cloud-based environment to do an industrial manufacturing use case and excavation.

Have a look at the booth if you're around. You can get some more stickers and swag there and have a chat with our fine folks about anything you need. Otherwise, yeah, come and join Nick and I, and thank you for spending time with us today. Thank you so much. Great job.


----

; This article is entirely auto-generated using Amazon Bedrock.
