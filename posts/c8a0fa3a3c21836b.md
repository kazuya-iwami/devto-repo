---
title: 'AWS re:Invent 2025 - Accelerate Telco Transformation: AT&T''s AI-Powered Migration at Scale (IND201)'
published: true
description: 'In this video, AWS and AT&T discuss how generative AI is transforming telecommunications infrastructure modernization. Efrat Nir-Berger from AWS explains that 70% of telco IT still runs on-premises with systems over 20 years old, making migration challenging due to mission-critical requirements, complex business logic, and legacy technology depth. The session introduces AWS''s agentic AI approach using Amazon Q Developer, AWS Transform, and Kiro to automate 70% of transformation work. Rama Raghavan from AT&T shares their mainframe modernization journey, highlighting challenges with COBOL applications and their strategy moving from code conversion to AI-powered reimagination. Demonstrations show automated Java version upgrades and how Kiro enables reimagining applications by generating new code from specifications rather than converting legacy code, dramatically reducing migration time from years to months while maintaining business continuity.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/0.jpg'
series: ''
canonical_url: null
id: 3093108
date: '2025-12-08T20:14:41Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Accelerate Telco Transformation: AT&T's AI-Powered Migration at Scale (IND201)**

> In this video, AWS and AT&T discuss how generative AI is transforming telecommunications infrastructure modernization. Efrat Nir-Berger from AWS explains that 70% of telco IT still runs on-premises with systems over 20 years old, making migration challenging due to mission-critical requirements, complex business logic, and legacy technology depth. The session introduces AWS's agentic AI approach using Amazon Q Developer, AWS Transform, and Kiro to automate 70% of transformation work. Rama Raghavan from AT&T shares their mainframe modernization journey, highlighting challenges with COBOL applications and their strategy moving from code conversion to AI-powered reimagination. Demonstrations show automated Java version upgrades and how Kiro enables reimagining applications by generating new code from specifications rather than converting legacy code, dramatically reducing migration time from years to months while maintaining business continuity.

{% youtube https://www.youtube.com/watch?v=aeg6fNVQJpE %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/0.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=0)

### Introduction: The Challenge of Modernizing Decades-Old Telecom Infrastructure

 OK. Hello, everybody. Good afternoon. Welcome to our session. Very happy to have you here. Today, we're going to tackle one of the biggest challenges that we have in telecommunications. How do you modernize an infrastructure that has been running for decades? We're talking about systems with millions of lines of code. It's running, it's processing billions of transactions per day, and it's serving millions of customers, all of this while maintaining business continuity. So this has become a big puzzle throughout the years.

Until now, all the migration, the traditional migration couldn't really handle it because it didn't have the ability to run it in the speed and the scale that is required to run these transformations. But now, we're reaching a different era. We have generative AIs that allow us to unblock these capabilities, and our session is going to be talking about this. My name is Efrat Nir-Berger. I'm a Principal Solutions Architect at AWS handling AWS for telecom.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/70.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=70)

 I'm going to be very happy to have you on stage later on, Rama Raghavan from AT&T. He's leading Enterprise Architecture and Hybrid Cloud Platform Engineering, and also my peer, Sanjay Aggarwal, who is going to be talking to you. He's a Senior Solutions Architect from AWS. They're all going to work with you about the latest and greatest services that we have for you.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/90.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=90)

 What's the agenda? So we're starting by understanding why telco transformation is so challenging. What's unique about these transformations? Then we're going to walk with you through the AWS generative AI capabilities, our latest and greatest services that allow us to run transformation differently than the way we've done before. We're going to have Rama on stage talking to us about their mainframe modernization project, a very interesting project that is being run right now. I want to close with a little bit of takeaways. How do you start your own journey?

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/130.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=130)

### Why Telecommunications Transformation Is Uniquely Complex

So let's start with the baseline. What do we have today? 70% of telco IT is still running on-prem. We're talking about 2025.  Interestingly enough, 70% of it, according to McKinsey, is actually systems that are more than 20 years old. We're not talking about the back office long tail systems. We're talking about the billing systems and the customer management systems that drive your key organizations that are in the critical path. So how do we actually modernize these? And then, one of the statistics that we know is that it takes around 1.5 years on average to do migration, but we know from practice, especially for some of the telco workloads, it actually can take even longer. So we're talking about multiple years to run them.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/170.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/180.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=180)

 So what makes telecommunication  uniquely challenging? We're going to talk about multiple dimensions, and each and every one of them amplifies the other. Starting with mission critical, many of these systems, think about online charging systems, are having very rigid availability requirements. We're talking about five nines, meaning that I can have five minutes downtime per year. We cannot afford to have any kind of impact while we're doing any modernization.

Same about data volume and complexities. It's not just about the traffic that goes through the system. It's also about the business logic. Now, we're talking about business logic that has been added for 20, 30 years. Just think about it, international roaming, enterprise hierarchies, family plans. So all of this logic has been added throughout the system and needs to be maintained through transformations.

Going into regulatory compliance, whoever is in telco knows that so many compliance requirements need to be met for customer data. We can talk about payments policies. We can talk about in Europe for data privacy. So all of these, if we're doing transformation, we have to comply and ensure that we are complying with them through the modernization. And also about integration complexity. Everybody knows the spaghetti of the ecosystem, so you have an ordering management system, it's like dozens of applications surrounding it. Lastly, this is what we're going to talk about now, is the legacy technology depth.

So according to Forrester research, we are talking about 20% of the IT spend actually going to maintain these legacy systems. And one of the things that we want to do is we want to invest in new capabilities in our system. So each and every one of them is challenging, but together, they're actually creating a real challenge for us.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/290.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=290)

### Time-Consuming Bottlenecks in Traditional Migration Approaches

 This is also about how much time it takes to run traditional migration as we know it right now. Think about the assessment. One of the biggest problems that we have is that we don't have enough knowledge. You actually need to look at the code itself to get the business logic behind it.

The same thing is happening, so it's kind of like reverse engineering the code in order to understand it. The same thing happens for documentation. You don't have documentation, so the documentation of the business logic is within the code itself, which takes a lot of time to actually understand the flaws.

Second is about the actual work for code transformation. One of the things that we're seeing is that if you need to do any kind of COBOL to Java transformation, this is something that can take a lot of hard manual work. And again, I'm talking about maintaining all the complexities of the business logic that has been added into the code in the 20 plus years that we've been running it.

Third one is about migration testing. Just imagine how many edge use cases have been added throughout the years. Think about it: leap years, time zones, all of these, we need to test it. If we're not testing it in the right way and the testing is not done in a comprehensive way, then this can impact customer production when we switch the systems.

What is our goal? Our goal and our vision is that we would like to accelerate and automate 70% of this transformation, actually allowing us to move away from this legacy, but again, without any additional risk to the system.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/380.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=380)

### Understanding Agentic AI: From Assistants to Autonomous Agents

 So it comes to the question of what is Agentic AI? How is it even related? Why is everybody talking about it? And how is it even connected to telecom IT transformations? That's what we're going to cover in the next few slides.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/400.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=400)

 When we look at the Agentic AI or even the evolution, it actually goes through the transformation of three stages. It starts with the generative AI assistance. It's kind of like what you used to have. It's a prompt, you ask a question, you get an answer, very simplified, but you need to drive all the interaction to get the prompts.

Now, what happens next is that we evolved into generative AI agents. They are actually now much more autonomous. They are able to reason and act on your behalf. So you can actually give them specifications, they are able to perform, but they still need well-defined goals that they need to accomplish. They still need some human interaction in order to perform what they do.

Now we're reaching an era that allows us to really hit these bigger transformations because we're hitting Agentic AI. These are actual bigger workflows. We're talking about a multi-agent system that can be fully autonomous and allow you to really execute complex processes. And the whole idea is that when you have these capabilities, then you're able to run from kind of like delivering something that you need to run all the interaction into something that the process itself can be executed in a fully transformative way.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/480.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=480)

 To understand how it goes, I'm going to go one level deeper into what is an AI agent. So this is kind of like a cycle that allows you to understand how it behaves. You have four steps: observe, reason, act, and in some cases, reflect on how it's being done.

Assume that I give you as an input goal a problem. I have a problem, I need to do something. First, it's doing observe. What's the current situation? What kind of information do I have about it? Then the next step is about reason. What kind of steps do I need to take? What kind of an approach do I need to do in order to resolve the problem that has been given?

Then it acts. Act allows us to actually take control, to call APIs, to use tools. We have an ability to use external sources of information, really orchestrate change. And then reflect: how did it go? What did I do? Can I do it any way better?

Now, this is a cycle, meaning that the agent is always learning. So when I have an AI agent and it's specializing in something, there's an ever-growing and learning capability that goes through it. So it really is improving over time.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/550.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=550)

### Reactive vs. Proactive Agents and the Power of Multi-Agent Systems

 Let me give you two examples of the way it works. First of all, let's do kind of like a differentiation between a reactive and proactive agent. In a reactive agent, I've got for you a telco question. I might ask, hey, how much am I paying monthly? And then the answer is going to be, okay, you're paying $85 per month, and it shows what you're paying. It's like you have unlimited texts, calls, and 10GB of data. Now, answer, you know, there's a question, you got an answer, that's fine.

What's going to do a proactive agent? A proactive agent can do beyond that. So first of all, it observes. I have a customer who's asking me about the plan, but let me see what I know about it more. So I know what plan it is, but I can also look at his six months past usage to see how much he actually consumed. What other plans are available for me? So then it allows me to reason.

He asked me about the payment, maybe he would like to do some changes, maybe he would like to create some optimization. Then the actual magic happened because it's going to be acting, analyzing the data, understanding that there's going to be a better plan, and then coming back to my customer and saying, "Okay, you're now paying $85, but I'm seeing on your past experience that you're actually not using more than 4 gigabytes," and then he's offering you a better plan for your needs. This is the kind of cycle that we're talking about. It's a completely different experience. And if I'm a customer saying, "Oh yeah, great, I would like to change that," then that's the other act that you can do. They can switch your plan, you're going to get a mail about the changes you've done. So again, it acts on your behalf and it's autonomous.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/650.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=650)

But we need to understand that a single agent is unable to perform very complex,  comprehensive processes. Just imagine that if I want to ask a single agent to do a full core transformation for me, there are so many steps on the way that one specialized agent will not be able to perform it. So we're moving from a single agent into a multi-agent. What multi-agency is doing is the same way as we behave in a telecom company. You're having different teams specializing in different activities, and they can run it.

So I can have inside a telco transformation or any kind of big legacy transformation, I can have my code analysis. This is the type of agent who will learn your code and understand all the relationships, all the integrations, and give you more information about the way you can run it. Then we can have code transformation that's actually the one who will do the code conversion. We can have the one who's going to build the unit. You understand it. It's kind of like a multi-agent working in tandem. You have in the middle the internal agent communications that orchestrate it all. That's what allows a Generative AI to perform these complex transformations in an effective way.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/730.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=730)

### AWS Generative AI Stack: Amazon Q Developer, AWS Transform, and Kiro

So we talked about agents. Now I'm going to take you to the broad level. Probably if you've seen,  until now, you've seen this slide once or twice at least. This is talking about our Generative AI stack. And the whole idea is the same thing. We have a suite of services that allows you to work through your business per your needs. So we're going to be very brief because we're going to reach the point which is interesting for transformation very quick.

On the bottom, on the foundation, we have the infrastructure. This is purpose-built infrastructure for optimizing AI workload. It can be AWS Trainium, NVIDIA, it can be GPUs, and then on top of it comes the second layer, which is about models and tools. This is where you're going to have Amazon Bedrock that allows you the access of any foundation model for your services. It also allows you to have agent runtime, orchestration capability, and so on. So on this, we can stay a lot of time, but I'm going to move forward because what we need to talk today is about the application tier.

This is where the magic happens for transformation. You're going to have Amazon Q Developer, AWS Transform, and Kiro. We're going to talk about all of these throughout the next slides of the session. Just letting you know, Amazon Q Developer is not like your regular developer chatbot. It actually enables you to understand, to write, to embed more information into your code, to operate all your applications on AWS. So one of the things that I would like to show is afterwards, we're going to do a short demo showing the agents behind the scene for running it.

Next one is going to be AWS Transform. Now, AWS Transform is one of the more interesting, it's the first industry purpose-built service that allows you to run transformation. And we tackle one of the biggest ones: VMware, .NET, and mainframe. Now, think about it, by the way, now in this re:Invent, we just launched another capability in Transform, which is the full stack of Windows. As an example, it now is able to do SQL Server conversion to Aurora PostgreSQL, our managed database. So this is ever evolving and we're listening and seeing what's needed on your side.

Another thing that has been launched in AWS Transform is, whoever heard of Custom? What is Custom? The idea is that we gave you the bigger ones, but at the same time, there's a lot of patterns needed by customers. So we are giving you repeatable transformations that are either coming out of the box from us for any kind of transformation. It could be version upgrade, it can be runtime migration, it can be changes of core transformations that we're just going to discuss. But you're either going to be able to use our own AWS provided solutions or you're going to bring your own.

So if you have a pattern that you would like to do some transformation, that actually allows you to run things at scale. Lastly, we're going to have Kiro, and I'm going to give you a snapshot for the next section. Kiro allows you to reimagine your legacy system as a modern microservices-based application. We're going to save that for the end of the session.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/940.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=940)

### Amazon Q Developer's Specialized Agents: A Live Demo of Java Version Upgrade

I'm going to go one level deeper into Amazon Q Developer because what you need to know about it is that it's a list of specialized agents.  We've thought about it as having agents that are specializing in performing tasks. In this case, we're talking about your entire software development life cycle. Think about it, your developers are spending a lot of time on these repeatable but very time-consuming, important activities. Amazon Q Developer helps you with these. I'm going to give you a few examples.

Look at the software development agent. It allows you to implement application changes in minutes, and we're not just talking about code completions. We're actually talking about production-grade features that you need to develop. Another thing is the unit test agent. That's one of the things that takes a lot of time to write down the tests. It allows you to build these comprehensive tests to support your new code that you've edited. The documentation agent is one of the things that we constantly see, especially for legacy applications that are missing. One of the favorite capabilities that we see around it is explainability of the code, so it allows you to have a lot more understanding of what's happening in the code itself.

The code review agent, think about it as your senior engineer. It's looking for if you have any bugs, if you're having any regulatory constraints that you put in the code, or misconfiguration. It gives you advice around this. Lastly, this is what I'm going to demo soon. We're going to show transform agents. What the transform agent is, is the capabilities that you're going to have inside Amazon Q Developer, but at the same time, it's what I mentioned about AWS Transform custom. It's the same experience into how do I take one of the biggest challenges.

In this case, I'm going to take Java version upgrade. Java-based version upgrade, everybody who's in Telco usually has these applications that are running, I don't know, Java 6, Java 8, and they need to update them to 17 or 21 because the software is end of life and there are security vulnerabilities around running this code in this version. But if I'm trying to do this manually, everybody who's trying to do this project, it takes time, it's error-prone, you need to debug it a lot. It's just one of the most time-consuming activities, but a necessary one. I'm going to show you a short demo on how we can do it much more effectively.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1100.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1100)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1110.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1120.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1120)

Here we go. We have the dashboard. I'm hoping it's starting, it should be starting. Now, yeah. I have the dashboard and I just picked, I want to do an upgrade from 8 to 17.  I'm also able to say, hey, I want to do unit tests on my code. This is where the thing is starting. I'm saying, okay, give me my JDK. I'm pressing the JDK.  What is now happening usually takes around 10 to 30 minutes for it to be executed, depending on the project size. What you're seeing is what's happening behind the scenes. 

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1140.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1140)

What kind of activities need to be done? The first one is going to say I'm going to do the upgrade of the version. Now what's interesting is that it's doing a full analysis of the dependencies and it's now showing you what it identified, what are the things that need to be added, removed, or updated. Now as a person, that's a human in the loop,  you are able to see exactly as it's offering to do the transformation of the code. You have the full information that you can review later on and understand and maybe change it, so you can always prompt it.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1160.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1160)

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1170.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1170)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1180.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1180)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1190.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1190)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1200.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1200)

The second step that it's doing, and again rushing the demo quickly, is that it allows you to look at the deprecated code.  The same is happening if you do a Java update. In many cases, you have code that needs to be removed. It's able to identify all the deprecated code, and then  it will change the code accordingly. Now, when we reach step 3, that's where the magic happens. It's actually now doing the agentic  work of taking all the analyzed information, doing the actual code changes based on the analysis that it's done before. Then it's going to issue a formal transformation  report showing you all the changes that it's done. But better than this, you can actually open the old and the new code and see and compare what has changed. If something doesn't fit, something doesn't add  up, or you prefer to do something differently, then you're always allowed to prompt it and request changes.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1210.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1210)

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1220.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1220)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1230.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1230)

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1240.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1240)

Let me show you how it looks.  It looks like this, so you can scroll down and see all the changes. What you've seen is an update being done in minutes for an activity that usually takes between months  to even years. Sorry about that, months or even weeks or days, depending on the complexity of the application. So this is  closing the demo side and the first service. I took you through some of the bigger challenges of telecom and a little bit about Agentic AI in our stack,  but all of this is just technical.

### AT&T's Mainframe Modernization Journey: Challenges and Strategic Approach

The really interesting thing is actually here, what's happening in the real world, a real use case from a real customer, about the challenges that they're experiencing, how they're handling it, and what their vision is toward it. For the second stage, I'm very happy to have on stage Rama and Sanjay to walk us through the AT&T use case. Thank you. Thanks. Thank you, Efrat.

Rama, thanks for joining us today. I really appreciate your time. Rama, you're leading mainframe migration for the telecom industry in a big way. Can you talk about what are the challenges you have when you're dealing with mainframe applications and also your vision around when you try to migrate or modernize that application? How do you deal with it? If you can talk about it, that will be super useful for the audience. Thank you.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1310.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1310)

Yeah, thanks. Hello everybody. I'm Rama Raghavan. I drive the cloud engineering, platform engineering, and enterprise architecture for AT&T. We've been  on a fairly good journey to evolve to a very hybrid environment within AT&T. Over the years, a lot of our migrations have been on the mid-range Linux environment. We've been looking at what it looks like from a very legacy mainframe-oriented assets perspective.

If I were to ask this audience, does anybody know anything about what JCL stands for or CICS, I'm sure there'll probably be two hands that may go up. But then as you go out to the industry, you're going to find fewer and fewer people that still have any knowledge about some of the mainframe systems. Now, the systems though are going to be running there. It's going to be running, and not a moment goes by, and it'll be another 30 years that will go by, and they could still be running. But it does present a certain business risk across our entire environment from the impact to business-critical systems, the impact to finance, and impact to many of the things that tend to occur.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1420.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1420)

The key challenges with this landscape again is about the highly integrated systems, and over years we've been aggregating some of the affiliates, disaggregation companies. We do billing aggregations across telcos. Where do we want to go with this in terms of  our velocity, and how do we move from where we are now to accelerate and integrate it with faster evolving platforms that are more cloud-based capability sets? The scale, obviously, we've got to match and exceed. Oftentimes you'll find that these mainframe systems are high-performance systems. There are newer capability sets that enable us in the cloud platform.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1500.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1500)

Same thing with security. A lot of our security vulnerabilities tend to arise on the mid-range Linux platform compared to mainframe. However, when there are going to be events and incidents, we want to evolve our platform in a manner that we are highly secure and bringing some of that. All those are looking more on the technical side of things. And then the other key is what do we, how do we enable our business in terms of de-risking the business and then providing them more capabilities as we evolve into additional business needs. 

Let me discuss our approach towards that. Obviously, the current set of assets, all the way from code to documentation, presents challenges because many of the people who worked on them have moved on to do other things. So you're going to find challenges in terms of discovery of what exists right now and then determining where we want to go from there. We've done quite a bit of work to look at it very hard and ask ourselves, do we need to retire this platform? That's obviously been given a very strong look, and you'll find that yes, you could do 10% retirement, you could do 20% retirement, but then after that, you're finding that these applications are serving business-critical functions across multiple product lines.

So from there, we did not actively pursue just trying to re-host it. We didn't want to grab that entire environment and run it like another mainframe in an alternate hosting environment. Instead, we started pursuing our path down a code conversion approach. There are several products in the market, but again, when you start looking at the code smell, you'll start finding that the source code got converted and it starts to look like COBOL just written in Java or many other such things. So now, if you go out to the street and hire somebody, and they are top Java developers, and they look at the code, they're going to say, okay, what is this thing, right? So what have we done in the process?

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1660.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1660)

Now, there are additional improvements constantly in the last 18 months to 24 months. There are products that are constantly improving that, producing object-oriented Java code and other things. So when you get to that, we've only been able to get it to a certain level through transformation. As part of the evolution within AI and the possibilities and where we want to go with all of this,  is where we start looking at exploring with my partners here with the critical capabilities. So what might that look like? Now we're no longer limited, I would say, by some of the constraints that we have in our existing environment. We were able to move, let's say, 10% using some of the existing tool sets, but we feel like we could do a lot more with some of the great capabilities that we've heard through the last few days here as well.

### Refactoring with AWS Transform: End-to-End Automated Migration from COBOL to Modern Code

So Sanjay is going to help start talking a little bit about some of the capabilities that get us to more than just re-platforming the code, but to take us further out to more refactoring and reimagining the whole thing using the critical AI capabilities. Sanjay, thank you, Rama. Thank you for the insight. These are the real challenges, really. So over the next 20 minutes, I will walk you through how generative AI is completely redefining the way we used to do migration and modernization.

When we talk about migration, as you see Rama also mentioned, we are familiar with the 7 Rs, all the way from retire to refactor. But thanks to generative AI, it allows us to add one more R, reimagine, and that is what I'll be talking about today. Once we add this one more R, that changed everything, and I will be talking about how reimagine works. Not only that, by the way, the way we used to do refactoring our applications also dramatically changed with generative AI. So I'll be talking about how the new technology in hand is helping in reimagining as well as refactoring our applications.

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1780.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1780)

Before I go further, I would like to get on the same page with all of you about what refactor means, because that may have a different meaning for different people. So how I define refactor is  let's take a simple example. You have a COBOL application running on mainframe hardware, a mainframe application, by the way. You've been asked to refactor and move it to AWS Cloud. So the way I define refactor is the way the application works in your on-premises environment will not change when it migrates. The business logic, the functional requirements, nothing will change. It will be working as it is, but

what will change is the tech stack. What is the tech stack? You may be converting your COBOL code to more advanced Java code or Python code. You will be converting your database to a more modern SQL engine running on RDS, or maybe you are redesigning your application from a monolithic architecture to a more microservices-based architecture. What we're seeing is that generative AI allows us to do all of this in an automated fashion from start to end. That is what I want to show you, how AWS technology is enabling you to accomplish all of this from start to end.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1850.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1850)

 The first tool I want to discuss today is AWS Transform. What is AWS Transform? It's really a collection of pre-trained agents sitting side by side, doing this work from analyzing your code all the way to deployment. They're stitched together, and you have full control of it, but they are working for you on your behalf to accomplish all these tasks. Let's dive a little deeper.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/1880.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=1880)

 When you feed your COBOL applications to AWS Transform, or maybe a set of applications, not just one application, the first thing that happens is the code analysis agents take over. They go through every single line of your code and learn from it. They create a blueprint of your applications, identifying all the dependencies, performing dependency mapping, and finding all the interfaces. This blueprint creation is something very unique that we never had before.

Once we have a blueprint of your application, the next agent takes over, which is called the documentation generation agent, and that is where the magic starts happening. Just by looking at the code, generative AI is now able to produce two types of documents. One is a business document that tells exactly what that application is doing, and the second is a more technical document. The technical document goes function by function, explaining exactly how the system is designed for each function, each file, and each application. This is a game changer because anyone involved in migration will relate to this. The hardest part is migrating an application that has no documentation or very little documentation. This step alone accelerates our migration and can cut the migration time in half, and I will show you exactly how this agent works under the hood in a minute.

Now we have a blueprint of the application and an up-to-date document. Next, we need help creating a target state architecture, and that is where the code decomposition agent comes into the picture. What it does is it goes through your code knowledge and provides very prescriptive guidelines to developers on how you can break the monolithic architecture into small microservices-based architecture. With that, you can design your target architecture. Once we have that, we get to the work that Rama was mentioning as well. We start converting code from COBOL to Java code, and generative AI doesn't stop there. It also works with your new code, making it better and better to the point where it's ready for test and deployment. This is the end-to-end migration process that is taken care of by AWS Transform from start to end, but I would like to give you a little bit more detail about what happens under the hood when we talk about these agents. Let's start with the first one, code analysis.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2040.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2040)

 Think of it this way: once you upload your code to AWS Transform, as I mentioned, the code analysis tool will go line by line, look at your code, and it will basically act as your inspector. It will inspect and examine your application. It will first check if there are any missing artifacts. Maybe for your COBOL application, your runbooks are missing, or your copybooks are missing, or maybe there's duplicate code. It will create a full visual representation of what your application is doing.

One interesting feature I would like to highlight is that it also gives you a complexity score. The complexity score tells you upfront how difficult or easy it's going to be to migrate your application. I personally like calling it the X-ray vision of your application, which was never available to developers like us before.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2120.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2120)

This X-ray vision of your application is super useful, not only for migration but also for planning purposes as well. Once we have this X-ray vision of your application, next I talked about the documentation agent. Yes, it creates two types of documents, business  documents as well as technical documents, but why is this such a big deal? Why is it such a game changer? Well, we all can relate again. The longest time in your migration journey is the testing part. You can migrate the application, but if the thing doesn't work, what do you do? You have to fix the problem, and that is where this agent helps.

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2160.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2160)

If I have to fix a problem and I don't have documentation, or technical documentation I'm talking about particularly, that is the toughest part to fix, and this agent solves that problem beautifully. Once we have all these artifacts created, I talked about  how can I get help for my target architecture? And this is where the code decomposition agent comes into the picture. The way it works is a typical generative AI problem. It does a segmentation of your code based on the business logic and gives you a visual representation to a developer of how you can break apart all your monolithic applications into small components.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2220.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2220)

Microservices. What is a microservice? A microservice is a subset of functionality which you can independently design, code, test, deploy, and even maintain. And why is it so important? Day two operations and TCO. Your TCO will be reduced because if you have to fix a small component of a problem, you need not touch the whole application. Just fix the small problem in the particular microservice, deploy it, and you're good to go. So this step alone is super critical for day two operations.  Once we have all these artifacts, which were never there before, it used to take us actually months to create that mental model. With these artifacts in hand, which get created within hours by the way, and we will show you some demos as well, you already have a target architecture.

Now let's go to the real work, converting the code, and that is where the code refactoring happens. You convert the code to the language of your choice, Java, Python, or any other language of your choice. And then generative AI continues working and makes it better. But one important point I would like to emphasize very clearly here is that at every step you are in control. Any input to the agent or output from the agent, you can change. So think of this agent as your assistant working with you, doing the heavy lifting for you, but you are always in control, so you can pivot whatever way you want. In fact, we will talk about this in more detail in a few minutes, but these are the ones changing the way we used to do our refactoring.

### Reimagining Applications with Kiro: Smart Refactoring and AI-Powered Development

Let's take the problem one step forward. What if you have an application and that application's requirements, thirty to forty percent of them, are not even valid anymore because the application was deployed and developed twenty years ago, fifteen years ago? You have to migrate that application. What do you do? What will you do? There are two options. One, you refactor it. Yes, this agent will help you with refactoring. You can migrate it, refactor it, that's all good. But the problem is you are bringing some of the functionality which is obsolete. You will be developing it, converting it, testing it, but that is wasted effort.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2360.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2360)

The second option is you rewrite it. You can always do that as well, but it takes a lot of time to rewrite the application. So what if I tell you there is a way now where you can bring only a selective part of the application? And this is a new way of working, and that is what we call reimagine. So I will walk you through a little bit on the reimagine part and how reimagine comes into the picture.  So let me define first what reimagine means for me. Reimagine is where even the business function is modified. You choose what you want to bring from the application. Yes, the tech stack will be changing, but we are talking about even the functional requirements you can modify. So you're bringing only the portion of the application, and that is something we never saw before.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2400.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2400)

This was never possible without generative AI. So this is the first thing we call reimagine, but you can call it smart refactoring. You can call it all of these names, and the tool for that is Kiro. You might have seen Kiro House on  our expo floor entry. That's where Kiro lives, but I can walk you through what it does basically.

Kiro is AWS's new AI-powered development tool with a simple but different approach. Here we do not start with the code. We start by telling what you want to develop, and Kiro will do the job for you. It's a little bit different approach. Same thing, we will still do all the things we will be doing, but now at the last step, let me show you a little bit of architecture and how it works.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2440.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2440)

So far, whatever we talked about with the transform, we still will be doing that. We still will be looking into the code,  doing a dependency mapping, interfaces, creating documentations, all good stuff, right? But we call it reverse engineering. Once we have that artifact, we will not convert that COBOL code to Java code. This will become an input to the Kiro, and we will tell Kiro what functionality you want, how you want, create specifications, and with those specifications, Kiro will develop a new application for you. It's a very different approach. That's why we call it reimagine, and you will have a brand new application, again with the clicks of buttons.

Then you go to the next step, the third step, which is where you validate everything. But things may not work as you like, so you can repeat step two and three, and two and three again and again till you get the result you are looking for. In step three, once you are happy with the result, it will give you basically a brand new application. You are not leaving all the knowledge behind. You're still using the knowledge of the application, but Kiro gives you a new way of migrating that application.

I even have a short demo. What I did was a three-tier application, web application, very simple web application, and I want to add one functionality, single sign-on. I want to add a single sign-on with a third party, Google sign-on or enter ID, whatever way you want, and see how Kiro takes it forward. And I will also pay attention, by the way, so not only once you develop the new functionality, Kiro can help you in testing as well, automated testing.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2540.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2550.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2560.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2560)

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2570.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2570)

 Hi, this is Kiro, the new Agentic IDE that works alongside you from concept to production. Let me give you a tour of some of the features of Kiro.  First, we'll click on that ghost hand icon on the left-hand side. Now I'll click the generate steering docs button. This will help Kiro better  understand the application when we prompt it later. Now that the docs are created, you can find them in the Kiro steering folder. These files describe a serverless full-stack  web app that we're using with Next.js and an AWS backend.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2580.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2590.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2590)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2600.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2600)

Let me show you one of our most powerful features, Spec. I'll ask Kiro to add a  new way to sign into our app using Google. Kiro is going to help us code this up and give us a step-by-step process so it gets it right.  The spec feature generated this requirements document as its first step in helping us create this feature. It has user stories and acceptance criteria. I'll review it and add another criteria to  improve the way sign out is handled.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2610.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2620.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2620)

For this next step, Kiro generated this in-depth design document. It includes a mermaid diagram, infrastructure changes,  and configurations. This looks good to me. We'll continue on. Finally, we're presented with a task list. Kiro has broken up this feature into smaller parts that  it can complete one by one. I really like this feature because I could interactively have Kiro complete a task at a time, and I can verify or make changes if needed afterwards.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2630.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2630)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2640.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2640)

 I'm going to have it start on these tasks. I'll queue a few, and I'll check it along the way. This will take a short while. While it works, I'll set everything up inside the  Google Cloud console outside this video. It's been a few moments. We're now in task five, and it's updating the Prisma update to include the Google account.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2650.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2660.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2660)

 Great. It's now completed all the tasks. I did a few updates during each task to make sure everything is working. And now, let's take a look at the output. Here's the application. It has this new sign-in  with Google button. All the backend and infrastructure has been completed because of Kiro. If we test the login, it looks good.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2670.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2670)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2680.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2680)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2690.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2700.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2700)

This next feature is really helpful. It's called  agent hooks. To get started, I'll click the little ghost icon on the left-hand side, and then I'll click the little plus arrow next to agent hooks. I want this hook to create tests  anytime I save a component if they're not already there. So I'm going to describe this in the prompt, on file save, add a few basic tests if not already created for each component. And then I'm going to  click this enter icon. Kiro is going to create this hook for me, and it'll be listening in the background for that file save. So I'm going to see it now inside the dot  Kiro folder, this new hooks, and then I can give it a try. So I'm going to go into one of my components, and I'm just going to add a quick comment to it and make sure it works.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2710.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2710)

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2720.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2720)

So, I'm  going to type in test comment here, and then I'm going to open up the task list, and you can see it running already in the background here, and it'll start creating the tests. This will just take a few moments.  Here's the new test, the Google Sign-in button test. I'll look through it, created several of them, looks really good.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2730.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2730)

Kiro does more than just development with specs  and agent hooks. We also support native MCP and vibe coding. With Kiro, you can go from concept to production and beyond. Go ahead and give it a try. It's free during preview. So I hope we learned together something like how this migration is evolving with the generative AI. So, Rama, if you can help, like we started this journey together, so if you can guide us through where it's going, that would be super useful.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2780.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2780)

### AT&T's Migration Factory Blueprint: Accelerating Transformation with AWS Partnership

Yeah, thank you, Sanjay. That was awesome. And think of it, folks, in terms of if we are crawling right now, our plan is to run fast, right? So, some of these capabilities that Sanjay just walked through is going to get us  to a, let's call it, a swim faster model, right? Now, when you talk about model and our migration, what you see here on the screen is really our factory approach or a blueprint for how many of our migrations are organized. What you see as the Cloud COE, the Cloud COE, the purpose is really to take the AT&T Well-Architected Framework. So, you derive many of the base foundations from the AWS Well-Architected Framework, and then define our landing zones, how we define what our VPC model's going to be, what are the guardrails around it from a security and other requirements and policies and the OU policies.

So in many ways, the Core Cloud Foundation team helps define some of those prescriptions and patterns. And oftentimes you'll find that you will need, when you start off, you may need, say, 10 patterns, and that'll unblock about, you know, 5 applications. And then you're going to find, I need to unblock 1 more, 2 more other patterns, and that'll unblock about another 10 to 15 applications. And so part of that is a robust set of prescriptions that are, some of them are codified through policies, some of them become inputs into some of the automation that can go into the migration.

Now, overall, when you look at the Discover Plan, this is about how do we go find what doesn't exist, right? I don't have the people that know any knowledge anymore. I don't have, in some ways, I would even say, where's the source code for some of these things that are running in mainframe? And so, what's fascinating is to be able to create documentation, to be able to create test cases, and in order for us to then validate that the outcome is in fact comparable and or better than where it is right now, from a performance standpoint, obviously.

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/2980.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=2980)

And so, in that kind of the transform state, as we are reimagining this, it wouldn't be possible without many of our partnership, both from an AT&T and AWS standpoint, but also with some of our SIs that we have some relationship as well. And so, in a result of, if you think about this, if we're doing, let's say, I don't know, about 10 to 25, maybe 25 applications in a year, we're looking to get us to a much faster rate through many of the innovations that we're here and seeing, in terms of where we go with this, like what's our goal and how do we get to  something that, you know, what does it look like, right?

Obviously, the development velocity, we talked quite a bit about how do we progress from where we are right now to accelerating our migration. The other aspect is about making sure that it is, in fact, highly performant from an operational standpoint.

In fact, these systems are meeting the business needs and functional value. Many of these are billing transactions and ACRs and summary processes, which are pretty intensive and performance intensive. We're going to capitalize quite a bit on many of the step functions and other capability sets to get us to a complete parallel execution of many of those things.

We've also been looking at some of the capabilities where you may have some SaaS products. So not all of them necessarily have to be code converted into this outcome, but there may be some opportunities in getting us to the outcomes. Where we see all this is how do we enable our business partners to be able to do more of their business functions, and then how do we de-risk our IT infrastructure.

As you look at it right now, nothing's falling apart. But then, how far do you keep moving that goalpost? We took a deliberate action to start investigating this. We didn't need to get it all done last year. It wasn't the goal. Our goal is how do we get to it in a manner with the great innovation that's occurring around us, and how do we capitalize on some of this.

From a performance standpoint, many of these batch jobs and other billing processes involve both a volume of data that's coming through, the aggregations that occur, the billing groups that occur, and our compliance needs around it. There's quite extensive data analytics that's occurring right now in the existing set of platforms we want to bring forward through newer technologies and improve our availability and performance.

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c8a0fa3a3c21836b/3180.jpg)](https://www.youtube.com/watch?v=aeg6fNVQJpE&t=3180)

What it gets us to is future-forwarding our platform and capitalizing on many of the AI innovations. None other than AWS capability sets is what's going to get us there. Thank you very much, and thanks for the great opportunity here. Thanks so much. 


----

; This article is entirely auto-generated using Amazon Bedrock.
