---
title: 'AWS re:Invent 2025 - How Netflix Shapes our Fleet for Efficiency and Reliability (IND387)'
published: true
description: 'In this video, Netflix staff software engineer Argha explains how Netflix balances efficiency and reliability across its four-region active-active global control plane. He details their approach to capacity planning using concepts like success buffers and failure buffers, fleet shaping across hardware generations (C7A, M7A, R7A), and service capacity modeling through their Oasis library. Key techniques include pre-scaling for predictable events, dynamic traffic shaping across regions, reactive auto-scaling operating in minutes, and priority-based load shedding in seconds. Argha emphasizes running tier 0 services at intentionally lower utilization (30%) while pushing best-effort workloads to 60-70%, maintaining the same $100M compute spend but with better risk management. The presentation demonstrates how Netflix uses Kingsman''s approximation and risk-adjusted net value calculations to optimize their microservices fleet while avoiding the "utilization alone is a lie" trap.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/0.jpg'
series: ''
canonical_url: null
id: 3085365
date: '2025-12-05T05:20:33Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - How Netflix Shapes our Fleet for Efficiency and Reliability (IND387)**

> In this video, Netflix staff software engineer Argha explains how Netflix balances efficiency and reliability across its four-region active-active global control plane. He details their approach to capacity planning using concepts like success buffers and failure buffers, fleet shaping across hardware generations (C7A, M7A, R7A), and service capacity modeling through their Oasis library. Key techniques include pre-scaling for predictable events, dynamic traffic shaping across regions, reactive auto-scaling operating in minutes, and priority-based load shedding in seconds. Argha emphasizes running tier 0 services at intentionally lower utilization (30%) while pushing best-effort workloads to 60-70%, maintaining the same $100M compute spend but with better risk management. The presentation demonstrates how Netflix uses Kingsman's approximation and risk-adjusted net value calculations to optimize their microservices fleet while avoiding the "utilization alone is a lie" trap.

{% youtube https://www.youtube.com/watch?v=K-2u50e0VzA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/0.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=0)

### Introduction: Netflix's Journey Toward Balancing Efficiency and Reliability

 Good morning everyone. Welcome to Reinvent 2025. My name is Pratik Sharma. I'm a principal solutions architect at AWS. I've been working very closely with Netflix for the last six years, and over the last few years, Netflix has shared their journey with you. Netflix has been a pioneer in building resilient architectures that leverage AWS's scale and flexibility. Previously we talked about how Netflix efficiently scales its compute infrastructure and how they do service capacity modeling. Today we're going to peel more layers and dive deeper into how Netflix balances efficiency and reliability. I have the pleasure of hosting Argha today. Argha is a staff software engineer at Netflix who's currently responsible for ensuring Netflix's scalability for live operations.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/90.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=90)

Over the years he has worked on ensuring Netflix performs fantastically when it comes to scalability and availability at the Netflix edge. Recently, he along with his team has been redefining how Netflix does compute scaling and capacity modeling effectively. So let's dive deep into it and welcome Argha to the stage. Thanks, Pratik. Good morning, everyone. My name is Argha.  Like Pratik said, I'm in the infrastructure organization. For most of my time at Netflix, I've focused on what we refer to as the end-to-end streaming architecture. This goes from the global devices to the Netflix edge, which is our CDN, into our cloud edge, and then obviously our unique microservices architecture. I've mostly been focused on the network side of things, but in the past year, I got involved in solving some hard problems on the compute side.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/130.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=130)

So what are we going to discuss today? I'm going to warn you that I've been told this is  a pretty dense talk. It's intentional because I think we are putting together almost two years of investment across many different facets that we've spoken about, including at previous years and last year's Reinvent. The main takeaway is for us to get a closer look at how Netflix approaches efficiency without sacrificing reliability. What that means is we're going to look into things like what it means for us to plan for compute supply, how we understand supply, what demand looks like to us, and how we look at our workloads.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/200.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=200)

Then finally, to my favorite part, which is what I'm tasked with, is how we balance what the supply side looks like with what demand actually looks like from a resource usage perspective. However, it would be incomplete without getting into how we actually use reliable techniques, like how we manage and mitigate risk. By definition, as most of you know, we're a global product. We are all across the globe, everywhere except the non-red part of the globe, which you can guess where that is. This means people at any given time can wake up and want to watch and binge-watch K-pop, Demon Hunters, or our latest Stranger Things season, and they can do this across  a whole fleet of devices.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/220.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=220)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/250.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=250)

### Netflix's Global Architecture: Four-Region Active-Active Control Plane

These are mobile devices, PCs, iPads, and TVs.  What that means for us is we cannot assume that all of these share a single stable network. We cannot assume that they have the same compute capabilities on the device, and more importantly, we cannot assume that they have the same resolution. This has implications for how many byte streams we encode and how much storage it entails. The way we serve our global audience is we run what is called a four-region active-active global control plane.  The active-active piece is very interesting because, as you can guess, very few companies at our scale choose to run in active-active mode. We are in four AWS regions: US East 1, US East 2, US West 2, and EU West 1. There is a good reason why we chose and stick with this architecture.

Part of this is we want to be able to serve users from across the world, any part of the globe that you saw on that map, with low latencies, extremely low latencies. More importantly, when things go wrong in a region, like some of you might be familiar with recent AWS outages, we do not want to be trending on Twitter. You should not hear that Netflix is down. It's a big part of my job and part of what my colleagues do.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/320.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=320)

This enables us to do something we talked about previously called failover. There are detailed talks on how Netflix does it, but this is why that active-active mode is very important. Finally, we complement the global control plane with our actual global data plane.  It's our homegrown CDN called Open Connect. We have dedicated talks that go really deep into the CDN architecture, but this is how it all comes together. It's often referred to as metrics as the crown jewel in terms of engineering.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/340.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=340)

### Defining Efficiency: Business Value, System Performance, and Risk-Adjusted ROI

So this is the start of what we're going to dive deep into.  The emphasis here is that I'm going to talk about a lot of efficiency techniques and how we approach efficiency, but I'm also going to talk about how we do this while staying highly reliable and available. How many of you here have been asked to make your systems more efficient? How many of you have been asked to reduce your annual cost, compute spend, or network spend?

How many of you get a chance to explain to your bosses that while that's all good, what is the cost? What are we trading off in terms of risk? I would have guessed that's a very small number. The reason this is important is we cannot have one without the other. We can be very efficient, but if we do not have a reliable product, the business has a huge cost. So we're going to level set on some definitions because these two terms mean different things to different people.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/410.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=410)

I'm mostly going to focus on what we mean by efficiency at Netflix, what we mean by reliability at Netflix, and how we measure both.  Let's start with efficiency. The reason workloads exist, or the reason your services exist, is they must create some business value. There's no point running services that are not additive to the business. For every single service, no matter how tier 0 or tier 1, they have to be creating business value. That's the end goal.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/440.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=440)

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/450.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=450)

Now, the workloads that do create business value come at a cost. The simplest way to understand the cost of those workloads  is the amount of resources you need to run that workload reliably and efficiently. When we talk of resource usage,  it's not just the work that the service itself does. It's the amount of infrastructure needed to support that work in totality. Now you have to move from a single workload to supporting a set of workloads at scale.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/470.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=470)

Finally, this is something that's often overlooked: failures come at a cost to business.  I want to dive deeper on the cost of failures, but this is something important. You cannot have workload estimation or cost without factoring in what it means to the business if that workload goes down, and this is the risk aspect of it. So this is probably one of my favorite graphs that I use with leaders, and you will see this is essentially like a Poisson distribution.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/500.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=500)

What it shows you is toward the tail, like this green line, there's a very interesting number. That number is not just interesting in terms of the probability, but just the scale.  So to put this in perspective, I'm going to assume a really big business like Netflix or Amazon. If your revenue is an ARR of 100 billion a year, what would you think is a good estimate for a minute of downtime? The cost of a minute of downtime approaches 2,000 a minute.

Let that sink in again. Ten minutes of downtime for a business like ours, and I'm only taking 100 billion of revenue, hopefully we do more, ten minutes of downtime is equivalent to 2 million in revenue. This emphasizes the point that as your business gets more important and bigger, you really need to start caring about the tail costs. There is an exponential increase in the tail risk of failure, and this is what I mean by factoring in this as part of the cost.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/590.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=590)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/600.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=600)

So let's look at what efficiency means  first without going into systems in detail, to the business. What is the business value of efficiency? So let's understand the value part better. I really like this  visualization that talks about how you have a workload that produces some value.

You have cost for running that workload. Hopefully, the value it provides is greater than the cost of running it. But then one important part gets missed. We came up with a term called risk-adjusted net value. What is the cost if that workload fails? To prevent failure as a business, how much do you have to pay or invest to buy down that risk? This is what we refer to as risk-adjusted net value. That is why that cost shifts left. Your ROI, the true value of that service, is much higher when you look at risk-adjusted net value versus just a cost versus value comparison.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/660.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=660)

When I expand this  for my teams running thousands of services and batch workloads, what does this look like at the fleet level? The first one is what we refer to as a moderately efficient workload. The cost is lower than the value, and there's a good net ROI. The one in the middle is a set of workloads generating a tremendous amount of value for the business, and we are able to run them at relatively lower costs. It's like a huge ROI to the business with few dollars. Finally, this is what we want to avoid at all costs: a set of highly inefficient services where the cost to the business greatly undoes the aggregate value that's provided.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/710.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=710)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/730.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=730)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/750.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=750)

### System Efficiency Through Kingsman's Approximation: Beyond Utilization Metrics

We'll come back to business value in a bit, but let's also look at another aspect of efficiency that we care deeply about: how do we understand efficiency at a system level.  I promised minimal math in all my talks, but there is some math here.  This is probably one of the most used mechanisms, at least at Netflix, for how we think about system efficiency. It's pretty simple in that you can think of performance engineering in general through what we refer to as Kingsman's approximation.  I highly encourage you to read the paper. It's linked out if you haven't already. In simple terms, it basically says you can predict the performance of computer systems by approximating how fast a system responds to new work.

When you keep adding load and work to the system, how fast does it respond? In the graph on the right, what's happening is as you push utilization higher, meaning you keep adding work, your arrival rate doesn't really change. You're approaching a point where there's nonlinear, almost like a hockey stick spike in the amount of time it takes to service new work. In simple terms, it's essentially like a queuing delay. The reason this is important is for at least two key takeaways. One is that if you focus on this equation with system load, service time, and arrival rate, playing around with any of these should actually change the outcome.

You can be at low efficiency, meaning your load number goes down and your service rate will be high or stable. You can play around with load balancing and other techniques, and you'll still have very predictable service times. Finally, if you keep pushing your system higher, far towards the right in terms of utilization, without any other mitigation techniques to buy down that risk, you are approaching the tail, which risks congestive failure. This is the law of physics. There is no way to get around it; you just have to design around it.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/860.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=860)

What it also means is it points to what my colleague likes to refer to as  that utilization alone is a lie. The reason they say that is there are three examples in this graph. All three services actually report that they are running at 30 percent utilization. But the behavior of these services is radically different. Focus on Service B, the line in yellow. If you see how much it spikes, we would refer to this as service time. We would refer to this service as a highly unreliable service. You cannot just look at 30 percent efficiency if that's your target and say we've achieved it and it's good.

Service C, on the other hand, does have some variance, but it's much more within the thresholds. Service A is just flat. So you cannot just look at 30 percent efficiency if that's your target and say we've achieved it and it's good.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/930.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=930)

This is another point to emphasize where we cannot and should not just look at pure utilization as a way to measure efficiency. We should use other levers like arrival rate, spreading load to truly improve system efficiency. So now we've built up mental models of at least at a high level how we understand the business value  of a workload, how we can measure system efficiency, and what it looks like when we actually apply it to the fleetâ€”a fleet of services. Now we're talking thousands of microservices and batch workloads.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/950.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=950)

### Fleet-Level Efficiency: Running Different Workload Classes at Optimal Utilization

This is a very interesting visualization. In this graph, we are basically saying that you have four different classes  of workloads. The first one is what we care about deeply. These are tier 0, no fallbacks. If that thing is down, your product is down. You have downtime. The second one is degraded, which means if those set of workloads are down, you have some degraded experience, but by and large, the product still works. The final piece is best effort, and then batch. Batch workloads will run to completion, but they can wait. You can queue them up.

What's pretty interesting though is that we're running all of them, all these four classes of services, very close to 50%, roughly. Now this is typically what compute spend looks like for a business. Let's assume this is $50 million or $100 million or whatever the number is. So we've allocated $100 million to run these four classes of services at 50% efficiency. Everyone's happy, there's a report that comes in, and people think the business is doing great. But is this efficient? The answer is no. For Netflix, this reflects a highly inefficient fleet.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1020.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1020)

What does efficiency look like? An efficient fleet for us actually looks something like this  where we are intentionally choosing to run our most critical tier zero workloads at a lower efficiency, not 50%, but 30%. And of course, this is a median or an average. We're choosing to run services where we can accept fallbacks or degraded experience at 50%. But more interestingly, we are choosing to run a whole cohort of services, which is our best effort and batch, at much higher utilizations, close to 60% and 70%.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1090.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1090)

Now, can you guess what the total dollar spent is for running the fleet with this shape? The interesting answer is this is also exactly $100 million. We are not adding more dollars to be more efficient. While this is a visualization, this is really true for how we've approached running services at Netflix over the past two years, and especially with some of our efforts like live ads. This has necessitated us to redefine what efficiency means for us. I'll re-emphasize that between the last slide  and this one, the compute spend is exactly the same.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1100.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1100)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1120.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1130.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1130)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1150.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1150)

So, metrics. We really like numbers  at Netflix. To summarize how we think about efficiency and what we like to measure: the first is where are we spending our dollars, obviously, because that matters to the business. This next one is very interesting. It's wherever we're spending the dollars, do those dollars mitigate risk  for us in some form. The third, which is where Kingsman approximation comes in, is what levers can we pull  to reduce costs, including that other efficiency distribution I showed, while managing risk. This is very important, and I'll keep re-emphasizing this: we never at Netflix think about efficiency without also considering reliability and risk. 

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1180.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1180)

### Reliability as a Complementary Construct: Measuring Impact and Recovery

The final takeaway, this is my addition to that, is there is a fascination, for the right reasons, on a myopic focus on just raising utilization, which means that we can all just be efficient if we just run things hotter. Our counter to that is yes, but it's just one lever. We can pull other levers which do a better job of not just managing efficiency, but also doesn't ignore risk.  Before diving deep into reliability, I think the key takeaway is if we had to articulate how we think about efficiency and reliability, we look at reliability as actually a complementary construct, not a thing on its own. Reliability is a complementary construct to efficiency. And what does it mean for us? At minimum, it means that the services we run, especially tier 0 and tier 1, they can respond with predictable latencies.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1230.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1230)

The low piece is obvious at high scale, but when we throw more load, when we accept those hockey stick spikes during a live event or a mad rush of a new season, we do not want degradation or wild spikes in latencies. That's the first tenet. 

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1250.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1250)

For batch workloads, which we do have to run a lot because we do all these massive encoding jobs for all our content, we want to schedule with low latencies, which means it's okay for the work to take X amount of time to run, but we do not want wild spikes in scheduling delays for our batch workloads. These are non-transactional. 

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1260.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1260)

We do not believe that zero failures are a thing, or we do not believe that we can prevent failures altogether. But we do care that failures are infrequent. We also care deeply that when things do fail, they recover fast. 

Finally, another often overlooked piece of reliability is that we care deeply about isolating our failure domains. We have to be excellent at articulating what a failure domain should look like. When that piece of software, workload, or system fails, what else is affected? How can we minimize that scope?

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1310.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1310)

So for reliability now, what do we measure? Instead of just measuring lines, which as an industry we've spoken about a lot and continue to do and it's important, what else do we measure at Netflix? We care about impact to business, like the blast radius. When one system or a set of systems fails, what is the cost to the business? 

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1320.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1320)

We care about how often it fails. A very good metric for this is mean time between failures. We care about recovery time, which means we measure mean time to recovery a lot. 

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1380.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1380)

Now, with an understanding of efficiency and reliability, this is the part where I dive into techniques that we actually use. But I want to pause here for a bit and re-emphasize that everything I'm going to talk about is not a mere focus on efficiency. It will, by definition, include how we can be efficient while managing risk. 

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1390.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1390)

### Understanding Compute Supply: Capacity Planning, Hardware Availability, and Buffers

For this to be viable, the first piece is that we need to understand what our supply side looks like. In my case, that means compute supply. Thanks to AWS being our vendor, we have to be very precise about the amount of compute that we want in every single region, what does that get allocated to, and how do we use it efficiently. 

The first part, as many of you are familiar with, is something that most organizations do at scale. It's called capacity planning. This essentially talks about how we allocate our dollars for compute spend specifically.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1410.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1410)

Now, this is pretty interesting as a visualization because this essentially shows us that at Netflix, for example, we have at minimum two very different sets of services. The yellow is the stateful layer, so this is our persistent stores like Cassandra, EVCache, and our key-value stores.  They are interesting in that they don't really have wild patterns of auto scaling, which means we have to run those nodes very predictably with a certain amount of quorum. We have to pre-reserve capacity for them. They cannot scale fast by definition. When nodes come up, they need 30 to 60 minutes to reconcile. This means how we think about capacity allocation is we have to reserve compute for them all the time. That's simple.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1450.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1450)

But there's this other set of services, which are the microservices fleet.  They are stateless by definition, and they have very different scaling patterns depending on the load. This is just a diurnal pattern, but it looks very similar with the peaks being slightly different when there is additional load.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1480.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1480)

The trouble is we can't reserve capacity for that peak. I mean, we could, but that would be highly inefficient. So what do we do?  In practice, the answer depends on how, based on your pricing with Amazon, well you can articulate what percentage of your dollars must go to what is called reserved spend. This whole area in yellow can include and should include not just stateful, but a percentage of the stateless fleet. But then on top of that, which is very important, is determining how much you should be paying for on-demand capacity.

The interesting thing here is that if you spend too much on reserves, you're being inefficient. If you spend too much on on-demand, which is arguably higher priced, you're also not being efficient. Netflix is smart about reusing our reservation trough. Because our workloads are diurnal with global traffic patterns, we have invested over the years in shifting capacity. We move these large batch workloads that get scheduled on this capacity, which means we have reservations that we're already paying for, so we might as well use that capacity to run our batch when services can't.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1560.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1560)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1580.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1580)

Another very important input to capacity planning is the availability of the hardware.  In that reservation graph, I'll go into families and hardware shapes, but we can't just assume that everything we want will always be available on demand, as much as AWS tries and does a fantastic job of being great partners for us. What this actually means for us is we have to account  for compute availability. This is a very interesting graph, probably one of my favorite ones. If you look closely here, I've color coded this for simplicity. This is from about a year ago. When you have stable generations of hardware, looking at fifth-generation and sixth-generation mostly, capacity availability is guaranteed almost. You have deep supply because your hardware vendors have done a really good job of racking and stacking. But then when you get to new generation hardware, this is seventh-generation AMD and Intel. This is where we want to be. We want to keep moving our fleet towards newer generation efficient hardware, but then we are approaching what we refer to as the volatile frontier.

It's interesting that we can't assume the hardware vendor here will be able to guarantee the same capacity that they can in the deep supply pool. The way to do this is we want to adopt new generation hardware, but incrementally, in parts of our fleet, and we're really good about this. The other piece is you need to be benchmarking new hardware, and this is one of my strongest learnings. Even if you do not dip your toes in the pool immediately, you need to have solid references of benchmarks of how these different families compare. A big shout out to our performance engineering team who do a lot of this heavy lifting for us.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1680.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1680)

The other piece, which is complimentary to my previous graph, is it's not just about the family. It's also about shapes.  What this graph is showing is on your X-axis, you have shapes going right from one Xcel to sixteen Xcel. This is great because the more we move towards the right, the more efficient we are by definition. We can push those instances higher, we can run much hotter on those instances, which is nice. But then we are also taking on the risk of the supply. The risk here is only because as you go higher or bigger on these instances, your vendor needs to do a much more complicated job of bin packing. Some of you might know my joke is that bin packing is an easy problem, right? What can go wrong? Bin packing is a really hard problem. So it doesn't matter who the vendor is. If you keep pushing towards the larger instances while they're efficient, you also need to be cognizant of the availability risks here. It's yet another trade-off for capacity planning.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1740.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1740)

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1760.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1760)

The third piece is very unique to what Netflix has done. We have added a third construct to our approach of understanding supply. We refer to this as buffers. It's essentially a way to understand headroom.   Buffers, by definition, mean that given a service, you have two territories within your service's headroom. One is what we refer to as success buffer. This is the part in yellow. That part means that beyond a certain point, so beyond here where your service is humming along nicely, if you throw more load in this area, how much additional load can the service still absorb without any impact to predictable latencies or otherwise? There's no degradation. That's what we refer to as success buffer. Now, if you push beyond that yellow line, you can still go higher until you fall over. And this small area there is what we refer to as failure buffer.

The reason it's important is that these constructs help us capacity plan when we allocate workloads to hardware. We must account for both success buffers and failure buffers. The other reason, which is probably the most important, is that I'm not using constructs like CPU cores, memory efficiency, clock frequency, or hyper-threading in this presentation, but that information is included in the map. I'm just not diving deep into those details.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1860.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1860)

What this allows you to do is take high-level constructs and transform the shape details of the family, threading, and clock frequency into these constructs, which shift across both instance families and shapes. By definition, this makes things portable. I really like this concept of success buffer and failure buffer, and we'll look into how Netflix uses that for capacity planning. 

The second most important thing, however, is that buffers, by definition, also vary by workload. How many people in this room have heard stories of data center clusters going down because they were running compaction workloads, for example? What this means is that stateless services, which we are very JVM heavy, are easier to reason about. They run with a certain amount of load, they take on more load, and beyond that they degrade. When you go to stateful services like Cassandra or Memcached, they have their own oddities that you need to plan for.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/1930.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=1930)

This background buffer here represents us planning for what it would look like if a set of Cassandra nodes ran compaction. How do we plan for that as part of the headroom? This means the math looks very different. We do not pin the success buffer of a Cassandra workload at the same set of thresholds. These are CPU numbers for simplicity, but you can see how they vary. We are computing this additional buffer and factoring it in for capacity planning. 

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2000.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2000)

In addition to workloads, buffers also vary by hardware type. These are numbers from our internal benchmarks, and what this looks like in practice is that you have both 7th generation AMD and Intel here, but also different shapes. What this shows us is that I can push a 7th generation 8XL much harder. This is 88% CPU versus what I would push with a hyper-threaded Intel. Also, the larger the shape, the harder you can push them. Depending on your service, tier zero has no fallbacks and tier one is degraded. Because we are planning for different amounts of buffer, we choose to intentionally run them less hot. Tier zero runs at 35%, tier one at 46% in this example. That's yet another thing we need to keep in mind when we plan for buffers in capacity planning. 

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2020.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2020)

### Understanding Compute Demand: Workload Profiling and Traffic Patterns

Now that we've understood the supply side, we also need to understand what compute demand actually looks like and where it comes from. We need to have this mental model before we can even get to the balancing piece. The first piece of understanding demand is we need to understand at a workload level what resources they need to do their job and how we approach estimating for that. 

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2030.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2030)

This is not complicated.  We are basically using CPU, memory, and network, the three most common ways to estimate work. On the CPU side, it is much simpler. Regardless of whether you're running on VMs or containers, you hopefully have a metric that tells you utilization. Memory gets slightly more interesting. One of my most interesting times in the last year or so was figuring out that because we are a very JVM heavy fleet, we can't just estimate for memory based on heap size. I also need to care deeply about the allocation rate, and that changes wildly depending on the garbage collector you're using, whether it's a parallel allocator, and so on. You need to come up with your own math for estimating memory based on the workload.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2100.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2100)

Network is also interesting. For stateless services, they're really network bound, but for stateful services we see very interesting patterns. Depending on what that workload is doing, while you have a baseline, you do need to account for bursts of network. What all of this means is we need to have our mental model for workloads, but we also need to observe them in production.  This is a real graph of us looking at existing production workloads and trying to fit, in this example, a CPU to RPS ratio.

This shows how much efficiency or utilization is running at for a given RPS. It obviously assumes fair load balancing and other factors. What this allows us to do is fit certain targets and scaling targets, which we'll discuss later in our presentation. The key takeaway here is that while you can have your models, the best way to do this is to observe your workloads in production.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2140.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2140)

 Another often ignored or overlooked piece is that you need to care deeply about service startup times. While you can add capacity and hopefully add it quickly, you need to factor in the amount of time it takes for your JVM to start up, for your OS to start, and for scheduling. If you don't account for this, you will have regressions and won't be compensating for the buffer you need to preserve before new capacity is added. Service startup time is the third element we model in our workloads.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2180.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2180)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2190.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2190)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2210.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2210)

 Now that we've talked about workload profiling and understanding usage at the workload level, let's look at what this looks like globally in terms of Netflix traffic.  Here's a simplification of our microservices architecture. We have devices coming in through the front door, which is our open-source gateway called Zo. Traffic goes to a federation layer called services, and they call their data stored through data gateways. However, in terms of load, when there is a 4X spike in load through the front door,  it doesn't automatically translate to a 4X spike to all services.

This is very interesting. It might mean a 2X spike for a tier zero workload that is critical to playback. It might mean a 3X spike for something critical to discovery, which is when you're browsing your rows or finding your shows. And then a 1.5X spike for something like ranking, which determines what show you'd like to watch next. This means we cannot apply the same math in terms of buffer and capacity allocation across the board. The summary is that microservices call patterns and your unique architecture have a tremendous impact on how you must estimate and understand demand.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2250.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2250)

 This is what it looks like from production for us. Because we run in four regions, there are some very predictable elements, which is the good part. There's a 24-hour periodicity, meaning that the same region within 24 hours will roughly have the same amount of usage every day, week on week. However, there's going to be a 10X delta between a regional peak and trough. The reason this is important is that it would be highly inefficient to reserve compute for this peak. We cannot do that. When you multiply that across four regions, that's a lot of dollars we don't want to be spending.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2320.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2320)

However, there is value to this predictability. It allows us to do things like failover. When US East 1 has an issue, we get ourselves out of that region, and this predictability allows us to say that if we move X amount of traffic out of US East 1, how much additional compute do we need to balance this traffic in the three other regions?  Then there's the non-predictable or unpredictable demand, which we also need to account for. This occurs when a title gets wildly popular, like a live event, the Jake Paul fight, or the Stranger Things launch. The demand looks very different in these cases. So by definition, we have to account for that as well.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2340.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2340)

### Balancing Supply and Demand: Service Capacity Modeling and Fleet Shaping

 Now that we hopefully have decent mental models for understanding both the supply and demand side, we get to the fun part of my job. We are tasked with perfectly managing demand and supply. I often joke that the task is asking you to predict the future. It's similar to asking whether you can time the market or know which stock is going to go up or down, and what could possibly go wrong if you're incorrect. Let's dive deep into what we do, but first, the tenets.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2370.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2370)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2390.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2390)

 When we talk about balancing, we need to be very precise that the efficiency we talk about needs to be a global versus a local optimization. This means you often can end up with very locally efficient optimization decisions, but they add risk at a global level. That risk can be for a service or for dollars.  The second thing is that buffers or headroom, as I explained, need to exist for our most critical services when it matters. The emphasis here is that it's not just enough for buffers to exist. They need to exist at the right time and for the right set of services.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2410.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2410)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2430.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2430)

Finally, you can imagine that everything I'm talking about involves this heavy lift  of managing both the supply side and demand side. This does not scale at a service team level. You need infrastructure teams like ours to do the bulk of this heavy lifting on both the supply side and demand side. We've talked about balancing, and now we're getting into how we do the  management of supply. The first piece is that while we discussed hardware availability, applicability, and workload profiling, we need to do the matching game for which workload should run on what hardware. Let's look into how we do that.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2450.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2450)

This is a visualization of  what we typically refer to as service capacity modeling. At minimum, a good simplification is that there are two kinds of modeling patterns we need to account for: one for stateless and one for stateful. We do more than that, but that's the minimum. What it shows is that you need to account for three key pieces of data. One is the hardware data itselfâ€”what family, what generation, what clock frequency, whether it's hyperthreaded or not, and what cores. The second is pricing data, which is very unique to your relationship with your vendor. The third is obviously workload data that we saw earlier.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2510.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2510)

When we put all of those together, we want to end up with a set of recommendations for every single service that we run in the fleet. That says what could be the most optimal hardware to run this on. It looks something like this. In this case, this is one of our stateless services. The model is essentially saying that based on all  these three pieces of data put together, you should be running on C7A2 Excels as the most optimal hardware. Bear in mind we persist this for all our services in a central depository. Because it's a C, my guess is this is some compute-bound workload, which means it would be inefficient for us to run that on an R. That's a very important takeaway.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2540.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2540)

What is that second row in the recommendations?  It's interesting because we could just have a list of preferred hardware types across the services and call it a day, but that doesn't really work in practice because you're not guaranteed hardware availability. So when you're not, what do you do? We also recommend a bunch of fallback shapes that each workload can run on. But the key part is these also need to be validated in production through canaries, and we talk a lot about testing in production. You'll see the consistency here is that because it's compute bound, we are recommending both C-series, the difference being sixth generation and seventh generation. My guess is the sixth generation is also cheaper versus moving to a seventh generation but a different hardware.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2600.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2610.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2610)

Finally, while we've understood hardware supply and demand at a workload level,  we need to see what it looks like after the recommendations. How do we apply it across the fleet? We have thousands of services. So we refer to this as fleet shaping.  What this means is that at any given time, if you look at three different pieces of contextâ€”performance, cost, and capacity in businessâ€”we can come up with an approximation of what the distribution of hardware shapes and families should be for every single region. This is what we're showing on the right: playback, discovery, personalization, tier zero, tier one. This is the rough split of M7As, R7A, C7A, and sixth generation Intel.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2650.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2650)

But what it doesn't do is factor in that we might want X amount of capacity in a region,  but what happens when we are short in a given family? That's quite possible. With seventh generation, for example, we really went through this. When those pools are tight, what do you do? One interesting thing here is that while seventh generation AMD M7As are short in capacity, we do have the option to find opportunities for other hardware shapes we could run on, slightly less efficiently maybe, but with much more robust availability. This is where the alternative shapes come in.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2690.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2690)

What that looks like in practice is in that example,  we're going to start this fleet-wide shaping loop. That's going to say, without touching tier zero services because those are our most important ones, what other workloads can we touch that are currently running on M7A, but because we are short ten thousand vCPUs, what does it take to start moving them?

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2730.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2730)

So R7. Now this obviously assumes I'm oversimplifying that you're talking mostly about memory-bound workloads, but if you do find that cohort of memory-bound workloads, we can drive on-demand this fleet shaping loop of moving them from M7XL to R7 XL. Our vendor can tell us whether they have enough capacity for that. 

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2750.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2750)

### Pre-Scaling and Dynamic Traffic Shaping: Managing Predictable Load Spikes

The other piece is now that we have estimates for how much capacity we need in the region, we've done the shaping. How do we actually account for traffic coming in through the front door? This is something we refer to as pre-scaling. We need to do something like this. It's both a science and an art. By definition, it involves things that I refer to as crystal ball predictions. We have to estimate what our peak viewership would look like on the launch of, say, Stranger Things season 56, whatever. 

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2800.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2800)

What that means is we also need to convert that from peak viewership to peak traffic estimation globally. Then we have to split that traffic estimation into regional traffic, and from there, we can go to compute demands. But finally, we also need to scale the fleet up because we are likely running hotter than we would want to when such a thing happens. In practice, this is what it looks like. We know that traffic is going to arrive when we launch a certain thing at 7 a.m. here. 

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2840.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2840)

We will use our math and modeling to pre-scale the fleet. We refer to this as min pinning. So you see, we scale up the fleet here, traffic arrives here, we handle it well. But then my favorite piece here is not this part, and I'll go deeper into this. It's called auto scaling. The reason for this is we do not want to scale up the services so much that we run inefficiently. We do want this auto scaling at the end. 

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2850.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2850)

And then finally, after the traffic has gone away, we want them to scale down, all automated. Service owners shouldn't need to do anything here. You can do pre-scaling in many different ways. You can do pre-scaling very inefficiently, as I call it, which means overscale. You're so far below your target, and when you add this up across the fleet, it's millions, hundreds of millions of dollars. 

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2860.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2860)

You can also do this. You can underscale, which is pushing services so hard that they risk falling over, especially tier 1, tier 2, and tier 0 running like near capacity, which is pretty bad.  But we would like to be sure. We would want to pre-scale in such a way that tier zeros are running efficiently, close to the target, a little below. Tier ones are running hotter, but not so hot that they fall over, and same for tier twos.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2880.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2880)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2930.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2930)

Now, what is efficient pre-scale? If you've done a job well, what does this look like?  It's in that same graph where we had pre-scale. You see the traffic arrive here. This green line spikes up, which means your average RPS goes high. But then we have another tool, which I'll go into slightly. We have load shedding kick in, but importantly, it's very brief and minimal. And then immediately as new capacity is added, we have RPS normalization. So we are humming along nicely again. 

And then like I said in the previous part, there is this brief period of auto scaling. So this is a graph that I really like because I like pointing to this as a reference. I need all the elements of this if we are doing pre-scaling right. It's not right if we did not have any load shedding, which means we overscaled. It's not right if we did not have any reactive scaling, which means we overscaled. And if we had sustained load shedding, that means we underscaled. We don't want to be there.

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2940.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2940)

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2950.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2950)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2970.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2970)

So how do we actually achieve this? I refer to this as dynamic traffic shaping, and it involves two key things. One is we have to take our existing traffic that is already in the regions  and find a way to redistribute them across our four regions. What this looks like is if I didn't do anything at any given point in time,  you will have this weird pattern of regional deltas. One region like EU West probably is going into trough, and the other region is running at a very different capacity. 

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2980.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2980)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/2990.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=2990)

Now, the reason it's a trouble is it makes it very difficult to plan for capacity. However, I could do this.  And if you look in the middle here, we have actually managed to balance this traffic. So this wild delta is now reduced. It's important because doing that is a precursor to us also figuring out what we do with the new traffic that hits our data centers. There are deep talks from Sergei and others on this, where Netflix uses its own DNS and authoritative resolvers to steer traffic into regions.  This is what it typically looks like without any intervention. You can see here that US East 1 is taking a lot more traffic, while West 2 and East 2 are not receiving as much.

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3020.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3020)

This is not balanced. For new traffic, we also like to publish shaping rules and force steering,  which can be suboptimal for latencies for a bit, but results in balancing. We're pushing people who would otherwise have gone into US East 1 into the two other regions and thus achieving balance. This is about the new traffic, and these two steps are complementary. You take out new traffic, redistribute it across regions, and then re-steer the new traffic which would have otherwise approximately landed in some of your most loaded regions.

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3050.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3050)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3080.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3080)

When new load  comes in, those wild deltas would be amplified so much that almost invariably we would be getting iced, as we call it, which means we would have not planned for the capacity, it's impossible, or if we did, the compute spend would be so large it would be highly inefficient. But with shaping, this is what we're able to achieve. We taper down the wild spikes,  and we can account for capacity a lot better. The peaks do get amplified, but they're well within range now and it enables us to operate more efficiently. Traffic shaping is a massive tool to have in your toolkit.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3100.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3100)

### Reliability Techniques: Reactive Scaling, Load Shedding, and Lessons Learned

Now that I've talked so much about  balancing and perfectly balancing supply and demand, we all know that this is exceptionally hard in practice. We can hope that's the case, but we also need to have strategies for how to do this or how to manage risk when that's not true. I'll quickly go through some final levers that we pull to manage risk, and these are the reliability techniques.

[![Thumbnail 3130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3130.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3130)

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3140.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3140)

The first one is reactive scaling. When you do not get capacity planning perfectly accurate, you need to account for how fast you can add new capacity.  For this, we need three things. At a service level, we come up with scaling targets, and at least three different numbers.  We refer to something as target tracking, where this is the threshold when your service hits this. We want to start injecting new capacity, but slowly. We have another threshold called hammers, very aptly named, where when you hit this, you start adding more capacity, but much faster, both in volume and scale. Finally, if you're still not able to keep up with all the scaling, we want to shed traffic.

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3180.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3180)

[![Thumbnail 3210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3210.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3210)

At a cluster level, this is all automated.  Our systems fire events called CPU-based scale-up. It's basically saying that because the target tracking fired, we need to bump up the fleet shape, spin up new instances. This is basically where the desired compute should be, and this is where it is at the moment. The other complementary side of it is we cannot hold on to capacity forever.  When the traffic has gone down, we want to give that capacity back. We refer to this as scale down, and this is also very automated.

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3230.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3230)

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3240.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3240)

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3260.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3260)

What's interesting is that while I talked about the fleet loop previously, that can only operate in the order of months because it's hardware planning, procurement, and driving that across the fleet.  From there, however, we just talked about reactive scaling, and what it's done  is give us another lever that reacts in the order of minutes, not months. This is a complementary reliability technique where we cannot just rely on monthly or quarterly changes to fleet shape. We need something that operates in the order of minutes.  But order scaling can be slow. You can have delays in your control plane, delays with your OS startup, and if you're running JVMs, JIT and JVM startup can take a while. I really recommend watching Ryan's talk from last year's re:Invent. He goes a lot deeper into how we fix some of those issues. The key takeaway is we cannot rely on auto scaling alone if we truly want to mitigate risk.

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3290.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3290)

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3300.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3300)

So what do we do when we cannot add capacity fast? We need to  shed load. What that means is we can shed load in two ways. We can shed everything, which we refer to as bulk load shedding,  or we can be much more opinionated and smarter about which load we shed. We refer to this as priority-based load shedding.

[![Thumbnail 3310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3310.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3310)

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3330.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3330)

[![Thumbnail 3340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3340.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3340)

[![Thumbnail 3360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3360.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3360)

In the same graph, what this looks  like is your service operating in a good mode, functioning healthily, and then new traffic comes in. Additional traffic arrives and you're still in that success buffer territory. Everything is green. But then further load gets added, and what happens is we start approaching what I refer to as the failure buffer.  You can see this is where we must pay attention. This is the cut point where we want to start shedding load.  The thing we want to avoid is if we did not do that, we would end up here. This is us falling over, and you can see that this is mayhem. If you take the analogy of an actual traffic highway, the important thing to keep in mind is that while you can drop traffic, there is impact. There's going to be customer impact if you just drop traffic indiscriminately. 

[![Thumbnail 3400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3400.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3400)

So what we actually want to do is we only want to shed this traffic here. My friend's analogy here is that on a very packed highway, if you're trying to add people and you have an ambulance which really needs to get onto that highway, and then you have this other person out for a weekend spin with his Porsche, that person can probably wait. So how do we do that? We shed that load here. This is what we refer to as prioritized load shedding. When done well, this is what it looks like: we want to move  towards the right incrementally as we hit higher utilizations. We first start with bulk traffic where there is minimal degradation and best effort, and only later do we even touch shedding critical traffic.

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3430.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3430)

This is our way to ensure that you have minimal impact, minimal customer impact, even when you're underscaled or can't keep up with scaling and we need to shed load. This is what it looks like from a real production graph.  There are three things here that we start non-critical shedding much earlier. Critical shedding waits until much later. But in all of this, while these errors grow because we are shedding traffic, we have successfully maintained our RPS rate, and this is key. That means by definition, we had minimal, very minimal degradation to our customers and service experience.

[![Thumbnail 3460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3460.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3460)

[![Thumbnail 3470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3470.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3470)

[![Thumbnail 3490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3490.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3490)

Now I'll bring this back to fleet loops. What does this mean? We had months for proactive scaling, we had minutes for reactive auto scaling, and then finally, when that wasn't good enough, we were able to add another layer which operates in the order of seconds.   The hammer rules, which is adding bulk capacity fast and load shedding, operate for us fleet-wide in the order of seconds.  Now we have a full picture of what techniques add up together to mitigate risk in addition to just pure compute spend and capacity planning.

[![Thumbnail 3530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3530.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3530)

I'm going to quickly summarize with lessons and key takeaways that I think are valuable. The first is whenever you do an exercise like this, you must think about both proactive and reactive levers. Because I have a network background, I like to think about this in terms of pre-ingress and post-ingress. Pre-ingress are the things that you can do before traffic gets into your data centers. This includes shaping your fleet, predictive scaling, and traffic shaping like I showed.  Then you have to also account for what happens after you already have absorbed additional traffic into your data centers. These are the reactive techniques: reactive scaling and load shedding.

[![Thumbnail 3560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a86ec836346a60d9/3560.jpg)](https://www.youtube.com/watch?v=K-2u50e0VzA&t=3560)

Second, I think this is obvious, but I feel like this gets lost a lot: you need to have end-to-end systems thinking. You cannot look at pure compute and pure spend as the only way to buy down risk. The other thing is that if you operate at scale like ours, one great thing is that efficiency compounds by definition. Finally, you need end-to-end traffic management. The final piece is my favorite statement, which my dear colleague Joey appreciates: math is our safety blanket, as I like to call it.  If you take some basic concepts from economics and apply some math, you can solve some of the hardest problems.

I do want to give a shout out to Joey, who's been my partner in crime for the past year or two. You can please check out our Oasis library, which is for service capacity modeling, and we have a talk on that too. Thank you very much, and I hope you enjoyed the session. Have a good rest of the day.


----

; This article is entirely auto-generated using Amazon Bedrock.
