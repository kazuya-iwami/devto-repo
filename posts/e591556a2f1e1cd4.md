---
title: 'AWS re:Invent 2025 - The power of cloud network innovation (INV213)'
published: true
description: 'In this video, AWS Vice President Robert Kennedy presents comprehensive networking innovations spanning AI infrastructure to global connectivity. He details UltraSwitch, SIDR, and hollow core fiber enabling Project Rainier''s million-chip AI cluster for Anthropic''s Claude training. Key announcements include VPC Encryption Controls, Network Firewall Proxy, Transit Gateway Native Attachment, API Gateway Portals with MCP support, Response Streaming, Cross-Region PrivateLink for AWS services, Application Load Balancer Target Optimizer for AI workloads, and AWS Interconnect Multi-cloud with Google and Microsoft. The session covers Fastnet transatlantic cable, new regions in Thailand, Taiwan, Mexico, and New Zealand, plus flat-rate CloudFront pricing attracting 15,000 subscriptions in two weeks. Salesforce demonstrates multi-cloud private connectivity use cases, while Epic Games achieved 268 terabits per second through CloudFront for Fortnite launches.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/0.jpg'
series: ''
canonical_url: null
id: 3088245
date: '2025-12-06T06:40:39Z'
---

**ü¶Ñ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


üìñ **AWS re:Invent 2025 - The power of cloud network innovation (INV213)**

> In this video, AWS Vice President Robert Kennedy presents comprehensive networking innovations spanning AI infrastructure to global connectivity. He details UltraSwitch, SIDR, and hollow core fiber enabling Project Rainier's million-chip AI cluster for Anthropic's Claude training. Key announcements include VPC Encryption Controls, Network Firewall Proxy, Transit Gateway Native Attachment, API Gateway Portals with MCP support, Response Streaming, Cross-Region PrivateLink for AWS services, Application Load Balancer Target Optimizer for AI workloads, and AWS Interconnect Multi-cloud with Google and Microsoft. The session covers Fastnet transatlantic cable, new regions in Thailand, Taiwan, Mexico, and New Zealand, plus flat-rate CloudFront pricing attracting 15,000 subscriptions in two weeks. Salesforce demonstrates multi-cloud private connectivity use cases, while Epic Games achieved 268 terabits per second through CloudFront for Fortnite launches.

{% youtube https://www.youtube.com/watch?v=RkdPAFJEPSA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/0.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=0)

### Introduction: The Evolution of Networking and the AI Revolution

 Please welcome to the stage Vice President of Network Services at AWS, Robert Kennedy. All right, hey folks. You're all here for networking, right? Great. I'm glad to see there are as many passionate people about networking as I am.

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/20.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=20)

 Folks, I'm Rob Kennedy. I'm the Vice President of Network Services. Over the next hour, I'll walk you through the highlights from the past year. I'll introduce you to a few exciting innovations we're unveiling at this re:Invent and share some insights, hopefully you'll find valuable for your own business.

I have been with AWS for over 16 years. I have been in the networking industry since I graduated college. I have a strange fascination with networking. How far it's come has just been absolutely amazing for me. It has been the heart and soul, I think, in terms of our innovations and allowing us to move forward. Going from what I thought was basic network connectivity back when I graduated to some of the sophisticated intelligent infrastructure that we have powering our modern applications today is just fundamentally amazing.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/80.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=80)

 We've also over the time, back over those 16 years, really seen some amazing workload transformation, from simple web applications to massive artificial intelligence models and global streaming events that reach millions of users simultaneously. From personal devices to orbiting satellites, network infrastructure underpins about every aspect of modern day life, accelerates breakthroughs in medicine, scientific research, and next generation large language model development and use. At AWS we build networks that keep things running, while also pushing the boundary of what's possible.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/120.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=120)

 And at AWS that starts with owning the entire stack from top to bottom, and bottom to top, and the fiber in the ground. Under the sea, physical hardware, the operating systems that sit on top of that hardware, we control the infrastructure, the data centers, we build them from the ground up, power, cooling, every aspect, all the way out to the internet edge is owned and operated by AWS and really nobody else can say that. We've also built our own software defined network that sits on top of all the physical foundation and it gives us unprecedented control and flexibility.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/160.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=160)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/170.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=170)

 We don't just build networks, we engineer them from the silicon to the software. Full stack control means we deliver reliability while simultaneously pushing the boundaries of what networks can do next.  Networking supports workloads of every scale, from the smallest application to the most demanding high performance computing environments. Right now, no technology is pushing those boundaries more than AI. Models are getting bigger, more complex, and more demanding by the day, rapidly approaching the very limits of what traditional networks can handle.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/190.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=190)

 Models have already crossed the 1 trillion parameter threshold, but it's not just about size. Customers need to continuously train these models to improve performance and then deploy them globally to serve businesses and customers around the world. The network is no longer just plumbing in the background. It's the backbone that makes it all possible.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/210.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=210)

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/220.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=220)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/230.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=230)

### Why Networking Matters: From Atoms to AI Accelerators

 So let's start with that simple question. Why does networking matter?  Think about atoms on their own, they can be interesting, but when they connect and combine into different molecular combinations, they make amazing things.  Water, air, and my favorite one, which I may have had a little bit of before I came on stage here, being Irish is whiskey. And hopefully you all know that the Irish invented whiskey, and if you ever hear any Scottish person tell you differently, it's a lie, they have no evidence whatsoever.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/260.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=260)

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/270.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=270)

Same is true in AI.  A single AI accelerator, such as an AWS Trainium chip or GPU, it's quite powerful by itself.  But to train a large ML model or run inference at scale, you need something more powerful. You need a distributed system comprised of over a million GPUs, all connected through low latency, high performance networking. Think of it this way, those accelerators are atoms, the network, that's the bond that connects them and transforms them into something far more powerful. At AWS we work to strengthen those bonds at every level, from the smallest chip to chip link, all the way up to hyperscale clusters.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/300.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=300)

 So let's zoom in and look at the basics. Let's start with the simplest use case. Two AI accelerators working together to train a large language model.

AI accelerators work together by sharing learned knowledge, adjusting, and passing data back and forth like dance partners in perfect sync. I'm not going to do any dance moves because I'll probably fall over. But when these AI accelerators sit in the same server, connected by PCIe or NVLink, they communicate in nanoseconds. It's a beautiful partnership, but here's where our story gets complicated.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/340.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=340)

Today's AI models are massive. We're talking about 1 trillion parameters and growing, so we have no choice. We must break up the perfect partnership and scale out across hundreds of thousands of accelerators. Why do we do that? Well,  the instant we step outside that single server, physics becomes our greatest enemy. We move to the next server, latency penalty. Move to the next rack, bigger penalty. Move to the data center, even bigger penalty. Move to another region, now we're talking big challenges.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/360.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=360)

To help reduce that latency, remember we  go all the way down, even to the physical fiber itself that we put in the ground. So we're constantly innovating, even at that level. And that's where technologies like hollow core fiber, which we talked about last year and just launched, come in. This technology is a revolution in material science and manufacturing. Imagine the precision it takes to fabricate a long strand of fiber with a perfectly hollow core. The benefits are worth it. Instead of traveling through glass more slowly, the information travels through the hollow core.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/410.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=410)

Since last year, we have deployed hollow core fiber live on our network, and we've continued to deploy thousands of kilometers across our data centers. And it ends up with a 30% improvement in latency, which is seriously meaningful. But still, every boundary you cross, every step you take away from that perfect accelerator to accelerator connection,  physics comes into play. At scale, every connection matters exponentially. The bigger the system, the more latency compounds.

Training a large language model with over 1 trillion parameters and growing may require billions of messages to move between AI accelerators. Why billions? Because every microstep of the process, exchanging parameters, gradient updates, intermediate matrix multiplication, and transformer layers may be divided across 8 to 16 accelerators, and each of those accelerators must share partial results hundreds of times per second. Delays, even on the smallest scale of microseconds, can impact job performance. That's why optimizing communication isn't just important, it's essential as clusters grow.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/470.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=470)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/490.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=490)

### Building AI Infrastructure at Scale: Trainium2 UltraServers and EC2 UltraClusters

That brings us to the trillion dollar question. How do you build AI infrastructure that's fast enough, reliable enough, and scalable enough to train the models that will define the next decade?  So, let's start with a concrete example of how we think about scale at AWS. First, let's take our Trainium2 UltraServer. It's not just a server, it's a new category of compute. We pack 64 Trainium2 chips into a single UltraServer, connected with high bandwidth,  low latency NeuronLink interconnects.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/520.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=520)

That's 512 neuron cores working together, and here's the key insight. Those 64 chips aren't just randomly connected. They're arranged in a carefully designed topology, a 2D torus within each instance with corresponding cores linked in circular pathways that bridge across instances. This isn't just accidental. The network topology directly determines what's computationally possible. The depth of interconnectedness is what  makes 1 trillion parameter models feasible.

Whether you're training or running inference on models, the UltraServer's interconnect design ensures 64 chips can work together seamlessly, maintaining the performance characteristics you'd expect from a single massive instance. UltraServers are great for those models that need scaled up infrastructure. And when you need to scale beyond an UltraServer, when you're connecting hundreds of these units together, the network has one job: make it all feel local. Every AI accelerator should think it's talking to its neighbor, even if it's sitting racks away. That means microsecond latency, not milliseconds, and massive bandwidth across every connection.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/570.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=570)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/590.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=590)

That's exactly why we build EC2 UltraClusters. Petabit scale,  non-blocking fabrics that connect over a million training chips or GPUs with predictable ultra low latency. These clusters have already powered some of the largest AI training runs in the world. And the same architecture that drives training performance delivers rock solid inference too.  The same network fabric that synchronizes thousands of AI accelerators during training seamlessly coordinates inference requests across model replicas, deployed fleets of Trainium

or GPU instances for real-time apps, massive models, heavy traffic, and you'll still get consistent response times without congestion or slowdown. But scaling to these massive clusters isn't just about connecting more hardware, it's about choosing the right network topology. So let me explain the fundamental trade-offs we face.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/630.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=630)

ML servers have multiple accelerators, each with an index,  and ML clusters often use wired rails connectivity that creates mini networks to connect accelerators of the same index to improve performance. This approach is great when all is working well, but here's the reality. Any single link can affect your entire training run, causing work to restart from checkpoints. And in clusters with a quarter of a million or more links, failures are going to happen.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/660.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=660)

### Introducing UltraSwitch and SIDR: Achieving Performance with Resiliency

The conventional alternative is to connect all  accelerators using a top-of-rack switch. This gives you the flexibility to route around failures, but it decreases bandwidth efficiency and increases accelerator interference. The trade-off is raw performance versus resiliency. Training job utilization is driven by network stability, not just raw latency. ML training jobs create periodic checkpoints that pick up in the face of failures. However, recovering from checkpoint failures is relatively costly.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/700.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=700)

But can we develop technology that gives us the best of both performance and resiliency? Well, of course you know the answer. We're AWS, of course we can. This is exactly why we developed UltraSwitch, a new first hop networking device in our data centers,  homegrown at AWS. UltraSwitch uses a variety of information such as accelerator index, cluster topology information, and network capacity to align AI accelerator traffic consistently down dedicated adaptive rails.

We allow for optimal rails-oriented data flow when possible, and when faced with failures, we quickly recover to provide the network stability. In the event of failures or periods of congestion, UltraSwitch can adaptively route traffic around an impaired area while keeping flow collisions to a minimum, enabling the job to continue without disruption.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/740.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=740)

 We also examined the shorter links, connections between devices in a rack and between racks. We typically have used copper interconnects for their reliability, but the cabling density and the length requirements make copper impractical when you need to route hundreds of thousands of thick cables through limited rack space. So traditional optics solve the cable size and reach challenges, but they're more expensive and they're less reliable than copper.

We needed to reach the reach and size benefits of optics with the reliability and cost effectiveness of copper. So again, thinking about all of the things that we own across the stack, this allows us to get into even the optics. And so ultra short reach optics delivers exactly that. By limiting cable lengths and minimizing connectors, we enable innovations in optics design and manufacturing that are not possible with traditional optics. The result is reliability that surpasses copper with three times improvement in service to network link reliability plus enhanced power efficiency.

But networking isn't just about the hardware innovation. When we're training models at massive scale, hundreds of thousands of AI accelerators exchanging gradients every millisecond, the network can't afford to pause. But traditional routing protocols were never built for this, taking seconds to detect failures, propagate routing information, and recompute best paths for traffic flows. And at AI scale, a few seconds of delay could still stall an entire training run, wasting compute time and money.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/830.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=830)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/840.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=840)

 To fix it, we built SIDR, Scalable Intent-Driven Routing, a part of our intent-driven network  where UltraSwitch handles deterministic forwarding at the switch level, making sure packets never collide. SIDR operates at the network control plane. It continuously monitors the network, detects congestion and failures in milliseconds, and adapts paths instantly, so the ML layer does not see any network interruption.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/870.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=870)

The result is rapid convergence, with the ability to converge ML fabrics consisting of a quarter of a million physical links in less than 500 milliseconds.  No job restarts, no idle accelerators, just smooth, uninterrupted training, even when the unexpected happens. Think of it this way. UltraSwitch gives you predictable paths through each switch. SIDR makes the network itself adaptive and self-healing. Together, they form a fabric that's fast, predictable, and resilient, exactly what large-scale AI training demands.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/900.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=900)

 And in just three years, AWS has deployed over 300,000 switches and more than 40 million physical ports dedicated to ML traffic.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/920.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=920)

That's growing faster than our core network itself, and it reflects the extraordinary demand for AI. Just recently,  we activated Project Rainier, one of the world's largest AI compute clusters, which is now fully operational. AWS collaborated with Anthropic on Project Rainier, which features nearly 500,000 Trainium2 chips, and actually we've just gone up to a million. It provides more than five times the compute power Anthropic used to train its previous AI models. Anthropic is scaling to more than one million chips already, and they are actively using Project Rainier to build and deploy its industry-leading AI model, Claude.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/960.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=960)

### From Physical Networks to VPC: Software-Defined Networking Foundation

We built one of the world's largest purpose-built ML networks, delivering the scale, performance,  and reliability our customers need to train and run the most advanced models. These networks don't just accelerate training, they also power inference at scale, from serving interactive chatbots to running recommender systems in real time. Whether you're building or serving your models, AWS networks are designed to keep them fast, reliable, and cost efficient. Everything we do here passes on to our entire network as well.

These clusters don't exist in isolation. They must connect securely and seamlessly to the rest of your environment, your data pipelines, your applications, and your users. That's where VPC services come in. Now let's shift from the inside of the cluster to the outside and look at how we extend these innovations into VPC, the network foundation that every AWS customer builds on.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1020.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1020)

I've mentioned before, everything starts with an atom.  The atoms bond into molecules and then further connect and scale, extracted to more complex structures like DNA, cells, organisms, and ecosystems. You can think about networking services in a very similar fashion, allowing you to build complex applications from simple foundations, with each component building upon the one below while hiding complexity.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1050.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1060.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1060)

We handle all the physical networking we just discussed, from GPU communication  through PCIe and NVLink to UltraClusters and connecting across data centers across the world, so you don't have to. When you move from prototype to production,  you need networking that can isolate different workloads from each other, enforce security policy, and scale globally. Whether you're running AI training, inference applications, high-frequency trading, or retail platforms, you should focus on the application logic, not cable management or switch configurations.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1100.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1100)

While you see many providers offering racks of GPUs, you need a solid network that can isolate your workloads from each other. At AWS, we've spent nearly two decades perfecting software-defined networking. We provide you with a software abstraction that gives you programmable access to the network without worrying about the underlying infrastructure.  We built this from the ground up to deliver microsecond-level latency and deterministic performance while hiding all the physical complexity. That's exactly what VPC represents, the programmable foundation that every production workload on AWS builds upon.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1120.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1130.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1130)

 So what does that mean for you? It means you get your own private section of the AWS cloud, completely isolated from everyone else. Within that space,  VPC gives you secure boundaries with complete traffic control. You decide what connects to what, when, and how. Whether you're running AI workloads or traditional applications, production environments require strict separation between workloads, compliance domains, and security zones.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1150.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1150)

Under the hood,  AWS builds VPCs as a software-defined network with focus on security, high availability, and resilience. Customer packets never leave AWS-controlled infrastructure. We can do this because of another innovation that we've talked about many times, the AWS Nitro System, a combination of dedicated hardware and lightweight hypervisor, enabling more performance, enhanced security, and faster innovation.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1180.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1180)

VPC control plane programs Nitro-based network virtualization,  creating millions of independent routing and security domains. Every one of them is isolated, programmable, and elastic, all built on the same foundation. It scales without redesign. We operate the largest VPC deployments in the industry with hundreds of thousands of resources in each, handling billions of route updates and petabits per second of throughput daily.

Need a new environment for AI training with a strict blast radius? One API call. Need regulated workloads with compliance isolation? Same building blocks, same controls, instantly available. Whether you're running a single workload or managing more than 50,000 VPCs, the same foundational technology scales with you.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1230.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1230)

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1240.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1240)

### Application-Centric Networking: VPC Lattice and Cross-Region PrivateLink

When we first introduced VPC, the world was a lot simpler.  Most customers ran in a single network: one VPC, one region, one set of subnets. Everything lived in a single place.  Fast forward to today, startups launch on AWS with microservices spread across multiple VPCs from the start. Front-end services in one VPC, APIs in another, data processing in a third. Meanwhile, some of the world's largest enterprises operate tens of thousands of VPCs, each mapped to a specific team, application, or environment.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1280.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1280)

Your VPC foundation provides isolation and security, but true power emerges when we connect them intelligently. VPC peering creates direct private links between networks. Transit Gateway scales an architecture of hundreds  of VPCs through a single managed hub. Internet gateways open your applications to the world, and NAT gateways enable private resources to reach out securely without any inbound exposure.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1300.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1300)

But here's what's fundamentally shifted: developers have moved beyond infrastructure thinking. They no longer think in terms of VPCs and subnets.  They think in services, APIs, and connections. This is how applications work today. The code doesn't care about network topology; it thinks in business logic, and the architecture reflects this reality. A single modern application might span multiple VPCs, stretch across regions, and live in different AWS accounts entirely.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1330.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1340.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1340)

From the developer's perspective, the ask remains beautifully simple:  connect securely to a database, reach a machine learning model, or access object storage. That's why we built a comprehensive application networking suite that puts your applications at the center,  like molecular bonds that hold complex structures together. Let's explore how we're making this vision a reality.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1350.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1350)

 The real magic happens when we build application-centric networking on top of this VPC foundation. That brings us to VPC Lattice, the ultimate expression of intent-driven networking. You'll see this intent-driven networking even in our foundational physical network, and you see it at our software-defined level. Lattice is an application networking service that eliminates the complexity of routing tables, peering configurations, and IP overlap challenges.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1380.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1380)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1400.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1400)

 Instead of configuring network paths across tens of thousands of VPCs, you simply say this service is allowed to talk to that service in a secure manner and define the trust level. Lattice automatically discovers endpoints, enforces zero trust policies using IAM, and continuously monitors connectivity. No service meshes to deploy,  no gateways to manage, no complex addressing plans‚Äîjust application-centric networking that matches how you build software. With Lattice, you get networking for your applications without needing extensive networking expertise.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1420.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1420)

Your application connectivity requirements don't stop here.  As you scale, your applications don't stay in one region. They follow your users, your data, your business requirements all across the globe. However, multi-region architecture shouldn't mean multi-region complexity for your applications. Last year we launched Cross-Region PrivateLink for SaaS services: private connectivity to third-party services across any region without internet exposure or routing complexity.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1450.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1450)

Today I'm excited to announce that we have the full extension of this feature to meet your needs:  Cross-Region PrivateLink for AWS services. Now your applications can access S3, DynamoDB, and other AWS services across regions using the same private connectivity model. The abstraction handles the complexity while your application simply connects to the endpoints it needs. This is the power of layered abstractions. Each new capability builds upon proven foundations, creating increasingly sophisticated possibilities while maintaining simplicity for developers.

### Optimizing AI Workloads: Application Load Balancer Target Optimizer and API Gateway Evolution

But private connectivity is just a foundation. The next challenge becomes ensuring optimal performance when distributing requests to those services. We've learned something critical: infrastructure patterns that work for traditional applications can hurt AI workload performance. Our customers running large language models experience inconsistent response times and poor resource utilization despite sophisticated load balancing.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1520.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1520)

They'd have idle servers while others were overwhelmed, not from CPU or memory issues, but something more nuanced. When you're running large language models,  server capacity isn't just about CPU. It's about model state, memory utilization, token processing complexity, and the specific type of inference request. A server might be technically available but completely unsuitable for the next request based on its current workload characteristics.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1560.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1560)

Traditional load balancing algorithms based on CPU or connection counts miss these real bottlenecks in AI workloads. We needed something entirely new. That's why we built Application Load Balancer Target Optimizer, a load balancing solution designed specifically for the unique demands of AI and high-performance compute workloads.  So you install the agent on the target, and the agent can configure the maximum number of concurrent requests that you want that target to receive from the load balancer. The agent tracks the number of requests the target is processing. If the number goes below the maximum request number, the agent sends a signal to one of the load balancer nodes. The node registers that signal, and when the new request arrives, it knows that it can send the request to that target.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1590.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1590)

 This also brings us to a critical realization. In this world of distributed services and cross-region connectivity, every application becomes an ecosystem of APIs. And managing that ecosystem requires intelligent coordination. That's where API Gateway comes in, not just as another networking service, but as the intelligent conductor managing how your APIs are documented, exposed, updated, secured, and accessed. Here's what we've learned from processing over 140 trillion requests in 2024, a 40% increase year over year across 400,000 accounts. Most successful applications aren't just well coded, they're well connected.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1650.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1650)

So let me share a story that helps illustrate this evolution. Ita√∫, the largest private sector bank in Brazil, started with what seemed like a simple architecture, a mobile app connecting three microservices: front-end authentication,  and data processing, each in their own VPC using the patterns we just discussed. But applications never stay simple. Within months, they were integrating machine learning models for personalization, adding real-time analytics, connecting third-party payment services, and building partner APIs.

Today, Ita√∫ operates over 5,000 APIs across 4,000 lines of business, each with different security requirements, performance characteristics, and scaling patterns. This distributed architecture now handles more than 160 billion API calls per month across 13,000 AWS accounts, a scale that transforms simple networking decisions into critical infrastructure challenges. This is exactly why we've been innovating rapidly in API Gateway. In the application-centric world, your APIs aren't just interfaces, they're the control plane for your business logic.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1730.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1730)

### New API Gateway Capabilities: Portals, Response Streaming, and Model Context Protocol

That's why I'm excited to introduce three new major capabilities in API Gateway. First, API Gateway Portals transforms how you scale API ecosystems across your organizations and with external partners. In today's interconnected world, your APIs aren't just technical interfaces, they're the business enablers that drive revenue, partnerships, and innovation.  Traditional approaches create significant friction through fragmented documentation, manual API key management, and inconsistent developer experiences, directly impacting adoption and business outcomes.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1750.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1750)

API Gateway Portals deliver a unified self-service solution.  Developers get searchable API discovery, auto-generated documentation, and immediate interactive testing. Providers gain valuable usage analytics and automated governance for security and compliance. This reduces API management overhead from weeks to minutes, so teams can focus on building great APIs instead of managing infrastructure.

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1780.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1780)

Another exciting launch is Response Streaming for API Gateway,  purpose-built for the AI era. Traditional APIs follow a request-response pattern where the entire payload must be generated, buffered, and transmitted as a complete unit. This works for small JSON responses but creates significant problems for AI-driven applications. When serving large language model responses or processing complex analytics, buffering entire responses creates suboptimal user experiences.

Users stare at loading screens while the backend generates a complete 2,000-word response, even though the first paragraph could be delivered immediately. This also ties up valuable compute resources and memory.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1830.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1830)

Response streaming support enables real-time streaming of API responses as they're generated, delivering content immediately rather than waiting for complete generation.  This dramatically reduces perceived latency and improves resource utilization by freeing up memory and compute as data transforms and transmits. It enables entirely new user experience patterns: real-time collaborative editing, progressive data visualization, and interactive AI conversations that feel natural and responsive.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1860.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1860)

For API providers, you can handle more concurrent requests with the same infrastructure, reduce memory pressure,  and provide instantaneous user experiences instead of batch process delays. This is critical for the AI era. As applications become more conversational and interactive, users are going to expect things to flow naturally.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1890.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1890)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1900.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1900)

I'm also pleased to announce that API Gateway now supports Model Context Protocol proxy functionality, enabling seamless transformation of existing REST APIs into MCP-compatible endpoints through integration  with Amazon Bedrock's AgentCore Gateway service. Organizations face significant challenges making  existing APIs accessible to AI agents. Currently, you must build custom MCP servers for each API, implement complex security translations, manage data across protocols, handle capability discovery, and monitor AI agent access separately, all while maintaining enterprise security and governance.

API Gateway's MCP capabilities resolve this with automatic protocol translation between MCP and REST, including context management and capability advertisement. It provides API key and IAM-based authentication, supports both public and private APIs, and includes enterprise controls for governance and compliance. Now your AI agents can seamlessly connect with existing and new REST APIs through MCP, powering agentic workflows with built-in support for observability to troubleshoot and optimize your applications.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/1980.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=1980)

### Intelligent Security at Every Layer: VPC Encryption Controls and Network Firewall Proxy

Now that we've established the foundation of application-centric networking, let's address what makes this all possible: security. Intelligent applications require security that's equally intelligent and adaptive. Your network security must adapt and respond without manual intervention.  The more connected your applications become, the more sophisticated your security must be.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2020.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2020)

AWS protection spans every single layer of the network stack. It begins in the silicon with the AWS Nitro system enforcing hardware-level isolation, extends through the physical network with encrypted transit, continues through software-defined boundaries of VPC with network ACLs and security groups, reaches the application layer with load balancer security policies and DDoS protection at the edge, which extends to every public API. Security is the foundation that enables every abstraction that we build.  Security must be deployable anywhere, effective at scale, and intelligent enough to adapt. That's why AWS security is programmable and integrated into every workflow. It must be as intent-driven as networking itself.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2050.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2050)

From physical hardware isolation to logical network isolation, VPC carries the same isolation principle into software-defined boundaries. We're extending that integrated security approach to solve one  of our customers' most persistent operational challenges. Organizations across financial services, healthcare, government, and retail face operational complexity in maintaining encryption compliance across their cloud infrastructure. Without centralized visibility, customers resort to manually tracking encryption across different network paths, having to piece together multiple solutions, managing complex public key infrastructure, implementing application layer encryption overhead, and manually demonstrating compliance with regulatory frameworks like HIPAA, FedRAMP, and others.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2090.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2090)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2100.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2100)

VPC Encryption Controls  address these challenges with simple controls. Available now, we've already seen  a huge number of VPCs already start to be encrypted just over the last week. In just a few clicks, you can audit the encryption status of your traffic.

Identify VPC resources that allow plain text traffic, and modify them to enforce encryption across your entire network infrastructure. This extends AWS's proprietary native hardware layer Nitro encryption to major AWS services, including Fargate tasks, Transit Gateway, Application Load Balancer, and more. It eliminates the operational overhead and complexity associated with certificate and key management.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2140.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2140)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2170.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2170)

Next,  I'm also excited to announce another breakthrough innovation meant to make it easier for you to secure your applications at AWS, Network Firewall Proxy. Firewall Proxy is expanding the highly resilient and highly scalable NAT Gateway functionality to include comprehensive proxy capabilities. You can authenticate source clients, decrypt and filter internet egress traffic, and provide protection against sophisticated attacks. It uses NAT Gateway's IP address for address translation  and ensures egress-only connectivity for private workloads.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2200.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2200)

With this, you can easily enforce tighter security controls against data exfiltration threats, prevent data leaks, detect compromised workloads, and filter traffic based on domain names, IP rules, and HTTP header fields. Proxy offers multiple layers of protection, including domain filtering, DNS lookup, IP filtering, TLS interception,  and response traffic filtering. You can now simply enable this enhanced proxy functionality on your existing NAT Gateway and make it available to applications across different VPCs. No more managing proxy infrastructure. AWS handles the scaling, updates, and availability while you get enterprise-grade filtering and data exfiltration protection.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2230.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2230)

The firewall deployments at scale can create a paradox. The more connections you secure, the larger your footprint,  the more complex your architecture could become. Customers were building dedicated inspection VPCs, managing route tables, and having to incorporate operational overhead that grew exponentially with each new connection. But our focus is to make things easier for you, to focus on your applications rather than the undifferentiated heavy lifting of configuring complex network setups.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2260.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2260)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2290.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2290)

So I'm excited to share that we've solved this complexity with Transit Gateway and Network Firewall Native Attachment,  eliminating the inspection VPC entirely. Now your firewall integration happens directly at the Transit Gateway level, giving you centralized security control across all VPCs and on-premises networks with no operational overhead, while also solving cost allocation challenges even in large-scale connected environments, as we now launch Transit Gateway Flexible Cost Allocation. With AWS, you can connect,  scale, and secure across all resources, automatically protecting against new risks while enabling innovation. This is security that not only protects your infrastructure but actively improves it.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2320.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2320)

### The AWS Global Network: Expanding Physical Infrastructure and Announcing Fastnet

The services we've built are battle-tested abstractions that scale without limits, opening the door to building global connectivity that extends beyond regions, beyond continents, and towards planet-scale operations. Throughout this talk, we've seen how abstractions simplify complexity,  like atoms forming molecules that scale into larger purposeful structures. Each layer builds on the one beneath while hiding complexity. At AWS, our networking services follow the same philosophy.

We've covered physical networking, from GPU-to-GPU communication to multi-data center networks. VPC abstracts this hardware complexity into logical components like subnets and security groups. Lattice further abstracts application connectivity, making services communicate seamlessly. Security follows this model too, with protections at every layer culminating in autonomous global threat detection.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2380.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2380)

These abstractions don't exist in isolation. They're powered by something fundamental, the physical foundation of the AWS global network. I've spent over seven years building this network, so it is amazing to see just how far we've come. It's the lifeblood of our global infrastructure. The AWS Global Network spans continents through terrestrial  and subsea fiber, connecting AWS regions, Local Zones, and Points of Presence into one cohesive global fabric. It ensures your applications can deliver content and services to users anywhere in the world with the highest level of security, reliability, and performance.

AWS operates one of the most extensive cloud networks, spanning over 9 million kilometers of fiber. And that's a 50% expansion in just one year.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2420.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2420)

I'm also excited to announce Fastnet. This is a dedicated high-capacity transatlantic cable connecting the US  back to my home country, Ireland. The subsea cable will create alternative data pathways between Maryland and County Cork, delivering fast and reliable cloud and AI services across the Atlantic.

Operational in 2028, Fastnet will add vital diversity for customers by building a new data pathway with unique landing points, keeping services running even if other undersea cables encounter issues. This enhanced network resilience will improve global connectivity and meet the rising demand for cloud computing and artificial intelligence. If you look at our past record, some of you probably have seen some of those cable cuts around the world. It is not by accident that AWS has survived every single one of them. We focus very heavily on ensuring that there is real diversity across all cable paths, and this is why we introduced Fastnet because again, across the Atlantic, we noticed that there were certain points of failure with the various landing stations in the US.

So these projects like Fastnet represent investments that enable us to bring these cloud services closer to customers everywhere. Speaking of proximity, it's not just about network connectivity that continues to grow. We're also rapidly expanding our infrastructure footprint to AWS regions and availability zones. Today, we operate 38 regions, 120 availability zones worldwide, all interconnected through that network.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2510.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2510)

This year alone,  we've launched new regions in Thailand and Taiwan, expanding our presence across South Asia, a new region in Mexico, our second in Latin America, and most recently we've launched in New Zealand, expanding coverage to customers across the Pacific. Looking ahead, we've announced plans for six additional availability zones across two new regions, Saudi Arabia and Chile, further strengthening our footprint in the Middle East and South America, giving our customers more choice, redundancy, and proximity to their end users.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2550.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2550)

### Bringing the Cloud Closer: AWS Local Zones and Customer Success Stories

To extend its reach further, we offer local zones, infrastructure deployments that place AWS compute, storage, database, GPUs, and other services closer to large population centers and industry hubs.  Local zones enable customers to deliver applications that require single-digit millisecond latency or data residency capabilities to end users, bringing the cloud even closer to where it's needed the most. And our footprint already includes 43 local zones worldwide, 35 metropolitan areas, 18 of those already located outside the US.

And here's how some of our customers are benefiting from that. So Sophos, a security company, needed to solve a critical problem. Their cloud-based threat intelligence was too slow for customers far from AWS regions. So they deployed front-end services in AWS local zones worldwide while keeping core infrastructure in their parent regions, combined with Amazon Route 53's intelligent routing.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2600.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2600)

The results were pretty remarkable.  69% latency reduction in Germany, 35% improvement globally, and resilience that scaled from 146,000 to 2 million requests per second during a traffic surge without missing a beat. AWS local zones didn't just fix their latency problem, it transformed their entire global delivery model.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2630.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2630)

DraftKings, a sports betting company, needed to expand across 20 US states while meeting strict data residency requirements.  Federal Wire Act and state regulations require customer data to remain within state borders, and traditional solutions require costly physical data centers in each state with extended deployment timelines. They deployed in the local zones for state-by-state expansion to achieve compliance without physical infrastructure. Deployment went from weeks to days, zero upfront infrastructure costs, 25% better latency while processing 500 million transactions in the first month of NFL.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2670.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2670)

### Global Connectivity Simplified: Cloud WAN, Direct Connect, and AWS Interconnect Multi-cloud

When leveraging our global infrastructure, whether through regions, local zones, or extensive network backbone, organizations face increasing need to connect and manage at global scale  their on-premises data centers and branch offices. And that's where Cloud WAN transforms this equation, unifying your VPCs and on-premises locations into a single cohesive global network. Instead of managing dozens of individual connections, you define your network intent through policy and Cloud WAN handles the complexity.

Cloud WAN enables you to create network segments, isolate sensitive workloads, implement granular traffic control, and manage everything through a single centralized policy framework. And one of the most important updates to Cloud WAN this year is routing policy.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2710.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2710)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2730.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2730)

Previously, when implementing advanced filtering and summarization for better control of routes between Cloud WAN and external networks,  you had to invest in complicated, expensive third-party routers to implement advanced routing techniques. Now, you get AWS native advanced routing capabilities that eliminate the need to invest in these third parties and provide you fine-grained routing controls to optimize route management. You can set the advanced  BGP attributes to customize your network traffic behavior, and you get advanced visibility into the routing databases, enabling rapid troubleshooting of network issues in complex multi-path environments.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2750.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2750)

Cloud WAN gives you incredible control and automation for global connectivity, but you also need physical connectivity  from your on-premises to AWS. That's where Direct Connect steps in, serving as high-performance, dedicated connectivity that ensures your Cloud WAN policy can be executed with the reliability, bandwidth, and consistency that mission-critical workloads demand. Over the last 12 months, we've expanded to 150 locations and we continue to roll out 400 gig connections to meet the demand of AI. And this focus on connectivity reflects a broader principle fundamental to AWS: interoperability. We believe you should have the freedom to choose technology that best suits your needs, whether that's connecting your on-premises infrastructure or integrating with other cloud providers.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2790.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2790)

But here's the reality.  When customers try multi-cloud connections today, let's face it, the experience leaves much to be desired. We constantly hear from customers that the path is far more complex than anticipated. Often customers are left with a DIY approach, leaving them handling the complexities of managing global multi-layered networks at scale. Innovation shouldn't be constrained by networking complexity, so we're addressing these challenges head on.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2820.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2820)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2850.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2850)

Our vision is clear.  We want to radically simplify operations so you can establish private cloud-to-cloud connections in minutes. Security must be built in from the ground up, ensuring data protection across cloud boundaries. And you need single interface control that provides visibility and management across all cloud providers and on-premises networks. That's exactly why I'm excited to announce AWS Interconnect Multi-cloud, already in preview with Google,  and I'm also excited to announce that Microsoft will be joining us in the first half of next year.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2880.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2880)

AWS Interconnect Multi-cloud enables customers to quickly configure private high-speed connections with dedicated bandwidth between their VPCs and other cloud service providers. We've built extensive pre-cabled capacity pools between AWS and other cloud providers. So when you need connectivity, it's there, ready to be activated with ease.  Both AWS and your other cloud provider manage the scaling and operations, while support is directly owned by the cloud providers, all backed by an SLA. This is what multi-cloud networking should be: simple, reliable, and focused on delivering value rather than managing complexity.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2900.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2900)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2920.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2920)

So stay tuned because there are many other providers that are in the works. And again, with multi-cloud,  we have actually defined the standard. We have published the APIs and the standard of how to interconnect, going far beyond what anybody else has ever done. Now, I'd like to introduce Salesforce to come out and tell their story. So please welcome to the stage Jim from Salesforce. Thanks, Jim. 

### Salesforce Partnership: Multi-cloud Integration with AWS Interconnect

Okay. Hey, good morning, everyone. I'm Jim Ostrogni from Salesforce D360, formerly known as the Data Cloud. I'm excited to share how Salesforce leverages AWS networking innovations to deliver seamless experiences for our customers worldwide. As Rob just introduced AWS Interconnect Multi-cloud, something we're really excited about, I want to share with you how we're putting this and other AWS networking services to work at enterprise scale.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/2970.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=2970)

Salesforce operates one of the largest enterprise platforms, and AWS is the backbone of that scale.  Across 18 countries and more than 700,000 production and sandbox orgs, we rely heavily on AWS's global regions and multi-region capabilities. To deliver consistent performance, we lean deeply on AWS networking services: Transit Gateway, PrivateLink, and Network Load Balancers for the massive global throughput that we see. AWS networking isn't just infrastructure, it's fundamental to how Salesforce runs at global scale.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3010.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3010)

Here's the reality: customers are multi-cloud.  Their analytics may run in Google Cloud, their legacy workloads might live in Azure, and their customer engagement layer is powered by Salesforce. They choose these architectures for good reasons, though. There might be regulatory constraints, data residency requirements, and just the desire to have the best of breed investment.

At Salesforce, AWS is where the core of our business resides. It's where we run our primary platform, where we innovate fastest, and where we operate with deep expertise. But our customers need us to meet them where their data lives, even when the data is locked inside highly secure private environments outside of AWS. And that creates a fundamental networking challenge, right? How do we preserve the AWS-grade performance and security and operational excellence while extending Salesforce into private multi-cloud data sets that never touch the public internet?

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3080.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3080)

This is precisely the challenge AWS Interconnect multicloud solves for us.  Our customers are multi-cloud by design, especially the large enterprises. Their most sensitive or regulatory data sets live inside private Google Cloud environments, yet they expect Salesforce running natively on AWS to connect to that data with the same security, performance, and operational excellence we deliver inside of AWS. With Interconnect multicloud, we can extend our AWS native private connectivity model directly into Google Cloud.

We keep the same Transit Gateway policies, the same PrivateLink-based security principles, and the same operational tooling, but now it works seamlessly across clouds. For customers, that means choice without a compromise. A financial services firm can keep core trading systems and regulatory data sets in Google Cloud while still privately connecting them to Salesforce Data 360, all without touching the public internet.

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3170.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3170)

As for Salesforce, this means that we can finally erase the boundaries between clouds. We can secure the customer's crown jewels of data between wherever it lives, satisfying the strictest compliance mandates without asking them to move a single row. Consider this use case here. We have a large US health company  who's already successfully using Data 360 with a data lake hosted on AWS.

Previously, they were trapped in a compliance paradox. Strict security policies were forcing them to use brittle middleware and custom encryption just to move clinical data. It killed the real-time agility that they really wanted. Salesforce Private Connect broke this deadlock. We established a direct zero copy link between Data Cloud and their data lake. Crucially, we built this onto the secure AWS infrastructure their security teams had already signed off on.

With Salesforce managing the complexity, they didn't have to build the pipes. They just unlocked the data. Now, we're taking this to the next step. Using the new AWS Interconnect multicloud, we are expanding that secure fabric to other data lakes hosted on Google Cloud.

The result is profound. The customer doesn't see multiple clouds anymore. They see one unified experience. With Private Connect, they are securely integrating their data lakes across AWS and Google Cloud, and Salesforce and their enterprise systems are now a single governed real-time data platform, powering AI agents and member outcomes at the speed of their business.

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3270.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3270)

Looking ahead, the partnership continues to open up some incredible possibilities. We're continuing to invest in additional sources and mechanisms,  expanding our zero copy commitment to secure an open integration with Data 360. We're refining our security model to provide consistent zero trust networking regardless of where workloads are going to run. Most importantly, we're simplifying the customer experience.

Next, enterprises will be able to onboard with Data 360 and immediately connect their existing multi-cloud, hybrid, and on-premises infrastructure through a single AWS-powered networking fabric.

[![Thumbnail 3320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3320.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3320)

This is the future of enterprise software, not forcing customers to choose between clouds, but giving them the power to use the best of all the clouds.  Thank you, Robert. Appreciate it. Thank you. Thank you.

### Extending Connectivity Everywhere: Last Mile, Amazon LEO, and CloudFront Innovation

All right. We didn't stop at multicloud, by the way. We took that specification that we had built and we thought, hey, how can we actually go further even into the last mile. And so we know that hybrid architectures span on-premises and cloud environments, and they all require the same thing: secure, reliable, productive connectivity. Deploying last mile connectivity between Direct Connect locations and your on-premises environment can take significant investment, time, and effort. You've got to find a co-location facility that we're in. You've got to identify a suitable partner, negotiate contracts and technical requirements, plan capacity, and procure those circuits. The list goes on.

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3380.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3380)

However, with AWS Interconnect now last mile, and now in gated preview with Lumen, you can get a fully managed connectivity service that allows you to  connect your remote locations to AWS in just a few clicks. So think about it. You already have a location that's got a Lumen circuit in it. You can go to our console, you can basically set up direct connectivity over that same physical circuit. Nothing else is required. And Lumen is only the first. There are already many other providers that we've started to work with. We're going to bring this across the world and simplify this everywhere.

[![Thumbnail 3410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3410.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3410)

And even though we have an expansive global network,  there can be limitations to terrestrial networks. And that's where Amazon LEO comes in. Through our constellation of low Earth orbit satellites, we're extending the same global connectivity promise to every corner of the world, no matter how remote. Where traditional terrestrial networks can't go, LEO satellite networks seamlessly take over, creating an unbroken bridge between ground-based infrastructure and space-based innovation. So the same interconnects we just highlighted with multicloud and last mile, we've developed them for LEO as well.

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3450.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3450)

Now to wrap things up, let's take a look at CloudFront.  We have over 750 points of presence around the globe. We continue bringing our network even closer to your users. We have 1,100 embedded POPs, a type of CloudFront infrastructure owned and operated by Amazon, but deployed directly into the internet service providers and mobile network operators' networks. Embedded POPs are custom-built to deliver large-scale live video streaming, video on demand, and game downloads.

[![Thumbnail 3480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3480.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3480)

And we didn't stop  in terms of how we've been innovating to bring CloudFront to folks. And so we're equally committed, even with our large enterprises all the way down to startups and small businesses. We want to enable everybody to get access to the same world-class infrastructure. We've heard from developers that they want the power and performance of CloudFront. They needed the pricing that's as simple and predictable as our new onboarding experience. They want cost certainty that lets them focus on building great applications without worrying about variable expenses from traffic surges or security events.

[![Thumbnail 3530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3530.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3530)

So we introduced flat rate pricing packages that bundle CloudFront, Web Application Firewall, DNS, and storage with transparent usage allowances and no overage charges. The result:  predictable monthly costs and zero financial uncertainty. Because when developers stop worrying about bills, they start building the future. And developers are responding. In just two weeks, we've seen over 15,000 subscriptions to our flat rate bundle.

[![Thumbnail 3550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3550.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3550)

Whether you're building, whether you're a startup building the next breakthrough  app or an established organization like the NBA, CloudFront scales to meet your needs. The NBA leveraged our global infrastructure to deliver NBA to Prime to millions of viewers globally without interruption. And Epic Games, just look at what they achieved. For their last Fortnite season launch, CloudFront delivered 268 terabits per second, an all-time high that demonstrates the incredible scale our network can handle at your biggest moments. And everything was handled through our network automation, traffic engineering, no humans involved.

[![Thumbnail 3590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3590.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3590)

[![Thumbnail 3620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e591556a2f1e1cd4/3620.jpg)](https://www.youtube.com/watch?v=RkdPAFJEPSA&t=3620)

Over the last few weeks, we've had over 50 launches  across IP management enhancements, Direct Connect features, VPNs, Route 53, and security. From our physical hardware in regions, Availability Zones, and Local Zones to our services like CloudFront, Direct Connect, and CloudFront, we're going beyond the boundaries of connectivity, reaching not just across continents, but also literally to the stars above. Thank you very much. Really appreciate it. You're all free to go get lunch now. Thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
