---
title: 'AWS re:Invent 2025 -Stripe: Revolutionizing compliance workflows w/ agentic infrastructure (IND3300)'
published: true
description: 'In this video, Hassan Tariq from AWS and Chrissy and Christopher from Stripe discuss how Stripe built an LLM-powered AI research agent using Amazon Bedrock to transform compliance reviews. They explain how enterprises spend $206 billion annually on financial crime operations, with 33% of tasks being automatable. Stripe processes $1.4 trillion in payment volume and needed to scale enhanced due diligence reviews efficiently. Their solution uses ReAct agents with directed acyclic graphs (DAGs) as rails, achieving 96% helpfulness ratings and reducing average handling time by 26%. Key technical components include an LLM proxy service, prompt caching, and a dedicated agent service supporting over 100 agents at Stripe. The system maintains human oversight with complete audit trails while enabling parallel processing and autonomous investigations across multiple jurisdictions.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 -Stripe: Revolutionizing compliance workflows w/ agentic infrastructure (IND3300)**

> In this video, Hassan Tariq from AWS and Chrissy and Christopher from Stripe discuss how Stripe built an LLM-powered AI research agent using Amazon Bedrock to transform compliance reviews. They explain how enterprises spend $206 billion annually on financial crime operations, with 33% of tasks being automatable. Stripe processes $1.4 trillion in payment volume and needed to scale enhanced due diligence reviews efficiently. Their solution uses ReAct agents with directed acyclic graphs (DAGs) as rails, achieving 96% helpfulness ratings and reducing average handling time by 26%. Key technical components include an LLM proxy service, prompt caching, and a dedicated agent service supporting over 100 agents at Stripe. The system maintains human oversight with complete audit trails while enabling parallel processing and autonomous investigations across multiple jurisdictions.

{% youtube https://www.youtube.com/watch?v=NKF4qKnOaCE %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/0.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=0)

### Introduction: The $206 Billion Compliance Challenge and the Promise of AI Agents

 Hello, everyone. Thanks for joining today's session. For enterprises that operate in regulated industries like financial services, compliance is not just important, it's existential. My name is Hassan Tariq. I'm a principal solutions architect with AWS, and I work closely with customers like Stripe on their AI/ML initiatives. Joining me today is Chrissy and Christopher from Stripe, who led the development of this solution. I'll let them introduce themselves when they take the stage.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/50.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=50)

 Here is the agenda for today. We'll start by covering some background context on why agents can be a good fit for compliance use cases, then go over the background and challenges that compliance teams faced at Stripe. We'll discuss the solutions they implemented, the benefits they achieved from it, and ultimately we'll end by discussing what's next and some lessons learned along the way.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/100.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=100)

Before I go into the details, let's take a look at what's at stake here. According to a research study conducted by Forrester,  enterprises spend around $206 billion per year globally on operations related to financial crimes, with North America alone spending around $61 billion of this total. This is a very massive spend, but it reflects the complexity and resource intensity of meeting regulatory requirements.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/140.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=140)

 This is not just a static number; it's growing. According to some compliance teams based out of Europe, the compliance regulations and demands they're getting from the authorities are increasing by up to 35 percent year over year. This means they are working through different cases and have to produce a lot of documentation and spend a lot of extra time doing that.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/170.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=170)

 When Experian conducted a survey with these compliance teams to figure out what type of tasks they perform and what type of documentation they need to do, they discovered that almost one-third of these tasks can be automated. Just imagine giving back 8 to 12 hours per week to these compliance analysts so they can focus more on strategic analysis and decision making. That's exactly where agents can play a big role here.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/210.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=210)

 Think about the best compliance analyst that you work with. They're really good at asking follow-up questions and connecting the dots among different data sources. If agents can do the initial investigation and data analysis, these analysts can focus more on complex decision making and nuanced judgment calls.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/240.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=240)

 When agents can do the heavy lifting of the analysis and initial research, it frees up a lot of the analyst's time, and they can scale up. They can perhaps review a lot more cases than they are regularly doing. It's like giving each analyst a dedicated and tireless research assistant.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/270.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/280.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=280)

 This whole process brings consistency to the analysis, which means you're ensured that every case will go through a rigorous step process and none of the data sources will get missed.  While doing all of this through agents, when the analyst is focusing on making the decisions, the agents are doing the documentation and preparing the audit trail for those decisions. In short, analysts focus on the why, whereas the agents focus on the what and how.

This is all great, but you need a very solid foundation for these agents to operate, and we will hear shortly from the Stripe team how Bedrock provides this foundation. I will now pass on to Chrissy to go over some background and discuss the challenges that they faced.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/360.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=360)

### Stripe's Mission: Building Trust Through Financial Infrastructure at Global Scale

Thank you, Hassan. How are you doing? Happy holidays and happy Cyber Monday. If you care about shopping or payments, before I dive into our AI use cases, let's take a step back and talk about Stripe as a company. So many people know Stripe as a payment processor. Last year alone,  we processed 1.4 trillion in volume with a big letter T, and that is 38% up year over year. Take a guessâ€”that number constitutes 1.38% of the global GDP.

We are great in payment processing, and we bring good vibes to our developers. These are all great achievements. Over the years, we are also evolving into a comprehensive financial infrastructure platform. Our internet, which was invented in the early 90s, wasn't designed with a native protocol for payments. So Stripe built the financial infrastructure on top of that, and that is what's enabling emerging protocols such as agentic commerce protocol, ACP. If you follow us closely on Twitter, now called X, our platform is very modular, and you can choose exactly what you need.

May it be global payments, moving money, or platformization of your money via Stripe Connect, or automating back office tasks like tax and billing. We handle the heavy lifting by integrating with local banks across 135 plus local currencies. While doing all this, we're maintaining 99.999% uptime, and on average, we're processing over 500 million requests per day. That number might have gone slightly higher during this BFCM. If you're interested, please go to BFCM.stripe.com to look into further information.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/500.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=500)

Definitely we're solving very interesting engineering challenges, but that is just a means to a bigger end.  The bigger end is our North Star: growing the GDP of the internet. Sounds amazing, right? To realize this macroeconomic impact, we realized as a company very early on that we need solutions to scale with our explosive growth without sacrificing safety and compliance. That brings us to the heart of our challenge that we're discussing today.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/540.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=540)

So when you ask people what is FinTech compliance,  most people will say, well, just following the laws. But at Stripe, our answer is that it is a foundation of trust. It is a mechanism that ensures that our platform ensures our innovative technologies follow the rules of global finance. We view trust as a unified responsibility with two critical dimensions. One is ecosystem integrity. We enforce know your customer, know your business, and anti-money laundering. This is to ensure our platform will never become a haven for crime. So think about this: if a sanctioned entity tries to use a fake identity to onboard to Stripe, our compliance engine would be able to block that instantly.

You might think it's easy, but it is actually quite hard in reality because these bad actors are smart and constantly evolving. An equally important dimension is to protect our users. At Stripe, users first is one of our leading operating principles, and that's something I really love about Stripe. I've been with Stripe for almost five years, and I think we really care about our users, whether adhering to GDPR for privacy or UDAAP for fairness. We ensure our users are never exploited or compromised. We realized that if we don't protect the people who rely on the system, building a perfect system doesn't have much meaning.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/660.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=660)



### The Business Challenge: Manual Reviews, Cognitive Overload, and Cross-Jurisdiction Complexity

To build up this very effective defense, there are four factors that shaped our approach. First is growth. We're constantly growing into new industries and markets, which means onboarding new users fast and safely is paramount for us. The second thing is the scale required from a compliance perspective. We need to do enhanced due diligence reviews at scale. The short form is EDD, and we do this so we can manage users' risk effectively. Third, we are always striving to reduce the time to do these reviews to raise the bar for our operational excellence. The last factor is technology innovation, which is where Agentic comes in. We want to completely transform the manual compliance review process and build a solution to make it faster, more consistent, and infinitely more scalable.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/740.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=740)



What is the core of our business challenge? Due to regulatory compliance requirements, we need to do extensive reviews for high-risk users. Think about this almost as a detective looking into suspicious individuals, trying to figure out if they are actually suspicious. As you can imagine, this requires a lot of investigation time. However, we're doing this every day, so we cannot take a long time. Doing this in a timely fashion is critical for us.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/780.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=780)



To help you visualize the manual review process, which is completely done by humans, I'll summarize the mainly two blockers. One is from the manual process. We realized that many times our expert reviewers are acting as navigators, going through all these different systems trying to gather and locate information instead of being the analysts they're supposed to be. They're not really using their time to make high-value decisions. The second thing is scalability and the jurisdiction hurdle. The biggest bottleneck is the cognitive overload when switching between different jurisdictions.

Let me give you an example of this context switching. Say an expert analyst can move from assessing an entity in California, where the retrieval process is rather straightforward, to evaluating a complex corporate entity in the UAE or Singapore. For each jurisdiction and region, it requires a completely different mindset to decide what constitutes a risk. The threshold of risk is a variable, and the definition of a safe business is not static. One example is regulatory compliance. Ownership transparency varies widely between high-risk and low-risk jurisdictions. Your expert analyst is basically applying these ever-shifting rule sets constantly, and this is a very demanding and complex task that can sometimes even be error-prone.

Just to give you a little bit more of an analogy to help you visualize this, every year I go through this pain once during tax season. You need to go through a lot of rules.

I'm only doing it for federal and California, so imagine how much pain these reviewers are going through. At the bottom line, we are trying to maintain operational excellence for regulatory compliance by navigating through this fragmented jurisdiction map. Simply linearly scaling up our workforces with the complexity is not the solution. So you might ask, what is the solution? If this were a quiz in college, I think you would all get perfect scores because Hassan kind of gave the answers away already.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/950.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=950)

To solve this, our solution is to build an LLM-powered  AI research agent to do autonomous investigations. The agent has lifting in three ways. First, async workflow orchestration enables parallel processing. The agent can look into different aspects of the business in parallel instead of sequentially, like a human would do. Second, because we built this, it plugs directly into the existing compliance tooling, so it works nicely and fits naturally into the compliance workflow that our reviewers are used to. You don't need to train them on something new.

The last bit is very exciting: agents never sleep and don't need to rest. Think about this feeling. If you have a Roomba at home, you turn it on and go out to hang out with your friends, and then you come home and the home is already clean. That's an amazing feeling. Having an AI agent to do these analyses before you even open the ticket is exactly the same thing. It will do the investigation and summarize the insights before you even open the tickets. This is a game changer.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1040.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1040)

Now we have the AI agent to assist and the humans to maintain the final authority. Before I hand over to Christopher to do the deep dive into our solution,  I want all of you to leave this room with at least three takeaways. Obviously, if you have more takeaways, that's a bonus for you. The first takeaway is oversight. We use human-centric validation. We're not trying to use the AI agent to replace decision-making. Instead, we're using agents to empower it. All the expert reviewers have configurable approval workflows, so the agents are there to assist and the humans make the final call.

The second thing is that, as Hassan mentioned, we keep a complete audit trail. We record every single decision and the rationale behind it. This ensures that we will have enough data to serve as compliance grid evidence. The last bit is that with all these agentic work we're doing, we supercharge our reviews to enable our expert analysts to go deeper and faster with all these pre-analysis. With all these things, we are ensuring that we're not just building for speed, but we're building trustworthy, auditable, agentic compliance solutions at scale.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1150.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1150)

### Technical Implementation: ReAct Agents, DAG Decomposition, and the Power of Tool Calling

Now with no further ado, let me welcome Christopher to the stage to dive deep into our solutions. Thanks, Chrissy. Hey everybody, my name's Chris. I'm a data scientist at Stripe. I'm going to walk you through a little bit of the technical implementation.  Let's just go through what we're going to cover. We'll start with the problem setup of exactly what we're trying to solve and why we're using agents. We'll jump into the agent mechanics and how it works under the hood. We'll talk about how we actually plug into the LLM, which is the brain of the agent in this case.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1200.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1200)

Next, we'll talk about an interesting service that we discovered we needed as part of this journey. We'll finally zoom out and talk about the Stripe ecosystem of agents and then we'll get into the business side of the journey and the results. Let's jump in. So let's start with the problem we had.  Basically, we had a very, very long manual review, and I think the first thing we wanted to think about doing was just throwing an agent at the workflow and effectively automating the whole thing.

That's a nice idea in theory, but in our case it wasn't really going to work. The problem is that if you just took an agent and threw it at this workflow, it would probably spend a lot of time on things you really don't even need it to spend on. It would rabbit hole, and then it wouldn't spend any time on things that you needed to look at. Since this has regulatory reasons for existing, we have things we need to look at in many cases.

We realized early on that it made sense to have rails for the agents. There's a diagram on the slide that shows how we were able to decompose this really complicated workflow into what we call a DAG, or directed acyclic graph. The DAG is effectively these rails that we can use for these agents. We happen to use ReAct agents. One nice thing about this DAG decomposition is that the tasks become bite-sized enough to actually fit in the agent's working memory. Agents have a limited amount of thought processing, just the way we do. I can't keep everything in my head.

The reason we're even using agents in the first place, and I think this is important, is to answer the question: why not just use LLMs? Well, I think the reason agents work well is because they can call tools, and I actually think that's the main value. I have a biased perspective on agents, which I'll share a little bit later, but I think that's why agents are practical here. One interesting thing, and this goes back to the fact that it would have been nice to just throw an agent at it and have it be done, is that we actually keep humans in the driver's seat. In this case, they're making all the decisions. Agents are just there to inform our reviewers to make the best possible decision. They can use it or not. That's up to them. This is very much still a human-driven workflow, and we found a good way to make that work.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1350.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1350)



[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1370.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1370)



We're using a ReAct agent. Let me talk about what that is. I think agents think a little bit like I might if I have a complicated problem. Someone might give me a question, which we'll call a query. Let's say it's like ten divided by pi. I'm not going to know that offhand. Some of you might actually know that, but I don't. So I'm going to call an action to use a calculator, and I'm going to wait for that calculator to come back as my observation. At that point, I'm going to think if I now have the answer. Yes, I do. So therefore I'm going to progress to this final answer stage. Really what we care about in our review is ultimately this final answer. That's all we care about. What it does in the meantime is an implementation detail to our project.

In reality, the reason we even use agents in this problem is that we have much more complicated queries than something like that. I'm a data scientist, so I might get an analytics ask. I might need a few queries. I have an idea of an initial query I'm going to do that might be close to what I needed, but maybe I see some issues. Maybe I'm going to rerun that query a few times. You can imagine I'm looping through this thought, action, observation loop as many times as I potentially need. You can imagine this could actually loop around forever, and then it would maybe be true AI, whatever you want to say. But at the end of the day, LLMs have a finite context window, and so this effectively becomes a bit of a challenge for us.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1490.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1490)



In our investigation, the reason we're using agents is because we need it to rabbit hole sometimes, sometimes we don't. We always need it to cover all the bases. When we're going deep down in some of these rabbit holes, these prompts can get very long.

### Leveraging AWS Bedrock: Prompt Caching, Standardized Security, and Multi-Model Access

One thing I like about this slide is that it depicts a 1 + 2 + 3 + 4 type of behavior. I'm not a mathematician, but that looks quadratic to me. What that means is that as the agent needs to dig deeper and deeper to answer a question, it's effectively getting a longer and longer prompt that you're paying for every single time. It's going to be a quadratic cost over those number of turns. If you think about the entire cost of your agent operation, you're paying for input tokens and output tokens, and you're probably going to be dominated mostly by the input tokens in this case.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1580.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1580)

There is a solution to this, however. I believe that solution today is in the form of caching. Why is that a solution? Because if I don't need to reread that prompt every single time, it's almost as if I'm paying for that final prompt once, which is more of a linear situation, which is superior.  That's where we're going to get back to Bedrock. Bedrock is our provider that we can utilize effectively for this kind of prompt caching, but that's not all I'm going to talk about in this slide.

We do use Bedrock for our LLM provider. We also have our own internal Stripe services as well. We use this, which we'll call the LLM proxy service for this presentation. One way it really helps out a lot on this project is that we have a lot of people using LLMs at Stripe. What could happen is another team is doing some testing or maybe they're scaling up, and that could crowd out my bandwidth on a potential LLM provider. By centralizing this proxy service, we can effectively solve this noisy neighbor problem and constrain those neighbors from hogging all the bandwidth I might need for this project.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1680.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1680)

There are other benefits too. We have guarantees because we're doing it ourselves around the availability of this service. We can do authorization and make sure the right LLM is being used for the right use case. Maybe there are some LLMs we feel are too sensitive to run certain data on, and this allows us to funnel those use cases to the right place. Potentially this service also allows us to configure things like model fallbacks if one of the model providers is out of bandwidth or something like that. 

Why is Stripe using Bedrock and why we're using AWS in this situation? I think one of the biggest benefits I see for this project is around the standardization of privacy and security. If you've worked at a big company before, maybe you want to use a new LLM provider for your project, but that's not going to work because it needs to be vetted by security. One really nice thing about Amazon for us is that we effectively do that vetting once to get on that platform and then it's effectively standardized there. Bedrock provides us access to multiple vendors and models, and we get that standardization of security around that.

The second point I want to make is more on the bells and whistles that come along with it. We talked a little bit about prompt caching already, which can save you a lot of money if you're doing some sort of agentic flow where you're doing multiple dives deeper into the data. I also think fine tuning is a really interesting one too. The vendors have been providing really good models lately, so maybe it's kind of fallen off the radar for a lot of people, but I see a big opportunity for this project mainly because the vendor model comes with unforeseen deprecation schedules. Let's say we want to be focusing on adding new questions, but now this new model that we're having great success with is going to get deprecated. I prefer not to have to focus on old questions. I want to focus on new questions, and so by fine tuning, maybe I could claw back some performance.

I think the bigger thing is just that we can now deprecate on our own schedule, and that seems amazing. I love the bells and whistles that come along with Bedrock. That's been very helpful. I believe there's more as well.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/1830.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=1830)

The last point I'll make is that it's really one API but you're getting many models. This comes back to the standardized privacy and security thing where you plug into it once and you don't have to have that overhead of setting it up, connecting it, doing the integrations, and so on. 

### Building Stripe's Agent Service: From Monolith to 100+ Agents in Three Quarters

Another thing I'm going to talk about here is why we have an agent service now. When this project started, we didn't have this service. Well, because we didn't have it first, basically. What happened was we had this kind of classic ML inference system across Stripe. The original plan was to hack in this kind of agentic workflow into this ML inference system, thinking it was close enough to get us there, but it was shot down quickly for very good reasons. Those reasons are mainly that it really requires a totally different set of hardware, among other things.

With traditional ML, you're really compute bound. Let's say you're running an LLMâ€”you're going to need a GPU or maybe at least one GPU. If you're running something like XGBoost, you might want a lot of CPUs, but that is what you're waiting on. You're probably getting a pretty consistent latency with whatever that computation is, so you have very short timeouts, for example. You also have very deterministic control flow. The model always runs exactly the same way. You're basically paying a lot for these machines. They're hard to set up, and you're trying to minimize the number of machines you need.

Agents are kind of the opposite in all those aspects. Particularly if you're using a vendor model, you're just waiting for it to come backâ€”you're waiting for the LLM to respond to you. You're actually not doing a lot of compute at all. You're just waiting around. You need a lot of threads, a lot of lanes basically, to wait around for all the different agents that might be running on the same machine. You can expect very long timeouts in some cases. You can imagine what if a tool call needs to call a database. It could be minutes. I remember when we had engineers first spinning out the service and they're asking me how long of a timeout do you need. Is 30 seconds okay? And I said I think we're going to need like 5 to 10 minutes. So very different profile of compute.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2000.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2000)

Agents are also very nondeterministic. Some answers it might know very quicklyâ€”it only needs one tool call. Some may need to loop around in that thought, action, observation loop for quite a while. So you don't know exactly how long it's going to take. Tiny machines might be true and might be false, but quite honestly, you really just need a lot of lanes to wait for network calls. 

We'll talk a little bit about the history of this agent service. We'll start with the beginning of Q1, which isn't in here, which is like it didn't even exist. We didn't even think we necessarily needed it. We tried to hack it into our kind of traditional ML service again, and that got shot down. But we realized it was going to be very impactful for this project. We saw that it was going to be something that was going to generalize pretty well, and so, quite miraculously, we got this thing spun up in about a month after deciding it was really going to be useful.

We spun that up quickly by just making it a little monolith with a very primitive API that reflected the way we were hacking it into the ML inference system. You can imagine an ML inference system is going to have some sort of classic predict endpoint. In this case, we had this kind of synchronous approachâ€”you're going to have to wait for it. If we go back to the ReAct agent, let's just say you're giving it the query and then the output is you're just going to wait until it provides that final answer. That was enough for this project. It wasn't tremendously hard to get going, but I think it was amazing we got it up in about a month, so we started there in Q1.

In Q2, we started polishing it a little bit. We added ways to do evaluation for agents. We added in tracing so that we could more easily debug what's happening behind the scenes within that thought, action, observation block in this case. We had an interesting developer in Stripe that decided to actually spin up this no-code agents kind of UI where people could build their own agents with whatever tools they wanted.

This enabled the mass proliferation we ended up seeing in Q3. Q3 is also where we started really pushing the boundaries of this project and hitting the capacity limits of the entire system. We took what was originally a monolithic design and allowed every single use case to spin up their own service, which solved the noisy neighbor problem within the agent service itself.

Q4 has been an interesting development on the functionality side. You can imagine this product requires more of an ML-inspired API with predict on point capabilities, but there are a lot of use cases around chatbots. With chatbots, you want to be able to interact with them, see their thought process, and view all the different logs of the chat. This new API allows for stateful streamed updates, which really enables that functionality.

What is interesting is that we have over 100 agents at Stripe. I did a recent change and they are all over the place. I might rabbit hole a little bit here, but I do not know if you actually need 100 agents to be successful. From my biased perspective of working on this project, I would say you could probably get by with a shallow ReAct agent, maybe a deep ReAct agent in some sense, or maybe a to-do list that can spawn sub-agents. However, I would say the 100 agents may be an artifact of the fact that we allow people to move quickly at Stripe, and there may be a lot of prompts baked into the agents to get to that number. That is just my opinion.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2240.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2240)

### The Complete Ecosystem: Orchestration, LLM Proxy, and Human-in-the-Loop QA



Let us zoom out a little bit and talk about the full ecosystem at Stripe. We will start back with the reviewers. Remember that we talked a lot about agents, but actually the human reviewer is really the star here, and they are in the loop. They effectively interface with this whole system through their web UI, the review tooling, and so on. We talked about this big DAG in the beginning. Well, that review tooling is effectively the orchestrator of this entire DAG flow. You can imagine that before the review even begins, we can kick off a lot of these agents that front-run research for some of these questions.

As the reviewer progresses through the review, you can imagine what context may become available to actually answer deeper into that review. This review tool and application is what determines or triggers the agent to begin front-running some of this research. The rest will not come as too much of a surprise because we have already gone deep on all these pillars. The ReAct agent basically interacts with the rest of the ecosystem through two ways. The first is the LLM client, which will hit our LLM proxy that we have talked about before. That handles things like model fallbacks and potential token caching, solving that noisy neighbor problem and making sure that this system is actually robust.

The third is how does it actually get the signals that we use in our investigations? That is where the ReAct agent comes in. It is able to call tools that could be an MCP client, which has been a big thing, could be Python tools, or whatever works. Something that we do not show here is that we do have to QA this as well, so this is a sensitive space. Regulators need to know that this is working soundly, and part of this is keeping humans in the driver's seat in the first place. The second part is that we have a very rigorous QA process that is not actually depicted here.

There is a lot of craze about having LLMs be judges on many different use cases. For this project, we really want to keep everything passing the human quality bar, which means we need to have humans in the QA phase as well.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2420.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2420)

### Results and Impact: 96% Helpfulness Rating and 26% Reduction in Review Time

I'd like to see it become more systematic, but it's not really something that fits well into this diagram here. However, it does exist. Let's talk about the  whole journey now that we're through all the implementation details. This really started as a moonshot idea. Interestingly, we spent all this time talking about agents, and we knew agents were going to work for this probably about less than a month in. We set up a quick notebook to demo how it might look on a few particular questions, and it was very impressive right away. So we knew this was going to be applicable.

Quite frankly, I think that's what really allowed us to get the push to actually stand up this new agentic service, which was really necessary to make this happen. We went from this small notebook demoâ€”fail fast, if you will, but it succeededâ€”and eventually got to this full enterprise system with their own services stood up. It was really miraculous. We also did this by having humans completely in the loop, not just during the review but also during this rigorous QA process to make sure the agent wasn't hallucinating or was providing sensible answers and could be depended on enough. We were able to finally get our initial questions out by the end of the half.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2520.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2520)

Let's look at another timeline here.  We started with those notebook scripts in early Q1 that allowed us to know this would actually work and was worth investing in as a full product. We bootstrapped that agent service very quickly with that monolith, just the bare minimum of what we needed to get going, and we took it as far as we could. It did well, and we hooked that up with all the enterprise LMs. By Q2, we finally got that first question in, which was huge. A lot of it was really working with the ops team to understand exactly how they were going to get the best use of this as well, so that's a big component of this.

It's not just the agent, unfortunately. I wish it was that easy, but really getting it to the bar of quality where a human reviewer is actually going to depend on it is critical. If it's helpful only 20 percent of the time, they will never look at it, and then you're just spending money on LLMs and getting absolutely nothing out of it. Maybe even at 80 percent, you could get that kind of reaction. So it took a while for us to work with QA and make sure this was done on rails. We didn't want them just going to a deep research interface asking anything, because you don't know what it's even good at answering. That was actually a big part of Q2.

We got that first question out. In Q3, we started really trying to scale it up, so we started launching a lot more questions and looking into things like caching. Now we care about costs. Q4 is where we've really already got a lot of the easier questions, maybe even a lot of the questions before the review even starts that we can get. But now if we're going to continue making gains, we need to actually be able to use context within the review to build upon what we have. That data orchestration makes this a little more complicated, and that's something we've been working to really plug into in Q4. We're really just scratching the surface with this.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2660.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2660)

So what do we get for it? Well, one nice thing we have is telemetry  from the reviewer so they can tell us if it's helpful or not, and what we're seeing is they're finding it about 96 percent helpful. Remember I talked a little bit before about people needing to trust this system to use it. If this thing's good 30 percent of the time, 40 percent of the time, maybe even up to 80 percent of the time in my opinion, they'll just learn that they're not going to get any use of it. They'll do all the research themselves, and you're saving no time. You've spun up this very complicated system, so getting this kind of quality bar was a challenging process and took a lot of human reviewer involvement, talking to the control owners to get a sense of what they needed.

This informed our approach, and we went through many prompt iterations to get it right. We managed to keep human reviewers in the loop, ensuring that the outcomes of these reviews carry weight and are made in a sound way. Keeping a human in control at all times is a really good way to move forward.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2770.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2770)

We built this in a way where we can show a regulator exactly what the agent has found, how it found it, what tool calls it decided to make, and what the result of those tool calls were. This gives us complete auditability across anything the agent touches.  Ultimately, we were able to make this review wildly more efficient. We measure the performance of reviews by average handling time. With just these initial questions that we front-run the research on, a human reviewer can spend 26% less time on average on these reviews as a result.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2820.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2820)

Agents are not doing the work or making the decisions. They are not automating anything, but they are still able to deliver these massive efficiencies. This allows Stripe to keep up with user growth and changes in requirements as regulations evolve. This has been massive for us.  Looking ahead, we are working on expanding the orchestration and getting deeper into this very complicated review. I would love for the evaluations of how well the system is working to be more streamlined. LLM judges could be a good way to fail really bad models quickly, but I still suspect you want humans in the loop to determine if it is really good enough to ship.

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/2920.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=2920)

### Key Lessons Learned: Bite-Sized Tasks, Infrastructure Investment, and Keeping Humans in Control

The 26% reduction is massive, but it is just scratching the surface. As we get deeper into this review, we can imagine this is going to get much bigger. Fine-tuning is really exciting. I do not know if we are going to get improved quality with fine-tuning, but I definitely think we are going to have more control over what we want to spend our time on and not be surprised by deprecation schedules. Reinforcement learning is a fascinating area too, mainly because the answers here are verifiable. You can imagine an end-to-end training loop where you are actually learning a superior brain to the agent that could maybe have fewer tool calls in the future, which saves context.  So what did we learn as part of this? Probably the first lesson we learned is splitting these very complicated reviews into bite-sized tasks that fit well into memory and are very easy to judge as consistently good or bad. We have humans doing the evaluations of whether it is good enough, so keeping the task small enough is part of the fight to get there.

The bite-sized task was absolutely critical to make any real progress and to be anything more than a demo. These bite-sized tasks allow us to do this kind of orchestration, which allows us to actually start building upon context that is going to come up inside of that review and go deeper. This gets us to a point where we are getting more than just 26% reduction. Infrastructure has been a big help. We could not have done this probably without the agent service we had. It would not have worked hacked into this ML inference system. It would have been a complete abuse of resources, so definitely do not be afraid of doing that.

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/3000.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=3000)

What I want you to take away from this is do not try to automate everything right away.  I think that is a natural instinct, and everyone thinks

agents are just going to completely replace everything everywhere tomorrow, and that's just not how it's going to work. It's great to keep humans in the driver's seat. You should use these things as tools, and they may not even be good enough to completely automate it. The only way you're going to find out is to have a tractable way to incrementally attack some of these very long, complex problems into bite-sized tasks.

Agents are great and really amazing when you have almost an infinite amount of signals you could be baking into your answer and you don't even know which ones to be looking at. Really, the tool calling is what makes the agents useful, maybe generally, but definitely for this project. You still need rails for them though because you don't want an agent that's just spending all of its time on something you don't even care about that much and not spending any time on the things you need to care about. That's why I think those rails are critical to this kind of incremental approach to those efficiencies.

The last thing is, do not be afraid to build new infrastructure, particularly for agents. Stripe has seen a lot of success. We have so many agents, and you can imagine just how much we're using this now across Stripe. It doesn't have to take that long to build either. We had something in a month, and you could do the same if you don't have it yet. Maybe you do.

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a0206e4dbd6a7de3/3150.jpg)](https://www.youtube.com/watch?v=NKF4qKnOaCE&t=3150)

So that's all I have for you today. I hope this was a good learning session for all of you and you'll be able to take some of these learnings and implement them in your own organizations. We'll wrap up this session. The Stripe team will be here to answer any of your questions that have been left unanswered. One final thought is what Chrissy initially mentioned about Stripe's Agentic Commerce Protocol, or ACP. If anyone of you are interested in learning more about it, there is a session on Wednesday around 2:30, so you can look that up and come there and meet some of the other Stripe folks who have been working on this protocol. In the end, please make sure to give feedback  in the app. That's how we improve. Thank you very much.


----

; This article is entirely auto-generated using Amazon Bedrock.
