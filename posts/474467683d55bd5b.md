---
title: 'AWS re:Invent 2025 - AWS storage beyond data boundaries: Building the data foundation (INV215)'
published: true
description: 'In this video, AWS VP Andy Warfield discusses storage and data innovations at AWS, focusing on S3''s evolution after 19 years. He covers S3''s massive scale (500 trillion objects, 200 million requests/second), infrastructure innovations like metal volumes achieving 10% power savings, and S3 Express improvements with 85% price reductions. Key launches include S3 Vectors (now GA after 250,000 indices created in preview) for similarity search, S3 Tables enhancements with cross-region replication and Intelligent-Tiering support, and S3 Metadata for SQL-based bucket interrogation. Customer stories from GoPro (450 petabytes) and Indeed (85 petabyte data lake) demonstrate real-world applications. The session emphasizes AWS''s building block philosophy and continuous refinement of storage fundamentals.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - AWS storage beyond data boundaries: Building the data foundation (INV215)**

> In this video, AWS VP Andy Warfield discusses storage and data innovations at AWS, focusing on S3's evolution after 19 years. He covers S3's massive scale (500 trillion objects, 200 million requests/second), infrastructure innovations like metal volumes achieving 10% power savings, and S3 Express improvements with 85% price reductions. Key launches include S3 Vectors (now GA after 250,000 indices created in preview) for similarity search, S3 Tables enhancements with cross-region replication and Intelligent-Tiering support, and S3 Metadata for SQL-based bucket interrogation. Customer stories from GoPro (450 petabytes) and Indeed (85 petabyte data lake) demonstrate real-world applications. The session emphasizes AWS's building block philosophy and continuous refinement of storage fundamentals.

{% youtube https://www.youtube.com/watch?v=beWO7h7Ut44 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/0.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=0)

### Building Blocks: The Philosophy Behind AWS Storage Services

 Please welcome to the stage Vice President and Distinguished Engineer at AWS, Andy Warfield. Hi everyone. Thanks for coming. It's really wonderful to be up here. This is one of the highlights of the year for me because I get to stand up and spend a bit of time telling you about all the things that have happened over the last 12 months. Putting the talk together is really fun because I get to go and reflect on all the stuff we did and all the things we've learned.

I'm Andy. I'm an engineer across our storage and our analytics teams, and I'm going to tell you about some of the things that have happened this year. Thanks for coming. When we talk about the stuff that we build at AWS, we use this term a lot: building blocks. As I was starting to put things together today and thinking about the work that our storage teams do, I found myself thinking about building blocks a lot. When we say building blocks at AWS, we mean a fairly precise thing, and it's a thing that I think our engineering teams really internalize. I think it's useful to talk about it for a second.

The services and primitives that we build are rarely used directly by end consumers. They're often used by builders who build products on top of AWS and then deliver their own services out to consumers. Our building blocks are polished for you as builders. Building blocks have a specific shape. A good building block, if we are getting these right, is something that you don't have to think about very much. It's something that removes a thankless, effortful bit of work and allows you to move on and work higher up the stack from that level.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/130.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=130)

When I work with the engineering teams across storage and data at AWS, it's remarkable to see how they approach things thinking about building building blocks.  At the start of this talk, this is my third year doing the storage and data talk, and it's been really enjoyable to go through it. I try to take a little bit of time to talk through the lens of the engineers and the folks that work building these building blocks. Building building blocks is a neat thing because the measure of success is actually a lack of friction. If we're doing things right, you're not struggling with what we have.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/170.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=170)

That's really a mindset that you see in the folks that build these systems. They would like to be invisible. They would like you to not think about the stuff that we do. When you think about what that means, building those building blocks for a service like S3 or a lot of our other storage services,  S3 this year is 19 years old. It's almost two decades old. We've been working on this building block for almost 20 years. A lot of that work is finishing work, sanding, refining, and identifying those places where we're still not perfect.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/220.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=220)

When I talk to the engineers on the team, they're so acutely aware of those imperfections, and it bothers us all. We spend a lot of time working on those things. The problems and the work that we have to do change year on year because the scale just continues to change. At this point, S3 stores over 500 trillion objects and serves over 200 million requests per second worldwide.  We process over a quadrillion requests every single year. It's just a phenomenal scale. I've been working on these systems for eight years at AWS, and I was blown away by the scale on the first day that I started. I spent the first few months quietly reflecting on the size of the systems that I got to work with, and it's just continued to grow over that entire time.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/260.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=260)

When we think about that through the perspective of building blocks and working at this scale, there are things that are invariants in how we engineer. The teams refer to these as fundamentals: security, durability, availability, performance, and elasticity.  If we are getting these things right, you will find the tools that we build easy to use. All of these things are incredibly difficult to build, and we often have to rewrite whole sections of our systems as we hit the next levels of scale. At any given point, we're constantly rewriting things.

One example is that in the time I've worked on S3, we've rewritten almost the entire data path in Rust. Almost all of the code that touches requests from start to finish is different than it was eight years ago when I started. It's just continuous reinvention and improvement of the services we built. About 80 percent of what we do falls into this category of quiet, innovative work.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/310.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=310)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/330.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=330)

Within that 80%, we do announce quite a lot of stuff, but they either surface not at all because they're scaling issues, or they surface as small "what's new" posts or little updates on refinements to the system.  Since 2020, we've had over 1,000 launches. It's really cool to reflect on the rate of change and all of the work that's been done. Storage more than anything else is something that has very little tolerance for not working, and so the teams that build storage tend to be really conservative.  There's this tension in the teams between going fast and being careful. We're always working on that tension and trying to deliver with velocity while maintaining the level of quality that you expect from our services.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/370.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=370)

### S3 Conditional Operations and the Power of Small Features at Scale

I'm going to call out one example of the type of thing that I mean with the scale of these smaller updates. In August 2024, so a little over a year ago, we launched a conditional put operation for S3. This is a nerdy little feature.  What conditional put does is allow you to put data into S3 with a header on the put that says only let this put succeed if the thing that I am overwriting is the thing that I expect to be overwriting. It allows me to specify an invariant at put time that I'm moving the system from a known state to a new known state.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/440.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=440)

The reason that's important is if I'm building a distributed application with a whole bunch of clients writing data into S3 and two of them write to the same object, and those puts race, there's a risk that I silently lose one of those updates, which depending on the semantics of my application, may be quite a bad thing to have happen. Without having a facility like this, customers have to add complexity to their systems to track the potentially racing puts externally with something that has that level of consistency and tracking, possibly a lock or something like that. By launching conditional put, we start to see it pick up, and in fact, today conditional puts are actually used for about 2% of all puts to S3.  That may not sound like a lot, but it's actually a remarkable property to see at the scale of S3. Two percent of put traffic using this flag represents tens of millions of requests every minute.

The team was really excited when we launched this because there was a bunch of blog coverage. We're all technologists and we really love it when people express excitement about the stuff that we build. We got picked up in Architecture Weekly, for example, and there were a bunch of really cool examples of people building distributed systems constructs using just S3 conditionals, doing write-ahead logs and leader election and things like that. It was just kind of cool. The bit that I think we all found to be more rewarding than that, the bit that was really satisfying, was that after the launch of conditional put in customer meetings, we would occasionally have someone say, thank you for doing that conditional put feature. It let us delete a whole bunch of code.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/520.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=520)

It's so awesome when you sit down and you hear about an improvement that we made at the data and storage level, simplifying applications and being worth the effort to go and tear out a bunch of complexity because now we take care of that. This is the kind of improvement that the teams are just relentlessly curious and focused on. Here are four other examples out of the huge number of things that have happened over the past year.  We increased the size of the largest object that you can store in S3 to 50 terabytes. We didn't stop at conditional put; we've added conditional copy and delete operations. We've added an atomic rename operation in S3 Express, which is a performance zonal S3 used a lot for file-style workloads, and so having a rename operation turned out to be a really valuable thing in terms of having the ability to close out transactional interactions at the file system level.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/570.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=570)

With S3 batch operations, we've added the ability to instead of specifying a manifest of the objects that you want to work with, you can now just specify a prefix, which is a small thing. But when you have a manifest of millions of objects that you want to apply batch operations to, it's actually a really meaningful change. I've talked about the work that happens with scale and some of these small features that the teams are just continuously focused on.  I wanted to take an opportunity to go way down to the bottom of the stack and tell you about some of the innovation that we don't get to talk about as much, but I absolutely love. I wanted to talk a little bit about some of the hardware and software work that happens at the very bottom of our storage services.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/600.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=600)

### Metal Volumes: Reinventing S3's Storage Infrastructure with Disaggregated Architecture

S3 is built on top of hard drives. I don't know if any of you have heard me talk before, especially about hard drives, but I can stop right now.  We can talk for three hours about hard drives. I find hard drives really exciting. I'm going to spare you that. They're engineering marvels.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/620.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=620)

I love talking about hard drives. We put the hard drives into shelves, and we put the shelves inside servers, and this is how S3 was built for the first 10 plus years of its life.  We still build in effectively this form factor. This model is called a JBOD, and so there is a server sitting underneath a sled of hard drives, and we drive to get more and more efficiency in our data centers year on year. Hard drives get larger in terms of their capacity, and so we put larger hard drives into our shelves. We try and drive the density of hard drives per server to higher levels. We're really trying to optimize costs and optimize efficiency.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/660.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=660)

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/690.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=690)

It is a remarkable thing to me as we scale out even further, putting this JBOD in a rack, putting that rack in a row, building out into a data center, that in these situations, we actually have this metric that I'd never heard before working on storage at Amazon, which is bytes per square foot.  This is a bizarre way to think about systems, like a sort of aerial metric of storage capacity. This thing of us building denser and denser JBODs went on for years, and it got to a point where we actually decided to back off of it a little bit. Dave Brown talked about this in his talk last year. The sort of peak JBOD for us was a system design called Barge that we actually stepped back from. 

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/710.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=710)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/720.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=720)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/740.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=740)

Barge at the time was using 20 terabyte drives. We were packing as many of those things as we could into a single server,  and we were building single racks with 6 petabytes of capacity. That's a lot. We really liked the cost basis of Barge,  the density, and the economics that it presented. But as we were moving to these densely packed servers, if the server failed or if we needed to move things around, we were dealing with a really coarse chunk of data. We were starting to get to the point where the server scale of physical bytes in S3 was becoming unwieldy for us to manage and to really provide on top of. 

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/770.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=770)

We wanted to see if we could move to something that had a bit more flexibility. As we talked about it, we ended up pulling in folks from the EBS team, from EC2, talking to folks that build EC2 Nitro, and we decided to completely reinvent the storage construct that we use. What we ended up building was something called metal volumes. This is a picture of a metal volumes rack, and what you see is the teal sort of outline thing, which is a Nitro compute node.  Its job is to securely virtualize the physical discs in the shelf. The rest of what you see there is discs. Nitro manages a bunch of discs, it securely virtualizes them, and allows us to connect the physical storage fleet in S3 to our shard store host, the file system that we build that maintains our disks running on diskless EC2 instances.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/810.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=810)

Rather than running those servers on physical servers connected to the JBODs, we're moving and we're able to use whatever compute we want from EC2.  This is really neat. It gives us a greater degree of availability because when those servers do fail, we can simply remap the disks to an alternate server, and we can decouple the servicing task of working with that initial server that needed attention. We're able to move things around that way. Similarly, when drives fail, we can map an additional drive. We're decoupling physical service from the software job of maintaining availability.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/850.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=850)

A second benefit that came out of the design was the ability to change form factor for the compute host that we were using to manage the drives. As we realize software efficiencies, we can increase the ratio of drives to servers and move to smaller servers, and vice versa.  If we need extra compute for some reason to perform some job or to work with the data that's on the disks, we can provision that up as well. The construct has really been a cool thing. Now it's not without technical challenges. One of the challenges was that we added this network link between the disks and the servers. Like a lot of storage systems, we write in chunks of data on the disk. This is especially true with zonal media like SMR.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/880.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=880)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/900.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=900)

Not all data has the same lifetime. As data is deleted, you end up with these holes in the data, and what you really want is large physical extents that you can write new data into.  We have to do background maintenance in our system, defragmentation. This was even a thing on Windows 95. We're continuously rereading and scrubbing and cleaning the data on the drives to free up space. Except that when we were doing this with metal volumes, we were having to move all that data over the network.  The team worked with Nitro, and we built a facility that we called Nitro Offloads.

What Nitro Offloads let us do is basically extend the NVMe interface and push some of these low-level operations to be performed by Nitro at the drive. As a result, we get about a 60% reduction in this case in the amount of traffic that goes over the wire. This is one example of the kinds of innovative work that the team is doing with this disaggregated structure of hard disks.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/980.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=980)

We are early on in deploying this construct, and when we deploy anything interesting in our storage services, we do it very carefully. We take advantage of erasure coding to deploy it to a small subset of any given object at a time. We roll it out carefully and very methodically, limiting the exposure that it has. We are very early in that process. When I say early, we are only 53 exabytes into deploying this thing, which is remarkable to me. 

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/990.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=990)

Another really cool outcome of the metal volumes work was that in right-sizing the virtualization compute, the network, and the flexibility of host compute at the end, we were actually able to realize over a 10% power savings end to end in our storage fleet, which was a really great result. 

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1020.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1020)

### S3 Express Performance Improvements and Meta's Large-Scale Training Workload

The last bit that I am going to talk about is one of the core aspects of our services: S3 Express. S3 Express is a high-performance zonal version of S3. We launched it three years ago, and the Express team just continues to innovate. We have gotten efficiencies in how Express is built, resulting in up to 85% price reductions for the system. That has been really popular. 

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1040.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1040)

We have improved the performance of Express buckets up to 2 million requests per second. As I mentioned, we added object rename, and we have also added Access Point support. One customer who the Express team has been working with closely is Meta, and Meta is doing large-scale training on top of AWS using Express as a direct zonal backend. They are an incredible storage workload for Express. They have over 60 petabytes of storage, perform over a million transactions per second, and are driving 140 terabits per second of traffic into Express. The experience has been great, and the team has loved working with them. 

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1080.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1090.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1090)

### Introducing New Storage Primitives: S3 Tables and S3 Vectors

As I mentioned at the start, a lot of the work that we do is refining, sanding, and completion work. Even after 20 years of working on some of our earliest building blocks, we are still improving, and the team still feels like we are refining them. But occasionally, we get to carve entirely new faces.  We get to build and introduce new primitives in storage. Two of these primitives that we have talked about in the past year are S3 Tables  and S3 Vectors. Tables offer managed Apache Iceberg tables as a structured way of storing data on top of S3, and S3 Vectors are a vector database, a vector index API that allows you to store vectors and perform similarity search within S3. All of these things are built with the same sort of fundamentals that S3 has.

Last year, I was standing about 18 minutes ago, just back there, about to come out and announce Tables, and I was wearing the anxiety of the entire team. It was a remarkable moment because S3 is a service that we are all so invested in, and we have a lot of pride in the way we build it and run it. We were extending its identity in some senses. We were changing a little bit what the surface was for S3, and we were worried that while we had talked to a lot of customers about Tables and had the feedback on Iceberg that this was exactly what the largest Iceberg customers on S3 wanted, changing S3 from being an object service to being a more general storage service that also offered other data types was a thing that maybe would not land as well as we hoped it would.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1180.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1180)

I remember sitting back there and sweating about how the delivery of that slide was going to go. In fact, what happened was the response was really remarkable.  In fact, there was a tone of impatience from some folks around the fact that what took so long to get to this. People immediately had this reaction to the launch of Tables that I think the team really learned a lot from, which was that your definition of what it was to be a table on S3 was much closer to that list of fundamentals than it was to a specific API like objects. That is really where we have come from as we have approached building Vectors.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1210.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1210)

 This is a really neat story. When we launched S3 Vectors in July, we launched it as a preview, and it's probably been the most popular preview that we've had in our storage services. In the five months that we've been operating the preview of S3 Vectors, over 250,000 vector indices have been created, over 40 billion vectors have been ingested into the system, and we've served over a billion queries in total. It's really taken a lot of action.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1230.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1230)

### S3 Vectors: Enabling Similarity Search at S3 Scale and Cost

 So why is a vector API in an S3 form factor interesting? Why would the S3 team think about building a vector database? I think a useful way to think about the value and the interest that we're seeing with vectors is that the way we work is changing. I'm a huge music enthusiast, and I'm planning a summer barbecue. I've got a CD collection and I'm building an index of my CDs in Parquet in a table. I'm going to write some code for this.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1270.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1270)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1290.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1290)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1310.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1310)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1330.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1330)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1350.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1360.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1360)

 If we imagine what it was like to write this code even two or three years ago, I sit down and I write the software that tells the computer exactly what I want and exactly what data it should use. Over the last year, especially with more agentic-driven workflows and using generative AI for development, the way we interact is changing and is upleveled a bit.  I asked for something much more ambiguous: build a mix of 80s songs for my summer barbecue.  This goes through some kind of chain-of-thought reasoning process and produces the code.  At this point, producing that code is a reasonably unsurprising result. The bit that is really interesting from a storage perspective is not that part.  It's that the agent hopefully sees an awful lot of my data and needs to decide that to answer that question, it's going to have to go use that table of metadata for my CDs and the bucket of music that I have. 

It's worth reflecting on the fact that knowing I had those two data sources and plumbing them into the software is something I take for granted that my brain is pretty good at, but most data structures and algorithms in computer science actually have a pretty hard time with. The idea of finding the right data without pre-indexing, without knowing exactly what has to be done, is where vectors step in. That ambiguous problem of choosing the right data to answer this question is what vectors solve.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1400.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1400)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1420.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1420)

 When I go back to my CD collection, I've given it metadata with the columnar structure of artists, tracks, albums, and release dates. But there are so many other things that are properties of this music that are important in thinking about it.  There's the tonality of the music, whether it's upbeat, whether it was in a commercial, and so on. I can't conceivably go through and label all of these things upfront. These are just semantic concepts about the data.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1440.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1440)

This is what's really changed and been remarkable recently. I can take these things, turn the media into a vector representation, which is really just a list of floating point numbers, and embed that vector in a space.  This is a three-dimensional space. When you ask a graphic designer, even a very excellent one, to draw you a 1,500-dimensional space, they get really upset at you, so we decided to stick with three for the talk. The concepts that you see called out map to clusters inside the vector space, neighborhoods.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1480.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1490.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1490)

What I do now is use an embedding model, the same embedding model that I used to place those vectors in there in the first place. I take my query, generate a vector that is a point inside the space, do an approximate nearest neighbor search,  and find the results that are similar to that thing. This is how a similarity search works. It's the basis for retrieval augmented generation and is also used in a bunch of other emerging fields outside of AI, like identifying interesting images in radiology and doing fraud detection.  Vectors are very much emerging as a neat tool.

Now, a tricky thing and part of what really drove the S3 team with the work that they did on vectors was what we discovered about the size of vectors relative to the size of data.

We found some interesting properties. If you were working with photos or video, using ballpark numbers with various assumptions about how you build your vectors, vectors are typically about 0.1% the size of the data you're storing. Vector databases typically have to be stored in DRAM or SSD. When you start generating vectors on text, you find that text is a very dense source of information relative to its storage size. Vector indices for text are often 2 or 3 times larger than the text you're storing. Since that vector index has to be stored in more expensive storage, it represents quite an explosion over that type of data.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1590.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1590)

This text data that people are indexing includes everything from PDF documents to call transcripts from call centers to code and similar content. It's a really meaningful consideration, and this was a reason not to use vectors. A lot of what drove us into the work with vectors in S3 was wanting to achieve an S3 cost structure for vectors that made this very useful indexing structure available to any data that anyone wants to bring it to, at any scale. Today we're launching S3 Vectors.  The five months of preview was incredibly helpful for the product and the team. We worked closely with customers who adopted vectors on day one. We worked to improve the latency of queries down to a design of 100 milliseconds, which is a big improvement over where we launched. That was one significant piece of feedback we received.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1650.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1660.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1660)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1690.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1690)

### GoPro's Journey: Managing 450 Petabytes of Media with S3 and Vectors

We've increased the maximum index size to store up to 2 billion vectors with 10,000 indices per bucket, and you can upload at a rate of 1,000 vectors per second. I'm going to tell you a little bit more about vectors, but I thought it would be good to take a moment and let you hear about the experience of using our storage services from a customer.   Please welcome Cedric Fernandez, the VP of Engineering at GoPro, to the stage.  Thank you. I love that video, just the pure joy on their faces coming down the waterslide. It's so uplifting. Good afternoon, everybody. If I were to ask you about GoPro's line of business, you would more than likely say that we're a consumer electronics company that makes action cameras. You'd be right. We've sold 50 million plus of these devices to date globally. But we're so much more than that.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1710.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1710)

Our mission is to help people live and experience life's moments and then share that with their friends and family. To achieve that mission, we've got to deliver a GoPro experience that goes well beyond capture.  It helps you offload that content, manage that content, and discover that content effectively. It helps you tell beautiful stories in an immersive and evocative way and share that with your friends and family. Since 2015, we've been working on this platform. It's cloud-backed and runs across multiple devices, but it helps you do this in a painless and seamless way. That's the GoPro experience we want for our customers.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1740.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1740)

If I were to ask you similarly what you believe people capture with their GoPro cameras, you would more than likely say action sports because that's what we're known for. But interestingly, people also use these devices to capture all sorts of content beyond action sports.  Whether it's a backyard birthday party, a religious ceremony, a bar mitzvah, or in my own case recently, my daughter just graduated from high school and we captured her graduation party. So we have a lot of diverse content in our systems. It's not just short form; it's also long form. You all use your cell phones, mirrorless cameras, and other devices to capture content and your life's moments these days. The GoPro experience is very different.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1760.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1760)

What we found is people follow what we call the fire and forget model.  You go out, you turn on your camera, you let it run, you live your life, and then hopefully you've got some beautiful moments that you can share with the rest of the world. In that scenario, unlike your cell phones where you're actually framing the shot and composing the shot, the experience is quite different.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1820.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1820)

So we end up with all of this tremendous long-form content in our systems as well. Take, for example, a surfing session. You go out, you turn your camera on,  you're paddling, you're waiting for a wave, and if you're lucky, you catch a really good wave. Maybe 10 to 15 minutes out there, you're capturing content, but what you really need is those 5 seconds or the 7 seconds of you riding that wave. That can be a very painful experience. I've gone through this, and I'm sure many of you have as well, where you've got to go through reams and reams of video just trying to find those special moments.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1870.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1870)

That's the experience we want to solve for at GoPro. So why is all of this relevant to a conversation on S3 and S3 Vectors? Well, we have a lot of content. 450 plus petabytes of media today sitting in S3, a billion plus media assets,  and we're growing at about a million assets or so per day. As an aside, we've been able to scale out the way we have over these years because S3 interestingly also follows this fire-and-forget model that we experienced with GoPro. We don't have to worry about SSDs. I am so thankful that's someone else's problem and not mine. We've been able to scale out, honestly, without even blinking an eyelid. It's super helpful for us.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1920.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1920)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1940.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1940)

Now, our challenge, given that we've got all of this content, is how do we make sense of all this immense content in a cost-effective and scalable way for our customers. That's the challenge we need to solve for at GoPro. In the past, we've done this  through numerous mechanisms: computer vision models, machine learning algorithms, and we have a number of sensors on our camera that are collecting a lot of information that we can leverage to identify certain key moments in videos.  We've used those things, and these are all very complicated pipelines. They kind of do the job, but they're very complicated pipelines, and they're missing a very key component for good storytelling.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/1990.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=1990)

Now, you all knowâ€”those of you who capture video and want to tell storiesâ€”everybody tells stories differently. The way I tell a story of a particular moment in my life might be different from how one of you tells a story, even if it's a very similar moment. In order to achieve that level of experience, context is super important. It's the key ingredient we've missed for a number of years while we've tried to achieve our mission. It was really hard to bring context to that experience. Well, fast forward to todayâ€”that's the promise of AI, right?  LLMs, agentic core from memory, vectorsâ€”it's all about vectors these days. All of these things put together allow us to achieve that.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2030.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2030)

So we're really excited about what these technologies can do for us and for our customers, and we're really looking forward to making this happen in the future. Interestingly, it's a mouthful. You look at LLMs, vectors, multimodal embeddings. But when I look at the architectures we're deploying today with these capabilities, they're much more simplified than the architectures we've actually employed in the past. So there's lots of opportunity here, and we're very excited about doing this going forward. Thank you. Back to you.  It's so cool to see that. I have three kids that ski with GoPro cameras, and I'm very familiar with scrolling through GoPro film to find the air in them. So this democratization of data structures that let us search in really complex media like video is a really interesting thing, and I think we're going to see a lot more out of it.

### Technical Deep Dive: How S3 Vectors Achieves Cost-Effective Vector Search

As I sort of hinted at before, this is not just an AI/ML thing. The use of similarity search in these high-dimensional spaces is something that we are seeing customers, especially in healthcare and sciences, using to do structural search on molecules, as I mentioned, looking for differences with radiology images. It's something that feels very young in terms of the computer science that's behind it, and it's something that I think is super exciting to seeâ€”that both the algorithms and the applications are moving forward so quickly.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2120.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2120)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2130.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2130)

I think we're probably going to be talking a lot more about the kinds of innovative work that you are doing with vectors in the coming years. Now I wanted to tell you a little bit about the implementation and how we do this in S3 because it's quite interesting. As I mentioned, a vector database is usually an in-memory construct.  This is because to index these high-dimensional spaces, you typically build a graph.  You take these vectors and add metadata in the structure. If you can imagine using a tree to search lower-dimensional data, you build a graph that points to other vectors that are nearby in terms of a distance metric. As a result, finding nearby vectors means hopping back and forth to memory repeatedly.

This works great when your data is in memory or on fast SSD. However, it does not work nearly as well when your data is entirely in S3 and you have higher hard drive-style latencies to access that data. Your vector searches end up taking many seconds or even minutes because of all the round trips to storage. To drive the cost structure down and to get the elasticity that we wanted, we had to approach the design of vectors as an S3 problem.

What we ended up doing was observing that a lot of the vector data structures use clustering or a neighborhood-style top-level encapsulation of the vectors. They group them into nearby groups, and vectors may exist in one or more of these groups. When you do a query, you find the relevant neighborhoods and then search inside them. Rather than using S3 as a latency-sensitive store, we shifted to use it the way that it works really well, which is as a widely parallel throughput-sensitive store.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2210.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2210)

We took these neighborhoods, encoded them as objects, and when you do a query to S3 Vectors,  the query will actually promote a number of those neighborhoods into faster memory for the duration of your query. It searches locally and in parallel across those objects and leaves that data resident long enough for you to handle any subsequent queries that come by in the next little while. Eventually, it goes back to using the storage basis of vectors. The system is a co-design between a traditional vector database and a really throughput-optimized backend.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2260.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2260)

We borrowed ideas from some of the implementation under S3 Tables. It is structured as a log-structured merge tree underneath. We do a lot of aggregation of data in the background and continuous indexing. The system is really cool. Another example in addition to what we heard from GoPro's use is BMW.  I had trouble choosing which other customer story to tell you about with vectors, but BMW has built a vector-integrated data lake for understanding their own product quality. This is a really cool system in that the BMW team doing analysis against their data is doing natural language search. They have built a natural language search interface that sits above both Athena for querying structured data and S3 Vectors for querying less structured data, and then fuses those things together. This allows the analyst team to quickly work through enormous amounts of performance and historical data.

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2310.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2310)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2330.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2330)

### S3 Tables Evolution: Cross-Region Replication, Intelligent-Tiering, and Iceberg V3 Support

Now we will go back to last year's launch of Tables and talk about what has happened with that since then.  Tables was also a really wonderfully popular launch. We were really happy with the feedback that we got on it. At this point, there are over 400,000 S3 Tables being hosted on the system. An example customer for S3 Tables is Indeed.  Indeed already had a large Iceberg-based data lake. Indeed helps people find jobs, and in order to do that, needs data to be very recent and up to date. Every day they ingest over 0.5 petabyte of update data into the system. They maintain an 85 petabyte data lake and are continuously serving queries to help their customers.

Indeed is in the middle of a big migration into S3 Tables and is being driven to do it largely because of both the performance acceleration and the reduced cost and operational effort of running on top of Tables. Now I need to go back. I guess I am using CDs, so maybe I cannot say rewind. Let me skip back a few tracks and go back to my example of my CD collection. This is actually a good example of a Tables-based database. When we are talking about Tables in Iceberg, we are talking about structured columnar data. For years and years, people have been hosting these types of data lakes on top of S3. MapReduce jobs ran against Parquet in S3. Over the past several years, what has changed and what led us to build S3 Tables was that

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2420.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2430.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2430)

as analytics frameworks became more powerful and ran on top of this data, people increasingly wanted this data to be mutable and act like a database backend. That meant being able to do inserts of new data, for instance, adding rows to the table,  doing upserts, deletions, and all the changes you would expect to be able to do with your table store at the backend of your database. When we look at what is happening  in Iceberg under the covers of that workload, it is important to realize what it takes to build a construct like that on top of S3. Remember that S3 is immutable, so this mutable construct built on top of S3 is effectively a file system being built on top of the object store.

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2460.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2460)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2470.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2480.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2480)

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2490.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2490)

My catalog is a bunch of layers of metadata, ultimately pointing to data stored in Parquet. As I make changes to my table, those changes result in new metadata files being written, pointing to both the new Parquet files and the old data inside the table.  I have simplified this diagram quite a bit, but as that process proceeds, I will accumulate more metadata, snapshots of the database, and more Parquet files.   When we first sat down to design S3 Tables, we talked to the founders of Iceberg, the folks at Netflix who were very passionate about that work. They shaped our design enormously.  We worked with some of the largest Iceberg customers who were using Iceberg in production at the time, and what we heard from many of those customers was that they loved the flexibility, expressiveness, and mutability they could get on top of their tables.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2530.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2540.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2550.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2550)

However, they did not love running background tasks and the maintenance involved in doing garbage collection, cleaning up small Parquet files into larger Parquet files, which is required to maintain performance. Customers were asking us to make tables behave more like they were used to S3 behaving in terms of being a first-class storage system for tables. The expectation at launch was even ahead of what we had managed to deliver at the time,  and the team has been trying to fill the shoes that you set out for us, or perhaps that we set out for ourselves at launch.  The number of launches through this year has been absolutely incredible. One launch the team was really proud of, earlier in the year, was achieving a 90%  compaction price reduction for maintaining those tables.

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2590.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2590)

When we first built S3 Tables, we were running compaction with our own isolated full-scale Spark clusters, and the team has worked ruthlessly to continuously optimize this. We moved our compaction to run on top of a lightweight Firecracker container, and we have continuously worked to optimize and essentially own that compaction code to make it as efficient as possible. That has been one improvement. There are a few new improvements this week that I am really happy to share. One of them is cross-region replication.  With cross-region replication, customers have asked to be able to replicate their tables across regions. Conceptually, it is a pretty simple thing, and it is something that everyone is accustomed to doing with objects in S3.

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2620.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2620)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2650.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2650)

However, this is something where, as we find with a lot of features with S3 Tables, the feature is easy to explain but subtle and a little bit complex when you think about these data structures under the covers. The reason that replicating snapshots or views of your table across regions is tricky is that when we replicate objects,  we have the freedom to do it in any order. We replicate objects with an RTC guarantee of how quickly we will replicate, but as the objects arrive, we move them in the background as a massive batch operation. With tables, if you do that, you can cause trouble because the table internally has data dependencies down that snapshot.  If I replicate that top-level metadata and I do not replicate the metadata nodes that it points to, my query will actually fail because the engine that I am using, Spark or PySpark or whatever, cannot find the files or the objects that other objects point to.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2690.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2690)

The team had to innovate to clean this up. We built something that I am going to refer to as DLWKC consistency. This is not a super technical term. It stands for "don't leave without the kids in the car" consistency. What this really does is pretty much what you would imagine. We wait with publishing that top-level node until we know that everything is there, until we know that everything that sits below it is there, and then we make the top-level of the snapshot visible.  Now your snapshots atomically show up as groups on the remote end, and you can trust your workloads to run against them. Everybody is happy. A second aspect of what we have done is Intelligent-Tiering. Intelligent-Tiering launched eight years ago for S3.

I'm sure you're familiar with S3 storage classes. When we launched Intelligent-Tiering, we had heard that customers loved the gradation of storage classes that we have for data in S3, but it's a bit of work to manage which objects should live in which one. Intelligent-Tiering tracks the behavior of your objects, and as your objects remain cold for over 30 days, we will move them into an encoding that's more efficient and allow you to get cost savings. Similarly, at 90 days, if you go and access the objects, they're promoted back up to standard, effectively.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2750.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2750)

When you run these OTFs like Iceberg, those background tasks  act as a bit of interference for that. If the background cleaning tasks need to go through and touch those old versions of your tables, they end up pulling them back up to the front, and they end up not playing well with Intelligent-Tiering. The team sat down and worked out how to make those background maintenance tasks something that we could schedule in the background and genuinely get out of the way. We were able to structure it in a way that let us keep the cost of that out of the way.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2800.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2800)

When you turn on Intelligent-Tiering for S3 Tables, background access to the tables to do garbage collection and compaction does not change the tier of storage that your data is stored in. Only primary access changes the tier, which has been a really cool innovation on the team side. Finally, we've also rolled out support as we start to fill in features for Iceberg V3.  Roll lineage and deletion vectors are now in S3 Tables. There's more to come, and there's a bunch of really great engine support for this. That's the update on S3 Tables.

### S3 Metadata and Managed Tables: Curating Data with SQL Queries

S3 Tables, objects, vectors, and tables as these core data constructs have had this really neat knock-on effect in terms of how people are using storage. One thing that I hope resonates with all of you as you think about your own systems is one property that we're seeing a bit of work that almost every customer, especially with meaningful amounts of data on S3, is addressing. You see it in Cedric's talk. What so many folks are realizing is there's enormous value in their data and enormous opportunities to grow into new analysis, new applications on top of it, and do new stuff. But they have to curate it.

They end up building effectively metadata layers over the data that they have. In probably 60 or 70 percent of the S3 customer conversations that I have, somebody is working on this kind of thing. They're building this type of curated layer for metadata. Last year, a lot of the thinking that drove us to launch tables in the first place was we wanted to remove the undifferentiated heavy lifting of being able to build those metadata layers and also get you to a metadata structure in Iceberg and in vector APIs that you could trust other tools would be built to use, to further amplify the effort of collecting your own metadata.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2900.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2900)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2920.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2920)

At re:Invent last year, we launched S3 Metadata.  S3 Metadata was a captured journal of mutation events inside a bucket. Over the course of this year, we've continued to mature S3 Metadata, and now it presents a full both journal and inventory view of your bucket. You can turn on S3 Metadata, and this is the new data notebook that we've launched over the past few weeks. You can pull up that table  and interrogate properties of all of your objects using SQL queries.

You can interact with the contents of your bucket or prefix in S3 using queries, which is really accelerative. You can also use natural language queries converted to SQL to access information about your bucket. That's the thing that we're seeing folks do all sorts of cool stuff with in terms of understanding cost, understanding where they're growing, and things like that. The primitive of a managed table, which is what we use to build S3 Metadata, is a read-only table where the service manages it and fills it in with the content.

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2960.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2960)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/2990.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=2990)

In S3 Metadata,  the content is high integrity. It actually reflects what's in the bucket. You can't edit it, which allows you to have confidence that it is what you think it's going to be. That pattern was really popular across customers and across services. We were almost immediately asked to add it to other things. What you see with the announcement this morning of Amazon CloudWatch Logs being now available inside a managed S3 Table with S3 Storage Lens, SageMaker Unified Studio,  and of course the S3 Metadata data in there is a set of effectively AWS service or system tables that have state about your use of services.

The really neat thing about this is now you can do joins across all of that data.

This is not a simple SQL query, but it is quite powerful. You can see here an example query where I am joining across access logs to S3, VPC flow logs, and the objects in S3. I am triangulating on access denied errors and mapping those through to the VPC and the account that ran into the access error. It is a really powerful tool to be able to do this kind of debugging directly with SQL against logs that you already have access to.

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/3050.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=3050)

### Enterprise Migrations and FSx Integration: Bridging Traditional Storage with Cloud-Native Applications

The last thing that I would like to talk to you about today is enterprise migrations.  Enterprise migrations are something that continues to be an absolutely fascinatingly complex challenge, and it is something that we still, now 20 years in, often find ourselves supporting customers on, moving between enterprise data centers and the cloud. From a storage perspective, storage often is the critical component for those migrations.

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/3090.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=3090)

An enterprise that has admins managing loads of servers, often in VMs, often finds that it is not trivial, but it is workable to move those VMs  and application workloads into cloud compute constructs. However, the storage admin has a very complicated job. They have lots of bindings and mappings of that storage data across the enterprise, and they are familiar with the constructs that they have in terms of filers.

We launched AWS FSx as a set of managed storage offerings, managed filers that you could use to maintain the same experience of running the enterprise filer in the cloud. We have FSx for ZFS, FSx for NetApp ONTAP, and FSx for Windows File Server. It is a very popular way to initially land data in the cloud and start to build out new workloads.

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/3150.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=3150)

Now as FSx continues to grow in adoption, a pattern that we have heard in a lot of customer conversations is that they love the ability to replicate that data into the cloud, take advantage and burst workloads into the cloud,  but they would like to move into cloud-first development. They would like to be able to take all of that data that they have now replicated and light it up under other applications that are written in the cloud, often written against object APIs. Things like Amazon Athena, Bedrock, Bedrock Knowledgebases, AWS Glue, and Amazon SageMaker are bringing those tools to bear against your existing enterprise data.

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/3180.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=3180)

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/3200.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=3200)

Earlier this year, we launched S3 access points for FSx,  and what this lets you do is stand up an S3 endpoint in front of that existing FSx data and effectively attach to that data as if it was S3. Today we are extending that initial launch of access points for S3 on Open ZFS with support for NetApp ONTAP,  which is obviously very broadly deployed and an incredible filer for enterprise data centers.

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/474467683d55bd5b/3230.jpg)](https://www.youtube.com/watch?v=beWO7h7Ut44&t=3230)

With that, I am going to wrap things up. I hope that I am able to tell you a story about the quiet velocity of these teams. It is really wonderful to learn about the ways that you work with data and build these building blocks that help you move faster. 

I hope the sense that I am able to convey to you today is the fact that 80 percent of what the teams do is literally listening and looking for those sharp edges and continuing to polish. I am sure we will still be doing that polishing and sanding 10 years from now. But the work that we have been able to do in this moment of really enormous change that we are seeing over how we build over the past few years has given us this opening to grow and offer new primitives that are even more meaningful for the applications and the way that you build.

I really hope that you go and play with them and I would love to hear what you learned. Thank you very much for coming today and enjoy the rest of the week.


----

; This article is entirely auto-generated using Amazon Bedrock.
