---
title: 'AWS re:Invent 2025 - It''s About Time! Improving Distributed Systems with Amazon Time Sync (CMP409)'
published: true
description: 'In this video, AWS Principal Product Manager Josh Levinson and Principal Software Engineer Julien Ridoux explain how Amazon Time Sync Service achieves nanosecond-level clock precision through hardware-based synchronization using GPS signals and atomic clocks via the Nitro system. They introduce ClockBound, an open-source daemon that provides time with uncertainty bounds, enabling distributed systems to order events without network communication. Nasdaq''s Nikolai Larbalestier demonstrates using hardware packet timestamping to reorder transactions with microsecond precision for cloud-based exchange systems. The session showcases how YugabyteDB reduced retries by 1000x and how Aurora DSQL and DynamoDB leverage precision time for multi-region consistency, concluding with the announcement of a new AWS Time Daemon with cloud-native design and VMClock kernel feature.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - It's About Time! Improving Distributed Systems with Amazon Time Sync (CMP409)**

> In this video, AWS Principal Product Manager Josh Levinson and Principal Software Engineer Julien Ridoux explain how Amazon Time Sync Service achieves nanosecond-level clock precision through hardware-based synchronization using GPS signals and atomic clocks via the Nitro system. They introduce ClockBound, an open-source daemon that provides time with uncertainty bounds, enabling distributed systems to order events without network communication. Nasdaq's Nikolai Larbalestier demonstrates using hardware packet timestamping to reorder transactions with microsecond precision for cloud-based exchange systems. The session showcases how YugabyteDB reduced retries by 1000x and how Aurora DSQL and DynamoDB leverage precision time for multi-region consistency, concluding with the announcement of a new AWS Time Daemon with cloud-native design and VMClock kernel feature.

{% youtube https://www.youtube.com/watch?v=PecK_inxe5Y %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/0.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=0)

### Introduction: The Critical Role of Clocks in Distributed Systems

 Good morning everyone. As builders and as humans, we know clocks are important. Today we're going to cover why they're important and how they're good in the systems and technologies that we build. This is CMP409, It's About Time: Improving Distributed Systems with Amazon Time Sync. My name is Josh Levinson. I'm a Principal Product Manager for EC2. I'm joined by Julien Ridoux, who's a Principal Software Engineer, and Nikolai Larbalestier, who's a Senior Vice President of Cloud Strategy and Enterprise Architecture at Nasdaq.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/50.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=50)

 Today we're going to go very deep into clocks: making a clock, what is a good clock, leveraging and using a good clock, and then using good clocks globally at planetary scale on AWS. I want to remind you that this is a 400-level talk. You do not need to understand the concepts we're going to go through at the start of this presentation. Towards the end, we're going to share how we simplify things and we've done the heavy lifting for you so you could take advantage of this in your systems or using AWS partner systems to use these definitely tricky concepts in your workloads.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/100.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=100)

 I like starting with a quote, and this one from Barbara Liskov: "Synchronized clocks are interesting because they can be used to improve the performance of a distributed system by reducing communication." We're going to get back to this, how clocks reduce communication and how clocks make systems more simple when you take advantage of them.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/130.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=130)

 Starting just with a baseline before we get into what a clock is, let's look at what clocks are used for. I think we all understand clocks as a common scale of measurement. Historically, distributed systems have not been built to trust clocks. Clocks are used for human observability, that's log messages, metrics monitoring, UI dashboards. We think of sports, how much time is left in a period, or how much time it took for someone to run a race. Auditing is another place that I think we're all familiar with clocks in industries like financial services, healthcare, broadcast media, online gaming, making sure events happen synchronously at the same time. Then in the last quadrant, I have distributed systems with a question mark. Again, going back to that quote from Barbara Liskov, where if you have a reliable clock, you can simplify your systems, reduce locking, leader election, reduce those communications, and we're going to answer the how in this presentation.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/220.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=220)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/230.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=230)

### Building a Computer Clock: Hardware Fundamentals and Oscillator Drift

To start off, I'd like to talk about just what is a clock. Julien, could you share what is a clock? I will try my best. Thank you, Josh. So at the beginning of this journey, we really wanted to start with some of the basics, which is how to build a computer clock.  And at this level you really need two things, very simple things: one thing that ticks periodically and one thing that counts this number of cycles.  Usually this is a piece of hardware that is combining these two functions, but again, something that ticks, something that counts.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/240.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=240)

 If you have this information, you can then build a clock, and that's represented by the equation at the bottom of this slide. At a perfect time T, you could read the clock C here by saying that this is the number of cycles that have elapsed, divided by the frequency and some constant. Sounds a little bit esoteric, let me make it simple. Imagine that you have counted 1,000 cycles and you know that your hardware is doing one cycle per second. Well, 1,000 cycles, one second each, 1,000 seconds have elapsed. You have now built a clock. This is the fundamental component that allows you to build a clock on a computer.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/280.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=280)

All right, and just to talk about some hardware, what does that look like?  You have a range of options from the left-hand side here to the right-hand side. It ranges from small form factor crystal oscillators to equipment that you will find in a physics lab: caesium beams, hydrogen masers, and optical clocks soon to be released. In terms of form factor, you range from a few millimeters to things that are pieces of equipment that are the size of a cabinet.

Even these ones are a bit more difficult to fit into a computer, and of course the price point changes as well. On the left-hand side, it tends to be a better price point for cloud computing. On the right-hand side, this can range from hundreds of thousands of dollars if you want to buy this piece of equipment. So we have trade-offs in terms of small form factor or not, in terms of price points, but also in terms of the quality of the hardware you can purchase to build a clock.

The ability for hardware to keep accuracy to within one microsecond changes around that scale. On the left-hand side, you can hold one microsecond accuracy for about a couple of seconds. On the right-hand side, when you go to hydrogen masers and optical clocks, you can keep accuracy for years, if not centuries, or what's the promise. So you have a range of hardware that you can choose from to be the clock in the computer.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/350.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=350)

 But no matter what decision you make, all oscillators drift. Even the most accurate atomic clocks do drift. It's just that it will take them centuries for you to observe that drift at a microsecond level. In general, we tend to have a small form factor and a good price point, and these oscillators drift due to manufacturing processes or aging. These are mostly considerations that are of importance for physicists.

For us cloud providers building computers, we mostly care about small differences in power supply or temperature variations due to the impact of your workload on your processor, CPUs, and the server as a whole. So I did lie a little in my previous slide. I gave you an equation that said you can build a clock if you know the number of cycles and you know how long each cycle is. But the truth is that the frequency of this cycle does change over time, as is shown in this equation.

The net result is that clocks drift, and we are showing you an example here with this graph. Here we picked an EC2 instance that's been running for about five days, and we let the oscillator free run with no correction applied. What you see is that over a period of five days, the clock has been drifting by about 75 milliseconds, which is something you would expect. All right, so we have the hardware, we know how to build a clock.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/430.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=430)

### The Challenge of Clock Synchronization: Network Variability and Asymmetry

 We've observed that the hardware does drift over time. What can we do about this? Well, we need to track and correct for that drift. How does that work? Well, computers are no different than us humans. Imagine you have a wristwatch and that watch is not on time. You will ask a friend, Josh for example, what time it is. Josh gives you the answer, you can adjust your watch, and you're good to go.

Computers are no different. Here we are representing a small example that illustrates the Network Time Protocol that has been in use for decades now. Effectively, you have a clock that's the bottom line on this graph that needs to be adjusted. The top line is a server clock, a reference clock that you will have over the network, and the client clock sends a message asking what time it is. The server receives the time and sends a response back.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/490.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=490)

In this late example, the client can then have all the information it needs to adjust its time, realize that it's running two minutes behind, and will correct its clock and it's good to go. But is it?  The reality is that the frequency changes all the time. You cannot ask what time is it only once and be done. You have to continuously ask what time is it repeatedly.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/510.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=510)

So this operation repeats, and this communication over the network then sees messages being exchanged with slightly various delays and of course the risk of failure when messages are  dropped. All right, we talked about clocks, how to fix them, how to track the drift, track drift and correct them. So why is clock synchronization a hard problem still? The short answer is that the network is part of the problem.

This communication between a clock to be adjusted and your trusted friend who can give you the accurate time is problematic for two reasons. First, on the left-hand side, we'll show you an example. The first reason is that the variability and the delays of communications have an impact. Here, the blue line shows you the noise of the communication delays when a server repeatedly asks what time it is.

The hard problem is to identify this purple line here, which is actually the clock drift from this very, very noisy blue input. That's one key difficulty: seeing through the noise, seeing through the variability of the communication delays. The second problem that happens with the network is that the network is not symmetrical. So far I've shown an example where you can adjust your time, you have an exchange, and as you can see on the top graph, you have a nice triangle that is very symmetrical.

Imagine now that these two clocks are perfect. Imagine that we have achieved perfect synchronization. On that first triangle graph in here, it's great. Time is being exchanged, two clocks are in agreement, they are perfectly synchronized. Second example, clocks are still perfect,

but this time the network is not symmetrical. When the client compares the time from the server to its own time, it will observe a difference. That difference is not a clock error. We have assumed the clocks are perfect. It's just network asymmetry. But from a client point of view, it's ambiguous. It cannot make a difference between the clock error and the network asymmetry. So this is a very fast track view about what the problem is.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/630.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=630)

 A few years ago, we really wanted to have good clocks for the purpose of synchronizing our cloud. So we went through this analysis of what clocks are, how they are made, and what the problems are. We knew the problems of tracking clocks over a non-deterministic network is not something any of our customers wanted to have to worry about. So we put ourselves to work, and I'm going to let Josh take us there.

### Amazon Time Sync Service: Delivering Millisecond Accuracy with NTP

So Julien just covered a lot for 9 a.m. on a Wednesday, or if you're watching this recording. And again, as he said, the good news is you don't need to know that. We've done the work for you. We started with the Amazon Time Sync Service, which was an NTP Network Time Protocol endpoint, a link local for every EC2 instance worldwide. We invested in engineering and hardware to support this service globally, and we did this in 2017 for our customers.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/700.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=700)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/710.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=710)

I want to start by showing you what that looks like. So here's an EC2 instance.  You have the Nitro system down below, the instance up above, chronyd or your time daemon in your software. And what you have is an NTP packet  from our distribution system through the Nitro system to your server that you can use to correct your clock. The accuracy for our NTP service is under one millisecond. It's really in the 500 to 700 microsecond range. And customers were happy. This is easy, it's hands off. Again, you don't need to understand the science, the technology, the methodology behind doing this. It's handled for you.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/740.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=740)

### Achieving Nanosecond Precision: Hardware-Based Time Distribution via Nitro

 But customers have asked for more, and when you really want to use clocks in distributed systems, you need to get much better than one millisecond. And we challenged ourselves, how do you get to nanoseconds? Other cloud providers and scalers try to do this with software, and there are different ways that you could go through with lucky packets and very clever algorithms and designs to get close. We found in testing you really need to invest in hardware all the way down to the servers to get to nanoseconds. And this is how we did it.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/790.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=790)

Here's an animation of what it looks like. We have  dedicated cables from GPS, redundant atomic hardware clocks, and we send a signal, a reference signal, essentially a GPS signal to your EC2 instances. This is not over the EC2 network and the AWS network. It's not over the data plane, it's not over the control plane. This is entirely dedicated for clock synchronization.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/820.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=820)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/850.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=850)

 And I want to show what that looks like again at the instance, going deeper because this is a 400 level talk on what the infrastructure looks like. We really took the network out of network time synchronization, and everything's hardware based and Nitro based. It's a great story about Nitro and the Nitro system. So what does that look like? You now see there's a Nitro clock, hardware reference clock.  And if you're using NTP, your NTP packet, the Network Time Protocol, does not go over the network. It's just directly to the Nitro card and back.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/870.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/880.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=880)

We've also added a PTP hardware clock directly in the Nitro system.  And the way that that works for you is you read the PTP hardware clock, the PHC, and you can correct your system.  You correct the system clock right there. So as you see, there's no network communication. We give the PTP, the Precision Time Protocol for you, because that's what most software and systems are familiar with. But as you can see, there is no backend, there's no PTP distribution within AWS. It's all local to a reference clock. That is how we get in the microsecond and nanosecond range.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/930.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=930)

When you're using the Amazon Time Sync Service this way, NTP is going to be under 100 microseconds at the software level in your guest, and then PTP is going to be in the range of around 20 microseconds, again in the software layer at your guest. And then just to recap, this is built on  the Nitro system where we've added these reference PTP hardware clocks. What's really cool about that is we add them everywhere. It could be built on Graviton, on Intel, on AMD, on our latest GPU servers as well, because again the base is the hardware of the Nitro system and this large infrastructure investment we're doing worldwide to support this feature.

### Hardware Packet Timestamping: Unlocking Nanosecond-Level Network Observability

Now if you're paying close attention, you might realize I talked about nanoseconds, but that in the software layer, you're in the microsecond range. And our customers have called us out on this too. They said, hey, if you have these nanosecond reference clocks, how do we get to nanoseconds as well? Julian, can we share how we go further in getting to the nanoseconds?

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1000.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1000)

Thank you, Josh. Yes, we can definitely talk about one of the key applications. So far we went through explaining or describing what is a clock made of, how to correct for it, and the system we built at AWS to have this level of accuracy  in here. One of the key applications that our customers were really interested in is the ability to do hardware packet timestamping. This is a feature we've launched earlier this year and has a lot of benefit, first of all from observability. If you timestamp a packet closer to the wire, you get a more accurate view about the network performance, which is very important if you are running a distributed workload.

Some applications also are very interested in hardware packet timestamping because it allows them to order messages that you are receiving in your server. That facility is built in the driver and so you can access it today readily. It provides nanosecond resolution on received timestamps, for instance, for all the network packets that your server is involved with. How do you access this facility? You can access it through the classic socket API that most applications are built over or the DPDK toolkit that allows you to bypass kernel infrastructure and get direct access to the packets down to the wire. So with this facility, you get the benefit of a very accurate clock and gain a lot of insight into how your application operates over the network.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1080.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1080)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1100.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1100)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1110.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1110)

This sounds a little bit abstract, so I want to take you through the example of the before and the after.  Here on this slide, I'm going to illustrate how you would do the same things before this feature was released. So we are first looking at the application of timestamping packets, messages you receive at the application layer. Let me start with a simple animation. Here we still have the Nitro system that underpins our virtualization layers and the instance that runs on top.  A message arrives, passes through the Nitro system, goes through your instance. The application receives the packet, reads the time from the system clock. You  now have a timestamp attached to your message.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1140.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1150.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1150)

How much work does it take to do this? Well, on the right hand side you can see, it takes about four lines of C code in this example. You open a socket, here I'm using the socket API as an example. You receive the packet. Once you have received that message, you can read time. It's relatively simple. Let me now show you how you would do the same thing by using the hardware packet timestamping facilities. Again, the Nitro system is here, but now we have the Nitro clock that Josh described prior.  Again, let's go through a little animation. The message arrives. This time the packet is timestamped by the Nitro system and the Nitro clock before it reaches your  instances. Then the message and the timestamp created for you is then passed to your application up to the layers.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1180.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1180)

On the right hand side of the slide, a bit of an illustration around how much work does it take. Well, a little bit more. You can see the number of lines of code has increased. You still need to create a socket. You still need to receive a message, but you have to do a little bit more work to process control information, extract the timestamp that's been created for you. Nothing catastrophic, but just a little bit more work.  Now, what do you get from doing this extra piece of work in here and to access timestamps created by the Nitro system. So here we have a little illustration. We have two instances pinging each other across an availability zone, and we're recording the round trip time of these exchanges. We have two graphs, two lines on this graph.

They are both measuring the round trip time between these two instances. The line at the top in blue is if you measure this round trip time using userspace and your application, the first example. As you can see, we're in the range of 400 microseconds of round trip time between these two instances. The red line is measuring the exact same thing, but this time using the hardware packet timestamping offered by the Nitro system. This time we're in the range of about 100 to 150 microseconds of round trip time measured.

We measure the same thing, but our yardstick is much more accurate and much more precise. The difference between these two lines is around 250 microseconds in that example, and this is the amount of time spent in your operating system, the network stack, and passing this information to userspace. If you look at this example, this represents about two-thirds of the round trip time that is actually the time spent over the network. So you get a much more accurate view of what's going on. The observability of the network performance is becoming much crisper, and you can start identifying and breaking down what are the delays between your application and the network.

### Nasdaq's Journey: Using Precision Time for Exchange Systems in the Cloud

All right, I'm going to pass that back to Josh now. Thank you very much. Thanks, Julien. With this technology, we want to give examples of why this matters and how this is used. I'm very happy to introduce Nikolai, the Senior Vice President of Cloud Strategy and Enterprise Architecture at Nasdaq, to show how they are leveraging precision time with the Amazon Time Sync Service.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1320.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1330.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1330)

All right, thank you, Josh. Thank you, Julien. So building on all of that excellent background and work, we're going to walk you through a little bit of background on who Nasdaq  is and how we're using this technology to demonstrate progress towards our goals of running markets in public cloud.  So first, a little bit of background. Nasdaq is an innovator. We were the world's first electronic exchange, created in 1971. We operate more than 30 markets around the world in the US and Nordics that covers stocks, bonds, options, and other derivatives. We are known as the home of technology stocks globally, and we host the top four companies by market cap, of course, including Amazon, which we are proud to have as a listed company.

We also provide technology to more than 130 market operators, so exchanges, clearinghouses, and regulators around the globe, and provide technology to more than 2,300 financial institutions that could be for managing surveillance, anti-financial crime, capital flows, reporting, and a number of other things. We provide technology to more than 6,000 corporates to support their journey in both private and public markets through the full lifecycle of capital markets. We have a listing venue, and we have approximately 4,400 public companies listed on Nasdaq exchanges globally.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1420.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1420)

 Now we have been an innovator in cloud, and we started our cloud journey well over 10 years ago now, probably close to 15 at this point. But we had approached it very carefully and very thoughtfully, and so what you see here is a representation of how we started from the edge out. We started with our data repositories, data warehousing, so we had our first data warehouse in 2012, 2013-ish, created natively in AWS using Redshift. We've continued to evolve that. We now store about 60 petabytes of data in that data warehouse, and that underpins a number of key functions, mostly back office in this case, that support the operation of our exchanges, including billing and regulatory reporting.

But we didn't stop there. We continued our journey into more and more real-time processing, starting with things like our market surveillance, taking streaming real-time market information to perform that. Market data distribution is now available through the public cloud. You can subscribe to Nasdaq products and a number of alternative data sets.

And of course, in 2022, we announced and launched our first options exchange on AWS Outpost, NASDAQ MRX, and we've continued that cadence. We now have six plus market systems running on Outpost, so we continue that journey. In addition, a number of our critical surrounding systems that provide real-time insight to market participants on the trading activity that's occurring on the market are hosted natively in the public cloud. We take advantage of the scalability, the performance, and the right sizing to process the large volumes of messages we handle and manage the peaks in capacity.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1550.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1550)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1580.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1580)

Of course, we do have some specific challenges in moving to the cloud.  One of those is that, in particular in the US, there's a very specific geography in which trading occurs, and that's predominantly in northern New Jersey for equities and equity options. So that provides some constraints on where you can deploy the compute because there are latency dependencies between those exchanges that are all hosted in that general area. We also have ultra-low latency  transactions, and by that I mean something on the order of 20 microseconds measured as what we call Order to Ack. That's the time an order comes into our system, gets processed, booked in the match, potentially trades, and is returned to the customer.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1620.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1620)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1640.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1640)

Coming with that, we also have large volumes, well north of 100 billion messages a day currently, with any specific exchange system processing up to 2 to 3 million messages a second. We also have high resiliency and uptime expectations.  The capital markets depend on us. We are a primary listing venue for a number of public companies, as mentioned earlier, and us being there to provide that liquidity, provide that venue for people to trade and generate price discovery is a key function of how we keep our capital  markets efficient and operating well. And of course, we do have regulatory oversight of what we do, where we run things, how we run things, and what third parties we're interacting with.

So we put all these together, and I think you start to see the picture of why we've been very intentional over the past 10 to 15 years, continuing to move progressively to more and more real time and into the match. Okay, so now we're running these exchanges and many of our support systems on Outposts, and many of the supporting systems in public region, but we don't want to stop there. So this is where Amazon Time Sync comes in, and I wanted to talk through some of the prototyping and experimentation we've been doing to demonstrate some of the capabilities that will enable us to deploy markets into the public cloud.

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1700.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1700)

But first, a short primer on  an exchange system. So very high-level architecture view of an exchange trading system. There are really three primary components. Before I go there, I should mention that we operate three equities exchanges and six options exchanges in the US. And as I mentioned before, more than 100 billion messages a day, with a single exchange out of those six options exchanges running north of 30 billion messages a day. And as mentioned, the median latency is about 20 microseconds for that.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1740.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1740)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1750.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1750)

So if we look at  the components of the system here, we have, of course, at the top left, as you can see, the matching engine, and that is where the primary logic  to book orders and trades and match buyers and sellers and generate trades occurs. Going one level down, we have what we refer to as order ports, and those are the primary point of interface for customers who are sending order flow to our exchange and act as a point of validation. And in some cases, pre-trade risk and other functions occur there. Next to that on the right, we have what we call our public market data feeds. So this is a published view of what's happening in the matching engine to the market, NASDAQ ITCH for those of you familiar with it. And then, of course, at the bottom, but maybe the driver for the whole thing, are the clients that provide the order flow to the order ports to the matching engine through their inbound transactions for round trip through the system.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1810.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1810)

So,  with that background, an exchange trading system on-premises is a traditional model. It's a model we run today and most other exchanges in the world. You want to provide a deterministic and statistically fair market to participants. So how do we accomplish that?

Well, given that it's an on-premise data center, there are certain physical characteristics we can rely upon. That is the lengths of fibers in the data center and the number of switch hops between components of the system and customers. Those drive what you see here as D1 and D2, the primary sources of network latency in the system. And then D3 encompasses both the network components and the processing components, D3 being one half of the round trip. So you can think of that as roughly 10 microseconds. Because of those physical characteristics and the sequential processing of the matching engine, you result in an orderly processing of orders that are processed as they're submitted by customers and returned to customers and public market data feed in a consistent and repeatable time period.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1900.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1900)

 So that doesn't work so well when you're talking about public cloud, obviously. We definitely do not have fixed cable lengths. We definitely often don't have any idea really how many switch hops sit between nodes. And there can be various other things happening in the network. There can be other traffic that's delaying packets, or just the placement of compute nodes can be unpredictable. Even within the data center, it can be on one side or the other, or it could be across availability zones, or it could even be across regions potentially.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/1970.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=1970)

So what do you do about that? Given that we can focus on these three key time periods here, D1, D2, and D3, this is where the time stamps come in. So using hardware time stamps to record the time of each transaction at each point through the network, we can then  use this high precision distributed time source to understand when orders came into the system or were sent by customers. And hence then when we should process them to ensure the sequential processing and sequential distribution of those messages.

So if you look here at the key components that we have that are relevant, we have our reordering buffers at the top left sitting in front of the matching engine. And it's important to note that this proof of concept is really focusing, and what we're discussing here is focusing on the ordering of inbound transactions. Now there's obviously a similar problem domain for the network flow going the other direction. Similar techniques apply, and I think you can use your imagination to see how that would expand, but that's not the specific focus of this proof of concept that we did.

But using that reordering buffer and the hardware time stamps that are provided through Nitro and through the Amazon Time Sync Service, we're now able to determine, for example, T1 being the time of an order being received at the order port from client one, T2 being the time an order was received at the order port from client two, and T3 being the time of an order received from client three. Given that those are hardware timestamps with nanosecond precision, we can use that information to define a time-based boundary. And using the reordering buffer that understands those time stamps, we can put them back in the order that they originated from. Now this is obviously a specific challenge, and we'll talk just a little bit more about what that means.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2100.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2100)

So how are they used? If you think back to the prior slide  with the T1, T2, T3 timestamps,

in this case we have on the inbound side order T1 received at time T1 that gets flushed at time T11. So we have T1, and at time T3 we have order T3, and at T4 we have order T2. As a result of the reordering, on the outbound side we see it as order T1, order T2, and order T3, reordered to their originating timestamp. So this is clear, and we have a patent for this technique to use time for this. This is something we've been working on for a number of years, and we see it as foundational to continue to evolve our posture towards public cloud for these kinds of very sensitive low latency systems.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2170.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2170)

So what do we do this with? If I go back to the prior slides and think about the prototype that we built with this,  we're running on AWS public instances using M7I 2XL instances. We're using proximity placement groups to ensure that the compute nodes were placed close in relation to each other within a few racks or within the same rack, and then using these hardware timestamps to reorder those packets. So what we have here, if you look at this example and you take these timestamps, in our testing example we used two time boundaries to demonstrate the impact of this. One was a 5 microsecond boundary, and at 5 microseconds, as you might imagine, you're not successful in reordering all that many packets because that's probably more, if you will, many packets take much longer than that to arrive at the matching engine, so they fall outside the reordering boundary. But that was about roughly 25% of packets that got reordered successfully in that time boundary.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2270.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2270)

When we moved it up to 50 microseconds, we saw, depending on the message rate, up to 84% or higher. So we continue to iterate on that boundary and work on how we can continue to evolve the proximity placement functions so that we can get greater granularity of where to place them, leveraging the Amazon Time Sync Service to accomplish that.  So if we look forward with this foundation, and as I mentioned earlier, this is a key part of our innovation and working with AWS to solve problems that are hard for public cloud. As we believe that many of these problems will be solved or will get migrated to the cloud in the future, we want to be part of that solution.

So this is about for us developing scalable trading systems, continuing to ensure that our systems are the most scalable and performant systems wherever they are deployed, whether that's in public cloud or on premise. We know there's a lot of innovation happening in public cloud, and some things like the explosion of AI and how we're going to apply that, and certainly bringing your data to public cloud is a key part of that strategy. Enhancing our security and reliability, so as mentioned before, we have high expectations both for ourselves and from our clients and from the regulators in terms of being available and ensuring that the capital markets continue to flow. And public cloud has significant advantages in both of those, in that you do have to understand the resiliency considerations and make sure that you're building with those in mind. They're not the same as on-premise, but built responsibly and in the right way, they're much more resilient and secure.

And of course, part of the final point there is that leveraging our strategic market expertise, our operation of 30 exchanges globally and as a supplier to 130 marketplaces around the world, we are a leader. We need to think strategically about this and how we apply technology to this. So thank you. Thanks for the time to Josh and Julian for the time sync work, and we look forward to future innovation here. Thanks. Thanks, Nikolai.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2420.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2430.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2430)

### Leveraging Clocks in Distributed Systems: Understanding Time Uncertainty

I love that example in technical depth, and we get really deep into the hardware world and hardware timestamps. But most people are not building a financial exchange like NASDAQ. And we want to get into how you leverage really good clocks in your systems  globally and at scale and back into the software world. I'm going back to this quote from Barbara Liskov  on distributed systems and reducing communication. Because we now have a very good clock in the nanosecond range, where in the software world you're in microsecond range. And we have the hardware packet timestamping. This allows your applications to be better. You can measure your network latency. You have monitoring and observability in the micro nanosecond scale and consistent point in time snapshots.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2470.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2470)

But we still haven't talked about distributed systems, and distributed systems are all about ordering. Distributed systems  have multiple clocks. That's why it's a distributed system. So in the first example on the left, if you have a system with one clock and three events, it's relatively easy to know what event comes first. But as Nikolai pointed out, when you have multiple servers, they each have their own clock, and each clock is a little different. And as that system scales, it gets more complex.

In the past, it's been difficult to rely on the clocks because they were not good enough to compare. So things like distributed locks, leader election, consensus algorithms built by decades of very smart computer scientists, Turing award winners, have been done to do ordering on a complex system. But these algorithms put a limit on the performance, and as the system grows larger, so does that complexity.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2540.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2540)

In order to use a clock,  we recommend changing the way you read a clock. I look at Julian's example, asking a friend for a time. You ask what time is it? And we think as humans, as people, it's 2 p.m. But no clock is perfect. And each clock has an uncertainty, so if you're using our precision clocks in the software layer, when you say what time is it, you need to think about, oh, it's 2:00 p.m. plus or minus 20 microseconds. And perfect time must be within that uncertainty window. And you have to factor in a margin of error in your software application.

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2590.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2590)

I'm going to go a little deeper into why. And then Julian will show how we simplify that so that you could do it without needing to understand these details,  but I think it's important to share these really deep details. Here's a simplified example of a bank account. Let's say you're starting with a balance of zero and there's two places where there are transactions, maybe a deposit of $10 at the bank on node two that could be a direct deposit electronically, and then a withdrawal. Looking at those confidence intervals, you know that event one, that first deposit occurred before the other two with absolute certainty. But processing the other two is really important. If you get the order wrong, you might have an overdraft. You might not have enough money, funds in your account to withdraw.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2650.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2650)

### ClockBound: Simplifying Time Uncertainty for Applications

So Julian, knowing this, how do customers know what these intervals are? Well, that's actually a fairly complex question, and we thought we would start by an example, right?  We know that no clock is perfect, and so we need to estimate the error of this clock to build this window of uncertainty. So let's start with this example here. If you were to start an EC2 instance with a default configuration, you could inspect your time synchronization daemon to look at the sources it's using to synchronize your own clock.

I'm going to start from the bottom of this output in here, one row at a time. The bottom line represents a source that is over the internet, NTP servers on the internet, for example, time.aws.com will be a source of time here. The second row from the bottom up is accessing the local NTP servers offered by the Nitro system that we described before. And last, the row at the top represents the PTP hardware clock, which is our enhanced Amazon Time Sync Service in here that is provided again by the Nitro system.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2740.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2740)

What matters in this output with lots of numbers is the column on the right-hand side. That is what the time synchronization daemon believes is the size of the window of uncertainty that Josh was describing. Again, from the bottom, if you use an NTP source over the internet, in this example, 395 microseconds is the width of that window. If you use the local NTP sources, it shrinks to 90 microseconds, much better, much more accurate. And lastly, if you use a PTP hardware clock, the daemon on here announces 32 nanoseconds as the size of the window, something quite small. 

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2760.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2760)

But the reality is that there's a little bit more to it, and Josh, myself, and the team behind the Amazon Time Sync Service know that we have engineered the system from the ground up. It turns out that there's a little bit more to know behind this number than just what is printed out in this output, and I want to give you a bit more  information and details about what that is. But the reality is that getting this information is extremely complex, and we cannot ask any of our customers to know the details behind the engineering and the setup of the infrastructure that we have put in place.

So to solve that question, to answer that question simply, since 2021 we have released a software infrastructure, a software daemon called ClockBound. It's open source and it's available on GitHub at the URL printed here on the slide. The key information about this software is that it gives you the answers in one go. In a single operation, you get three pieces of information. First, you get the current time. It's Monday at 2:00 p.m. Second, you get the size of that window, plus or minus 20 microseconds. And last, because it's important to every single application, you get a status about the validity of the clock. Is it initialized? Is it free running? Is it synchronized? So in one call you get these three pieces of information, which gives you much richer information about time and the clock status and the quality of time upon which you can build your own business logic.

The ClockBound daemon has been supporting PTP hardware clock sources and NTP sources from the get-go. It scales to the most demanding workload. You can ask this question millions of times per second to make sure that you can match workloads such as the one that Nikolai was describing before. It is a piece of software that is written in Rust, something we like in Amazon for performance and memory safety. We really care about this concept. It's available in Rust as a crate and also for your C application if you want to backport it.

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2860.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2860)

Now, I want to give you a bit of an intuition about the  calculation behind the scene. Again, you do not need to know these details. ClockBound solves this problem for you. However, it's nice to have a bit of an intuition about what this is all about. If I were to summarize what the clock error bound is in one sentence, this would be the tightest bound on the worst-case clock error that is due to either oscillator drift and communication delays. And this illustration on the right-hand side gives you a sense for how errors accumulate.

At the bottom of the stack, we have the Amazon Time Sync Service with all the infrastructure that Josh described before: GNSS antennas, atomic clocks. We get into the nanosecond range of accuracy in the hardware. But of course this information has to be passed up the stack through layers of operating system and your application, and this is where some little errors do accumulate. When you target nanosecond and microsecond level, everything does matter. And so as you move up the stack, you move from nanosecond accuracy up to microsecond accuracy, and this is where ClockBound shines and gives you the answer.

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/2930.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=2930)

Now, again,  giving you some intuition about this. Let's imagine that you have started an EC2 instance that you are running the ClockBound client on. You can inspect it and ask what is the value and the size of this uncertainty window at any point in time. So in this example here, the white dotted line represents a hypothetical perfect synchronization, the perfect clock. And as you can see over time, the size of the window grows and shrinks. It grows linearly and shrinks when the clock is updated on every tick and grows again. At any point in time you can ask what is the size of the window, which is represented here by this interval between the earliest and the latest borders, boundaries of that uncertainty window.

So with ClockBound, the result is that all this complexity is handled for you, right? And you will get this result and be able to access it as shown here in this diagram. Now, with ClockBound, we now have a tool that allows applications and customer applications to leverage their local time with confidence. And some applications are particularly interested in this new functionality, and Josh is going to tell us all about it. Thanks, Julian.

### Real-World Applications: YugabyteDB, Aurora DSQL, and DynamoDB Global Tables

Now for the fun part: getting into who is using this and how they're using it. Again, doing all of the difficult communications for you, we have the primitive of the precision time, the hardware, the hardware timestamps, and then ClockBound doing that work to reduce the communications that we talked about in the quote at the start of the presentation.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3030.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3030)

The first example I want to share is YugabyteDB.  YugabyteDB is an open source database using our open source ClockBound software. They have a system called a Hybrid Logical Clock. They do some of the ordering logically as a traditional database would, combined with our precision clocks. When you're leveraging the Amazon Time Sync Service and ClockBound, the first thing that they saw was reduced latency for their customers by three times. They also saw increased throughput in the transactions, doubled, and then the big number I want to share and focus on is incredible: one thousand times fewer retries.

A retry would typically occur when you have two transactions, let's say a read and a write, at the same time, and there's a conflict. These retries are additional communication, load on their network communications, and load on their servers that no longer exist. The result is a better experience for their customers in consistency, isolation, but also just end user experience in their database. I have a QR code there of a blog I co-wrote with Karthik, co-founder and co-CEO of YugabyteDB, if you want to learn more about their Hybrid Logical Clocks and see some examples of this.

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3120.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3120)

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3140.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3140)

 At AWS, we are also using our precision clocks at scale. We're doing that with Aurora DSQL and with DynamoDB global tables. These are both multi-region, global, planetary scale databases. The reason why is shown in this diagram here.  So these are network latencies in milliseconds across three regions in the US. If you look at US West 2 to US East 1, you're going to have a latency of around seventy milliseconds. And again, we're talking about clocks in the microsecond and nanosecond range. So using our precision clocks, faster than sending a packet over the network, you can determine the order of your transactions.

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3170.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3170)

 And this is what it looks like with Aurora DSQL. This is the same example, the same diagram I showed you earlier with the bank accounts, getting a little deeper and more complicated. This time we're looking at two nodes from two different regions for Aurora DSQL, and getting this right is critical for consistency and isolation in a multi-region database. Using ClockBound's uncertainty window, you have Event 1 with a T start and T commit, which is guaranteed to be in the past compared to Event 2 or Event 3.

Now Event 2 and Event 3, as you can see, have some overlap, and they need to be processed carefully. One thing you can do is delay an event, so delay the start of Event 3 so there's no retry, if let's say that was a read and Event 2 was a write. But again, it depends on your database or system design, the type of events that are occurring, and what you want for your customers.

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3240.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3240)

 But this is not a database talk. I'm not going to go further into that. It could take hours. Here are three sessions going deep into the architecture of Aurora DSQL and DynamoDB. These are all later this week, and for people listening in on the recording, these are also all recorded sessions that you can find online.

Now we've covered a lot. We've gone very deep into the science of time. We have Nasdaq's example on leveraging hardware timestamps for in-region financial exchanges. We've talked about thinking and changing the way that you think about time and measuring not just a specific time but an interval and confidence. AWS partners and AWS databases are leveraging this. But I promised you at the beginning of this presentation that while it's good to understand this, you don't need to go that deep to put this to work in your systems.

### The New AWS Time Daemon: Cloud-Native Time Synchronization with Enhanced Performance

We have one more thing to share with everyone today. Because customers ask us, these primitives are incredible and very powerful, but how do we make it easier to use? How do we build our software and systems and have this work done for us? Julian, do we have an answer?

[![Thumbnail 3340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3340.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3340)

Well, that's the part where I'm probably going to get a bit more excited. So as we described, we have built a very accurate time synchronization service at Amazon. We support many workloads.  We support innovation. We have examples from NASDAQ, innovation in the new generation of databases. The point of this is that our customers put their trust in us, and so we have to continue to keep that trust and continue to improve the customer experience over time.

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3370.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3370)

And that's why I'm very happy to announce that we have just released a new AWS Time Daemon. The thing is, it's under the same software stack, the same ClockBound software that we have released four years ago, and it's still accessible on GitHub, but if you are aware,  the QR code will take you there as well. There are a lot of improvements we have made in there. A lot of work has been done to continue to improve and deliver higher accuracy and higher reliability for our customers.

One of the key changes and the key innovation is that ClockBound now includes this time daemon, which is a standalone time client and daemon. It does not rely on third-party software anymore. It will synchronize your operating system clock for you as well as continue to give you this information for your application to know what time it is, the size of the window of uncertainty, and the status of the clock all in one go.

It's important for many reasons. Nikolai mentioned regulatory reasons there, but also just simple observability for your distributed system. We have built-in metrics. Observability is a first-class citizen and must be absolutely a first-class citizen. Again, it's built on the same software stack in Rust that provides memory safety and high performance, and performance is achieved through different means. Here we have a shared memory segment that allows for inter-process communication that allows us to provide this information at a very high rate.

So this is very exciting, and there are two more things that I want to share with you before going into more details in the following slides. The first one is that we have redesigned the system to be cloud-first design. Time synchronization software has existed for decades before the cloud existed. Here we make a massive change in paradigm. Most of the historical systems use a feedback loop system that is not well suited to cloud applications. We've changed that around. We now have a system that feeds forward information in the system, which is much more friendly to cloud applications, and we'll talk a bit more about this in a second.

And last, if you're like us, you are a time nerd in the room or watching this online, I just want you to know that the new software stack ships with a simulator that allows you to test the software, see it for yourself, find what you like in it, potentially report some issues or things you would like to see improved, and we will welcome your feedback, of course, as you try it.

[![Thumbnail 3500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3500.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3500)

So let me go back quickly into the last components of the talk.  First, we talk about cloud-native synchronization. So you know that at AWS, availability is a top priority through all the layers of virtualization. We have made some innovation here and contributed a new feature to the Linux kernel called VMClock for virtual machine clock. And this feature, combined with our new ClockBound release, allows the direct transfer of time from the Nitro clock into your instance. It's an improvement over any kind of maintenance event to the point that you now have your workload running smoothly at any point in time.

[![Thumbnail 3540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3540.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3540)

Great. Let's have a quick look at how this looks as well in terms of performance.  Similar to the earlier slides, the top graph here shows the drift of an oscillator in a band of thousands of microseconds. How well are we doing? Well, the result is the graph below. Given this input, we managed to track and correct for the drift to within the band of plus or minus one microsecond over this period of multiple hours. This is how good the system is working today.

[![Thumbnail 3580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3580.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3580)

So this is for the internal of the system, but we mentioned that what matters to the customer application is the window of uncertainty. So here we have a little application, a little demonstration where we can see in real time that we are constantly querying the ClockBound daemon to plot the size of the window of uncertainty.  As you can see in the previous graph, it has this sawtooth behavior. It grows and shrinks as the clock gets updated. But what matters here is that the results are in a band of somewhere between twenty and twenty-six microseconds. That's the size of the window of uncertainty you can achieve inside your application by using this new application.

[![Thumbnail 3610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5038809daacb48df/3610.jpg)](https://www.youtube.com/watch?v=PecK_inxe5Y&t=3610)

And now I see that we're almost out of time, and it's now a good opportunity for Josh to conclude and take us there. Thanks everyone. Now it's time for you to build and take advantage  of the Amazon Time Sync Service and the precision time capabilities that we have. Again, we've built this into Amazon EC2 instances. You could use AWS databases, AWS services built to take advantage, using NASDAQ, AWS partners like YugabyteDB who are leveraging time for you. Again, Amazon Time Sync and ClockBound do the work for you, and I can't wait to see what you build using this. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
