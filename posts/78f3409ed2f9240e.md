---
title: 'AWS re:Invent 2025 - Reliable AI at Scale: How Data + AI Observability Powers AI Products with AWS'
published: true
description: 'In this video, Bar Moses, CEO of Monte Carlo, interviews Madison Sargis, Head of Analytics at Morningstar, about data quality in financial services. Madison explains how Morningstar manages investment data for 401k and brokerage accounts, emphasizing the critical importance of accuracy since people make financial decisions based on their data. She discusses the Morningstar Portfolio Risk Score (MPRS), a key metric advisors use to ensure portfolio suitability for clients'' risk tolerance. Madison shares how consolidating calculation teams and partnering with Monte Carlo reduced QA time from eight hours to one-to-two hours per week for their first model, enabling faster data delivery without compromising quality. She outlines their three-phase maturity journey: establishing best-in-class observability, implementing AI-powered workflows for troubleshooting, and adding observability to agentic solutions. Madison notes they currently spend $20,000 validating a $2,000 agentic solution, hoping new agent observability tools will reduce validation costs while maintaining trust in outputs.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/78f3409ed2f9240e/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Reliable AI at Scale: How Data + AI Observability Powers AI Products with AWS**

> In this video, Bar Moses, CEO of Monte Carlo, interviews Madison Sargis, Head of Analytics at Morningstar, about data quality in financial services. Madison explains how Morningstar manages investment data for 401k and brokerage accounts, emphasizing the critical importance of accuracy since people make financial decisions based on their data. She discusses the Morningstar Portfolio Risk Score (MPRS), a key metric advisors use to ensure portfolio suitability for clients' risk tolerance. Madison shares how consolidating calculation teams and partnering with Monte Carlo reduced QA time from eight hours to one-to-two hours per week for their first model, enabling faster data delivery without compromising quality. She outlines their three-phase maturity journey: establishing best-in-class observability, implementing AI-powered workflows for troubleshooting, and adding observability to agentic solutions. Madison notes they currently spend $20,000 validating a $2,000 agentic solution, hoping new agent observability tools will reduce validation costs while maintaining trust in outputs.

{% youtube https://www.youtube.com/watch?v=AG3L2oL_T9Y %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/78f3409ed2f9240e/0.jpg)](https://www.youtube.com/watch?v=AG3L2oL_T9Y&t=0)

### Introduction: Monte Carlo, Morningstar, and the Critical Role of Investment Data Quality

 Hi everyone, can you all hear me in the back? Fantastic. Well, congrats on surviving AWS re:Invent. This is the last day if you made it here. I hope you had a great first few days. My name is Bar Moses. I'm the co-founder and CEO of a company called Monte Carlo. Monte Carlo is creating the data and AI observability category. Imagine a world where you can ship agents with confidence knowing that both the input and the output is trusted and reliable. That's really our mission, what we help the best data and AI teams in the world to do.

I am honored to be here today with an amazing leader, Madison Sargis. Thank you so much for joining us, Madison. Madison is Head of Analytics at Morningstar, and Morningstar has a very interesting approach and work with data. In particular, we're going to talk about the Portfolio Risk Score product, which some of you may be familiar with. I'd like to ask you, Madison, first of all, welcome. Thank you so much for joining. Thanks for having me.

Maybe just to get started, Madison, could you help us understand the data landscape? I know data is really foundational to the work that you're doing, so maybe share a little bit. What kind of data are you working with? Where does it come from, and why is data quality so important? Yeah, so Morningstar is one of the largest global providers of investment data. Think when you log into your 401k or your brokerage account, any information about your investments is either collected or calculated by an investment data firm.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/78f3409ed2f9240e/110.jpg)](https://www.youtube.com/watch?v=AG3L2oL_T9Y&t=110)

 Morningstar has its roots in mutual funds, but as you can see, we've changed and grown our collections practice as the data landscape has changed over the last few years, last decade. A couple of trends. One, ETFs have surpassed mutual funds in their popularity. So ETFs, if you don't know, have very transparent reporting requirements daily, and so there's a lot more data coming through our pipes with a lot more data that we need to take off and collect.

On the other hand, you have this growth in what I would call a little bit more opaque data sets, so things like model portfolios, semi-liquid vehicles. These have either lighter or non-standardized data requirements, and so at Morningstar we have to collect it and we collect it globally. The way we do that is either through feeds of regulatory filings, documents, websites, or in some of these opaque parts of the markets, directly from asset managers themselves. Going back to the example, why is data quality so important is because people are making decisions, financial decisions with our data, and so they have to be able to trust that it is accurate and correct.

### Consolidating QA Practices: From Eight Hours to One Hour Per Week

Awesome, love that. Maybe, Madison, tell us a little bit about your role. Earlier this year you brought in different teams under one umbrella. I think one of the questions that a lot of folks have is how do we actually set ourselves up for success in this new world? So pulling all of these data teams together, what would that journey look like and why was that so important for you all?

So my role at Morningstar is I'm responsible for all calculated data at Morningstar. That ranges from things like returns, so how much your investment earned, to what is in your investment, so a lot of portfolio statistics, to more of the sophisticated analytics like forecasting risk or any Gen AI investment analysis text. Earlier this year, we brought together all of the calculation teams into one department. Previously I was just responsible for the advanced analytics, and then we had some maybe asset class specific or calculation specific teams that were all kind of doing the same thing, and so we merged the teams together.

We have an engineering discipline, a product discipline, an analyst discipline, and then for each data set or model that we produce, we have a cross-functional team responsible for it. The thing that kind of jumped out at me when I started managing this larger global department is that there is a wide variance in our QA practices. You can think of that as MLOps or model validation or software engineering unit tests. That framework really varied across the data sets. For example, we had a range of internal tools, external tools. I had eight different role types of people who were touching QA, and so if you're trying to run a global scalable department, that's probably not the way to do that. Earlier this year, we were looking at this, okay, how do we want to do it.

Looking at this, we had to decide how we wanted to approach it. The context for that is we moved our models into AWS, started moving them in 2015. Monte Carlo was founded as a company in 2019, so there was no ability for us to buy industry standard QA checks. There was no vendor for that. We were where we were because we had to build these things in-house. But fast forward to today, you don't have to do that anymore.

When we were rearchitecting our QA practices, we took a step back and thought, okay, what are the checks that are industry standard that we can outsource? Those went to Monte Carlo, and the stuff that was really bread and butter, core to the methodology, core IP, we kept that. We went, it felt painstakingly slow, but we went model by model, check by check, and said, okay, what can we outsource and what are we going to insource.

I think the first model we put out there, we went from eight hours per week to one to two hours per week, and that's end to end, monitoring, QA, triage, maintenance, anything in that vein. Then we went from the next model, it was twelve to six hours, sort of seeing that twelve to six. Then a light bulb went off. We have a very complicated model, and one step of the process went from twelve to three hours.

I hear from my clients that, hey, you have to update this model quicker. You need to be able to reproduce it or regenerate it faster. I sort of always was thinking, oh, we have to do a full history refresh. You don't want to degrade our quality standards. But we're going to be able to do that now because of some of the rearchitecture of our QA practices.

From an operations perspective, our analysts are happier. They're doing more engaging work. I'm able to add more data sets without adding more headcount, and I'm able to produce data faster for our clients. I love that. And for folks who are not intimately familiar with Morningstar, can you tell us a little bit about how do clients actually interact with the model? What does it mean for a client to be able to have the model within three hours instead of thirteen hours? For folks who are consumers, what does that mean for their world?

The quicker you can get data that's accurate and more reliable, that just means you're able to get insights faster. You're able to make decisions versus waiting. All this data is going to update at the end of the week or end of the month. You're able to make decisions on a more timely basis.

### The Morningstar Portfolio Risk Score: Building Trust Through Proactive Data Observability

Maybe just to double click into one of the most critical data products, the portfolio risk score. I think that's something that we spent a lot of time talking about earlier. Maybe for those who are not familiar, what exactly is the risk score? What goes into producing a highly reliable risk score, and why is it so important for you all? Why are you all spending time on it?

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/78f3409ed2f9240e/520.jpg)](https://www.youtube.com/watch?v=AG3L2oL_T9Y&t=520)

The Morningstar Portfolio Risk Score, and if I say MPRS, that's what I'm talking about, is basically a single summary metric that evaluates the risk of your portfolio. It's widely used across, will you flip the slide. It's widely used across, thank you, Madison,  financial professionals. It's a single number that can assess, are you, is the risk of your portfolio in range for the client.

If you walk into a financial advisor's office, the first thing they'll probably have you do is fill out a risk tolerance questionnaire. At the end of it, it'll say what level of risk that you're comfortable with, all the way from conservative to aggressive. What an advisor needs to do is then build a portfolio that's in range with your risk comfort level. That's where the risk score comes in because it's a third party metric that evaluates, hey, your advisor built a portfolio that is in range with your risk comfort. It's something called suitability.

Advisors typically aren't able to move forward with a proposal or making any sort of portfolio adjustments unless they can show that the portfolio that they're putting you in is suitable for your risk comfort. It becomes really critical and core to an advisor workflow. When you think about it, that number, that risk estimate has to be correct. It can't be almost correct, and it can't be late. It has to be the most up-to-date information that an advisor has so that they can make sure they're acting appropriately on behalf of the client.

Obviously, I can understand why if I'm highly conservative and the result that I get is very aggressive, that has implications on Morningstar's brand and revenue and trust and the partnership with clients. So it's very critical. What are some of the biggest failure points or vulnerabilities that happen when you create such an important score like this?

Great question. There are sort of two answers. One, it all comes back to your input data. The input data has to be correct. With models like these, any input data issue can propagate. For example, if you just have a single corporate action that's incorrect, that can flow through to your factor exposure, to your residual, to your covariance matrix, to your forecast, and then to any portfolio that holds that security. When you're building these models, you can't get to the end and say, oops, one input data point was wrong. You have to be able to check at every single spot before you proceed that your data is correct.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/78f3409ed2f9240e/690.jpg)](https://www.youtube.com/watch?v=AG3L2oL_T9Y&t=690)

What we have to do is really scrutinize the quality of the data every single step before we can proceed, because otherwise you just compound your problem as you move forward in your production process. Makes a lot of sense, and as you mentioned, when you started this in 2015, there were no solutions. You had to build a lot  in-house with every single step. As we started working together and you started working with Monte Carlo, what was the process like? How did you incorporate data observability and data quality into this?

The first thing is that with Monte Carlo, we were able to move data observability from just before we ran production to ongoing. That means that we're able to catch issues and then have time to fix them or do something more intelligent with that data issue before we run production. Whether that's sending it back to our data teams to fix it or adjusting our models to potentially not include it in an estimation, we're able to monitor and find these issues more proactively before we start production.

### The Future of Data Quality: AI-Powered Workflows and Agent Observability in Financial Services

Fantastic. Maybe changing gears a little bit here, how are you thinking about the future of data quality? I'll ask that in particular with an eye towards two things. The first being generative AI taking over, and the second being how do you operate in such a regulated environment. I think the standards for companies like Morningstar are a lot higher, and there are different challenges. So how do you think about the future and what that looks like?

It all comes back down to trust and the integrity of your data. As AI comes into making trading decisions or risk management, you have to have accurate, consistent, timely data. It's more important than ever. But even without the advent of AI, I hear all the time from clients that if I have an API that maybe it's up and running but the data pipeline is delayed or the data is not fully updated, that's an issue. That's a data quality issue. We have to make sure that we can do everything we can to make sure our data is accurate, timely, and consistent.

On the regulation piece, I operate a global data company. We produce data all around the world that's subject to different regulations. As a financial services company, we have to have very robust data governance processes. The trends that I'm seeing are that it's not just enough to scrutinize your output data. You have to make sure that your whole data pipeline, your whole data processes have the same rigor around it as you do with your final output. I'm seeing more and more that the scrutiny is coming upstream, and so we're taking that thinking and applying it globally, not just in certain markets that are a little bit stricter than others.

Awesome, yes, certainly very different with the EU AI Act and many others coming into play. Obviously, long before AI, that was very important, of course. Amazing. Maybe one thing that we see at Monte Carlo working with folks like Madison and others, there are kind of three stages, if you will, or three big problems that folks try to solve. I'll just explain them briefly. The first is maybe what you have done in the last five years or so, which is really building the foundations and putting together what best-in-class data observability looks like. I think that means primarily moving from a world that's very manual and homegrown and maybe separated by eight different people doing QA, and then moving to a place that's a lot more proactive, a lot more automated, and really getting their processes together.

That's really the first phase. The second big phase that folks are working on today is actually taking this to the next level and introducing AI-powered workflows. If you think about it, a lot of these workflows can be done using agents. One of the strongest examples that we see in this case is what we call a troubleshooting agent, where a data steward or data analyst might typically take weeks or even years to troubleshoot an incident all the way from receiving the incident to understanding what's the root cause to figuring out what's the remediation process. That can take a very long time and actually can be done pretty well with an LLM.

So that's the second bucket: how do we introduce agents to automate and make data and AI teams more productive and more effective? And then the third category of problem, or the third step in the maturity, is when folks are starting to introduce agentic solutions in their own environment, moving from dev to production. We're no longer in pilot land. And when we ship agents to production, we start worrying about not only the input, as you mentioned before, but also the output of agents. So how do we make sure that we can trust that the agent isn't hallucinating or misbehaving, or even just producing an answer that's way too long to process for a human, as an example?

In that world, one of the things that we found is that thinking about data and AI as one state and actually thinking of observing that whole system as one is one of the most important turning points. And so I'm curious, Madison, I love your story because you all have really been touching on each of these boxes in very different ways. What's your reaction to that, and where do you all see yourself investing across these areas?

Great question. So I kind of think of it more as a pyramid. The first bucket that you talked about is best-in-class industry standard checks. I don't want anyone on my team building those anymore. All of those need to be outsourced to companies like Monte Carlo, and we've seen a lot of value and a lot of efficiency gains in that. And then the next step is, how can we use some of these AI solutions to take the next action after we've been alerted? Observability is great, but then you have to do something about it. So how can we triage faster? How can we submit PRs faster? How can we submit compliance reports faster? And I think we're seeing those efficiency gains in all of those areas.

And then in our world, we have a few agentic solutions out there, and so we are experimenting and working with you guys about how we can put more observability on there. I think we have an example where we have a fully agentic solution. It costs roughly $2,000 to run on an ongoing basis, but then we spend another $20,000 to validate and revalidate that data before we publish it because we want to make sure that we can fully trust the output. And so as more of these agentic observability tools come online, can we apply those to our output to then potentially bring down the cost of our validation?

So you heard it here. We've actually announced the GA availability of our agent observability this week, and I'm signing Madison up to try it out for her agentic solutions. And again, just to double click on that, I think it's not only the agent output, it's also the agent input, and observing both of those really gives the full end-to-end view and ultimately confidence when shipping agents in production.

Amazing. Well, Madison, this has been so fun. I have one final question, which I think is maybe the most important question for today. When you're not thinking about risk scores and QA practices in AI, what's one thing that you've gone into that's outside of work?

Great question. Needlepointing. So I am pretty deep into the Reddit and YouTube needlepointing subreddits and needlepointing videos. I'm currently trying to learn how to self-finish. If anyone who needlepoints knows, that's the skill I'm trying to learn right now, so very different than data.

I love that. I would never guess, but if anyone has tips or recommendations for Madison, please let us know. Thank you so much, Madison. This was amazing. Thank you for your partnership. Thank you for being such a leader in the data and AI space, and thank you all for the time. Really appreciate it.


----

; This article is entirely auto-generated using Amazon Bedrock.
