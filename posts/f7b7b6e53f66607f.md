---
title: 'AWS re:Invent 2025 - How FanDuel''s AI Chatbot Revolutionizes Sports Betting Experience (SPF203)'
published: true
description: 'In this video, FanDuel and AWS present AceAI, a generative AI betting assistant that evolved from a hackathon idea to production in weeks. Jordan Ratner, Rohit, and Santosh detail the journey from prototype to rollout across multiple states and sports, handling 16.6 million bets during Super Bowl with 3 million active users. They explain the RAG-based architecture using AWS Bedrock with Nova Lite and Nova Pro models, Lambda functions, and DynamoDB for conversational memory. Key challenges included reducing latency from 12 to 5 seconds (P99), handling vague customer queries, implementing responsible gaming safeguards, and creating evaluation frameworks with thousands of tests. The solution reduced parlay construction time from hours to seconds while maintaining user control throughout the betting journey.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/0.jpg'
series: ''
canonical_url: null
id: 3088336
date: '2025-12-06T07:21:04Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - How FanDuel's AI Chatbot Revolutionizes Sports Betting Experience (SPF203)**

> In this video, FanDuel and AWS present AceAI, a generative AI betting assistant that evolved from a hackathon idea to production in weeks. Jordan Ratner, Rohit, and Santosh detail the journey from prototype to rollout across multiple states and sports, handling 16.6 million bets during Super Bowl with 3 million active users. They explain the RAG-based architecture using AWS Bedrock with Nova Lite and Nova Pro models, Lambda functions, and DynamoDB for conversational memory. Key challenges included reducing latency from 12 to 5 seconds (P99), handling vague customer queries, implementing responsible gaming safeguards, and creating evaluation frameworks with thousands of tests. The solution reduced parlay construction time from hours to seconds while maintaining user control throughout the betting journey.

{% youtube https://www.youtube.com/watch?v=XmBDchavCW8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/0.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=0)

### Introduction: FanDuel's AceAI Betting Assistant

 All right, welcome everybody. My name is Jordan Ratner. I'm a Senior Generative AI Strategist at AWS. I have a question for you. How many of you have made a bet since being in Vegas this week? Oh, almost everybody. Okay, a lot of risk takers in here. Well, we have some really great news for you. With FanDuel's AceAI, a generative AI betting assistant, your experience for bets just got a whole lot easier. I have here a couple of friends with me, Rohit and Santosh. You guys want to introduce yourselves?

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/50.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=50)

Hi folks, I'm Rohit, Senior Staff Engineer at FanDuel. Hey folks, I'm Santosh Vallurupalli, Senior Solutions Architect with the AWS Betting and Gaming Team, and I work with FanDuel very closely.  We know a lot of our customers have questions about how to go from nothing to a full-scale production generative AI solution, especially in such a highly regulated environment like FanDuel is, and we want to walk you through that journey step by step in this talk. We'll start off with a baseline of who FanDuel is and what their goals are. We'll walk through the entire history of AceAI from inception and prototype all the way through to production rollout. Then we'll dive into how we did it. What does the architecture look like? What challenges did we go through, and how did we overcome them? Finally, of course, the metrics. Is this working, and how do we know? Why don't we start off with that overview of FanDuel?

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/110.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=110)

### FanDuel's Scale and Regulatory Challenges in Sports Betting

Thanks, Jordan. February 9, 2025, Super Bowl Sunday. I was in Porto in Portugal. The office was buzzing with excitement, and after the game, we sat down and we looked at a few figures.  We had over 16.6 million bets just during that game, nearly 3 million active users during the time of the game, and we peaked at 70,000 bets at the very height of it. FanDuel is America's number one sportsbook, and there's a reason for it. I want to talk to you a bit about, before we get into AceAI, what FanDuel is, what we do, and also what are the challenges we face generally.

We can think about FanDuel as a sportsbook, as an e-commerce platform. You have to give the customers the product and list it out to the customers. You need to make sure that the customers can use the product, pick the product up, and order it. You have to have an order processing system. You have to have a way to fulfill that order and make sure that the customer gets what they want. This is the same thing in a betting platform as well. We have Jalen Hurts is going to score that touchdown. That's the product. The ability for the customer to pick up that product, bet on it, place their wager, see how the wager progresses, and finally settle that bet so that they get the money. This is what we have to do in an e-commerce platform. It's similar to that.

We can also liken it to a trading flow. Prices change constantly. Imagine a basketball game. It's so quick, it's so fast. We need to make sure that the prices are up to speed. Jalen Hurts is going towards that goal line. We need to make sure that the probability of that event happening is so much closer now. We need to make sure that the prices are correct at that time. These are the two extremely challenging scenarios, and that's what we face every day in a sportsbook as well.

Some of the challenges that we see otherwise in general is sports betting in the United States is very regulated, a highly regulated industry. We've got regulations within the state itself, so one state would allow college betting. Some other state might not allow that college betting. Some states allow college betting for only sport teams which are not within that state itself, so it's very complex. Some states don't allow some sports at all. We've got to make sure that these rules are centralized so that they're available across all states. Then there's the data requirements as well. In terms of data requirements, they are strict. They are mostly regulated by the Wire Act. The Wire Act makes sure that we have to ensure that all bets placed within the state are within the confines of the state itself,

so we need to make sure that the infrastructure, the data, and the bet transactions all remain within that state. So the data requirements are quite important as well for us. And you can think about the latency. The central rules lie in a different place. The state-specific rules lie within the state itself. So there's obviously a latency problem, and that's where AWS helps us a lot.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/360.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=360)

### Core Principles: Customer Focus, Leadership Support, and Responsible Gaming

I've spoken about the fast pace, so I won't bore you again, but there's one point that I'd really like to hone in on, and that is responsible gaming. FanDuel takes responsible gaming really seriously. We aim to set the standards as a responsible gaming operator. With that, I would like to go to the next slide where we talk about a few principles of FanDuel. Don't worry, I'm not going to bore you with all of these. It's just the three that I really want to  talk about because that's what we use for AceAI, and they kind of relate to AceAI as well.

One of them is everything begins with the customer. That's really ingrained in our culture. We need to make sure that everything is for the customer, that the customer is safe, again coming down to responsible gaming, and also that the customer is entertained. The second thing is AceAI wouldn't have been possible without the support that we have from our leadership. Anything is possible is there within FanDuel, everywhere, and the support and the push that we got when LLMs came out, when generative AI came out from the leadership team was phenomenal, and that's how we got AceAI as well.

And again, lastly, winning with integrity is important for us because, as I mentioned earlier about responsible gaming, it's winning but with integrity, making sure the customer is safe, making sure that they're entertained. With that, I'm going to hand over to Jordan to talk about how we went from inception to prototype. Thanks, Jordan.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/450.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=450)

### From Hackathon to Production: Partnering with AWS Generative AI Innovation Center

Thank you, Rohit. Much appreciated, and Rohit was serious when he said that FanDuel took generative AI seriously. FanDuel started in 2024, very early  on, and they brought together a global hackathon with hundreds of people from all different walks of life: engineers, data scientists, lines of business leads, and all the way up to their senior executives and C-suite. Everyone was a part of this hackathon to try to find the best opportunities to apply generative AI to help their customer experience and to improve their internal operations. And of course, being the customer-centric company that FanDuel is, they landed on this idea of AceAI, an autonomous betting assistant that would help customers create a much more seamless experience than they ever have had before in their gaming life.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/510.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=510)

Now, there was a challenge. In 2024, especially early 2024, very few people actually knew how to deliver a generative AI solution. And so to build that muscle, this is where FanDuel reached out to AWS's Generative AI Innovation Center. Now the Generative AI Innovation Center is a global team of strategists and scientists who are fully dedicated to delivering generative AI solutions at scale. That's all  we do all day long. AWS has actually invested over $200 million in this team to make sure that we had this capability to support our customers in delivering what they needed, even when starting from absolutely nothing.

Now FanDuel started from zero. They had incredible engineering talent, but they didn't understand the generative AI toolsets. They had never built out prototypes before, and so they needed a little bit of support. And for any of you wondering how to get started, you don't need hundreds of millions of dollars. You don't need a team of 50 people or 100 people. FanDuel actually started with just one engineer. That's it. It was one engineer and three or four data scientists from the Generative AI Innovation Center helping to guide them through in an advisory engagement, which you can think of like office hours, meeting and chatting about what the solutions look like, what the services would be used for, how to set up the solution, doing research, and sharing code.

And that's where it started, so really humble beginnings going from this idea, bringing in that single engineer to learn about this work. And that engineer actually became the start of a snowballing effect within FanDuel. And very quickly, this engineer became the trainer, so we had the train-the-trainer moment. And we've snowballed this just from AceAI at the beginning, all the way up to 15 engagements with FanDuel, 10 of which are already in production, and we have another 20 or 30 on the docket for next year alone. So we're really excited to be partnering with FanDuel to help them build this muscle and continue forward.

Something else I'll mention is that we have a bunch of different programs available to our customers. You'll notice there's a ton of them on the screen here. I'll focus you on advisory and hands-on. Advisory is what we did with ACI. It was just the engineer from FanDuel building and us guiding. We also have a hands-on engagement where the Gen AI Innovation Center actually goes into your environment and builds out a solution alongside your engineers.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/630.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=630)



### Understanding the Customer Journey and Reducing Friction

So how do we walk through this entire engagement? How do we go from nothing to something? Well, the first thing we do is we validate the idea. This is so important. If you have the wrong idea at the start, you're setting yourself up for a lot of challenges that are just unnecessary, and it doesn't take forever to do it. We did it in a single conversation. We made sure we had what we call a diagnostic call for 60 minutes where we brought in all the senior executives and the engineering leads, the data team, and we walked through what were the challenges, what were the opportunities that they wanted to tackle and why. Within that single conversation, it became very obvious that a betting assistant would be really valuable for their customers. So with that one conversation, we sprinted forward and we decided to take this on and continue to build.

The next step was the discovery workshop. This is where we introduced the Gen AI Innovation Center's incredible data scientists. They came on alongside myself to walk through with FanDuel's entire staff of engineers, data scientists, and business leads to identify the customer, understand what the challenge was, determine the KPIs of how to measure success, determine the challenges of getting this going, understanding all the requirements, sharing screens, looking at data, and really diving in to understand how to make this thing successful. Once we had all the requirements, we then started off with the engagement. This is when that engineer went to work alongside our three to four data scientists. Now, of course, once we were ready, we handed it off to Rohit's team to actually productionalize the solution.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/730.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=730)

So what did we find out? We did all of this work. We walked through with all the different stakeholders.  What is it that we found out about the customer? Well, the first thing was there's a consistent, repeatable format that the customer walks through. They first research what they want to bet. They look up all the information. They understand what is the record of Federer versus Nadal over the past 10 years and in recent months. Then they go through the discovery phase. Now I have all this great information. What can I actually bet on? What is available for me to deploy this knowledge?

Then they construct the bet. They say, great, we have all this information, we know what we want to bet on, we see the bet in front of us. How much money do I want to put down? Maybe I want to build a parlay, which is much more complex, and then finally executing, pressing that button to actually put down that bet. What we found was that experience was actually filled with friction because when somebody determined that they wanted to make a bet and they wanted to research something, they would leave FanDuel's app, they would find their favorite search engine, and they would ask the question of how many points, who are the top five leaders in hits, as you can see in the questionnaire, and they would get this information from Google.

They'd get a notification on their phone from TikTok, and boom, five to ten hours is gone. They look up and all of a sudden their bet has expired. So this is a really common occurrence, and so we knew in order to make that customer experience better, we had to create a single in-app flow that the customer could walk through entirely within the FanDuel app. That was really critical.

Now, having that single experience that handled the entire journey was great to drive conversion, but we made sure to keep it an assistant. We did not go to full autonomy. When you're considering what to build, you're going to hear a lot about autonomous agents that handle things end to end. That's really great, and when you want to deploy that in certain scenarios where you don't have any business with human judgment in the decision, that's great. But in this case, our customers wanted to retain control. Throughout the betting experience, they wanted to be able to do the research themselves, stop, consider things, do the discovery themselves, stop, consider things, and onwards.

### Measuring Success and Building the Prototype

But what they didn't want to do was the hours that it takes to look up all the information and do the physical searching, putting all the information together and doing that analytics. So we focused this assistant on making sure that all of those elements were seamlessly handled while maintaining that control at every single step of the journey. Of course, we wanted to measure how we were successful with this solution, and we did it in a couple of different ways.

One was an event-based way. This is something that's just being counted on its own. The system is understanding how many times somebody's actually used this AceAI program, what are their drop-off rates, did they leave something in the cart and not actually continue forward on that bet and let it expire. Then we have time-based considerations. So how long does it take to go from the beginning of the journey all the way through until the execution? And then how long did it take at each individual subsection?

How long did it take to construct the bet? All of these things are really important when you're starting to understand how successful you are with the customer experience. And the last one is interactive. So one of the best ways to get feedback from customers is just ask them, and this is where something like a CSAT would come in.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/930.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=930)

So what does this prototype look like after all this work? Well, you can see here it's really rudimentary.  It's not that pretty, and we asked a point here how many points has LeBron had in the past three games? And the answer is this paragraph. It's unformatted. It's a string of metrics. It's really kind of hard to read, but that's okay. That's the point of doing these rapid prototypes. Your goal is to handle the tough job of proving customer value. That's it. And so all these edge cases that you might be thinking about, you don't need to worry about that in this first stage.

So if you ask this AI in the prototype stage to make a poem about your favorite tennis player, it would gladly do so, and that's perfectly okay. We want to make sure that we're proving customer value, and once we did that, we handed it over to Rohit's team to actually go to production. So Rohit, do you want to walk us through what it took to go from prototype all the way through to rollout?

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/990.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=990)

### Production Rollout Strategy and AceAI's Customer Experience Today

Yes, thanks Jordan. Great. And then the video takes us back. Good memories.  Yeah, when we started the rollout, we had three things in mind. We wanted to ensure that the customer gets the most accurate information. Thinking about how Gen AI was working at that time, we were very unsure, so we wanted to make sure that that information is correct for the customer and accurate, and if not, then we needed to roll back and we needed to find that strategy as well. Second thing was to learn because we didn't know a lot about this. We didn't know how our customers are going to react to it as well. So we wanted to learn about this as well. And the third thing is iterate.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/1070.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=1070)

And that's why, as you can see over here, we went one sport at a time, one state at a time, so we slowly moved out into the bigger United States. The Alpha pilot was a very small number of customers, friendly customers that we used to give us feedback as soon as possible, understand how the chatbot works and how they work with the chatbot as well, and then we slowly went out to the beta release earlier this year in quarter one, and then in quarter two we went to the beta two, and then so forth, and now we're in the beta three release. Yeah, so that, let's look at  what AceAI looks like today. This is a short video.

As you can see, much richer experience than the prototype. Yeah, this looks beautiful, right? Yeah, and you can see how the suggestions come out. The customer types in something very vague like bets. Yeah, that's interesting. So our customers are putting in individual keywords instead of full sentences? They do, yes. They treat it much like a search engine. So they just type in something vague and expect answers really quickly.

And I noticed here that you have a lot of rich UIs, so these are dynamically being pulled based on what the customer is looking for. Absolutely, yes. Yeah, so it's more conversational like think about you as a person talking to another person. What would you think when you say bets, for example? Yeah, well, I would have a question and I'd have a follow-up question, but what do you mean? Yeah, exactly that. Yeah, you can see it's come out to stats for a player.

Now, this is interesting, so build me a parlay. So I can see here as it's thinking about building that parlay, how long does it typically take for a customer to build a parlay? Internal research shows that it takes a customer hours to research and build and construct a parlay. For hours to construct it? Wow. Some customers, that's amazing. And we saw here it only took a couple seconds. Yeah, wow, that's an incredible improvement in the experience.

This is quite an extreme example of responsible gaming, and we do see things like this, but AceAI is very sensitive to these kind of questions and will respond immediately with a link to the responsible gaming page so that you can go there. I know I've been hopping on about responsible gaming for the last three slides, but this is very important for us in FanDuel as a responsible operator. Yeah, people do unfortunately in betting take risks that they can't afford, and so it's really important that we make sure that AI is responsible. Now, we also mentioned in the prototype we didn't handle edge cases.

So in the prototype, if somebody asks, say, "Hey, I'm going to put all my money and lose the house," it would say, "Great, good luck." It doesn't handle that, and that's okay in the prototype stage. But once you go into a customer-facing realm, it's really important that this responsible gaming is properly handled.

One last thing I wanted to mention: AceAI is not a recommender. It researches for you. It allows you to prompt it to provide and discover the bets that you're looking for, the opportunities that you're looking for. With that, let's get on to some of the meaty stuff, a little bit of a technical deep dive. Great, and Santosh, why don't you join us up here? Absolutely, let's do it.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/1260.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=1260)

### Technical Architecture: Leveraging AWS Services and Amazon Bedrock

I'll take you to the architecture. FanDuel Sportsbook, FanDuel.com up there,  is your device or your application. When the customer first gives us a prompt, the first thing it hits is the AWS API Gateway. And obviously the gateway will help us rate limit, will help us do the authentication and the authorization, et cetera. And then it comes down to the Lambdas.

Now Lambdas have been the best thing for us because when we first started out, we didn't know about the volumes and the traffic that we were going to see. We didn't know what kind of volumes we were going to see. So they were the ideal solution because they're serverless, we can launch them when we want to, and memory is very low as well. Lambdas serve as our main compute here. They do the routing, and I'm going to talk about that in the next slide when I talk about the data flow, but they do the routing for us, and also they do the data collection.

On the left there you can see a couple of EC2 instances and also EKS clusters there. Now those form our main services and data that we get for the LLMs. Those are the Sportsbook Services. Yeah, so essentially that's the data, those are the tools that we have. Lastly, Elasticache Redis and DynamoDB offer the caching for the data and also the memory for the LLM, so I'll talk about that a little bit more when we go through the data flow.

The RDS over here is where we store the stats for players and teams, et cetera. So that's where the main stats for the players and teams come from. Bedrock is the shining star over here for us. When we started out, Bedrock was phenomenal because we could quickly get the team working on it. It was pretty easy as a learning curve to come up and start using Bedrock.

The choice of models was phenomenal. We could use any different models we wanted. We tried Claude Haiku, we tried Meta Llama, but we finally settled on Nova Lite and Nova Pro because that delivers what we want for us and it's very cost effective as well. And scaling is not a problem as well. Being serverless, Bedrock scales very well, and if you speak to Santosh very nicely, he'll put up the limits for you. And costs obviously are your tokens, so as long as you control the tokens, your costs will be controlled as well. What do you say, Santosh?

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/1440.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=1440)

Yeah, I think, you know, we've heard my buddies Jordan and Rohit talk about  how AceAI really started off as a hackathon idea, you know, went into rapid prototyping, experimentation, and the production rollout. And this just happened in a matter of a few weeks. You know, we're not talking months, years, quarters. It was just a few weeks. And so to be able to have that kind of rapid consistency, rapid application development cycles, it was really important for FanDuel developers to have consistent APIs, you know, constant uniform platform that they could experiment with, deploy it, test it, and then move on to production. And that's where Bedrock has really shined really, really well.

And so you just heard Rohit talk a few seconds ago that they've experimented with a bunch of different LLMs like Haiku, Sonnet, Anthropic models, Amazon models, and they finally landed on Nova models. But then before that final decision, there was a lot of prototyping, testing, evaluation cycles that they had to go through. In the absence of Bedrock, what it meant for FanDuel developers was they had to use several different SDKs that were proprietary to all these LLMs, build custom authentication frameworks around that. Observability is like another huge pain.

Another huge pain point was all the cycles that developers would have otherwise spent trying to deal with and solve these challenges, but then Bedrock just provides you with a single consistent API that you can use to interact with hundreds of different models we host on Bedrock. All it takes for the developers is to just flip a model ID, and then boom, now you're talking to the LLMs of your choice. So that was really amazing for developers to rapidly prototype, understand what LLM works best for their use cases, and then quickly move on and focus on something that's more important.

And also Rohit just talked about scaling and cost. As you've just seen, FanDuel went through several different rollout cycles. They had alpha stages, they had beta stages, and several new states are coming live. I'm sure AceAI would probably end up there. And so it was really important for FanDuel also that the model or the medium where they're actually deploying these LLMs should also scale seamlessly. FanDuel doesn't want to necessarily invest a lot of time in maintaining that infrastructure, patching that infrastructure, and keeping them always updated. And so that's where Bedrock, being serverless, seamlessly scales to whatever your requirements are.

All we had to do with FanDuel and Rohit's team was just ask them what are you expecting as your new throughput to be after your rollouts, how many tokens are we talking about, and what are the requests per minute. And that's it. We just put in a request, we got the service limits bumped up, and Bedrock did its magic. So Bedrock has really reshaped the way in which the Gen AI development life cycles happened at FanDuel and especially for AceAI.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/1610.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=1610)

### Data Flow and Overcoming Technical Challenges: Context, Latency, and Caching

Yeah, thank you, Santosh. That folks is a very high level view of the architecture.  Let's go on to look at the data flow. The data flow is a typical RAG flow. So you've got your prompt coming in to the Lambda, the AceAI backend services, what I'm going to call it. As soon as it hits there, the first thing we do is we make an intent call, the get intent call that goes to Bedrock immediately. And that tells us what's the intent of the prompt and what does the customer want.

And that goes back as a response to the Lambda, which then decides what tools to use to collect the data from, and that's where it would go either to the RDS or the EC2 instances or the EKS instances to pick up that information. Once that information is picked up, it'll then send that back to Bedrock to formulate and enhance that information. That goes back to the Lambda, which goes back to the customer and is rendered back to the customer in the form that you saw earlier.

So let's take an example. We saw in the video, which was quite an extreme example, I'm going to lose my house. If that kind of a prompt comes in and it goes to Bedrock, Bedrock responds with this is a responsible gaming intent. Once it finds that, it's quite a static response to go back to the customer, so we put that page quite quickly. This kind of information is quite easy to send through because things like customer service or responsible gaming, we send it through really quickly.

The second thing is, let's take the other response. There was a question from the customer on the video for stats about Donovan Mitchell. And if that kind of question comes through, if that prompt comes through, Bedrock will say, right, this is a stats response. It'll go to the RDS, pick up the statistics that the prompt has requested, go to Bedrock, and then Bedrock will formulate it and enhance it and enrich it for the prompt according to the prompt and send that back to the Lambda, which then renders it to the customer.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/1790.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=1790)

So the data flow is quite simple. It's essentially a multiple intent-based workflow, acquiring information from many tools and sending that back to the customer. Both, that is in a nutshell, the architecture and the data flow of the system. Let's look at some of the challenges we faced as we went through and also how we solved them.  Context is quite key to LLMs. In the video earlier, you might have noticed the customer said, give me stats about Donovan Mitchell. The LLM responded with the stats. And then,

the customer asked the LLM to create a parlay for him. What's the "him"? It's Donovan Mitchell. In a conversation, we know this. We know the pronouns when a person uses a pronoun, but the LLM doesn't know. We need to provide that context back to the LLM.

This may seem quite naive, but at first we didn't realize that. But as we went through the alpha and the beta stages, we realized what was happening. The LLM would work for one player, one team, but then when there were multiple teams and multiple players, it became difficult for the LLM to understand what the context of the prompt is. So, with a lot of experimentation and iteration, we managed to put in the DynamoDB from my previous architecture diagram, if you remember. And DynamoDB now serves as the memory for the LLM.

So when a new prompt comes in, we pick up the previous prompts of the customer and provide that to the LLM so the LLM knows what the customer is talking about. What I want to say here is Gen AI is a very different paradigm compared to our normal develop, test, fix bugs, and release cycle. It's quite different from a functional perspective. We need to think about it differently and we need to understand how the LLM works because it hallucinates, it gives out wrong results, and so on.

The other thing is betting is quite a fast-paced industry. As you know, imagine a basketball game. LeBron James is, if it's live, LeBron James is going to score that basket for 20 points, and he gets 20 points. Is he going to meet that bet? How quickly is that going to happen? That's difficult for the customer and difficult for the LLM to understand as well. So when a person in a live environment asks for a parlay, it becomes really difficult both for the customer to type that all out, but also for the LLM to respond back as well.

So in this situation, what we've done is when the game is live, we provide them only with singles. Even if the customer asks for a parlay, we provide some answer back. So we inform the customer that a live parlay is not possible, but here are some singles for Donovan Mitchell or for LeBron James. That's how we work it out.

And in terms of latency, we had very high latency earlier when we first started out. Typically, we have this one-second rule. When somebody asks for some question or searches for something, we need to respond back in one second. But for LLMs, that's not possible because they take a long time. So we had a latency of about 12 seconds when we first started out, and you can see that on the graph over there. The P99 was 12 seconds.

We did a lot of work on traditional work like trying to cache things. We saw the Redis cache. That's where most of the data is brought together. So for example, if I want to bet on tonight's game, Jordan might want to bet on that as well. So the data available in the Redis cache makes it really easy for the LLM and the Lambdas to pick it up. Most of the work that we did somewhere around August was traditional work, but we also did a lot of work on the models as well, and AWS was really helpful in figuring out where we could fine-tune the models to get response back quickly.

So then we rolled that out and we reduced our latency. We've come down to around a P99 to 5 seconds now. But over here, what I wanted to say was that the models will respond back to a prompt, but you can do only so much with the model. You need to do some work behind the scenes on traditional architecture so that you can get faster responses as well.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/2080.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=2080)

### Testing, Customer Vagueness, and Evaluation Metrics for Gen AI

 Moving ahead, for these three points that I'm going to talk about, I'm going to talk about the challenges that we faced, but I'll talk about the solution to that in the next slide. Testing is again slightly different in the Gen AI paradigm. It's not the traditional way we can test.

We can have test suites, but not the test suites that we have right now. Functional testing and exploratory testing are difficult because we don't know how the AI will respond back. So how we do that testing was interesting for us.

The other thing was customers are quite vague when they ask questions to the LLM. Jordan asked me a question earlier when we were going through the video about how customers behave. Customers behave with the Gen AI just like they would behave with any other search engine. They think it's a search engine. They ask questions and those questions are pretty vague. If you remember the video, the customer just asked for bets. What kind of bets? And that's where we need to think about the LLM as a person. As a person, what would you ask back?

So when the customer says bets, you would probably ask them which sport do you want bets for. Is it NFL, NBA, NHL? And that's exactly what we did. So the LLM responds back with a suggestion, saying, hey, these are the suggested bets that you could get to. And when the customer clicks on NBA, then again, that's a vague question. Do you want NBA bets for tonight's game? Do you want for the season? Do you want for the game next week?

So the vagueness of the customer means you need to prompt the customer and suggest more things to the customer so that you get the information out from the customer. And finally, the customer goes to, I want the bets for Donovan Mitchell, I want the stats for Donovan Mitchell, and that's where they wanted it to be, really. So we need to prompt the customer as well as the customer prompting the LLM.

The last thing in vagueness is the slang and the colloquialisms that people have within the sporting industry. So people will say something like, give me a bet for LBJ, which is LeBron James. So we need to be able to translate that and understand what that means, because the LLM doesn't understand. Or maybe a parlay for the birds, the birds being the Eagles. How do you deal with that? It's not an exhaustive list. So this is something that we're still working with. We don't have a perfect solution to it, but it's happening.

Models are also very sensitive. I want to bring two points over here along with the models. One is they're sensitive to changes in the data, so the tools changing any kind of data. Tools are used by not just the Lambdas for the LLMs, but they're also used for other services. So when these services change, that affects the LLM as well. So we need to be careful about that.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/2330.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=2330)

The other thing is the prompt. If the prompt changes and the way customers evolve and start looking at the prompts and creating better prompts, that is also something that we need to track. So how do we do that? How do we make sure that we have an accurate response and how we track that accuracy? Let's look at that in the next slide. 

When we talk about metrics, we have a lot of metrics. Obviously, if you're a data scientist, you'll have your precision and your recall, and you'll have your false positives, your false negatives. But for a product at FanDuel, we've picked up on accuracy as our metric for understanding what the response is going back to the customer. And evaluations have helped a lot in that. So evals is what we use at the moment.

The traditional development, test, and release cycles don't work very well with evaluating and understanding the response from an LLM. So what we have done is we have created a test suite and we've created thresholds for every response that comes out. We've got categories, and each of these categories have got thresholds for any kind of response that comes out. So when we have any change, any change to the model, any change to the prompt, any change to the data, or even the codeâ€”if any of these four factors change, then we test against these thresholds.

If the response that comes out has gone below the thresholds, then we've regressed, so obviously we need to make a change and understand where we went wrong. If it's progressed, then that's great. It might be a new threshold, but we never know. We need to keep understanding why it has progressed, and then we have a new threshold.

So that's what we do today. We have thousands of tests in our test suite, and we run those every time we make a release so that we know our benchmarks are met. We know that we have the right responses from the models for whatever changes we have made. You'll ask me, what about how the customer interacts? Because this is just the test, this is just the changes that we do. What about customers?

Customers, as I said, they keep changing. They're constantly learning and evolving. So how do we track that as well? Our data scientists and our data team work with understanding the feedback loop from the customer, so they use evals as well. They make sure that they have a list of static and dynamic tests where they're constantly testing the prompts that are coming in live as well as statically behind the scenes.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/2540.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=2540)

They have ground truths on these, and with these ground truths what we do is make sure that the information that's coming through is evaluated against new categories. So we discover new categories and new ground truths, and then that feeds back into the test suites that we have for our testing as well. Moving ahead, that's about the key metrics and how we test. So going back to the learnings and the challenges testing, that's how we take care of it  by using the evaluations and the models as well.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/2570.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=2570)

### The Future: Bedrock Agent Core, API Gateway Streaming, and Continuous Evolution

We make sure that when there's a new model coming, so for example, we had Nova Lite 2 released yesterday, we can test that now and make sure that it meets the standards that we have put out and that we don't regress from what we have right now. The future.  So we saw how we started out from inception as a hackathon idea, how we went ahead with the partnership of the Gen AI Innovation Center, how we implemented and rolled out in iterations, the architecture, and you saw the data flow as well and the challenges that we faced along the way.

We continue to learn from customer behavior and understand how our customers are evolving and how we need to evolve as well. And we're going to use new things as well, like MCP for example, for internal use. Santosh wants to talk a little bit about Bedrock Agent Core as well and what we're doing with that.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/2640.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=2640)

Yeah, absolutely. So we've seen how Rohit talked that Lambdas were really central to how AceAI was actually built. You know, there's a lot of different nuances that Lambdas handle really good today and get the job done properly. But then we all know that Gen AI applications are unique by themselves. They have different requirements and different characteristics to them.  And then you also need to have a platform that is efficient enough to support all of your agentic AI applications and LLM applications.

And that was the reason why at New York Summit, I believe in July or August this year, we launched Bedrock Agent Core. So Bedrock Agent Core has a lot of different primitives onto it such as runtime, identity, and observability. It's a very helpful feature of Bedrock that a lot of our customers are experimenting with, and same with FanDuel as well. Lambda's got them so far and got the job done really well, but with Agent Core is what FanDuel is now evaluating to see how that actually fits with AceAI runtime platform.

You can now run workloads as quick as a few seconds or a few milliseconds and have those workloads run until many, many hours. I believe eight hours is the absolute limit that you can run. So as AceAI is exponentially adding more features to it and as it's getting more and more robust, we're currently actively evaluating Agent Core runtime as a way of scheduling all the work and all the workloads.

And secondly, I wanted to ask, how many of you are following up on all the launches that we've had in the serverless space? We've got quite a few. Very good, I see some hands. We've got some quite exciting announcements with Lambda and API Gateway, and one that is very much of interest for us here with the FanDuel team is that API Gateway now has the capability of streaming responses. So when you're interacting with Bedrock, when you have Lambdas running in between, you can actually stream the response as it is happening live and then send it out to users instead of chunking it or batching it.

To make the user experience even better, there's a lot of things that we're doing, and API Gateway is one such area that we're investigating and experimenting with right now. I'm sure we can talk a lot more here. We're also looking at SDKs, the Bedrock SDKs that we've launched as well to provide you with a consistent manner in how you can build your generative AI applications. There's a lot more to it, and we're very excited to see the future of AceAI and how Amazon Bedrock is going to help shape that for FanDuel.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f7b7b6e53f66607f/2780.jpg)](https://www.youtube.com/watch?v=XmBDchavCW8&t=2780)

I think the API Gateway streaming is quite powerful. Our customers have asked for these features, and we're very excited to have launched this as  part of re:Invent just recently. I would also encourage all of you to try it out, give it a spin, and let us know what you think about it.

All right, it was really great presenting to all of you. Thanks for taking time to be with us today, and I hope you found this session helpful. Thank you very much, everyone. All right, see you folks.


----

; This article is entirely auto-generated using Amazon Bedrock.
