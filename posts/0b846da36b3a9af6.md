---
title: 'AWS re:Invent 2025 - Next-Gen Media Production: Amazon Nova reinventing Bundesliga Content (SPF309)'
published: true
description: 'In this video, Bundesliga and AWS demonstrate how Amazon Nova enables scaling content production and enhancing fan engagement. The session covers three content automation products: Match Reports (saving 90% of editor time), Bundesliga Stories (80% time savings with 40% higher user engagement), and video localization (75% faster processing, 3.5x cost reduction). A key highlight is Match Mate, an AI-powered fan companion using dynamic workflows with Amazon Nova Lite and Pro that reduces costs by 35% while delivering personalized statistics, live updates, and video search through natural language queries. The presentation showcases Bundesliga''s comprehensive approach using Amazon Bedrock, Rekognition, Transcribe, and OpenSearch to serve over 1 billion fans globally across their direct-to-consumer ecosystem.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/120.jpg'
series: ''
canonical_url: null
id: 3085216
date: '2025-12-05T04:06:35Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Next-Gen Media Production: Amazon Nova reinventing Bundesliga Content (SPF309)**

> In this video, Bundesliga and AWS demonstrate how Amazon Nova enables scaling content production and enhancing fan engagement. The session covers three content automation products: Match Reports (saving 90% of editor time), Bundesliga Stories (80% time savings with 40% higher user engagement), and video localization (75% faster processing, 3.5x cost reduction). A key highlight is Match Mate, an AI-powered fan companion using dynamic workflows with Amazon Nova Lite and Pro that reduces costs by 35% while delivering personalized statistics, live updates, and video search through natural language queries. The presentation showcases Bundesliga's comprehensive approach using Amazon Bedrock, Rekognition, Transcribe, and OpenSearch to serve over 1 billion fans globally across their direct-to-consumer ecosystem.

{% youtube https://www.youtube.com/watch?v=4_M_QdjJiL4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Bundesliga's Partnership with AWS and Vision for Fan-Centric Innovation

Welcome everyone. Thanks for joining today. You can learn how Bundesliga uses Amazon Nova to scale content production and enhance fan experience. For those who don't know, Bundesliga is Germany's premier soccer league. My name is Jean-Michel Lourier. I'm a Principal Data Scientist with AWS and the technical lead of the partnership between Bundesliga and AWS. I'm delighted to be on stage today with Alexander Altenhofen, Director of Product and Technology at Bundesliga, and Artur Hartwig, Senior Software Developer also at Bundesliga.

As an introduction, Alex will talk about the partnership between AWS and Bundesliga before walking us through Bundesliga's product strategy and core principles. Then we're going to dive deep into scaling content production by showing you three products of Bundesliga, which are prime examples of scaling content production. After this, we're going to move into fan engagement and again show you an example of Bundesliga. Throughout the whole talk, we're going to emphasize how Amazon Nova's price-performance enables Bundesliga to scale content production and enhance fan engagement. With this, Alexander, please kick off.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/120.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=120)

Thank you. So welcome to any football fans here? Please raise your hands. Perfect. Any soccer fans? Yeah, perfect. So let's start. Welcome to Bundesliga.  There are over one billion people around the world who are interested in Bundesliga, and our goal as the DFL, the governing body of Bundesliga and Bundesliga 2, is to become the most fan-centric football league in the world. AWS is helping us come closer to this goal day by day.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/140.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=140)

We actually started building on AWS since 2016, and since 2020, AWS is the official technology provider for Bundesliga.  We have been changing the game since then. In 2024, when we renewed our contract with AWS, many companies were still stuck experimenting with GenAI and working on prototypes. But we were already in production with our first GenAI solutions and served them to over 100,000 Bundesliga fans in our app. AWS and the DFL both share the same mindset when it comes to customer centricity, and AWS helps us drive our innovation and deliver a groundbreaking fan experience and deeper insights around the game of football.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/190.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=190)

 Within our partnership, we have set up three pillars. The first one focuses on data, specifically sports data, where we build on the success of Bundesliga Match Facts and develop new technologies for referees to take faster and smarter decisions. The second pillar is about media production, where we leverage GenAI technologies to scale our content production and localize content for our global audiences. The third pillar, my favorite one, is creating unique personalized fan experiences to engage fans within our direct-to-consumer ecosystem.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/240.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=240)

How are we going to do this? First of all, compared to other football leagues, we are in full control.  The DFL covers the whole value chain, and we call this our glass-to-glass strategy, which starts at the glass of the camera lens in the stadium and ends with the glass of the smartphone or the TV where fans are watching Bundesliga matches live with our partners like ESPN and Sky, or consuming viral content within the Bundesliga app. With this unique setup, we are able to build and commercialize products end to end.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/270.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/280.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=280)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/290.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/300.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=300)

Through this organizational setup, we have various touchpoints with our fans.   There are over 50 million people on social media following us, and we are on track to generate over five billion video views on our social channels this season.  Over 10 million people every weekend alone in Germany are watching Bundesliga with our partners, or they take part in our self-hosted events like the Franz Beckenbauer Supercup. 

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/310.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/320.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=320)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/330.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=330)

These are opportunities where fans may already be customers of our commercial licensing partners like EA. Our goal is to bring all those fans into our D2C ecosystem, especially the Bundesliga   , where we collect first and zero party data to personalize their fan experience within the app. This enables us not only to increase loyalty and retention with our fans, but also to develop new business models and generate future revenue streams for the DFL and our stakeholders.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/350.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=350)

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/370.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=370)

When we rebuilt our app last year, we set up five guiding principles for the Bundesliga app . The first one is about personalization. Everything when a fan comes into the app should be personalized based on their favorite club, their region, their heritage, and their fan interaction. The second one is continuous pathways , so we want to bring users into constant content consumption loops where they can interact with content and find new stuff.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/380.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/390.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=390)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/410.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=410)

The third one is about video and story centricity , so we believe that video enriched media is the core of our app and is designed to drive retention with our fans. The fourth one is about finding new content, so discoverability, where users can find new and relevant content for them, and if they have a question, they will get the answer from us. The fifth one is engagement  . We really see in our data that engagement drives retention and that people are actually interested to take part in quizzes, voting, and engaging with our content.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/440.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=440)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/460.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=460)

### Automating Bundesliga Match Reports: Using Amazon Nova to Generate Editorial Content

In the next few minutes, my colleagues will show you four examples where we are trying to get closer to those core principles, and we start with scaling our content. The first project I'd like to present are the Bundesliga match reports . Our fans love to read about match reports after the game has concluded. They might have missed the match or were unable to watch due to time zone differences, but our editors are swamped with work during such a match .

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/480.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=480)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/490.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/500.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=500)

They have to watch the game and compose live ticket entries. Meanwhile, they also compose Bundesliga pushes and match stories. We found that if we automate this task, we could alleviate the stress of our editorial staff and increase the performance of such products . Let us take a look at how such a match report actually looks. We have an introduction, embeds, videos and paragraphs with associated images , and at the very end we have statistics with the fantasy heroes .

We can see that we have a rather predefined structure when it comes to match reports. We have an introduction, a pre-game discussion which is often covered in the live blog commentary, the first and second half which is also discussed in the live blog, stats and MVP which is decided by the editor, and all of those paragraphs are complemented with images which were taken during the match by licensed photographers. We can tap into vast data sources like the live blog commentary which is composed by the editors themselves. We have match event data which contains the clubs that play and everything else, but we also have match statistics where we have, for example, the winning probability which might drive an LLM to focus on different aspects of a match that has been played.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/580.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=580)

We have historical match data which is interesting when it comes to derby matches where we want to focus on the teams rather than the outcome. We have team lineups where all the correct spellings of all players are part of this data . So we have all the recipe parts we need to actually come up with an LLM workflow to generate such match reports. In the beginning, an editor kicks off this whole process. We ingest all of the relevant match data into our OpenSearch database either during the game or after the game, and we use AWS Lambda to transform this data into the type we need to actually pull off the prompt.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/620.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=620)

Once the match report has been generated, a human editor can review it and publish it to the web and app. However, the question becomes: how do we select the appropriate images for each of the sections that the LLM generates?  We opted to use a multi-modal prompting approach for this. We first generate the match report text, supply assignment instructions, and provide blobs of all the images that were taken during the match. We then get back references to those images and recompose the match report.

The downsides to the often-used vector search approach are that we have quite high latency when generating such match reports, we use up a lot of tokens, and most LMMs are limited to 20 images. With our approach, we know which images to take. We don't have to search any database, and we don't have to come up with search phrases because we can supply the image blobs to the LM directly. This seemed like the perfect approach for us.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/680.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=680)

When it comes to actually prompting,  we have a straightforward approach as well. We supply all the content blocks we need, and we have the matchup and all the relevant data available. However, in generative AI applications, we are facing very common pain points. These include hallucinations when it comes to incorrectly referencing stats, incorrect paragraph styling when it comes to editorial content guidelines on how such text should be generated, and quotes that were sometimes incorrect. We also wanted to use British English instead of American English.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/720.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/730.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=730)

Our solution was to not only have a human in the loop in charge of everything from start to finish, but also to introduce a review by Amazon Nova at the very end to help and assist   the editor in correcting those generated match reports even quicker. The human in the loop, the editor, kicks off the production of such a match report by selecting the match and selecting a persona, which might be different depending on the matches played. For example, as I mentioned before, derby matches often focus on the teams rather than the outcomes, but some matches were rather boring in the first half and were very interesting in the second half. You can supply additional instructions to focus on specific parts of your generated match report, all from a central content management system as an editor.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/770.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=770)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/810.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=810)

The generated match report then gets reviewed, and Amazon Nova tries to find all the style inaccuracies and fact-checks everything that has been provided and validates all the quotes.  It's not 100 percent correct all the time, like most LLMs, but here are two examples of what Amazon Nova has found and suggested to the editors. For example, spelling has been corrected, and the editor can then manually edit the text appropriately. Additionally, the top scorer was incorrectly mentioned for the statistical element at the very end of such a match report. 

We can summarize that having such an end-to-end solution where the editor kicks off everything and is in control of every step saved us about 90 percent of the editor's time. Previously, when an editor started writing such a match report in the second half, they now can with one button generate such a match report and save 90 percent of the time. They can use this time to write other articles and other content pieces, which amounts to about 20 additional content articles for editors during such a matchday. Amazon Nova, as per our editors, found approximately 70 percent applicable corrections, which is very good and very fast.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/860.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=860)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/880.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=880)

### Creating Bundesliga Stories: Transforming Articles into Engaging Short-Form Content with AI-Powered Image Selection

With this, we move over to the second product  we recently released with Bundesliga Stories. The Bundesliga match reports are long-form content. We see that engagement in apps is driven by the younger cohort by more engaging and image-centric media.  With Bundesliga Stories, we wanted to create highly engaging short-form swipeable content, much like Instagram Stories, basically where we focus on images and storytelling. Our solution would be to repurpose existing articles which editors have already written and the images embedded in those articles, alongside our vast database with about 150,000 new images coming in every season.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/920.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/930.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/940.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=940)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/950.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=950)

We transform them into multiple sets of slides of different types. We assign proper and engaging images to all of them.  Let us take a look at how this works. With the help of Amazon Bedrock and Amazon Recognition, we create Bundesliga stories.  We transform articles into engaging slides for our app users within seconds, and our fans love it.  We are now able to scale our content production like never before by automating routine tasks with AI. We are empowering our editors to create better experiences for Bundesliga fans worldwide.  As you can see, we have three examples of slides generated by this process. We have a regular slide with text and a description. We have a quote slide with an engaging image and a preview image of a video which you can play in the app.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/980.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=980)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1000.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1000)

How did we proceed in architecting this LLM solution?  We used a three-step function approach for this. In the first step, we take the Bundesliga article and transform it into separate slides, each with only text and associated metadata. In the second step, we take all the images which are associated with such an article  and, much like in the match stories I presented earlier, we use computer vision to assign the appropriate images to the generated slides. We know that those images are relevant because they have been used by the editors for the original article already. We use Amazon Nova for this because it offers the best price and performance and responds very quickly to our queries.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1030.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1030)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1060.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1070.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1070)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1090.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1090)

However, most of the time we generate more slides than there are images in such a story. That is why in the third step  we use a vector search approach where we tap into our metadata store database where all our images are contained and find the appropriate images to complement the slides. We asked the editorial team which images to pick, and they gave us four aspects on selecting such images. First, relevance: we want to complement the storytelling with relevant images.  We do not want them to be disconnected from the actual content presented.  It has to match the mood and the intent of the slide. Second, focus: we want to have the teams or the players focused in each of our slides. Most of the time we have player images from matches, but we also have interviews and other media which we use. 

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1120.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1120)

The third aspect is motifs. We do not want to have stadium images on all of our slides. We want to have a different variety of images present throughout the whole story. That is why we want to select them by scenery, for example, not only stadium images but also interview images and close-up images. That is what we call motifs. The fourth aspect is recency.  We want to use the most recent images available. We want to have the most recent jerseys visible in those images, and we want to feature the most recent games. When we take all those four aspects into account, we have to think about how we ingest our images to make this all possible and to select by those four aspects.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1150.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1150)

We came up with a synchronous and asynchronous approach to the image ingestion workflow.  In the synchronous task, the image is uploaded by an editor or automatically ingested, and only the active data is extracted and stored in our metadata store. The image is directly available to be embedded and resized dynamically and served via CloudFront. Asynchronously, we then determine the match-related metadata, for example, competition ID and season ID. We do this often heuristically, where we determine based on the fixtures and the teams and players visible which match day and which match a particular image is about. We detect all the player faces using Amazon Recognition, and for embeddings we are using Amazon Titan multi-model embeddings. For the last aspect, the motif, we trained Amazon Rekognition's custom labels.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1230.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1230)

We trained Amazon Rekognition's custom label feature with different categories such as action, celebration, and goal, along with several subcategories like general shot, tackling, behind the goal, and sideways from the goal. We trained each category with approximately 1000 images. This is how it looks for an editor when composing or uploading an imageâ€”we have all the appropriate tags directly available in those images. 

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1260.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1260)

We know we have a very high detection rate of our players because at the beginning of each season we take high resolution 360 degree shots of all of our players during the DFL media days, which is very important to actually detect all of the player faces for all of our images uploading. 

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1280.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1290.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1300.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1300)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1310.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1310)

When it comes to actually scoring the images for selecting them, we let the LLM generate not only a search phrase but also the motif and the player supposed to be visible on those images. Then we came up with a scoring algorithm where we take the similarity of the generated search phrase from the LLM, which is a normalized value between 0 and 1, and multiply this by the motif boost, where we detect whether or not a suggested motif is present on an image being considered, and we multiply this by 10 percent.    

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1330.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1330)

Lastly, we give an additional 20 percent boost to any images which are most recent. As a developer, I thought we should just give a wholesale 20 percent boost to all recent images like the last 250 days, but this was actually not what the editors wanted. They wanted a distribution, and my suggestion was a linear distribution, but they actually value more recent images exponentially higher than older images. That's why we took an exponential bias curve where we apply all of our calculations. We stopped giving any multiplier boost after 250 days of the image being taken. 

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1400.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1400)

When we combine all of those aspects together, we can filter for the images and supply in Amazon OpenSearch Service a scoring script which lets us calculate the score for each of the images in our database. In the first line we calculate the similarity which the generated search phrase has provided. In the second paragraph you can see we calculate how old the image is, and if the image is older than 250 days, we don't give any additional boost to those images. We apply our exponential distribution bias curve, and at the last step we only look at whether or not any of the motifs or subcategories of our suggested motifs are present. We multiply this value and we automatically choose the first image which has the highest score, which is most of the time the image the editor would have picked. 

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1440.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1440)

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1480.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1480)

This is how the output looks for an editor. Based on what the LLM has generated, it suggested to use a text slide for this particular story and came up with a title and description. It suggested that Harry Kane, who is part of Bayern Munich, be visible and a goal scene to be present. We have a search phrase which has been generated, and a depiction is only there as a helper to steer the LLM as a train of thought to help generate such a cohesive picture.  

Now that we have this whole solution in front of us, we know that without this comprehensive image ingestion workflow we weren't able to provide such an algorithm for our editors to use in the generation of such stories.

We know our custom scoring script finds the perfect image in our vast database, and with Amazon Nova we can assign images quickly that are already present in the article. We know those Bundesliga stories have impact. Bundesliga fans who use the app and engage with stories have 40% higher time spent in app. They occur for 70% more sessions and 20% increased one week retention. If we compare this process by creating a story from an already existing article to writing such a story from scratch, editors save about 80% of time in that endeavor, and Amazon Nova reduced the cost for this crucial step for us by 70% and is as fast as other LLMs. With this I give it over to Jean-Michel for the next part.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1580.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1580)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1600.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1600)

### Video Localization at Scale: Breaking Language Barriers for Global Broadcasting Partners

Yeah, thank you, Arthur. So we looked so far in  scaling content for fans, and we're now going to segue a little bit into scaling content for partners of Bundesliga. Here you can learn how to build an on-demand video localization at scale to content that you might also offer to international customers.  Bundesliga has more than 1 billion fans around the world across 200 countries. However, Bundesliga produces most of its content in English or in German. If you put yourself into the shoes of an international fan, how would you prefer to consume soccer content? Most likely you'd like to consume this in your own native language. It's important to know that Bundesliga does not stream content directly to fans. They offer a high quality product to broadcasting partners.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1660.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1660)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1700.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1700)

At the moment, these partners need to invest time to localize that content, which limits the global distribution of that content. That is why Bundesliga built a video localization solution that removes that language barrier and increases the global market reach. So first I'd like to show you a small example of this  and I would ask you to pay specific attention to the voices in the following video. He is the undisputed leader up front for Bayern. His special qualities lie in how he controls the ball and gets his shot off. It's a joy to watch Kane. In great control, Kane, brilliant. Game after game, week after week, King Kane is delivering. Beautifully controlled, expertly dispatched. His ability is clearly in finishing, but he can also score with his head, having done so 8 times.  Harry Kane, a poacher's goal.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1710.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1710)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1750.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1750)

I think you could observe the two voices. First of all, it's a narrative voice which was translated from German  to English, and there were live calls which were kept in the original English language. This video is editorially produced with different specific components and to meet the high bar of Bundesliga, the localized video needs to have the same editorial design. Just one word on the languages: we picked a video in English to make it easily accessible here for this presentation, but Bundesliga targets markets from Latin America to Middle East to Asia and according languages. This solution is offered to media partners of Bundesliga through  the Bundesliga media portal. In the media portal, the media partners can discover the content in English or German and then request it in their native language.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1770.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1770)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1780.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1780)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1800.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1800)

Once they make the request, that hits our API powered by Amazon API Gateway.  An Amazon API handler takes that request and puts it on SQS and that's when a step function kicks in.  The step function can be composed of different steps for the localization. A few examples are the translation, which we do with Amazon Bedrock, transcription with Amazon Transcribe, and voice generation with an AWS partner solution from Deep U AI. 

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1810.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1820.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1820)

Once the video is localized, we put it back on S3 and make it available for download to the media partner.  The core logic of the localization really happens in that workflow.  We learned that the localization workflow needs to be tailored to the actual products that Bundesliga produces. Bundesliga produces more than 20 products, and all of them have different editorial design. We now have a list of different steps that you can always choose depending on which product we want to localize.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1880.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1880)

It also allows us to use different inputs for different products. Some videos might have a shortlist that describes what is present in a video, and that is very helpful for our localization. Only if we use all inputs that are available to us and preserve the editorial design of the video do we meet the high quality in the localized video that is actually required. As an example of such a workflow, we start with demultiplexing a video.  That means we take away the audio tracks from the raw video signal, and we found that processing the audio separately makes it faster and more cost efficient.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1900.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1900)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1910.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1910)

Then we transcribe the audio using Amazon Transcribe.  We also use an LLM-based correction, for example Amazon Nova Pro, to iron out a few mistakes in that transcription.  Once we have the transcription, we run Amazon Rekognition to segment the video. That is really important to assign the editorial design to the video that we are processing. Once we have the segmentation, we go into the actual translation.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1930.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1930)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1940.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1940)

With the translation, we generate the new content.  Voiceovers are generated and new subtitles are created. Then we multiplex together the video and the audio tracks and possibly also the subtitles.  For translation, we found that LLM-based translation is the best choice for us. We use either models from Anthropic or from Amazon Nova. To make that decision about which LLM to use, we first need to establish an evaluation.

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/1960.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=1960)

 We settled on the word error rate for evaluation. A word error rate measures how many words need to be changed, removed, or updated depending on the translation made by AI to make it into a correct translation. With translations, there is the problem that for a given input text, there are multiple correct translations. We solved this problem by using machine translation checks. In that case, we make the AI translation first and then give it to a professional translator.

This professional translator takes the AI translation and makes minimal changes to make it correct, and that becomes our ground truth going forward. Here is an example. We have an AI translation to English which has three sentences. A human translator checks this translation and changes two words. Changing two words in the sentence results in a word error rate of approximately 5 percent, and that gives you an intuitive understanding of what these numbers mean.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2050.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2050)

A word error rate of around 5 percent means changing two words in a text of about three sentences. With the metric at hand, we can now compare different methods for translations.  As a first approach, we compared Amazon Translate against Amazon Bedrock, which is LLM-based translation. We found that with LLM-based translations, we got lower word error rates, meaning higher quality translations. That was mostly because we are able to avoid idiomatic errors in the translations for the sports commentary.

Then we compared different LLMs for different language pairs that we want to use for the translation. We get word error rates as low as 2 percent or up to 5 and 7 percent depending on the language pair. Our approach is that we always choose the LLM which gives us the lowest word error rate for that language pair. If we see matching results, we prefer to use Amazon Nova because of the price-performance ratio. We can reduce costs and latency of the translation.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2110.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2110)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2150.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2150)

Speaking of quality, there's one  cool feature I haven't even mentioned. Our solution can automatically learn from human feedback to improve the translation. If you remember from the beginning, the media partners use the media portal to request a localized video. But they also have the option to review the result and resubmit the localization to get a corrected version of the localized video. This is a very important feature for the media partners because it gives them control over the translation  and ensures they have the high quality of translation they need.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2170.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2170)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2190.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2190)

At the same time, this is also very valuable to Bundesliga because we can now process this human feedback. In this case, we do this with Amazon Nova to turn the feedback into a correction rule  which is then applied to other translations going forward to avoid the same mistakes. That is how the solution learns automatically. Let me make that a bit more tangible with an example. Let's assume a media partner requests to localize a video from German to English,  and it contains this sentence here with the word "uncomfortable," which is not a great fit in this sentence.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2210.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2210)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2220.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2220)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2240.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2240)

The media partner goes in and changes "uncomfortable" to "challenging." We process that change with Amazon Nova and derive a rule.  The rule could be: when translating from German to English, do not use the word "uncomfortable" when describing how tough a team plays. We put that on Amazon Bedrock knowledge base.  Next time a media partner looks into a similar translation, we first look into the knowledge base to see if we have a similar translation in the past, and then we avoid the same mistake. So now we use the word "challenging" instead of "uncomfortable." 

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2270.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2270)

To sum it up, Bundesliga built an AI-powered content localization solution which streamlines the content production for international content at scale. We found that the solution reduces the processing time when localizing a video by 75 percent. We also found that when we make use of the price-performance of Amazon Nova Pro, we can reduce the cost by 3.5 times. 

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2290.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2290)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2330.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2330)

### Match Mate: Building an AI-Powered Fan Companion for Personalized Live Match Experiences

Now let's shift gears. We've looked into scaling content production through three different products from Bundesliga. Now we're going to look into enhancing fan engagement. Here you can learn how to use GenAI to deliver a highly personalized experience to your customers.  Bundesliga knows that during live matches, around 80 percent of the fans juggle with different apps to get live data. In a younger cohort, 70 percent chat during the matches. Bundesliga wants to put an end to this second screen chaos, and that's why they want to provide a one-stop shop experience for fans which democratizes the access to statistical data. 

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2340.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2350.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2350)

That's why Bundesliga built a fan experience which is currently under private preview and has the working title Match Mate.  Let's assume Berlin versus Munich is on, and of course we support Bayern Munich. Berlin scores the first goal and we get a notification of that goal in Match Mate. Then our team scores. In that case,  Harry Kane scored the equalizer and again we get a scorecard with a celebrating image of that player. But we also see moreâ€”Match Mate does research on that goal and provides some context to it. What does this goal mean to the player? What does it mean to the club? That is just how a live commentator would do this.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2380.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2380)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2390.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2390)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2400.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2400)

Now we want to know what is the impact of that goal, and we ask Match Mate through natural language: show me the top 5 of the table.  We get the live standings and we see no surpriseâ€”Bayern Munich is on top of the table.  We want to look a little bit further and ask what is up next for Bayern Munich. Again, we just ask that through natural language.  Then we get the next lineups and the next fixtures of Bayern Munich.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2410.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2420.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2420)

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2440.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2440)

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2460.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2460)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2470.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2470)

The conversation could stop here, but Match Mate re-engages with us  and offers a statistical analysis of the upcoming game. We can see that Bayern dominated the matches . However, we want to dive deeper, so we ask a statistical question to compare both teamsâ€”Freiburg and Bayern Munich. Match Mate shows us how the last matches went between the two teams. We can also look into  rich content in Match Mate, so we can easily ask to show all goals that Harry Kane scored in the second half at home. First, these goals are listed, but Match Mate has a video search in the background which then presents these videos directly to the fan , and they can consume it in the app .

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2480.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2480)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2500.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2510.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2510)

Let's look at how this is made possible. When a fan raises a question, it goes through a backend service  put on an event bus powered by Amazon EventBridge and then passed on to the chatbot service. The chatbot service responds with a natural language response to that question. We've also seen that fans can ask for videos, and that is enabled by integrating the chatbot service  to a video service. What makes Match Mate really fun is that it also proactively pushes relevant content to us , and that is what the nudging engine does. The nudging engine uses events from the matches to understand what is relevant content for the fans and pushes it proactively into the app, like the research of the goal, which is interesting to us.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2540.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2550.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2560.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2560)

Now I'd like to dive deeper into how the chatbot service works and how the video service is enabled. Let's start with a simple question: which teams score most in the final 15 minutes?  This is a statistical question, and we need to turn this question into  an SQL query to respond to it. That's what we do with a text-to-SQL workflow. Once we have the data at hand, we can formulate the natural language response back to the fan .

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2580.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2580)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2610.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2610)

For the first step to generate the SQL query, we use LLMs, and here we use either Anthropic Sonnet or Amazon Nova Pro, and we'll look into this in a moment. Once we have the SQL query, we run that against Athena to pull the data from S3 . With that, we can formulate the answer back to the fan. Now let's think about what questions fans can ask. There will not always be such simple questions as we see here on the slide. There will be a vast variety of questions that will hit Match Mate. If we would use one static workflow for all these questions, that would not be very cost efficient. That is why we introduced dynamic workflows .

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2620.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2620)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2650.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2650)

In a dynamic workflow, we try to understand the question or the type of the question first and then adopt the workflow to have a very cost-efficient  workflow to answer that question. In the first step for a question like how many goals did Harry Kane score, we try to answer what type of question the fan has, and we do that with Amazon Nova Lite. We look into the query type, which can be an individual player stat or a team stat. It can also be a comparison, and we look into complexity, where we differentiate between simple, medium, and complex questions. Once  we have the complexity, we route that question through different paths. For simple questions, we can take advantage of the price-performance of Amazon Nova Pro to answer it. For more complex questions, we route it through Sonnet 4. We found that around half of the questions can go through the simple path, which allows us to reduce costs quite significantly.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2680.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2680)

Once we know through which model  we want to route that question, we now build the actual prompt to run it, and that also makes use of the classification that we have done upfront. We also use it to pull a few examples which we then use for few-shot learning in the prompt. With that, we run the SQL query against the LLM which we picked, then run it against Athena to get the data and come back to the fan with the answer.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2710.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2710)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2730.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2730)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2760.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2760)

 We've seen how the chatbot service is able to respond to fan questions for statistical questions, but if you remember from the video, fans also have the chance to ask for videos in Match Made, and that is made possible through the integration of the video service.  If you think of video search, you might not think about a chatbot in the first place. But in our case, the fans, when they're looking for videos, are looking for specific moments in Bundesliga matches, and that's what we use the chatbot for. We use the chatbot to identify these moments and once we have them, we run a search on the metadata of video  to find the videos and play them back to the fans.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2780.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2780)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2790.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2800.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2800)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2820.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2820)

Let's have a look at this. So let's assume the fan asks, "Show me the last goal of Harry Kane." First of all, we need to identify the moment in the match when the last time Harry Kane scored. The chatbot service, coming back to the classification of the dynamic workflow,  understands that this is a query of type match event, so a match event should be returned. In Bundesliga, that is an event identifier  which says for each event in a Bundesliga match what the exact time stamp is when it happened.  Once we have the event identifier, that can be passed on to the video search, which runs a metadata search on Amazon OpenSearch to return the videos which capture this exact moment. Once we have the videos, we can play them back to the fans. 

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2860.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2860)

### Conclusion: Amazon Nova's Role in Bundesliga's Journey to Becoming the Most Fan-Centric League

We discussed how Bundesliga makes personalized and interactive content accessible to fans. We found that Match Made enables Bundesliga to scale their personalized content by five times per user. With the dynamic workflows, we can make use of Amazon Nova, which allows us to reduce the costs by thirty-five percent. So coming back to our conclusions, Amazon Match Made, as you've just seen, is not live or public yet,  but I've been personally testing it for the last few weeks, and let me tell you it is awesome. Match Made is everything what we've built over the last few years, from UX, content, sports data, AI, and tech finally coming together in one personalized product within the Bundesliga app. So watch out for the release next year.

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2930.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2930)

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2950.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2950)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b846da36b3a9af6/2960.jpg)](https://www.youtube.com/watch?v=4_M_QdjJiL4&t=2960)

Wrapping it up, today you should have learned that Bundesliga is leveraging GenAI and data to become the most fan-centric football league in the world. We are using this to develop new digital business models. We showed you how we localize videos for global audiences and scale content production with Match Stories and Match Reports, and offer every fan an AI-powered fan companion with our Bundesliga app. In all examples, we use Amazon Nova as a key infrastructure to run those GenAI workloads in production in a cost-efficient way. That's the end of our presentation. We have another session tomorrow from Bundesliga where we'll give you more insights about the chatbot  service that was just presented, and we finish it up with a small video.  


----

; This article is entirely auto-generated using Amazon Bedrock.
