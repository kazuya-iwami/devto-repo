---
title: 'AWS re:Invent 2025 - How Allianz designed AIOps at enterprise scale (IND3321)'
published: true
description: 'In this video, Philipp, a Global Solutions Architect at AWS, and Tivadar from Allianz Technology discuss building scalable AI and GenAI platforms. They address the challenge of proliferating capabilities, frameworks, and diverse user groups while avoiding vendor lock-in. Tivadar presents Allianz''s dual pathway approach: Data Science Workbench 2.0 built on Amazon SageMaker for rapid experimentation, and a flexible self-service lane with staged governance for production. Key strategies include using "low regret standards" like Git for prompt management, OpenTelemetry for observability, and containers for decoupling applications from runtimes. The platform supports multiple personas from business analysts using SageMaker Canvas to engineers using JupyterLab and Bedrock. Allianz demonstrates practical implementations including document processing with Textract and automated infrastructure provisioning, achieving faster development cycles while maintaining enterprise security and compliance standards for over 100 million customers.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - How Allianz designed AIOps at enterprise scale (IND3321)**

> In this video, Philipp, a Global Solutions Architect at AWS, and Tivadar from Allianz Technology discuss building scalable AI and GenAI platforms. They address the challenge of proliferating capabilities, frameworks, and diverse user groups while avoiding vendor lock-in. Tivadar presents Allianz's dual pathway approach: Data Science Workbench 2.0 built on Amazon SageMaker for rapid experimentation, and a flexible self-service lane with staged governance for production. Key strategies include using "low regret standards" like Git for prompt management, OpenTelemetry for observability, and containers for decoupling applications from runtimes. The platform supports multiple personas from business analysts using SageMaker Canvas to engineers using JupyterLab and Bedrock. Allianz demonstrates practical implementations including document processing with Textract and automated infrastructure provisioning, achieving faster development cycles while maintaining enterprise security and compliance standards for over 100 million customers.

{% youtube https://www.youtube.com/watch?v=GYNeA7NZE3w %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/0.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=0)

### The Growing Complexity of AI and GenAI Platforms: Challenges at Scale

 Welcome everybody. My name is Philipp. I'm a Global Solutions Architect working for our financial customers, and next to me is Tivadar. He's one of the builders behind the platform we are going to talk about in this session. Tivadar is from Allianz Technology, and I will start with a small introduction about what the actual challenges we currently see when building AI and GenAI-based platforms at scale. When I'm done with this, I will hand over to Tivadar, who will give a brief introduction of who Allianz actually is and then walk you through how we tackle some of these challenges at Allianz.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/60.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=60)

 When we look back over the last, let's say, five to eight years, AI and ML has always been a hard problem. It always came along with a lot of heavy lifting. But the point is, the capabilities we had to provide to the actual users were pretty stable. We always knew, okay, my folks need a place to train models, they need something for experimentation, monitoring, inference. Although it was complex, it was stable, and for that heavy lifting we invented things like Amazon SageMaker to make this simpler.

If we then look back over the last two years, there was an explosion of capabilities and, along with that, of complexity when GenAI entered the stage. This is just an illustration. It's not a 100% complete list of all the capabilities you will need to provide such platforms. But I think what you can easily see is that there are a lot of new capabilities, and the important part here is these capabilities are just stacking up to our platform. They're not replacing what has been there. It's a simple addition of new stuff we have to take care of.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/160.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=160)

 If we dive deeper into this problem and look at the thing from the application side of things, we'll find that the complexity on the application side is even more complex. For each of these capabilities, people are creating frameworks to help us abstract these things a bit. But the point is, again, there is not a single framework or a single solution which solves all of these problems. Indeed, it's more of the opposite, actually, that all of these frameworks are pretty good, and you might want to use all of them because all of them solve a certain problem for you, might be doing this better than the other framework for this specific problem. I promise you, if your company is large enough, you will most probably find all of them somewhere, and this is just an example of a GenAI framework. The same holds true for, let's say, memory, RAG, whatnot. There are tons of these frameworks.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/220.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=220)

 If we change perspective a bit, what we can also see, which is in fact a good thing, is that GenAI democratized a lot who can work with AI these days. In the old days, and old days is funny because it's actually two years back, we had a very strict and narrow scope of who was building and working with AI. We had our researchers, data scientists. They even had degrees or PhDs in what they were doing, so we could assume they pretty well knew what they were doing. Then we had some ML engineers who were building the infrastructure around the platform, so it was a very well-defined world, if you will.

Nowadays, these people still exist, which is good, but if we look on the bottom level, we find a lot more different and highly diverse user groups. We have now business users using visual orchestration tools and starting to build agents all over the place. We have people who are interacting with chatbots, all these things, so the user groups are highly diverse and they want to build AI agents. The problem is, given their diverse skill sets and educational background, they need support in different stages over this process.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/310.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=310)

 So if we take all of these things into account, like the proliferation when it comes to capabilities, the amount of frameworks, and the different user groups we need to build our platforms for, the question is how can we provide a platform and an operating model for these users that on the one hand does not lock them in by being prescriptive and saying, "Okay, from today on you use this framework to build an agentic application or a GenAI-based application," but on the other hand, identifies the standards, these stable elements you can provide as part of the platform.

This approach allows us to first standardize our operations and processes, and on the other hand, use these capabilities and stable elements to scale and accelerate the adoption of AI within your company. To say it more in an architect's language, what we actually want to do is identify the stable elements, make them part of our platform, and decouple them from these fast-moving elements so that these fast-moving elements do not force us to rebuild, refactor, and refactor again and again. And by this, I'm handing over to Tivadar, who will now start with the introduction of Allianz.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/440.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=440)

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/450.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=450)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/460.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=460)

### Allianz: A People-First Company with 134 Years of Transformation

Thank you very much, Philipp. This is really great to be here. When I was a child, I saw a really great movie called Casino with Robert De Niro and Joe Pesci, and I am here in this city. Unbelievable. Yesterday I was in the desert, and I really would like to share what I was thinking in the desert.  But first, who we are, what we are doing, and what our purpose is. We are Allianz, and we have really strong numbers.  We are stronger than ever, but behind it we have a lot of people. 

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/510.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=510)

And for a developer like me, it has a meaning. Two years ago I visited Croatia and I was able to see the Allianz office, and I thought immediately, "I am at home." It happened also in Greece in the summer, and that has meaning. For 134 years we are here, and how we managed it is that we always realized how important it is to change. We are changing and we are still here, stronger than ever. 

I belong to Allianz Technology, and here we can see also really strong figures. But behind all those figures we have a lot of stories. For example, we have a really strong feedback culture. If I have a one-on-one with my boss, the first question is always, "What can I do for you? How can I support you?" And in my life I have never seen so many PhDs in one place, so many engineers, and we have just one purpose day by day. We would like to do it better and better. We are full of ideas, but we are in the people industry. We are doing insurance.

I am sure you already heard of Allianz, but you might think insurance is boring. Let me prove this is not true. We are doing the people industry, not just for external customers but also for us, for internal customers. A couple of months ago we were developing with one of our internal customers a new tool. We were really excited, and he said to me, "A couple of years ago I was really sick. I was fighting for my life. I was really confused. I really didn't know what I should do."

"I was already working at Allianz at this time, and Allianz came to me and said, 'You really need to put focus on your fight. You really do not need to think about your position, your salary, your living. We are behind you. We are supporting you.' And he won. He survived, and we were working at 8 p.m. and he told me the only thing that I can do right now is try to give something back."

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/650.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=650)

 We have the best company in the world. If you ever have a chance to visit us, please do it. Best service, best food, and we had the tribe review in this building, and I was standing next to this building and I thought, I belong in this company. This makes a difference.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/680.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=680)

### Democratizing AI Access: From Jenny I Love to the OneDIA Tribe

Really important, as I already mentioned,  we need to change. We need to move, and we realized this and we already started it. We have a vision. We have different groups, and with those different groups, we would like to support all those changes. We would like to bring AI everywhere. We would like to support the process, support the development, support every aspect. We have a target. We would like to achieve this by 2027, and we already started. We are not doing it from scratch.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/740.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=740)

We did the first step on how we would like to achieve it. We would like to provide training. We would like to provide documentation. We would like to provide tools that you will be able to use out of the box. First one, Jenny I Love, is unbelievably good. You can imagine you have immediate  access into one URL, and each employee who belongs to the technology has access to it. You can choose which model you would like to use. This is not just for technological people. This is also for the business. If you need to translate something, you can do this. If you would like to compare a couple of things, you can do this. But it can provide more, not just for the technology, and we have this out of the box. But we have another aspect also, agents. We have hundreds of agents with which we can make the onboarding easier. We really do not like to read documentation. We would like to get the results immediately. Jenny I Love can bring us this.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/790.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=790)

 And we organized another group, the GenAI Focus Group. With the GenAI Focus Group, we can bring to the people the coding assistant, coding assistants such as Amazon Q, which is a really great tool. This is the lower hanging fruit. If you would like to accelerate the speed of developing, if you would like to achieve higher efficiency, then the coding assistant would be the first step, the easiest way. And our focus group is providing this, which is really important because we need to measure it. We need to measure how many users are using and adapting the coding assistant. We need to check the productivity, what we achieved with the productivity. We can make interviews. We can check it, and we also need to check the enablement, how many users we trained. We are also providing training for them.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/900.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=900)

We have champions, a really great group. You can imagine we represent different teams. We represent approximately 200 people, but from 200 people we provided 12, 16, 19 people, but they have the background. They know what their team is doing, and we can provide a forum. And in this forum we can show new technology, new processes. And now I would like to talk about  Allianz in Germany.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/910.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=910)

We have our tribe where my team also belongs. This tribe is  OneDIA, Data Insights Analytics and AI. This tribe is for data, but data without process and without extraction really doesn't have a lot of meaning. We need to process the data, and for that we have this tribe, a really strong tribe. Here we can provide for our customers platform as a service, data as a service, data products, and AI-related things, for example, the Data Science Workbench. We can provide training as well.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/970.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=970)

You can imagine you are a use case team and you have a lot of ideas and you would like to bring your ideas into production. You can come to us, you can ask us, and we can provide the solution for your ideas, and we did it many times. We have a really, really  strong relationship with AWS, and AWS has really strong tools. We provided the platform for our customers to bring everything into production, and my Product Owners and my Lead Architects have a really strong motto: do it one time, make the documentation, and after that reuse it and repeat it, and we did it many times.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1000.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1010.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1010)

### Data Science Workbench 2.0: Empowering Multiple Personas with SageMaker

I  would like to talk about Data Science Workbench 2.0 because as we have seen in this picture, this is already the production,  but we also need a place where our developers are able to develop things. This is the place where the Data Science Workbench is based on SageMaker, a really, really strong tool. With SageMaker, we are able to provide instances for our customers where they can use model training and AI-related things like machine learning, and this is decentralized. That means each team can make their own onboarding and can start immediately.

It's really, really fast. It requires two days, you know. We know the feeling when you would like to develop something and you would like to put everything together. It takes days, weeks, months until you are able to install all the drivers and until you have connectivity in the infrastructure with the databases, but we have here everything out of the box, really easy to handle, really easy to get, and really strong.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1080.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1080)

 We have different personas because, as I already mentioned, we really do not want to only provide solutions for technical people. We also would like to provide solutions for non-technical people, for business analysts, because they can better understand the business. For different personas we have different solutions. If you are an engineer, you can take SageMaker. If you are a data scientist, you can use Bedrock, which is also available out of the box. Or if you are a business analyst, you can use SageMaker Canvas, such a strong tool where you really do not need really strong technical knowledge. You can use drag and drop, which is really great.

As you can imagine, with these things we can provide JupyterLab for developers who would like to use Python. JupyterLab is the best thing. If you would like to use a coding assistant, we also have coding assistant possibilities in Code Editor. If you have knowledge about R, our language for machine learning, we have two possibilities: one possibility is Python, another really big possibility would be R, and we can provide it. We really do not say sorry, you need to learn Python. We have RStudio and we can provide it for our customers. And we have Canvas, as already mentioned, a really strong tool.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1190.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1190)

But  why are we doing these things? Why do we have all those things? Because we would like to be more efficient. Amazon's tax measurement is really easy. We have a really long history as a company, and we can measure how many times it took to process 1,000 documents?

How many hours behind is it? We can measure it and compare it. How many times does it take with Textract? How many times does it take with Bedrock? We did it, and we already brought a lot of things into production. We have access to Secret Manager. You know it. In the normal case, you need to open a ticket and go to support, but we can provide direct access to the Secret Manager. These are really small pieces, but all those small pieces make a difference.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1250.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1250)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1280.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1280)

It's already happening. Our customers started to develop something  here in the Data Science Workbench, and after that, we were able to bring it into production. We provided the whole thing: governance, pipeline framework, and infrastructure. It works, and it's still working. I prepared some demo because I just would like to show you how  this whole process looks like, but really, these are the customers who do the magic. It is not the same as what they are doing, but maybe with it you will be able to see how much possibility we have with SageMaker.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1320.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1320)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1350.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1350)

First, we have a lot of forms. These forms could be medical forms or such things, and we need to process them, and we can use Textract for it. After that, we have the data, but we also need to process the data. We can use Bedrock for it, and after that, we can do the machine learning.  Let me start the demo. This is the code I wrote for it, and I already started. The first step would be Textract, and with Textract we will be able to extract the information and data from PDF. After we finish, we will be able to see immediately the result, a really strong feature. 

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1360.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1370.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1380.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1380)

Maybe our customers really do not understand German. They would like to use English. It's also possible. We can also do it. Send it to Bedrock, and Bedrock  will be able to translate from German into English. It's already happening. After that, we would like to somehow summarize all those  things. Bedrock can also provide that for us, and this is already happening. I created the CSV. We can see the CSV here, and  based on all that information, we can do the machine learning, and the business analysts can do their job with it.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1400.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1400)

### Enterprise-Scale Solutions: Infrastructure as Code and Git-Based Prompt Management

Also really important are the lessons learned.  We are working at enterprise scale, and at enterprise scale belongs the need to do everything as Infrastructure as Code. We need to use the four-eyes concept. We need to follow what happened and why it happened. I struggled with one of the pieces I was working on. I just wanted to write the Terraform code for the Code Editor, and at the end of the day, a new release happened on the Terraform side, and it solved the problem immediately. This isn't a product. This is a process which develops and is always moving.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1480.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1480)

Really important is the governance point of view because we are at enterprise scale, and we need to secure our customers. We need to secure the data. Custom images are important because we are working with custom images. We need to put certificates and proxies inside the custom image. Scalability is also really important because we need to provide a solution for 20 people, and we also need to provide a solution for 200 people or 300 people. 

This is really great. We discovered a pattern. Now I just would like to talk about different pieces. In the end, all the pieces are coming together. We discovered the pattern. Our platform engineers are doing the infrastructure. They are doing the governance.

They are doing the pipeline, but for business users, they are doing the prompt. They are closer to the business and can better understand it, but we need to provide them with a tool where they will be able to use evaluation and check their work. They can iterate their prompts. We already started this work three years ago, and we know that most of the time we are going and changing the prompt again and again. We are fine-tuning the prompt, and our business users would like to do the same, and they need the platform. They need the portal just to do this.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1590.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1590)

Second, we can provide the platform for them. We can provide a really strong platform, and we also need to make the evaluation in this platform. For our technical teams, we need to provide the pytest integration, and it is crucial that we combine the prompt management with evaluation.  But how can we do these things? We have ideas, we have prompts, and we would like to somehow track what happened in the prompt and how we can do all those things.

First, we put all our ideas into the box. Maybe we can try to use a database for prompts. We tried it out, it cost a lot, and it requires extra database knowledge. Then we put together what we would need for this purpose. We need versioning, we need traceability, we need tags, we need branches. After we put all those pieces together, we realized Git already knows all those things. We really do not want to reinvent the wheel. We would like to use what is already available in the market.

We have versioning, we can check what happened two or three years ago, we can create another branch for it, and we can do all those things already. It is already provided, and this is crucial. Git is a low-cost standard. It does not cost a lot, and all our developers know how we can manage it and what we need to do. But our business users really do not know this, but we have found the solution for it through automation.

They can only see the visual interface. But what is happening in the background is already automated. If they would like to save in this visual interface, then this will automatically trigger the Git commit. This will automatically trigger the pull request and everything, the Git push and everything which belongs to it. With it, they will be able to work and they will have such an interface, such a place that they will be able to evaluate immediately, and they can do this job and they can go back, and everything is automated. AI ops, really good things.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1740.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1740)

### Dual Pathway Strategy: Balancing Speed and Security Through Selective Standardization

 But how can we bring all those pieces together? What will be the steps for all those pieces? We recognize these aren't problems to solve, but tensions to manage, and our solution is a dual pathway. The first is the Data Science Workbench. I already talked about it, really easy to get. You can start, you can test, you can use it. The second would be the flexible self-service lane where you can do your job there.

In traditional IT at enterprise scale, we already realized if we are getting bigger and larger, then it takes more time to move in any direction, because you can imagine a tanker ship, as we heard already in another aspect. Small IT companies can react really, really fast, but somehow we also need to find a way where we can also react and move faster.

The goal isn't to eliminate the security aspect because we have more than 100 million users and we need to save our data. We need to protect our customers, but somehow we also need to find a way where we can react faster. We have a happy path, which will not be a Data Science Workbench, but we have another possibility: stage governance.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1840.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1840)

You can imagine  such a funnel. It's really, really wide at the top. You can go to a Data Science Workbench, you can test it out, you can make your experiments there. You have immediate access to Amazon Bedrock, Amazon Textract, you can do AI-related stuff, you can build LLMs, you can train them. But all the things which are going into production, all those things need to meet security standards. This is absolutely important because we need to save our customers, we need to secure our customers, and all those things need to meet compliance and such requirements if they would like to go into production.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1890.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1890)

Selective standardization.  The platform team is going to create the infrastructure, we are going to create the pipeline for it, and all those things are standardized. But it's really important to give them the flexibility at the application layer, and they can use what they would like to use, but we are protecting them.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/1920.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=1920)

And the next one.  We are not forcing everyone into the same box, into one box. We are providing a platform where they will be able to use what they would like to use. First step, going to the Data Science Workbench, the happy path. They can start fully integrated, they have connection everywhere. Second, self-managed teams. We have defined clear handover standards. They can put their application into the container. After that, we can use OpenTelemetry because we need to check, we need to keep an eye on the cost, what is happening there, we need to trace it, we need to follow it, and after that they can do the prompt management we already talked about.

You are not really doing it from scratch because behind it, you can imagine you would like to bring something to production as a use case, we can provide the agent for you. We already created the repository where all our template infrastructure as code is existing, and you can talk with this agent. You say, "Hello, I would like to deploy to London, I will need Amazon Bedrock." Then this agent is going into this repository, grabs the template, and provides it, and we can automate a lot of things. But at the end, we have crystal clear handover because we need to use our standards, we need to use security standards. What we can win from it, what is the main purpose for developers, is they can react faster, they can move faster, but we are also saving the environment.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/2040.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=2040)

 We also need to talk about decoupling to maximize flexibility. The first would be prompt from application, already mentioned. They can use a portal, they can create infrastructure, and they can do their own thing. The second one, really important, is OpenTelemetry to trace and follow what is going to happen. The third one is runtime. They can use it for long-running jobs, they can use event-driven Lambda for orchestration, they can use Amazon EKS, it's up to them what they would like to use for it.

We can provide this business interface and they can work with it. We already have it for more than 20 years ago, and this will be also here 10 years later, 20 years later. This will outlive a lot of things, and we have OpenTelemetry.

Why don't we use it? We have containers, a really strong and great solution. Do you still think insurance is boring? I don't think so. Yesterday I was in the desert, and do you know what I was thinking? I was thinking this is amazing. Thank you very much for listening. Thank you very much, Allianz. Thank you very much, AWS. Thanks a lot, Philipp. I am handing over back to you.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ddc392789619c683/2150.jpg)](https://www.youtube.com/watch?v=GYNeA7NZE3w&t=2150)

Yeah, thank you Tivadar. To sum it up a bit, the key takeaways of what we wanted to show you is first of all, build on what we call low regret standards.  What I mean with low regret standards is not necessarily that they cost you less money. It's more about standards that are already in place in your organization, which do not require a lot of change to get installed and build on, because they're there for twenty years. They're stable, they won't change. We could have done a talk about other technologies and all this stuff, but the idea was to show you a bit the standards which are there, which are maybe not super obvious, like OpenTelemetry. These are things which are worth it to build on.

Tivadar showed you how you can use Git to decouple basically the prompt management and the UI from the actual application, and in this way make sure to create portable AI applications where you can then decide on your runtime. We've decoupled the code and the runtime by OpenTelemetry. Don't forget OpenTelemetry is more than just tracing. There is a Generative AI standard built into OpenTelemetry. You can look this up and check it. It provides you things like the amounts of tokens consumed, the LLM you're using. You can use all these details to build a lot of governance around your LLM just by building on these standards.

Another super simple obvious standard is to use containers to encapsulate these fast moving, fast changing frameworks from your actual runtime. This is super important. And this is also important: don't treat the streamlining of the operation as a platform capability in that way. What I mean by this, if you remember the picture from the beginning on the right hand side, everything is standardized by decoupling the actual application with the low regret standards. Then we can apply the same operating model no matter which tool we are using to build or create these AI applications.

Don't forget what we call this dual pathway. This is also important. We provide two entries into the system, two ways to get into the system. There's this fully automated thing which means we have a pattern, we found a pattern that works well, so then we invest into making this thing become a standard. We build a pattern which is then later reusable for our teams, and it will become part of this fast heavy path on the top. But we also give the people the freedom to experiment, to play with new technology, as long as they comply with these lightweight low regret standards. Give us your prompt in the GitHub repository. We don't care which prompt management tool you were using to create these prompts in the beginning. Same holds true for the framework. Same holds true for the runtime.

Yeah, and by that, also a big thank you from my side. If you want to discuss or have some more questions, happy to meet us outside. Thank you very much.


----

; This article is entirely auto-generated using Amazon Bedrock.
