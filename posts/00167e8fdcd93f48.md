---
title: 'AWS re:Invent 2025 - Integrate any agent framework with Amazon Bedrock AgentCore (AIM396)'
published: true
description: 'In this video, AWS demonstrates integrating open source frameworks like LangGraph, CrewAI, and Strands with Amazon Bedrock AgentCore for production-grade agentic applications. The session covers AgentCore Runtime''s ability to deploy agents with minimal code changes, support for MCP and A2A protocols for standardized tool and agent-to-agent communication, and deep integrations with memory management. Aris introduces AgentOps methodology using OpenTelemetry and Langfuse for observability, experimentation, and CI/CD pipelines. Keith from Cohere Health shares their real-world implementation using AgentCore for healthcare prior authorization, achieving 30-40% faster clinical reviews while maintaining precision through specialized sub-agents, LiteLLM gateway integration, and comprehensive evaluation frameworks with Arize.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/0.jpg'
series: ''
canonical_url: null
id: 3093269
date: '2025-12-08T22:05:41Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Integrate any agent framework with Amazon Bedrock AgentCore (AIM396)**

> In this video, AWS demonstrates integrating open source frameworks like LangGraph, CrewAI, and Strands with Amazon Bedrock AgentCore for production-grade agentic applications. The session covers AgentCore Runtime's ability to deploy agents with minimal code changes, support for MCP and A2A protocols for standardized tool and agent-to-agent communication, and deep integrations with memory management. Aris introduces AgentOps methodology using OpenTelemetry and Langfuse for observability, experimentation, and CI/CD pipelines. Keith from Cohere Health shares their real-world implementation using AgentCore for healthcare prior authorization, achieving 30-40% faster clinical reviews while maintaining precision through specialized sub-agents, LiteLLM gateway integration, and comprehensive evaluation frameworks with Arize.

{% youtube https://www.youtube.com/watch?v=YmXszEVI-x8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/0.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=0)

### Introduction: Building Agents with Open Source Frameworks and Amazon Bedrock AgentCore

 Hey everyone. Welcome to AIM 396, where we're going to talk about how to integrate open source frameworks with Amazon Bedrock AgentCore. My name is Shreyas Subramanian. I'm a Principal Data Scientist at AWS. I also have Aris, who's a Senior GenAI Specialist SA at AWS, and we're happy to also share the stage with Keith, who's a VP of Engineering at Cohere Health.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/40.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=40)

So before we get started, quick show of hands, how many of you have built an agent with either a no code tool or low code or an open source framework? Quite a lot of us, yeah, and so you might recognize some of the frameworks up here and also the protocols.  So building a basic agent requires piecing together all these different complex components, right? So you have picking a framework, picking what tools to integrate with, you also have state management, but fortunately some of these open source frameworks came first and it really simplifies the way you can build these agents. We also have open protocols like MCP and A2A that help with connecting agents to agents or agents to tools. We have open protocols and standards like OAuth for authorization. We have OpenTelemetry for observability, and all of these help you start off really quickly and build a POC. But going from POC to production is a totally different ball game, right?

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/80.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=80)

 So at AWS since its inception we've been contributing to open source and we're proud to do that. We think open source is good for everyone. We've been contributing to Linux Foundation, the open source projects on GitHub. We're in the agents related talks, so we thought we'd contribute to Strands, which is our own agentic framework, and also with other open source frameworks. We believe open source is good for everyone. We're committed to bringing this via managed services to our customers and you'll hear from Keith at Cohere Health more about this as well.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/120.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=120)

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/140.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=140)

### AgentCore Platform Overview: An End-to-End Agent Technology Stack

 So today we'll be talking about specifically how AgentCore works with open source frameworks. If you haven't been in other AgentCore sessions, so AgentCore is our end to end agent tech platform that helps you build and operate agents at scale. And so zooming out, you know, a 30,000 foot view of AgentCore, if you look at all of these components  that you see on your screen, AgentCore is comprised of all of these individual services. You can pick and choose what you want. In the center of your screen is AgentCore Runtime. So with Runtime, you can deploy any open source agent as well as MCP servers and A2A servers with no changes in code or minimal changes in code.

These runtime agents run in a session isolated environment. It can be a back and forth agent where you have a conversational interface or you have a long-running agent that can run for hours and hours, like a deep research agent, right? Runtime also integrates with all of these other AgentCore services. So for example, Runtime integrates with Identity, which is our agent identity and credential management service. It also integrates with Memory, which is one of the first enterprise grade memory capabilities that you can have zero infrastructure set up, but you have short-term and long-term memory that can attach to your agents.

We also recently introduced Policy and Evals in AgentCore, so this helps you constrain and also judge how well your agents are doing in production. What connects all of these different services together is observability or AgentCore Observability, which works on the OpenTelemetry or OTel format. The nice thing with AgentCore, as I said, was you can pick and choose whatever services you want to build your agentic application. And today we'll see different flavors of observability, for example, so Keith will talk about Arize and how they use that in production. Aris will talk about how we can use Langfuse with AgentCore as well.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/250.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=250)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/260.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=260)

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/270.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=270)

### From Customer Service Scenario to Production: Choosing the Right Framework

So before we get into the details, let's start with a really basic scenario, right? So let's say you have  your customers, you want to track an order and you're usually contacting customer service and this  customer service representative might have to look into like order management system or external shipping provider API or maybe even call them to get order status, right? They might also have to work with other technical  support, you know, human resources, so you have two teams working together to actually get your final answer, which a lot of times is have you tried restarting it. So, you know, after all of that effort you might get back an answer in a couple of days.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/290.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=290)

We're going to try and replace these  two humans or human resources teams out there with agents, right? So you have a customer service agent and a technical support agent. Now these agents may not replace the entire functionality of the jobs to be done of these persons. It might just have to do a partial job or might be a co-pilot that's domain specific.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/310.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=310)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/330.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=330)

So  how do you get started? The first gut reaction we have is to look into open source frameworks because open source frameworks have deep and wide integrations and community support, and we have great examples that you can start off with, right? So our gut reaction is usually there, but how do you pick which framework you have to start with? There are a lot of different factors including how your team is  working with certain frameworks already, but we're listing three top frameworks in our minor list, so LangGraph, CrewAI, and Strands Agents, and we're just listing a few factors that you can choose your agentic frameworks with.

So LangGraph offers a graph-based execution with extensive open source integrations. So it's powerful, but then it has a really steep learning curve compared to some of the other frameworks. So it is still good for complex agent orchestration with strong community support. CrewAI, on the other hand is really easy to get started with and especially for role-based and team-based agents where you're running a multi-agent crew, this is the best place to start with. Strands, which is our own open source agentic framework, you provide, we provide like a model-based approach and has access to local as well as AgentCore memory, for example, and this is ideal for a starting point for enterprise grade agentic applications that are running on AWS, right?

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/400.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=400)

So you have all of these choices. You might have many more. We in fact have many applications and use cases that we have put in our GitHub repository they can check out. But the good thing is you can pick and choose  the frameworks for the right job, right? So in this case we're picking Strands for the customer service agent and LangGraph for the technical support agent. You don't have to choose two different frameworks. You can choose the same framework as well, but in this case, as an example, we're saying where you're choosing Strands for customer support and LangGraph for technical support.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/420.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/430.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=430)

 So what's the best way to get started with that, right? So you have, let's say, a Strands customer support agent that's running locally on your laptop. You have  local credentials in there. The credentials let you access APIs that are hosted in AWS, and these AWS APIs, you know, you can access because you have local credentials. The technical support agent, you're doing some hard integration between the customer support agent and technical support agent. And what you also see is that the customer support agent has an API key that may integrate with an external service provider. But you can quickly notice that this is not going to scale, right? So this is running on your laptop. It's not something that you can host and you can have multiple users interact with. But your goal is to, you know, put this out for, you know, tens of thousands of users to use. So how do you do that?

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/470.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=470)

### Deploying Agents with AgentCore Runtime: Simplified Hosting and Scaling

 That's where AgentCore Runtime comes in, and we spoke about AgentCore Runtime a little bit. This is where, you know, we mentioned LangGraph, Strands and other agentic frameworks. You can very easily host these agents, and you get a dedicated endpoint. These endpoints scale automatically based on your requests, and you can go from zero to production really quickly, right? You can also connect these agents to any model provider, so it doesn't have to be Bedrock. It can be OpenAI. It could be any model provider pretty much, and you can also work with really strict security guidelines, so it could be a VPC only mode or you can only use IAM.

We also introduced resource-based access policies recently. Today we also launched full support for the MCP spec. So if you have an MCP server, you can go and host the MCP server without any change in code. Also, bi-directional streaming was launched recently, which is useful for something like our customer service agent, right? So you're doing right now imagine a text-based interface, but imagine you want to convert that to a voice-based interface. You can very easily do that with Runtime because Runtime now supports bi-directional streaming. All right. So how do you actually, you know, host these two agents we said there's a customer support agent and this LangGraph agent.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/540.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=540)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/570.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=570)

 How do you host that on Runtime? There's two main methods to do that. You can either package up your code in a Docker container. That Docker container gets pushed to an ECR repository, and that ECR repository can be used as the starting point for deploying to Runtime. The other even easier way is you can zip up your deployments in a zip file, and you can push that to S3 bucket, and then that S3 bucket becomes a starting point for your deployments in Runtime. So it's two different methods, and to do this you have to actually  make a few lines of code change, right?

And this basically says, you know, make the right imports from the Bedrock AgentCore SDK, initialize this app which is actually a Starlette app, and then you have a decorator. It's easy decorator at app.entry_point just points to this handler or this entry point as the starting point for your agent execution. What happens inside that my_agent function that you see, it's completely up to you to be any agent, any framework that you're integrating with, any business logic, and then finally you return a response.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/620.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=620)

What you expect to work, whether it's sync or async responses or if it's streaming responses, all of those should work as though it was running locally. And this will work with runtime, right? So now that you've made those little changes in your code, the next step is actually to use our Bedrock AgentCore starter toolkit.  This is a really simple way to go from code that exists locally to code that can be deployed.

You have three steps. The first step is to do AgentCore configure. So AgentCore configure will basically set up your things like ECR repository that you want to land in, your role, whether you want IAM or OAuth, all of that just sits in a config file that's local. You can go back in and edit it on your own as well, but it's easy to manage it with AgentCore. The AgentCore CLI also has this next step which is deploy. So you can do AgentCore deploy, which will deploy to runtime depending on those two choices that if you recall from the previous slide, so you have either the container version or the zip deployment version.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/680.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=680)

And finally, within a minute it'll be deployed and you can start invoking your agent. All of the scaling and infrastructure management, all of that is happily managed behind the scenes, and you don't have to worry about it. Once you've launched that agent, of course, you can start invoking it. And in this case, we're saying, okay, we're going to use two different runtimes,  one for the Strands agent, one for the LangGraph agent. There are use cases where you want both to coexist, but in this case, we're saying you can have two different containers that are running up for you.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/720.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=720)

Everything else remains the same, meaning the AgentCore runtime has an execution role. It lets you access AWS APIs. It also, you can use Identity Store to store secret credentials, and that can be an API key, for example, for your external shipping provider if you want to track a shipment through that API, for example. But if you notice the integration between these containers and the API as well as from the one container to another container, something that you manage, right? 

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/730.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=730)

### Standardizing Agent Integrations with MCP and A2A Protocols

Before we dive in even deeper there, let's talk about context engineering for agents. This is,  you know, you might have heard about context engineering and prompt engineering earlier, but when applied to agents, if we zoom out, zoom into one of those agents, so either the Strands one or the LangGraph one, we can see that the context that you provide to that agent is extremely important, right? So you can have user instructions, you can have system instructions, you have retrieved information from RAG systems, for example. You might have short-term memory coming in with previous conversations, and you might have external or internal tools that are giving you additional context.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/780.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=780)

So imagine if your tool gives you a wrong context or a wrong response, or worse, if you call the wrong tool, the LLM calls the wrong tool on your behalf. That still remains in your context for the agent and your agent is going to respond based on that. So how can we make sure that all of this is handled, but we don't have to actually handle the integration points, right? So you as a developer are  basically responsible for connecting those different nodes, right?

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/810.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=810)

So you have the customer service agent which is Strands, you're writing integration code to connect to your order management system, writing separate integration code to do an API call. And then when you're connecting from one agent to another agent, you're also writing another integration code. So quickly, your integration code becomes the bulk load of your entire code base, right? You're not focusing on your business logic, but it's all of this integration code. But what if you could use some of these open protocols to  help you out there?

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/820.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/830.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=830)

So one of the things that you've already probably heard about is MCP. MCP allows you to securely access external data and tools through a common interface.  So there are two connection points to MCP within AgentCore. One, if you host your own MCP servers or you have the code for your MCP servers, you can  host that on runtime with no changes in code and literally no changes in code. If you have external APIs or Smithy models that connect to internal AWS tools, or even API Gateway setup that you want to connect and MCP file, you can use Gateway for that.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/860.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=860)

So AgentCore Gateway helps you create an MCP interface to all of your existing APIs. So there's two ways you can take AgentCore components and then connect it back to MCP. So that's from your agent to tools, right? So what about agent to agent? So as the name suggests, A2A is  standardizing the interface when you want agents to talk to each other, right? So it was originally developed by Google, but then it was donated to the Linux Foundation soon after.

Agents may be built with different frameworks, different languages, different models, and might be coming in from different providers as well. But you still want them to talk to each other natively. And so on the right we're just showing an example of three different agents talking to each other with A2A. Since A2A is a native protocol like MCP that's supported in runtime, once again you don't have to do any changes in code and you can directly host your A2A servers. You can have an A2A client similar to an MCP client connect to these A2A servers to test it out as well.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/910.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=910)

### Deep Integration with Open Source Frameworks: LangGraph, Strands, and CrewAI

So great. So now we're going to change that little architecture diagram a little bit. So you have a customer service agent  that's connected through the blue line, which is A2A to another LangGraph technical support agent. The red lines are now MCP, so that's a standard interface to go from the Strands agent to an order management API, which is, by the way, an internal AWS setup that you have. It also has another connection to an external shipping provider API.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/940.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=940)

So for example, if you have an OpenAPI spec for your external shipping provider API, you can import that into an MCP gateway.  What we also have is deeper open source integrations into some of these agent core components from the frameworks itself. So we're constantly pumping back some of this development back into the open source frameworks. One of the things that you can do is you can have the Strands customer service agent use the built-in session manager, and you can add agent core memory to it. So both short-term memory, which is STM on the slide there, and then long-term memory as well.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/990.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=990)

Similarly, in LangGraph, we have some deep integrations as well. So you can use what's called agent core memory saver, and what that does is you can save and load your checkpoints. Checkpoints store information like your user AI conversation or your graph execution state, and that can be loaded onto memory. Then once you restart your applications or your agentic container, you can rehydrate the container and restart where you  left off. So we're excited to continue our partnership with LangGraph and LangChain, and all of our examples are based on some of these open source frameworks.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1020.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1020)

For LangGraph specifically, we do this via the LangChain AWS library, so you can pip install LangChain AWS. This gives you all of the AWS LangGraph superpowers,  like the one we saw earlier for the memory checkpointing, for example. With Strands, of course this is our own homegrown agent SDK, but it also has 4,000 stars and a million downloads, and it's growing. Definitely check it out if you haven't already. It's easy to get started with and we have a bunch of examples. We are the primary maintainers of Strands, but it's a completely open source project.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1040.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1040)

Lastly, the way we  collaborate with some other frameworks like CrewAI is that we help make direct contributions to their repositories themselves. So Aris actually has a blog up there where you can scan and take a look at that. It's to build agentic systems with CrewAI and Amazon Bedrock. Separately, I helped LlamaIndex with their RAG implementations and also integrations into agent core. So we're happily continuing to do this over and over again.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1090.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1090)

So let's get back to our application at the end. So now you have agent core runtime that is used to run your customer service agent and your LangGraph technical support agent. You have A2A being the interface between those two agents, right? Then you have memory resources that are attached to each agent. There's also another alternative. You could have the  two agents have a common memory resource as well if that's the way an application is set up. But in this case, imagine you have two separate memory resources for the customer support agent and the technical support agent, and then finally you have a gateway that connects to the external tools as well.

### Transitioning from DevOps to AgentOps: Infrastructure and Operational Excellence

With that, we'll have Aris come up and talk about how engineering teams are transitioning from traditional DevOps to what we're calling AgentOps. Thanks, Shreyas. So now that we have learned how we can build powerful agents with AWS services like agent core and the open source frameworks, and about the powerful integrations between these two, we want to take it one step further and think about how we can put that into production with operational excellence, obviously, right?

So in the next 20 minutes, I want to talk about a couple of things that we have worked on in the last months together with some major players in the open source space here as well. And I want to start with operational excellence. If you think about operational excellence in traditional software engineering, probably everyone would say DevOps is really a thing here, right? And we would probably also agree that DevOps is not one dimensional. DevOps has a lot of different dimensions that we really want to get right if we want to be good in that field.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1190.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1190)

Starting from infrastructure, we need to manage our infrastructure from top to the bottom. We need to pick the right tools with a lot of automation. And then we are all people, we are an engineering community, and we work in processes embracing a specific culture. Now, in the first days of DevOps,  all of these things were really not easy because we didn't have, we had a cloud, but we didn't have a lot of managed services, right? We didn't have that set of comprehensive tooling that we have today,

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1220.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1220)

and we hadn't yet agreed on a new common culture. A lot of the things were still very waterfall-like, so engineers really had a hard time. But we at AWS have pioneered that in the last decade. A lot of organizations out there and also the open source space  have been inventing a lot of cool things over the past 10 to 15 years here, right?

So we at AWS have launched powerful services like Lambda, like EKS, like S3, like CloudWatch, and many more services that make all of our lives easier. We have entire companies that have risen out of the open source space. If you think about GitHub or Docker, and then we all as a community have embraced a new way of doing software engineering that is super agile with a lot of automation with CI/CD, which is actually benefiting us a lot, right?

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1260.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1260)

So what I want to talk about today is how can we take what's working well in traditional software engineering  and kind of evolve it into the agentic era that is starting now with all the new functional and non-functional requirements that are arising. And I want to do that sequentially, one pillar after the other, before bringing it together at the end.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1280.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1290.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1290)

Starting with infrastructure, as I  said, we have been pioneering that field for traditional software engineering with services like Lambda, S3, and many more. And we haven't stopped reinventing  on your behalf. Shreyas has mentioned a service that we have launched in July at New York Summit. AgentCore is really specialized for agentic workloads.

I'm not going to spend a lot of time once again. You have heard a lot from Shreyas. You have heard probably a lot about this service in the past days, but it is really absolutely useful in this specific scenario because it is a serverless service. It spins up agents quickly. There's a lot of things that the service is basically managing for you, so we strongly recommend using a service like AgentCore, and you will see how you benefit from it down the line as we go through the other pillars sequentially.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1330.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1330)

### Observability with OpenTelemetry and Langfuse: Tooling for Agent Operations

 Moving on to tools, tooling is actually a pretty vibrant space or has been a pretty vibrant space in the last one to three years. A lot of stuff that organizations have built, we have built a lot of tooling within Bedrock, but also in AgentCore if you think about observability, the primitive or evaluations that we have launched here at re:Invent. But also the open source space and the startup space was pretty vibrant, so I was working with a lot of those startups across or alongside the last one to two years.

The Y Combinator cohorts were full of those, and a lot of great tooling has been popping up. Since this is kind of an ecosystem open source session, we're going to focus on that space today. And as Shreyas has already said, I'm going to focus on Langfuse, which is one of the largest open source providers for observability for agent ops as well.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1400.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1400)

I've been collaborating strongly with those folks in the last months to develop the approach that we're going to be presenting, but again, whether proprietary  or open source, you can really pick and choose because everything is built upon open standards. And if you think about open standards, observability, operations, DevOps, agent ops, really one of the most important here is OpenTelemetry.

What is OpenTelemetry? It's not a new thing. It's been around for years in traditional software engineering, but what we have done as a community is really we have further developed it again to work with the new arising functional and non-functional requirements. It defines a set of semantic conventions, which you can see here on the right side. So we basically standardize how we are capturing the data points, the telemetry in objects.

And then we have also standardized the way how we actually collect them and ship them into so-called OpenTelemetry backends. So the way that works is we all use these open source frameworks. They are auto-instrumented. That means there is already code in there that is collecting those metrics automatically for you if you configure them right.

There are standardized exporters that are being built by organizations or by the open source which are exporting that data, streaming them to the backends, and these backends can be the platforms we have talked about. So our observability platform that is tightly integrated with CloudWatch or a third-party open source ones like Langfuse.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1490.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1490)

 Now, I want to spend a minute speaking about Langfuse to really make sure we are all on the same page. What is Langfuse? It's an open source LLM engineering platform that is one of those OpenTelemetry backends.

This means you can stream your traces and your telemetry there is being stored. On top of that, there's a rich set of features that you can leverage. You have core observability where you can actually look in a visual way at what's really happening in your agents. There are also a lot of other features as well, like prompt and dataset management, evaluations, experiments, human annotation queues, and many more things. We're going to use a couple of those further down the line.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1540.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1540)

So to come back to the solution architecture that  Shreyas has been building up sequentially in the session, you might recognize we have removed a couple of components that are not necessarily crucial for AgentOps. What we really want to do is use Langfuse, so we need to host it. It comes as open source under MIT license, so you can just host it in your AWS account if you want. There's a marketplace offering where you can choose the fully managed SaaS version of Langfuse called Langfuse Cloud. This is also the one that you will find being used in the code repository we're going to share at the end of this session.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1590.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1600.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1600)

Now you see two black lines here connecting AgentCore Runtime and Langfuse. I want to focus on the top one because this is really the last thing we need  to do before we are up and running. We need to actually configure how the streaming of the traces works.  It's not difficult, but it's just a couple of things we need to configure. We need to set a couple of settings, like for example which endpoint we're going to stream our telemetry to and things like how we're going to authenticate against Langfuse. A good way to do that is through setting environment variables when we are deploying the agents in AgentCore. You can see that on the right side.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1650.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1650)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1680.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1680)

On the left side, this is basically a boilerplate agent implementation in Strands for AgentCore. You will recognize a couple of things that Shreyas has shared before. All we really need to do is add two lines of code where you are initializing an exporter. This will pull the environment variables in,  and then the configuration is being set up and you will see traces flowing into Langfuse as you're using your agents. This is how it looks like. You have a rich set of information like the timestamps, the input and output of your agent on a high level. You can see how many observation levels the agent was going through, so how many turns it was doing until it solved the problem, token counts, latency, and a lot of information. Then you can also zoom into the different traces. 

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1690.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1690)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1700.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1700)

With the observation levels on the top left again, we have a graph execution view that can be useful for us humans, a visual way to see complex executions. On the right side you see  the data points we have collected in the trace based on the semantic convention we have defined. 

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1720.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1720)

### A Three-Stage AgentOps Process: From Experimentation to Production

All right, so now we've connected infrastructure and tooling. We know what's going on. The next question is obviously how can we build processes around that that are leveraging this, that are kind of making our life easier towards production. We were actually thinking a lot about what  would be a good approach to start together with the Langfuse folks. We started working backwards from DevOps, right? So we were thinking about what is working good in DevOps and how can we evolve it.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1760.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1760)

The first thing we actually realized was a crucial concept in DevOps is testing. DevOps doesn't work without testing. The analogy, even though it's a bit more complicated to testing in Gen AI and agents, is evaluation. So we were thinking about how does evaluation play a crucial role alongside the entire lifecycle of an agentic application. We found a pretty cool illustration from Evidently  that is actually showcasing that.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1770.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1770)

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1790.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1790)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1800.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1800)

We usually start kind of in a mode where we are trying to figure out a good implementation.  So we are changing the code, maybe different frameworks, different models, prompts, sets of tools. We want to figure out what works best, so we are running experiments based on some datasets that we have put together and we are running evaluations on top of that. It is an iterative process, and at some point we come to, or  at some point we basically have a state that is, as we think, good enough to deploy into production. We deploy to production, we collect the traces  as we have seen earlier on, and then we enter a kind of a phase where we are striving towards, first of all, operational excellence.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1810.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1820.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1820)

By  moving into a more online way of running evaluations, building dashboards, and creating automated alerts, we focus on operational excellence. We want our systems to be running well.  But we also want to close the loop. We want to learn from what's happening in production, which means we can feedback learnings from production into our test datasets. If we realize that the dataset distribution is not matching the distribution in production reality, we can also get inspired by what's happening in production to implement new things, whether this is fixing bugs, fixing issues, or just the next best feature.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1850.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1850)

 The next step was really thinking about how we can make this great theoretical framework a bit more tangible, a sequential process that we can implement later on. What we came up with was this three-stage approach. We start with an experimentation and hyperparameter optimization phase where we want to try out what's working well. Then we move further towards production where we have guardrails to production. Basically, we want to really make sure that everything is working, so we have a QA and testing phase, and we want to automate that with CI/CD because that's working really well in DevOps as well. Then we go into production, operational excellence, learning from what's working in production and closing the loop.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/1910.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=1910)

Now, moving from theory to practice and going one step deeper,  we have talked about the process and the culture that we want to evolve. Now I want to come back to the infrastructure and tooling pillar to speak about how we can actually implement that. I want to start with infrastructure, then go over to tooling, but before I do that, we have two core assumptions that we are building all of this upon. The first assumption is that we strongly recommend using a multi-environment setup with different staging environments: dev, test, and prod. It really makes sense because you don't want to experiment in your production environment and break the backend of your production application. Probably a lot of you have already implemented that, but if not, we are strongly recommending it.

The second thing is maybe a controversial one for developers, but we strongly recommend end-to-end remote cloud-based development, especially when we have serverless services at hand. I know a lot of people like to develop locally, to get started for a proof of concept locally with notebooks maybe, and this is great. But as you mature in your work, as you have a lot of teams doing that and a lot of use cases, you reach a point where it actually doesn't scale anymore. You have Model Context Protocol tools that are remote that you're not owning, which you might not be able to access. You really don't want your developers to spend weeks of development work just to realize that the stuff they have built is working on their local machine but not in the target environment. So with those assumptions, we're moving further and getting started with the implementation.

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2010.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2010)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2040.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2040)

 The first phase is experimentation and hyperparameter optimization, where we want to figure out what's working well alongside a set of hyperparameters or implementations. But we don't want to do that by rule of thumb, just trying a couple of things out and seeing what works. We want to have solid data points and a proven methodology. What we can do is actually use automation concepts like infrastructure as code, but also leverage the cloud and the fact that you can spin up agents in a matter of seconds with AgentCore.  This comes really handy because you can just go ahead and launch a fleet of agents, for example, one agent per hyperparameter combination. This is then a grid search in hyperparameter optimization. You spin them up, you run your evaluations, you tear them down again in a super frugal way, you get your data points, you have tested out all combinations, and you have a solid database basically that you can ground your decision on.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2070.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2070)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2090.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2090)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2100.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2100)

Then  we move on. Once we have found the ideal setup, we push this into the code versioning repository, GitHub, CodeCommit, whatever you want to use. This triggers an automated CI/CD pipeline that is here for QA and testing. It's going to deploy another ephemeral agent, just one agent now. We run our tests,  we get the results. If we are below the bar that we have set, we go back to the first phase. But let's assume we are good enough, then we can deploy into production. Now it's a  persistent agent that we want to use as backend for our production application.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2120.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2120)

And we close the loop by learning from what's happening in production. So this is the infrastructure side. Now how does that look like on the tooling side? And I want to start with  recognizing that all of these three phases are based on the concept that we have traces available, right? Without the traces, we don't know what's going on in the agents. We basically need them. This is the core assumption we are building the rest upon.

Now, the first two phases are actually pretty similar in a way. We are running experiments of different sizes based mainly on offline evaluations with datasets that we have configured. The main difference is we might want to use different evaluations because we want to test other things in those phases, and then the size of the datasets can vary. If you think about it, in the first step you have a fleet of agents. If you use a huge dataset, it gets expensive. Now in the second step, you have only one agent to test on, so you can maybe with the same budget do some more testing cycles, but this is really a trade-off we need to think through.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2180.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2180)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2200.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2200)

And then in the second phase,  we are basically transitioning a bit more into an online way of doing evaluations, even though I think offline evaluations are also possible. And we use, or we might want to use, a cool feature in Langfuse, the annotation feature where we can actually have humans annotate traces  and feed them back into our datasets.

So to bring it all together, we have talked about how to actually evolve DevOps into something we might want to call AgentOps, satisfying the requirements that have been popping up with powerful services like AgentCore that are integrating with third party observability and agent tooling platforms out there. And we are really flexible because they integrate well. And we've also spoken about a new culture with new processes that we are proposing here, and we hope you will all embrace in order to have a new era of successful software engineering now here in the agentic space.

### Cohere Health Case Study: Transforming Prior Authorization with Review Resolve

Now, before I close it up and hand over to Keith, I want to point out that we have a couple of resources for you. We have actually really built that. There is a code repository that you can fork and use as a starting point within your organization. There's going to be some things that will be different to your organizations, but I think it's a pretty good starting point. And then if you want to dive deeper than what I could do now in 20 minutes, you might want to check out the YouTube video here on the right side. It's a deep dive that I've recorded with the CEO of Langfuse a couple of weeks ago.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2300.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2300)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2310.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2310)

And with that I want to hand over to Keith who is VP Engineering at Cohere Health, and he's leveraging a lot of the things that we have spoken about today with his teams and in their day to day work. Thanks, thanks Aris.  So at Cohere recently had the opportunity to partner with the AWS team on the beta and launch for AgentCore.  And we built on top of it what we call Review Resolve, which is a product that we're using to help drive improved prior authorization processes.

So for those of you who aren't familiar with prior authorization, in the US healthcare system, it's common practice where prior to a clinician performing a procedure, they need to make a request to a health insurance company to get that procedure approved. And the determination about whether or not that procedure can be approved is dependent on a policy. That policy can be defined by the insurance company. You have state level policies, you have federal policies. In those policies, you've got medical necessity guidelines that have to be met prior to a procedure being approved.

So for example, you might have a knee injury, you're talking with your doctor, and she recommends that you get surgery for your knee. Prior to performing surgery, she would have to submit a prior authorization request to the healthcare company. We would take a look at the policy. The policy might indicate, for example, as one part of the medical necessity criteria, prior to knee surgery, more conservative therapy has to have been attempted like physical therapy, for example, over two months before you can get approval for knee surgery. So that example is a very straightforward one.

The reality is that the prior authorization process is a very complex one in healthcare in America. It has an estimated cost of about $35 billion annually. It creates an enormous amount of administrative overhead for clinicians, which reduces their time with patients. It also affects the timeliness with which patients get the appropriate treatment that they need.

So at Cohere Health, we're really focused on helping clinicians and patients get to yes faster. And we do a lot of auto decisioning, so based on the information that we receive in a prior authorization request, the clinical information that we have about a patient, and the policy that we're examining, in some cases we can get up to 85% auto approval rates. However, in that 15%, it needs to go to a clinician for review. We don't use machine learning or AI to ever deny a claim or deny a prior authorization request. But what we do use AI and agents for is to help surface clinical information in that review process for the clinician to help them figure out whether or not medical necessity guidelines are being met and that procedure can be approved or not. This results in an uptick of 30 to 40% faster reviews for clinicians, doctors and nurses who are interacting with our review tools to make a determination on prior authorization cases.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2520.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2520)

So I'm going to talk a little bit just about the problem  space here and some of the issues that we're solving with agents. I think, typically if you're working with agents or language models, you're doing some sort of optimization between latency, cost, and some accuracy measure. In this case for us, latency and cost aren't the primary factors. Precision and accuracy are paramount. We're making decisions that, our clinicians are making decisions that determine whether or not a patient gets the appropriate care. We're looking through potentially huge amounts of data for very precise information.

The language here is complex and specialized. Frontier models, state of the art models, do a reasonably good job of picking up on some of this. But there are some complex problems here. Lower back pain as an example might be expressed as lower back pain, low back pain, pain in the lower back, LBP. And everyone who's worked with language models or NLP is thinking, well, that's a problem that's fairly easily solved to semantically group those. The problem though is that in order for you to do that in a way that's effective, it needs to use the same ontology and taxonomy that the policy is using, right? So term normalization, concept resolution, match up to the language in the policy, or you're taking the policy and your clinical information and a third ontology and taxonomy and mapping everything to that. So this is a very complex space where just out of the box language models aren't able to handle a number of these problems.

Agent management is also complex, so we're dealing with both structured and unstructured data. So structured data might be claims history, for example. Unstructured data could be clinical records. There can be an enormous amount of either of those two. And I think that if you've done a lot of work with language models at this point, I think intuitively based on precision and accuracy, you can sort of see that something like traditional vector RAG search, similarity search isn't going to be particularly useful here. There are some times when it is useful, but you're not looking for something similar, you're looking for something that is a very precise match.

So we need to do a lot of in context learning with our models, so passing the structured data in. We also need different representations of the data. So combining the structured and unstructured data into, say, a graph representation, and giving the agent options for how it might want to fetch that data to reason over is critical.

There's also an important temporal component here. I mentioned previously for the knee surgery conservative therapy, you need to look at potentially clinical data or claims to see if, over the last two months, a patient has undergone physical therapy. You might be looking at blood values over time, you might be looking at very specific dosing information over a time window. That kind of temporal data retrieval is something that language models typically don't handle very well.

There's also a context window management problem here. You can easily overflow a context window with claims and clinical data. So how do you narrow down what it is you're looking for? What kind of heuristics can you use to determine what you pass to the model in context? The scope of your evaluations here expands rapidly. You're dealing with different data representations, you're dealing with a lot of data, you need a lot of ground truth data to run your evals against. The complexity here grows rapidly.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/2790.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=2790)

### Production Architecture and Evaluation Strategy at Cohere Health

 I did want to talk, obviously this is an AgentCore session, about why we're using AgentCore. As Shreyas and Aris have talked about, bring your own tooling is a key principle here. We were already in the LangGraph and LangChain ecosystem, so we were able to fairly quickly move out of ECS into AgentCore. We're using Arize for telemetry and observability. Aris was just talking about Langfuse, which is great. Arize has an open source product called Phoenix, which is also great. We do use Arize's Enterprise edition, Arize DX, but their open source Phoenix is a great point of comparison against Langfuse. We also use LiteLLM for an LLM gateway. I'm not sure how many people here are using LLM gateways, and I'll talk about it a little bit on the next slide.

Something that really appeals to us as a startup is the minimal, which is no infrastructure management here. Anytime my team's not managing hardware or infrastructure, they're focused on product development. We're in a regulated industry, the micro VM architecture, fast spin up spin down, no red memory is important for us. The CLI fits really neatly into our existing CI/CD pipelines, and OpenTelemetry works really well to hydrate Arize. All of this works pretty seamlessly out of the box with minimal code changes.

Two things that are really important for us are AgentCore gateway and governance and security, and I see them as very tied together. Let's say that we have a specialized subagent that is a claims related subagent, so it's able to pull claims for a patient and search for data in claim history. We might have an API that we have internally that has different endpoints to interact with claim information. What's nice is we can put AgentCore gateway in front of it, but what's critical is that from not just a security by design perspective, but compliance by design, we need to know who's interacting with the main agent in a review session.

If there's a call to a subagent to look at claims data, what's the identity of that user who made that call to the subagent? What are they authorized to see, and can we ensure that they only retrieve information for a patient that they have access to? This is critical for compliance and audit. Shreyas also mentioned short and long term memory. Short term memory supports text and binary data. He mentioned LangGraph checkpointing which needs to use binary data. It's very straightforward plug-in from the AWS team to drop into LangGraph to get short term memory storage.

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/3000.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=3000)

Long term memory is more complicated, I think, regardless of the framework you're using. One thing that we found useful is namespaces for managing a multi-tenant environment. You need to spend some time thinking about your memory architecture here. This is, I'd say, significantly more complex than short term memory, especially if you have multiple agents.  So I'm going to just talk about our architecture a bit here and it is, yeah, I wish I had a pointer here.

It looks busy, but the complexity is actually reduced. I'm only showing one agent which is in the Cohere Health platform, and just in the middle it shows the chatbot supervisor. That's running in LangGraph, and we're connecting to AgentCore short-term memory there. Like I said, very seamless integration. The chatbot itself, or this particular agent, is running on AgentCore infrastructure on the micro VM. All of our traffic is running through LiteLLM to Bedrock. So LiteLLM is an AI gateway. If you haven't worked with one of these, it functions as sort of a reverse proxy for talking to different model providers. It gives you really nice features like rate limiting, insight onto token usage by user, cost controls, and you can put common guardrails in place.

We're mostly working with Bedrock, but it's seamless for us to pull in Gemini, anything from OpenAI behind LiteLLM. If a client in front of LiteLLM wants to use those, you're really just changing an API key and the name of a model. All the other work is being handled via the gateway. You can see on the left we're using Arize prompt management. I'll talk about that in a minute. For us, we tend to separate our prompts from our application code so that we have a lifecycle and a deployment lifecycle for prompts that's separate from code. You can make updates to prompts without having to redeploy containers.

At the bottom you can see indication level LLM prompt in the lower left there. So thinking about that claims agent for a moment, you might have this supervisor agent, and that's what the doctor or the nurse is interacting with. You might make a call to a specialized sub-agent for claims. Something that I feel like we talk about a lot when we're talking about Model Context Protocol is tools. I think how we use prompts in Model Context Protocol is really interesting. I don't see as much about it, maybe I'm reading the wrong authors. But in this case, you could pass the medical necessity criteria to that claims agent. It could decide what claim history it wants to grab and also select what is the medical necessity criteria, or the indication level is the terminology we'd use. What prompt do I want to grab in order to look for this information in the claim history?

This is, so there are multiple ways we handle clinical data extraction, including with fine-tuned models. But this is particularly interesting, and it also adds to the complexity of your evaluations. It's a powerful methodology, but as your clinical areas grow, as the number of prompts you have grow, being able to evaluate whether or not you retrieved the right prompt and applied it in the right way for that sub-agent against that data to get the information you're looking for becomes another set of evaluations you have to run. The announcement of session level evaluations is really exciting. Being able to run evaluations across the whole set of agentic interactions is really critical.

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/3240.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=3240)

 And then I'm just going to touch on evaluations here briefly. My slide here is much busier than Aris's. So like I mentioned earlier, we're already in the Arize ecosystem. We have online evaluations, so from LangGraph and AgentCore, we have data going into Arize. That can get piped into our online evaluation. We could have an LLM as a judge there looking at things like hallucination rate. We could be looking at guardrail violations. There are a number of things that you might want to be looking at for online evaluations, especially if you wanted to determine if performance is changing in any way.

We also hook into our offline evaluations here. So we have clinicians on staff, doctors, nurses, subject matter experts, so we're able to have human annotators in the loop here. That can all be fed into our offline evaluations process where we're determining the effectiveness of a particular prompt, of a fine-tuned small language model, of the agentic system or the set of agentic calls as a whole. And just like with Langfuse, we already had something like this. We're able to keep this mostly working as is and plug it into AgentCore and get all this data hydrated the way that we needed it to.

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/00167e8fdcd93f48/3370.jpg)](https://www.youtube.com/watch?v=YmXszEVI-x8&t=3370)

That's it for me. I'm going to hand it back to Shreyas. Thank you Keith, and you know, in summary what we saw is we saw AgentCore that can integrate with multiple open source frameworks as well as protocols. Aris walked us through AgentOps as well as a deeper integration with Langfuse, and we saw a really exciting real world application with Cohere Health as well. I'm going to leave this slide up there for you to scan.  I know folks are trying to hurry up and scan quickly when we change. So thank you so much for attending our session and I hope you found this informative. We're going to hang out here for a little bit to take any questions later on. Thank you so much.


----

; This article is entirely auto-generated using Amazon Bedrock.
