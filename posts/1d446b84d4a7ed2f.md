---
title: 'AWS re:Invent 2025 - AWS ProServe''s Game-Changing Partnership with Sports Customers (SPF205)'
published: true
description: 'In this video, AWS Professional Services presenters Abhisek Rath and James Kellar discuss how the sports industry can capture $130 billion in untapped revenue through cloud and AI solutions. They explore four key trends: AI-powered biomechanics analytics, hyper-personalized fan experiences, revenue diversification through D2C and betting integration, and sustainability. The session covers ProServe''s capabilities in cloud migration, real-time analytics, live content delivery, and data lake solutions. Two detailed use cases are presented: WBD''s live sports metadata synchronization using audio analysis (achieving 96% accuracy by detecting referee whistles and crowd noise), and migrating complex on-premises data processing algorithms to cloud using ECS, Fargate, and API Gateway. The presenters emphasize their collaborative approach, working alongside customer teams with 50-50 engineer splits to ensure successful implementation and knowledge transfer within 12-24 weeks.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - AWS ProServe's Game-Changing Partnership with Sports Customers (SPF205)**

> In this video, AWS Professional Services presenters Abhisek Rath and James Kellar discuss how the sports industry can capture $130 billion in untapped revenue through cloud and AI solutions. They explore four key trends: AI-powered biomechanics analytics, hyper-personalized fan experiences, revenue diversification through D2C and betting integration, and sustainability. The session covers ProServe's capabilities in cloud migration, real-time analytics, live content delivery, and data lake solutions. Two detailed use cases are presented: WBD's live sports metadata synchronization using audio analysis (achieving 96% accuracy by detecting referee whistles and crowd noise), and migrating complex on-premises data processing algorithms to cloud using ECS, Fargate, and API Gateway. The presenters emphasize their collaborative approach, working alongside customer teams with 50-50 engineer splits to ensure successful implementation and knowledge transfer within 12-24 weeks.

{% youtube https://www.youtube.com/watch?v=cDw5-rWp3UU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/0.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=0)

### Introduction: The $130 Billion Opportunity in Sports and AWS Professional Services

 All right, thanks everyone again. Let's begin. I wanted to start with where we are today from a sports industry standpoint. A recent research from Morgan Stanley in August 2025 says that the sports industry is leaving about $130 billion of revenue on the table. That's roughly 25% of the overall revenue calculations they have. I want you to focus the next 40 minutes on how much of that upside you can get for your organization, your partner, and your customer.

Hi everyone, my name is Abhisek Rath. I am part of AWS Professional Services. Professional Services is part of AWS, which helps customers accelerate their journey onto the cloud. Whether you're innovating or migrating, we work with our teams as well as with partners to help you in that journey. I'm a ProServe account executive, so my job profile allows me to work with C-Suite executives and many customers in the media entertainment industry, sports, and games industry, and provide them innovative solutions in their journey to the cloud.

With me, I have James Kellar, and let him introduce himself now. Hi, thanks Abhisek. So yes, James Kellar, you can probably tell from my accent that I come from the UK, so I'm just a visitor here for this week. It's lovely to see so many smiley faces there. I'm quite glad that they've moved the beanbags because we were worried you were going to make yourselves just a little bit too comfortable. I've been in the media industry for almost 30 years now, and I reckon quite soon it's going to make my hair go gray and fall out, but it hasn't happened yet, so I feel like I'm doing okay there.

I've worked a fair bit on different supply side companies. I spent some time at Cisco. Before that, I was at NDS and Quantel, which perhaps some of you have heard of. I've been with AWS for just over four and a half years now, and honestly, I'm really enjoying it because we get to build some great stuff, and we're going to try and give you a feel for some of that later on through this afternoon.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/140.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=140)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/150.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=150)

 All right, I actually forgot to talk about where I worked before. I've been working at AWS for the last four years. Prior to AWS, I spent about 17 years working for Sony. Some of you are here today. I worked with the studio for a long time, being in a technology leadership position there.  Between us, we have spent quite a lot of time over the past 25 years working with media entertainment, games, and sports customers, so we have a lot of understanding of the partners, the players, the industry, and so on. We'll try to present that in front of you.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/190.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=190)

What we have here today is a very business and functionality-based presentation, not super technical. So we hope that you understand why we are doing certain things, how we approach to do those things, and for technical deep dives, we can have separate follow-ups.  Today, we're going to talk about a few things. First, we're going to look at what some of the industry trends are as far as sports is concerned. These are the latest trends and how we look into the future. Next, we're going to talk about some of the focus areas based on the trends and the challenges that the sports industry is facing today.

Right after that, we will jump into what our capabilities are in that space. What does ProServe do? When I talk about ProServe, it's AWS Professional Services along with any partners that we choose and work with, which is a wide variety of them, hundreds of thousands of them, to realize the benefits that the customers are trying to get to. Finally, we have two use cases we're going to talk about very specifically in terms of how we've approached innovation, how we've approached migration and modernization in the sports industry.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/250.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=250)

### Industry Trends: AI-Powered Analytics, Fan Experiences, Revenue Diversification, and Sustainability

And that's the fun bit because that's when I'll be talking. All right, fine. Come on, man. Let's go.  So challenges, right? There are a lot of challenges that we see or business trends that we see across the sports industry. We chose these four because they resonate a lot with many of our customers we work with. First and foremost is AI-powered analytics and performance. It's essentially looking at real-time biomechanics and data-driven insights to sports agencies, teams, and coaches, and so on and so forth. There's a term here, biomechanics. Since we have a smarter person in the room, I'll let him explain what that really means and what we do with it.

Yeah, so I think you probably know most of it already, but basically when we're talking about biomechanics, we mean, let's imagine you've got a big stadium with dozens of cameras around the edge of it. You're capturing multiple speeds of camera, multiple angles, and you can fly the spider over the top.

Using this approach with multiple angles, we can build up quite a complex real-time view of what the players are actually doing. Wouldn't it be good if you could capture the position of their elbows, wrists, and shoulders? If you can work out where the player's knees are and where their arms are holding a hockey stick or something similar, you can build up a huge amount of very useful information.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/350.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=350)

You can work out where all the players are in the field and determine who's about to hit the ball, depending on what the sport is, whether it's football or soccer.  You can work out where all the players are in the field and determine who's about to hit the ball, depending on what the sport is, whether it's football or soccer.

The next thing is about fan experiences. We have solutions and are working with many customers. This is another important field that has exploded in the last few years. I'm sure there are a lot of sports fans in the room right now. As business providers to sports, we understand just how important those fans are. The people who would like to analyze every single move of the game from all of the different angles and would like to be able to consume the sport in a way that suits them.

Maybe they would like to always track their favorite player. Maybe they would like to be able to generate automatically a highlights package that they can review again after the game. They can go through it with their friends showing all of the plays, the players, and the goals which are important to them. Getting all of the data and the masses of information coming in real time during the game, wouldn't it be good if you could stream that into the player experience? They can get pop-ups and informatics during the game, showing them exactly what's going on and making predictions about where the next play is going to go, but tailored to their requirements. That's what we mean by hyper-personalization, and it's a very growing part of the industry.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/450.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=450)

 Let's move on to revenue diversification. We understand from our customers that rights fees and tickets are not sufficient for sustaining long-term revenue. Many of our customers are looking into creating digital subscriptions, D2C campaigns, and integrated betting experiences. That's a big thing right now. What that really means is you need the ability to test, iterate, and release to market fairly quickly. That's where cloud and AI come into play.

Finally, it would be a miss not to talk about sustainability in today's world. Whether it's your brand, your fan base, or venue economics, sustainability has become very important. AWS takes that very seriously. As we look from the stadium all the way through mobile devices or any devices where customers are consuming content, we are looking for ways to make sure every step of that is sustainable in a way that makes sense for your business as well as for the fans.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/530.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=530)

### Challenges: Data Explosion, Zero Downtime Requirements, and Synchronization Complexity

 Just to add to the point about operational efficiency, if you spin up all of the cloud resources for the entire duration of a soccer season, that can be quite expensive. If you have all of those big instances sitting there not being used except when the game is going on, that's clearly not desirable. One of the things we're asked for a lot is whether we can arrange some kind of automated orchestration which will ingest the station schedule and spin up those resources in sufficient time to make sure that the video feeds from the stadium are working properly. Wouldn't it be good if all of that could be automated so that you're only running your codecs, only running your packager, and only running the ingest into storage when it's actually needed?

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/580.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=580)

 Now let's move on to some of the challenges. Based on the trends, some of the challenges we see is there is an explosion of data. You're tracking data from broadcast, tracking information, tracking probably medical information from athletes, there is betting information, and there are app events occurring. There is an explosion in terms of the amount of data that needs to be managed in a real-time fashion.

At the same time, we also have to ensure zero tolerance in terms of uptime. All of those factors together make a serious case for examining what kind of architecture and solution you need to build to support all of that.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/630.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=630)

Yeah, so I think it's fairly obvious,  but if you imagine that literally every frame, maybe from some quite high-speed cameras, captures huge amounts of information. If you have 2D and 3D data being processed and generated in real time, that generates a tremendous amount of numbers flowing through the system. There is the video, there is the audio, you might have cameras running at double time or even more, and there are all sorts of sources of information. You have to make sure that all of that is properly synchronized. We will talk later about some things you can do to synchronize, maybe using audio to synchronize different video feeds. But if all of those are being captured at different speeds from slightly different parts of the stadium and so on, maybe some of them have had graphics overlaid on top of them as well, it is a real challenge to bring all of that together and synchronize it in a consistent, reliable way.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/710.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=710)

### Four Focus Areas: Fan Experience, Performance Analytics, Data Monetization, and Next Best Action

Yeah, so the real question is how do you take advantage of some of these opportunities you have without taking too much risk and without maybe having resources at your disposal to do this faster. That is where our focus areas come in, and we have four of them right here. The first one is fan experience. We talked about that a little bit, which is really providing low-latency live experience deliveries.  It is basically tailored for every single fan in the way they want to consume that information.

Yes, I mean, we have talked a bit about how sensitive fans are to the data that they really care about. It is so critical that you get the timing of all of those things and the reliability exactly right. Think about what it would feel like if you are a fan watching a game, and then the players are about to score perhaps, and then before the ball has actually left the player's foot you get a graphics pop-up saying the goal, the score has changed. That is a disaster in terms of the experience. We have to work very closely with our customers to make sure that kind of thing is organized and works as well as possible.

Cool, let us jump into performance analytics next. This is a topic that has been discussed with quite a few of our customers, which is essentially around athlete safety, their performance, their health, and also about the quality of adjudication we have, which is the officiation or officiating as they call it. Why do not you go a little bit into the details and talk about what that means.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/830.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=830)

So look, there is a goldmine of information there. We talk quite a lot about the biomechanics, but think about the things that you can use that for. If you have players who are being scrutinized very heavily by their own coaches, by their own fitness people on the team, there is a lot of information that you can get by watching very slow motion replays, just watching how they move. Maybe not quite as athletically as this, but you get the idea. There are also a number of other quite important use cases that can be taken from that data as well. I am sure you have all enjoyed watching things like VAR on the soccer matches, but anything that we can contribute to making that a better experience for the viewer  and also perhaps for the betting public as well, I think is very important.

One use case that we have not called out here specifically, but I do want to mention also, is anything to do with match rigging. Now I realize that is rather a controversial thing to bring up, but that we are starting to get interest in is whether there is anything that can be done using AI perhaps on the stats of the games, but also just looking at the behavior of the individual players just to see if there is anything that ought to be flagged there. I think that is worth being aware of, and I think it just gives you some idea of how rich the things are which we are being asked to look at now.

Cool. Now let us move on to data analytics. Irrespective of the industry, across every media entertainment company today, building a customer 360, whether you call it Fan 360 or Customer 360, is an initiative that everybody is embarking on. What that really means is there is a movement within all these organizations that we work with from a sports and media entertainment standpoint to build what is called data products. What that does is essentially enable you to monetize the data that you have.

This creates new opportunities for you to do business with other partners in terms of making sure you have sponsorship evaluations, making sure you can do targeted marketing, and so on. Anything you want to add there, James?

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/930.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=930)

Yeah, just to give a little bit more color to that, what do we mean by monetization? The kind of thing that we're being asked to do is to provide some kind of normalization  of all of this vast amount of data which is available, and also then to expose that potentially to partners or customers of our customer through standardized APIs and so on, where they can do unified discovery and then search and then actually access that data. If you can provide some kind of consistency in how that is presented and how that works so you as the consumer of that data don't need to know where that's come from, that is a very attractive proposition for our customers. That's something that we're working actively with quite a few of them to start building.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/990.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=990)

The final point here that we want to make is really around the next best action, right? Whether it's calculating win probabilities, whether it's calculating what should be the next best move that a particular player can take or a team can take in any kind of sports, there is a lot of traction there.  There's a lot of conversation we are having with our customers and building solutions in that space which is essentially around innovation acceleration. Now, why are we talking about all these areas and what's this got to do with professional services? Our role in this entire journey for the customer is really to de-risk it. That's why we're here.

### AWS ProServe Capabilities: Migration, Real-Time Analytics, Content Delivery, and Data Lake Solutions

We help you in picking up the right place to start. There are a lot of things that you can do here, and all of these four focus areas may have many projects that you can take on to create revenue efficiency, cost efficiency, or anything else you're looking into building, even new products. We know that it's tough to choose from, so we also help you in the process by prioritizing that by working with you on a business case. That's one thing we focus on. Second, we really focus on understanding what the architectural patterns are and what is the right way to do certain things because technology is changing very fast and we want to make sure we set you up for success in the long term.

Finally, we also work with you side by side along with partners, so you are never left alone. We work with the customer alongside their teams to make sure that we're able to build the right kind of solutions for you. So next, we will shift gears a little bit into more practical examples at our delivery models, how we work specifically in which areas, and then we'll jump into the two use cases that I will have the smarter person here cover.

In terms of the two use cases we're talking about, let's jump in a little bit into what our capabilities are. I could have listed many things here, but we chose to only list five key things that we focus on. The first and foremost is the bread and butter for what AWS has been doing for the last many years, which is essentially helping you to accelerate your cloud migration journey and give you modernization patterns and so on, so it's easy for you to go through that process.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1120.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1120)

The second thing, as we observe through challenges, trends and focus areas, we are heavily entrenched in building solutions which are around fan engagement and we're talking about real-time analytics versus anything which is static in nature.  The third core focus area for us in terms of AWS ProServe is also live sports content delivery. Whether it means that we are packaging it, we are delivering it or we are ingesting multiple data sources end-to-end supply chain, we're able to work with the customer to build those solutions.

Last but one is performance analytics. James kind of touched upon that a little bit. We definitely focus on that space as well and you'll get some examples of that today. Finally, one thing that's a common theme across any media, entertainment, games and sports company is really helping them in building data lake solutions which enable you to unify your data sources which is a core concept now to make sure you have AI solutions built on top of it not only to provide you insights, provide you recommendations, provide you next best action, but also give you real-time information to be able to make decisions.

And they let you do that monetization which we talked about just now. Absolutely right. Anything else you want to cover here? No, I think if you forget anything else then I'll let you know, but otherwise good. Very kind of you, let's move on. All right, so now we go into a little bit of how we really work.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1230.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1230)

### Delivery Model: Discovery, Foundation, POC, and Continuous Enablement

Some of you may have worked with us, and some of you may not have worked with us. We have a classic model by which we do our deliveries, and when I talk about deliveries here, we're talking about solutions that we are building for our customers in those areas we discussed. We do start with a discovery process because we believe in working backwards from our customers, which basically means understanding what the KPIs are,  what you are trying to achieve, whether it's athletes' performance, whether it's monetization opportunities, whether it's building a Gen-AI-based agentic chatbot or a solution. We want to understand what that is, and that's the discovery process that we do.

We will try very hard to understand where you're trying to get to. We want to know why you're doing this and what you want the outcome to look like so that we can work backwards from it rather than just say we're going to build such and such. It's much more important for us, and we think for customer success, to focus on where you're trying to get to and why. If we can understand the why, then we'll be very much more helpful to you.

Once we have that in place, we really go into what's called foundation and POC. We use the POC term very seriously because POCs should not be science experiments. POCs should be ones where you're actually able to take it and move it to production. Hence, you see a combination of foundation and POC together because we want to make sure the foundation is built out. When I'm talking about foundation here, in technical terms, it could be building your pipelines that are necessary to move code from lower environments to higher environments. It could be building your data strategy and building your overall architecture. Even if we're doing smaller POCs to prove our points, we want to make sure everything is built and everything is automated for you in terms of infrastructure as code so that you can take that POC and move it forward.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1330.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1330)

For me, the most important thing of a POC is defining what the success criteria are. It's more important than executing it.  If you just do some stuff and it's a science experiment, as Abhisek says, that's great fun and we've probably ended up with a better understanding of what the technology space is, but it doesn't move the business forward necessarily. So if we can get a clear statement understood and agreed by everyone about what success or failure looks likeâ€”and failure's fine in a POCâ€”then it's going to be a much more effective experience from a business point of view.

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1360.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1360)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1380.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1380)

The final two phases that you see is really building the end-to-end implementation and also providing you with abilities to do feature enhancement on top of that.  If you see this rough timeline, the reason I put some timeline here is it's directional and depends on your use cases, but we move really fast. That's the way we want to work, which is within 12 to 24 weeks,  you will have a full capability built out that you're looking for. Of course, if the complexity increases, then the timelines change, but we believe in moving faster through the process.

When we are doing this, we are actually building with you, not for you. What that means is as we start to design our architecture, we look at what kind of resourcing we need, not just from a build perspective, but also to make sure to run and operate efficiently as ProServe pulls out of it, because we don't provide managed services ourselves. We want you to be able to feel comfortable operating this and running it in production. So as we're doing this process, we're also looking at cost, security, reliability, and all the good aspects that come with AWS as a platform, and making sure you're set up for success as you move through your production journey.

It's really important to stress that this is not a one-size-fits-all approach. It's absolutely tailored to what your needs areâ€”what resources you have available, what your level of experience is, what your sweet spots are, and what you're most concerned to get assistance with. We will build a custom resource profile and custom set of milestones to match all of those things so that we are doing what you need rather than just giving you a complex pre-packaged solution.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1480.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1480)

Now I'll jump into the bottom section, which is really important. The big block that you see here in black represents our belief in continuous enablement.  Every customer is on a different path of maturity in terms of cloud. We have some customers who are really mature and have already been using cloud for many years, and some customers are just getting onboarded to cloud. So we believe as we go through this journey of implementations, we want to make sure there is enablement. What we mean by that is we have a lot of trainings that are available online as well as instructor-led, depending on what kind of training you want to take on.

In addition to that, as you go through the process in the journey of building out the implementation, we will have our teams providing very targeted enablement sessions of what we are building, what services we're using, and how you're going to get to use them, as well as know how to maintain them going forward.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1550.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1550)

And we can make that quite reactive on the way through the engagement. So typically what will happen is it'll become obvious after a month or two that there are some services which are very complex or critical and really need to be understood in depth and fully embraced by the collaborative team that has both AWS people and customer people in it.  And we can react to that and do very deep dives. We'll bring in specialists as needed, or it could be that the consultants that we've got engaged can already do that. But that's something that we generally find gives a really good experience.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1580.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1580)

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1590.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1590)

### Accelerators and Partner Collaboration: Leveraging Generative AI and Agentic AI

So let's move on to the last bit of this presentation. It'll be a miss not to talk about what are some of the capabilities  that is coming up with generative AI and agentic AI that can be used to do the implementations faster.  So we believe in building accelerators, and what we mean by accelerators are, let's say you want to set up a data lake architecture within your organization for the FAN 360 we talked about. There are already ready to deployable code that's available which is approved by our security team members and taking that and customizing it for your use cases is something we provide as a benefit.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1620.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1620)

What is the advantage of that? Your time to market in terms of how much time you take to build some of these things from scratch  reduces significantly. Not only do we do that, we also work with our partners in these engagement opportunities and bring their accelerators in our journey because we don't believe we have to build everything ourselves. If a partner who is very purpose-driven in terms of working on the agentic space and we partner with them in one of these engagements, we also bring in their accelerators, we also get them go through our security process to make sure we are comfortable with our customers deploying them into their environments.

So you get that benefit of both agentic AI capabilities to accelerate your process to production. You also have expertise of the partners that work with ProServe to make sure we do end-to-end implementations for you. With that, all the boring bits are over. So I'm going to transfer over to this gentleman for the two use cases we really want to talk about today. So you let me know when you want to keep switching, but let's go to the first one.

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1700.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1700)

### Use Case One: WBD Live Sports Metadata Synchronization Using Audio Analysis

Right, so this one is based on some work that we did with WBD over the last probably nearly two years altogether. The basic scenario is that you have a bunch of video feeds coming in live, you have a bunch of metadata  describing those feeds also coming in. And what we were asked to look at was a number of technical challenges to do with the timing quality and the reliability of the data in those feeds. So what you have to do is think about where does that data come from?

Let's imagine it's a soccer game, because it was, and that therefore makes it a good example. So typically what happened in this case is that you have a human sitting, let's imagine, just picture in your mind, a noisy English pub. There's a big screen up on the wall of the pub, and there's a guy sitting there, or a gal, with a pint of beer, a laptop, and a delicious plate full of chips, maybe with some mayonnaise that they're dipping them in. And you understand obviously that when I say chips, I mean I'm using the British term. Not crisps, obviously, that would be silly.

Yeah, so this is a noisy, slightly fragile, slightly easily distracted environment. So therefore, what does that mean in terms of the fan experience? It means that some of these timing markers can be a bit late, or maybe a bit early. So if you think about a really complicated soccer play, let's imagine it's a penalty. Think about what actually happens. First of all, the players are running around, and one of them commits an egregious foul on one of the other players. They fall down, and so what happens? The referee is likely to blow the whistle. There's likely to be some kind of indication at that point. Oh yes, it's been a penalty awarded. And then the kicker, if that's the right word, maybe the striker, from the victim team will go and take the ball, stick it on the spot, have a look around, and then perform the kick.

Perhaps it will go into the goal, or perhaps it will bounce off the goalie's head. The point is that there are quite a lot of things happening, and it is very easy for the human logging all of this to make errors. You will get the main things in there, but if you simply take those events as they arrive and translate them into live graphics, imagine the hyperfan watching on their iPad at home as well as on the big screen. The last thing they want is lots of variability in those events being revealed to them.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1870.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1870)

You can also have other technical issues. There might be a timing offset, so perhaps the time codes tagged onto those metadata events do not line up quite with the source time code coming with the video, and therefore you can get permanent offsets. That describes the context  of the problem we are trying to solve.

So how did we deal with this? The most important thing here is to understand that we picked a solution well-suited to the customer requirements. Oddly, we ended up doing quite a lot of this using the audio rather than the video. The reason for that is you can get a huge amount of very quick information from processing the audio, which is much quicker, much cheaper, and has much lower latency than if you did this using the video.

A really good example is the referee whistle. If you take the audio from this feed coming in, do a frequency transform, and then look at the audio spectrum, you are seeing the frequency domain as time goes by. The referee whistle is a standard design. When the referee blows the whistle, it always sounds on two very distinct, well-specified frequencies. That makes it super easy to spot in the audio spectrogram. We can get a very high level of confidence of exactly when the whistle was blown. As soon as you have that information, you can lock it in and nail down the timing of the metadata.

Whether it was the kickoff whistle or the whistle blown when a foul was committed, we can see exactly when the metadata for that event ought to have been captured. We built this with the customer. Andrew Lee, who built it, is not here right now but sometimes hangs around near the Live Sports Forum. He is actually doing a live workshop over at the Mandalay Bay. He wanted to be here, but I need to give him credit for this work as well. He and I and a few others from the customer wrote the blog that we will give you a link to at the end of this presentation.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/1980.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=1980)

What was the outcome? We ended up basically eliminating the human errors. We had a 96% success rate. We had to do a few other things as well as the audio. We had to use the overshoot right at the start of the game to be confident we were actually seeing the kickoff. As soon as you can see all the players  standing there ready to go in formation, that gives you a high level of confidence of what part of the game you are looking at.

This last one is really good fun. We were not expecting to do this at all, but we noticed that there is a lot of information to be had from the reaction of the crowd. We basically used the crowd noise and particularly level changes in the crowd noise to understand when things were happening. Obviously, when the ball goes into the back of the net, everyone in the crowd is going to stand up and roar if they are on the winning team at that point. We were able to use that to get additional confidence of when the events were happening.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2020.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2020)

It turned out that you can also use that to gauge the level of interestingness of a particular play that has just happened. You kind of get that information for free. As long as you look for deltas in the audio level and take into account that as you get to the end of the match things are probably getting a bit noisier anyway, you can learn quite a lot from that. You can actually generate additional metadata that might feed into things like highlight generation and so on. 

I want to actually ask you something there. You and I worked on something for World Cup Soccer back in 2022. Do you want to talk a little bit about the sync, the challenge of sync as you get live feeds?

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2120.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2120)

It's really challenging because if you have a load of free-running cameras, which hopefully are genlocked but may not be, it's quite possible that the batteries in the timecode generators near them  have gone flat or been replaced and haven't been reset properly. All of those things can happen, and you can get quite noisy, out-of-synchronized video data coming in. We also talked about timing problems in the metadata. We looked at a use case last year related to Formula One, and I think I heard some Formula One things happening earlier.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2150.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2150)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2170.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2170)

If you have cameras on the cars and they're driving around the track, it's a bit late to be resetting  the timing generators there as well. However, we've started looking at things like the beep, beep, beep, beep that you get when the green lights come on at the beginning of the race. So there are things that can be done in order to deduce what the timing information is for those things that need to be synchronized together. 

### Use Case Two: Migrating Complex On-Premises Sports Data Processing to the Cloud

Let's move on to use case number two. This use case relates to migration, but the overall context is still real-time data for sports. We're talking about something where a customer has built a lot of very complex, very mature data processing algorithms which originally run on-premise, probably at the stadium. Not always, but usually. These are quite mature, probably quite complicated, and very high value. The customer derives a lot of their identity and unique offerings from the quality of those algorithms.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2250.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2250)

So these things are precious. They're also complex and quite fragile. What we needed to do was think about how to get those complex on-premises workflows moved into the cloud and working well with all of the scaling benefits that provides, but without damaging the very important, precious things that have already been built. These things are very high performance. They're typically using the fastest GPUs and CPUs that you can get on-premises. We needed to think quite hard about what is the correct way to run those to get maximum benefit as though it had been born in the cloud. 

The kind of technology we're talking about here is likely to involve C++. There's probably FFmpeg in there and so on. These things have implications. Not all that many developers these days love working in C++. They'd rather be writing Python inside Lambda functions. There's no better or worse thing. They're just different. So how did we do this? We had to work out a way to get the benefits of being in the cloud and doing that refactoring, but without spoiling the precious things.

In general, what we had to do was think about things like how to move to more horizontal scaling than vertical scaling. Some of my customers got quite bored of me asking what's your multi-threading strategy in this algorithm? Because if you relied on having a single super-hot vCPU and then you tried to put it in a regular instance, it might not do quite what you'd like. So we needed to think about things like that. Really important bits are the glue around the edge.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2340.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2340)

Right at the beginning of this talk, we mentioned plugging into the customer schedule systems and ingesting those, the scaling, and the need to optimize costs. So one of the most important things we can do is orchestrate all of these services now running on computing cloud in a cost-efficient way. We needed to try to work out how best to use the serverless technology which is available in the cloud to maximum effect. 

But to get that position on the spectrum right, at one end you could simply say we're going to take everything that's on-premises, do a full lift and shift, run it in the cloud, and just leave it like that. Which is all great fun, but it doesn't get you that magical benefit. The other extreme would be to say we're going to rewrite everything using Lambdas and queues. That doesn't work either because you then end up having to throw away those precious algorithms that they've spent so much time on.

I think things I'd call out here are particularly important. We always end up spending a lot of time designing the APIs. That's things like the control API for the deployments. It's things like the schedule ingest and all of those things. I've mentioned here some of the services that we typically end up using. You normally expect to see an API gateway with a really good hard API that's thoroughly designed and agreed with the customer.

We'd expect to see lambdas as the handlers behind that. I mentioned ECS and Fargate here because I think it's quite interesting to think about the choice of what I'll call instance type, but really I mean is this something where we're going to run it and just let Fargate take care of it all, or do we need to be a little bit more prescriptive and intimate in exactly what it looks like? The short answer is Fargate's good and quick to get you started. But if you care very much about what instance type you've got, maybe you've got some extended instruction sets on the CPU and so on, suddenly Fargate suddenly looks a little bit less attractive. It's a question of choosing the right thing.

I mentioned AppStream and DCV and I could mention LeoStream here as well as a key part of the system to make it fly in the cloud. This has got to be something which keeps the operators happy. We all know that sports operators are under a lot of pressure to deliver. We talked earlier about there being zero tolerance of black frames or freezes and so on, and naturally the operators are going to feel the pressure of that. So it's really important that we give them a good experience with the control surfaces, the latency of what they see on their screen locally, and also the picture quality that they're experiencing. So these are things which we've built with the customers, both the data processing and also all of the interfacing, the integration with the schedule and so on, and that's what you have to do to make the system fly.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2500.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2500)



I'll talk quickly about something Abhisek mentioned already, but I want to come back to this point about cultural transformation. How do we actually work with the customer team? Do we just turn up with a load of engineers on a bus who come and sit in a small corner of the customer's office and type away not talking to anybody, or do we actually work with the customer? Obviously the answer is the latter. One of the things which I always specify in any of these proposals is the personas that we need from the customer. I want to draw your attention to the product owner there. That's probably the single most important thing.

What I mean is the customer is usually a single person who is able to turn up at the sprint reviews every two weeks at the end of the sprints and agree that the functionality that we've delivered is what they were asking for. Getting that intimate connection is really important. What we love to do is to have a team which is roughly fifty percent customer engineers and fifty percent which has come from AWS. I'd probably try to get them in a single scrum team or at least multiple scrum teams, but each with a combined breakdown, because it's the best way to make sure that all of the design decisions are surfaced and fully embraced by the customer team.

Also, that handover at the end of the engagement is almost a non-event because they already know what it is that they've been built. They've been involved, they've probably written at least half as much of the code, and we find that to be an incredibly good way to get that collaboration done and to have a customer team which is super confident that they can operate, they can maintain, they can extend the system. It's theirs, they own it, they love it, they built it. It's not something that's been inflicted on them.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2610.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2610)



So what was the outcome of all of this? The engagements that I've just been talking about, both the first and the second use cases have launched successfully. They're in production, they're working now on live sports leagues, and we have successful series, we have successful customers, and customers of customers.

### Call to Action: Identifying High-Impact Use Cases and Leveraging AWS Ecosystem

So we've spent a good forty-five minutes already talking about all this stuff. So what does it all mean to you guys and gals over in the crowd? We wanted to talk a little bit about what's your call to action here. So we do have the blog reference here which is publicly available for you to refer to. It's on the AWS website.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1d446b84d4a7ed2f/2660.jpg)](https://www.youtube.com/watch?v=cDw5-rWp3UU&t=2660)



Now, we talked about trends, we looked at challenges, we looked at the focus areas that come out of those trends and challenges, and where we are focusing from an AWS Professional Services standpoint to transform customers' business. So what do you do next? We want you all to identify what's that high impact use case you want to start with, because that's where everything begins. It could be a FAN 360 initiative that you're currently working on. It could be improving your live event data accuracy and latency. It could be a GenAI assistant as I talked about, or it could be just modernizing your on-premises system as one of the use cases James described.

So what we want to do is we want you guys to leverage AWS's broader ecosystem that is a dedicated section for AWS in sports. There are lots of articles, lots of design patterns that are available for you to consume. Professional Services and our technical teams are available to help you in your journey as you start to look at one of these use cases. So all we want to say as a closing thought is you're not in this alone. We want to be part of your journey. We want to bring in the right partners. We want to lean in if required in certain cases to help you to achieve the innovation and the acceleration that you want to achieve throughout your process. Thank you so much for your time.


----

; This article is entirely auto-generated using Amazon Bedrock.
