---
title: 'AWS re:Invent 2025 - From EC2 to EKS: Tipaltiâ€™s transformation to Windows containers on AWS (MAM339)'
published: true
description: 'In this video, Tipalti shares their journey migrating a .NET 4.7 monolithic payment platform from EC2 instances to Windows containers on Amazon EKS. Danny Teller and Maya Morav Freiman detail technical challenges including logging configuration with Log Monitor, containerd signal propagation bugs, and zombie pod issues caused by host networking service race conditions. They achieved a 50% performance boost, reduced deployment time from 6 hours to 6 minutes, and enabled horizontal scaling through Karpenter. Key optimizations included EBS configuration tuning, internal image registry implementation, and AMI-level fixes. The session covers Windows container fundamentals, ECS vs EKS decision-making, cost considerations showing 60% EC2 savings, and practical lessons for .NET Framework modernization without complete rewrites.'
tags: ''
series: ''
canonical_url: null
id: 3085128
date: '2025-12-05T03:20:32Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - From EC2 to EKS: Tipaltiâ€™s transformation to Windows containers on AWS (MAM339)**

> In this video, Tipalti shares their journey migrating a .NET 4.7 monolithic payment platform from EC2 instances to Windows containers on Amazon EKS. Danny Teller and Maya Morav Freiman detail technical challenges including logging configuration with Log Monitor, containerd signal propagation bugs, and zombie pod issues caused by host networking service race conditions. They achieved a 50% performance boost, reduced deployment time from 6 hours to 6 minutes, and enabled horizontal scaling through Karpenter. Key optimizations included EBS configuration tuning, internal image registry implementation, and AMI-level fixes. The session covers Windows container fundamentals, ECS vs EKS decision-making, cost considerations showing 60% EC2 savings, and practical lessons for .NET Framework modernization without complete rewrites.

{% youtube https://www.youtube.com/watch?v=RkjMcQ4Avw0 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/0.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=0)

### Introduction to Windows Containers on AWS: Setting the Stage for Tipalti's Journey

 Welcome to MAM 339, where we'll be diving into Tipalti's transformation to Windows containers on AWS. I'm joined here today by Maya Morav Freiman, Technical Account Manager with AWS who was part of the journey, and Danny Teller, lead DevOps architect for Tipalti, who joins us today to give us his experience and insights on that journey. Quick question before we kick off today to make sure these headphones are working: who here is actually running Windows applications and Windows workloads in their environment? Not Windows containers, all right, perfect. How many have actually considered containerizing these applications? Alright, so this is going to be a great session because Danny has quite the roadmap coming up here for you, where you'll be able to see all the pitfalls and successes that they've engaged over the last 12 months doing this.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/60.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=60)

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/70.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=70)

 Our quick agenda for today: we're going to do a quick background level set on what containers are, and then we're going to dive into Tipalti, their journey, how they got where they are today, the lessons learned, and what's next, what happens after this.  Danny, if you want to let the audience know what they'll be in for over the next 60 minutes, that would be perfect.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/80.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=80)

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/90.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=90)

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/100.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=100)

Definitely, thank you. So what we're going to do today is walk through from ancient to modern all the technical decision making that we had to go through, the challenges we faced, and our performance gains, optimizations, all this fun stuff, some truly weird things and our results, and finally how it impacted our business. So stay tuned. Thank you.    So, like I mentioned, we just want a level set so everybody's familiar with what Windows containers are, how they came about, and that way if you're an advanced user or just beginning, you'll see how easy it is to get started with them. We're all pretty much familiar with this. Back in the day, one app per server wasn't a great use of the hardware, a lot of high costs, and it wasn't very efficient at all.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/120.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=120)

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/130.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=130)

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/140.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=140)

 Next up, as we all know, hypervisors virtualized the operating system and the hardware, which is great, but unfortunately it still wasn't that efficient. You could only run so many hypervisors in one box, and costs again are a major component.   Lastly, this is where containers really came into its own, virtualizing the operating system. The benefit this has is you can now have a shared kernel with multiple containers using it, and really optimize your costs on this. So the big takeaway here is your cost, your ability to spin up new systems, be agile, and just be much more efficient in the use of all these resources.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/150.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=150)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/170.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/180.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=180)

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/190.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=190)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/200.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=200)

 So a lot of people forget that the first Docker spec was created about 10 years ago in 2015. So it's a pretty solid solution to go with today. It's not like it's brand new. I know people are familiar with Linux containers, that's what everybody thinks.  But fast forward to 2017, that's when Microsoft enabled Linux support on containers, which opened up hybrid strategies. Again, we won't go through every single item, but containerd came up with Server 2019.   2023, Karpenter support enabled, Amazon supported at the same time, and it's continuing to evolve today.  So the big takeaway here is that 10 years later, this is still an evolving process, but it is really a tried and true method for doing Windows containers.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/210.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=210)

 Some quick benefits: being agile is one. Being able to spin these up, shut them down, rapid development, that's essential. Cost-wise, we do see customers save about 60 percent on their EC2 costs, roughly about 68 percent on their storage costs, and reducing your management overhead. If you are using something like EKS, an orchestrator, you know, patching, getting them back, getting them up, failovers, all this is much simpler management as well.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/240.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=240)

 When we're choosing these, there are some considerations to be brought up. For example, this is 2025, as we go into 2026. If I have a new application today and it's going to use .NET, I'm going to choose Linux. Let's be honest, it's just going to be modern. However, for Windows applications, they're a little bit beefier, so you're going to start off immediately with a 3 gig image, which means that the host has to be larger, so cost is an issue. The size of the image is also an issue. Only certain applications can be used as well to be containerized, and we'll dive into what some of those applications can be.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/280.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=280)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/290.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=290)

We do see across the financial industry, healthcare industry, a lot of use cases in AWS for customers taking their applications on Windows containers, and the types of applications that we do recommend to take would be your ASP.NET,   WCF or Windows services applications, and finally console applications. Anything that requires a full GUI, absolutely skip that one for right now. These would be your best bet. If it runs on Windows Server today, there's a pretty good chance that you can containerize that application.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/310.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/320.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=320)

To get started with containerizing applications on Windows Server today, you only need five items.  The first item you need is your Dockerfile, which is your blueprint for your application and contains everything you need. The second item is your proprietary base image.  Typically, that's your Windows Server Core image that you can pull down from Microsoft. The third item is your registry to store your images, which in this case would be ECR. Fourth, you need your host for worker nodes, which would be EKS. Finally, you need your orchestrator for failovers, bringing instances up, and spinning them down. EKS is your frontier for this. With just these five simple items, you are off and running with containers.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/360.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=360)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/380.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/390.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=390)

One thing you can do to speed up deployment from pulling the actual image to starting it is to use the EC2 Image Builder cache strategy.  How this works is you create a custom AMI and bake in all the components through the Image Builder. There is a blog that explains this much deeper that you can find online. By doing this, it does all the work for you up front. When you go to launch this, it actually launches  about 65 percent faster than without a caching strategy. I have two screenshots here to show you the difference. The top one uses a vanilla ASP.NET application with the image cache strategy, and as you can see, pull to pull takes about 54 seconds.  On the bottom, you see two arrows showing a pull, and then finally it's pulled and up and working and started. It takes seven times longer. So you can see the image cache strategy absolutely benefits you and gets that launch much quicker.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/420.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=420)

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/440.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=440)

I mentioned back in 2023 that AWS supports Karpenter.  It's highly recommended and works with the Kubernetes scheduler, so it automatically spins up nodes as needed. They'll be cost effective and work with the compute provider. It's absolutely something that should be used when you are running Windows containers. This week you're probably hearing a lot about AWS Transform for .NET.  We're not going to dive too much into it, but during your modernization journey, what you will be considering is also modernizing the code afterwards. The first step is to get it into a container. That way you can modernize without having to do any code adjustments. The follow-up would be to use AWS Transform for .NET, point that at your application, and it generally will increase the conversion time above four times, which is what we're seeing from customers today. If you want to learn more about Transform, pop by the Expo Center and go to the Amazon booths. They'll discuss code analysis and everything else for you.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/490.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/500.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=500)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/500.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=500)

### Tipalti's Early Architecture: From Startup Success to Scaling Challenges

On that note, I'm going to pass you off to the team from Tipalti.  Maya and Danny, if you want to take it away. Thank you. Thank you, Aidan.  Danny, tell us what Tipalti is all about. Well, Tipalti is a finance automation platform powered by AI.  It basically streamlines all your CFO operations, anything from global payouts, accounts payable, procurement, tax compliance, treasuryâ€”you name it, we've got it. It actually just makes your life a lot easier. Everything in financial operations just makes life a lot better. We distinguish ourselves by combining a single suite platform with simplicity and all the complexity you can imagine, just making it simple.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/540.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=540)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/560.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=560)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/600.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=600)

Now let's talk about the technical foundation.  Tell us about the application architecture that started this journey. Alright, so picture the early Tipalti days with a .NET 4.7 framework, a monolithic architecture, and a payment platform. All of that was actually hosted on EC2 instances, which is like most monoliths.  Our process was actually multiple processes running on these EC2 instances. It was fast to develop, reliable, and a great match for a startup phase. That's a good setup. Early stage priorities are all about speed and focus, where enterprise scale demands something else entirely. How did that initial success play out? It played out great. For the initial phase, it was wonderful with quick releases, stable and reliable production. It was really fun to work with, but as you expand, all these things kind of hit a small bottleneck, and we quickly outgrew its capabilities.  You started with a clean, straightforward monolith. Serve me where it first. What happened when Tipalti began to grow? When growth happens, it happens fast.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/620.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=620)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/650.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=650)

When it does, you need more features, more resilience, and pretty much a lot more of everything. Our single process evolved to multiple child processes from a single parent.  That became increasingly complex when all of them needed to run on a schedule, and sometimes they move out after 18 years. That growth is pretty common, and I bet it produces some serious operational headaches. Definitely, headaches is just putting it lightly. Debugging was a nightmare. Imagine taking a single process out of hundreds and figuring out which one was actually the culprit. Meanwhile, we were still running on EC2 capacity.  It was fixed, and we had no scaling. It was truly challenging. So we had architectural complexity and infrastructural limitations. When did it start impacting the business?

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/690.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=690)

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/700.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=700)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/720.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=720)

The turning point was when our hundreds and thousands of transaction volume exploded into millions. Our systems struggled to keep up. We had frequent crashes, delayed payments, slow performance, and most of all, angry engineers. That is not fun. That is what every scaling company dreads. The architecture that powered the initial success starts holding it back.  The party experience is not unique. We see in companies that started as monolithic a few common pain points.  The first is that your application was built for a fixed world. You buy a server, you run your application on it, simple and predictable until it is not. You are overprovisioned just to handle peak loads, and you are wasting resources the rest of the time.  If you have any capacity spike, there is no elasticity. Your application simply was not made for it.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/730.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=730)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/770.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=770)

And then there is the day to day.  Your deployment should take minutes but stretches into hours, and you are often flying blind. Troubleshooting feels more like detective work than engineering. Then there is the point when the monolith becomes the bottleneck. Release cycles slow to a crawl. Every component in the system is intertwined, and every bug ripples through the entire system. If you try to adopt modern development practices, it becomes an uphill battle. The architecture is not built for it. What really hurts is the  business level. The architecture that fueled your initial success simply was not built for it. So you are faced with two choices: spend years rewriting or live with the constraint.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/790.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=790)

### Choosing the Path Forward: EKS vs. ECS and the Replatform Decision

 What option did we consider, and how did we arrive at our solution? We decided that containerization was the right way to go, but then we had a choice. Was it ECS or EKS? We decided on EKS because it felt a lot more natural for us, and we were already heavily invested in it using our Linux workloads. So it was great for us. It also addressed the scalability challenge far better. Finally, we needed machine access because, well, sometimes you need to understand the intricacies of the stuff. We had that need, and finally, Windows and Kubernetes is kind of uncharted territory, so that was interesting to go with.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/870.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=870)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/890.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=890)

Where did the confidence come from? Absolutely from our DevOps team. Those guys are relentless. They tore into that stuff and really remodeled the entire thing. Also, the container ecosystem was very mature, so it helped out. It was great, and we decided on a minimal risk approach. We wanted to stay as close as we could so we could focus on the application instead of the platform. So ECS was naturally the perfect fit for us. Basically, when you are ready to containerize your Windows application, you get to a fork in the road.  Do you go with Amazon Elastic Kubernetes Service or Amazon Elastic Container Service? Let me go over both and explain what each offers so you will know why we made the choice.  Let us start with Amazon ECS. Think of it as AWS knows the best approach. We have taken hard-won lessons from running containers at massive scale and baked everything right into the service.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/920.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=920)

The good thing is that you don't have to become an orchestration specialist overnight. ECS is opinionated in the best way and is relevant for a team who wants to focus more on their application and less on the infrastructure.  Amazon EKS tells a different story. This is for a team who wants the full power of Kubernetes experience, or maybe for teams like Tipalti who are already running Kubernetes on other workloads and want to get the same experience everywhere.

So how do you choose? Basically it comes down to your team's DNA and where do you want to spend your energy. Focus on the application and less on the infrastructure, while EKS offers you that Kubernetes flexibility when standardizing across environments is part of your strategy. The good thing is that both are 100% ready for Windows production workloads, so the question is not which one works better, it's which one fits your team's strategy. It comes down to control versus simplicity, and Tipalti chose EKS because of their extensive development expertise and their need for deep debugging of their environment.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1050.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1050)

Now when it comes to .NET applications on AWS, this flowchart is here to help make the decision on how to move forward. It's prescriptive and requires some investigation and deep dive, but eventually there are three migration paths corresponding to those approaches. The first one is rehost, which is simply moving your application as is to traditional servers like Amazon EC2 instances. The second, which is replatform, is making some changes  to your application to accommodate the underlying platform compute changes, like moving to run on Windows containers.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1100.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1100)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1110.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1110)

The third one is refactor, which is rearchitecting your application. This pattern helps you use cloud native solutions such as Serverless or porting your .NET Framework application to cross-platform .NET and run on Linux containers. Tipalti chose the replatform path. They were already running on Amazon EC2 instances and decided not to spend years rewriting their codebase. Instead, they moved to run on Windows containers.  Now let's dive into the nuts and bolts.  Walk us through what it took to get a Windows container running.

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1130.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1140.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1140)

### Building the Foundation: VPC CNI, IAM Permissions, and Windows Container Image Selection

So first of all, we read every single AWS document that mentioned Windows and Kubernetes in the same breath, and we found out there are a couple of key nuances that you have to set up right from the start. If you don't do that, you're probably not going to have a smooth experience in the process.  Once you had the lay of the land, what were those set of steps? Well, the first one is actually setting up your VPC CNI.  That's the actual component that's responsible for handing out IP addresses to your pods and tying it to your VPC. We had to tweak it with the right flag, and it was very straightforward.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1170.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1170)

However, unlike Linux nodes, Windows nodes don't run the AWS node pod that actually does the tying of your IP address to your VPC. That actually happens internally, and that's a fact we had to really pay attention to going forward. Next, we had to tweak the AWS IAM permissions  specifically for Windows nodes. It has a small addition to Linux nodes, so we had to tweak that as well. That actually used to live in AWS Auth config, which is now conveniently replaced with access entries.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1190.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1190)

Once the foundation was in place, how did you decide which Windows container or which Windows version or which container setup you want to use?  We decided to stay as close to home as we could, so we decided on Windows containers Server Core 2019 running on Windows Server 2019 server nodes.

It proved to be very close to what we needed. At some point we actually tried to match the build of Windows, but that proved to be very tricky, so we stuck to versions and it was very simple for us to do that. This pragmatic approach is especially important for business-critical workloads. Compatibility can make or break a deployment, and what drove this level of caution was risk reduction.

This is a payment platform with zero tolerance for downtime or any interruptions, so we wanted to stay as close and identical to something we already knew was running. By moving to the same setup on Kubernetes, we were actually able to reduce operating system overheads, so it played out well for us. However, that's only part of the solution. Sometimes you have to pay attention to what Windows images you're going to choose.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1260.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1260)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1280.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1280)

Microsoft offers four container-based images, and each one exposes different Windows components, which influences the final container image size and footprint.  Let's start with the minimalistic Nano Server.  This image exposes just enough Windows to run cross-platform .NET or modern open source frameworks, so it's excellent for building sidecar containers or when every megabyte matters in your deployment. It's clean, it's fast, and it gets the job done.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1310.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1310)

 The next is Server Core, and there's a reason this is the crowd favorite. This one strikes the sweet spot between efficiency and functionality. It exposes the Windows sets that support the .NET Framework and those bread and butter Windows Server features, and it covers the majority of enterprise application needs without going overboard.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1350.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1350)

 The next one is Windows Server, which is a little smaller than the full Windows image, but don't let that fool you. It packs the complete Windows set, making it perfect for applications needing to run directly with graphics capabilities. It's like having a compact car with a surprisingly powerful engine under the hood.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1380.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1380)

 The last one, the heavyweight champion, is the full Windows image. It exposes every single Windows component you can think of. It's massive, but if you need to run a graphics-intensive application or cutting-edge machine learning framework, this is your go-to option.

When you come to choose, you need to consider everything you bring along with you: those drivers, your application calls, or those specifics you need to run. If you go too small, your application won't run, and if you go too big, you're hauling around unnecessary weight, which will slow down your deployment.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1450.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1450)

### First Challenge Conquered: Solving Windows Container Logging with Log Monitor

At this point your cluster was configured and ready, and what were the next steps? Well, the first step was encountering a significant blocker.  Let's talk about those blockers. The first challenge was about logging. Launching a container is usually the easy part, and debugging what's misbehaving is actually the most interesting part about containers.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1490.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1490)

Picture this: you're running on an EC2 machine and what you usually use is an XML format, which is standard and reliable. You'd install a Fluent Bit engine and you'd get those logs right into your logging system, which is very easy to do. But when you go into Kubernetes with Windows and you need JSON format, all that goes out the window.  That's not going to work that easily. So you're actually faced with a couple of choices: do you bake the agent into the Dockerfile, do you run a sidecar, do you look for a vendor to solve it for you, or do you rewrite the application completely just for the logs, right?

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1510.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1510)

 Here's where Windows containers throw you a curveball that sometimes catches teams off guard. In the Linux world, logging is beautifully simple. Your containerized application dumps everything to standard out. The container runtime catches it, and you have logs flowing. It works every time. Now in the Windows world, pods don't generate standard out by default, and here comes a solution called Log Monitor. Think of it as a universal translator for your Windows logging. It formats everything from ETW, from your Windows event log and your application-specific logs, and pipes it to standard out, something that Linux does by default. It's like a bridge between the two worlds.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1580.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1580)

How to set it up is pretty simple. You take Log Monitor,  integrate it into your Windows container during the build, configure your log specifications as part of your pod, build, deploy a log forwarder like Fluent Bit, and configure it to send everything onwards to hopefully CloudWatch. As Danny mentioned, I will share something from experience. Formatting is very important. Always log in a structured format like JSON or C slog, because you will be very grateful if you're debugging at 2 a.m. and you have logs that you can parse. Second, less is more when it comes to verbosity. Keep it quiet unless you are actively debugging. Important events can get lost in the noise.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1680.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1680)

So this is the technical side. How did it actually play out? Well, we started off with Log Monitor just as you mentioned, and once we plugged it in, we actually saw logs. It was great, worked really well. We were able to see our logs within our system and it played out well. It actually confirmed for us that we could see a Windows container with a legacy application and logs flowing through our system. It was good. Seeing the past live logic was kind of depressing, I'm guessing. So Log Monitor stayed in your configuration for a long time. Initially it did, until it didn't.  Because we actually found out that Log Monitor introduced some unwanted baggage and overhead. It made the application crash sometimes. It crashed on its own. It wasn't very fun to hear.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1720.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1720)

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1740.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1740)

One thing about it is when you hook one process into another, it's bound to have some mysterious activity in it, so it's not very effective. What we actually did was modify our application logging configuration to log directly to standard out and actually just solve the problem for us. Once we did that, we had logging flowing into our centralized logging system and the first challenge was marked as completed. That was good to go.  So once this hurdle was behind you, what came next on the road to production readiness? Our next challenge was a completely different set of difficulties, something completely unexpected. We were testing various application versions. It was great until we stumbled upon something really nasty. What happened was we would roll out a new version,  and we saw pods getting stuck in terminating state for quite a long time, I mean like 5 minutes or something depending on your configuration, but they were just stuck there instead of terminating as they should.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1760.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1760)

### The Terminating Pod Mystery: Debugging Signal Propagation in containerd

So first of all, that sounds very frustrating. Can you tell me what was happening under the hood?  Yes, so we actually suspected something between the Kubelet and containerd. In a nutshell, once you shut down a container, Kubelet actually sends that signal to the container, executes any pre-stop hooks along the way, and then the container actually shuts down and reports back to Kubelet that it's gracefully shut down. But actually for us it wasn't really happening because in Linux you have SIGTERM and in Windows you have something different, an equivalent of that which at that point wasn't really propagating properly. So we had to really investigate to understand what was going on. Kubernetes was doing its job, but the signal was not what the application was expecting.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1830.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1830)

Our application was actually coded to handle that specific Windows event for shutdown just fine, but Kubernetes tried to send that signal and nothing was happening. We figured that something was going on during that stuck period where Kubernetes was force killing the pod using the graceful termination period. It was stopping, but not gracefullyâ€”very brutally, in fact. 

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1870.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1870)

We dug in to pinpoint the root cause. We found that there was a bug in containerd at the time. AWS was running two different versions for AWS Linux and for Windows. For Windows, they were running version 1.6, and apparently in that version, signal propagation for Windows was not supported. That's why it wasn't working. 

This is where Windows diverges from Linux again. Kubernetes sends a signal, and the Windows runtime translates it to something called a control shutdown, which is the Windows native event. If your Windows application receives the event, it will behave normally and shut down the application. The translation is crucial because Windows applications are built to respond only to control shutdown events and not to signals. The container runtime acts as a bridge between the Kubernetes world and the Windows world.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1940.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1940)

We reached out to AWS through GitHub, and it was a great collaboration. We opened an issue and somebody replied very quickly and straightforwardly. They confirmed that a later release fixing that issue was coming later that year. Unfortunately for us, we couldn't wait. We had to solve these things right here and right now. 

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1960.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1960)

Of course, we found a workaround. We relied on Kubernetes native features. We introduced the lifecycle hooks we mentioned earlier and tuned our graceful termination period. This gave us more control over when our application was shutting down and how it was shutting down. All we had to do was sit back and wait until AWS released a newer containerd version. 

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/1980.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=1980)

Once that was behind us and everything was up and running, we decided on the ultimate head-to-head battle to validate that the container version was behaving as expected. We compared our EC2 environment to our Kubernetes environment. They were running side by side with the same configuration and same environment, and they were both plugged into the same RabbitMQ routing queue. 

What we saw was a mysterious new client just consuming these messages. Our developers came over asking what this new container was doing and what this new agent was consuming our messages. Of course, it was our Windows pod pulling its own weight. It was very satisfying to watch. Finally, we could see our Windows container working properly on real business logic, and it was really great to see.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2020.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2020)

We bulletproofed everything after that. We touched performance, we touched the baseline, and we continued with testing. Then we were very eager to start the next phase. 

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2040.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2040)

### Achieving Cloud Native Scalability: KEDA, Karpenter, and the 50% Performance Boost

We were ready to test the core promise of cloud native transformation: scalability. This was the moment we had been waiting for because we wanted to test scalability. We plugged in our Windows pods and tuned them to listen to messages on RabbitMQ using KEDA. 

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2070.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2070)

We wanted to scale based on the amount of work to be done instead of just server load, CPU, or memory, which is classic but not enough today. The other half of the battle is infrastructure. You cannot scale an application if your infrastructure does not support it. Since we were on Kubernetes and already using Karpenter, all we had to do was extend it with another CRD. 

We saw great scaling results. We were able to see our pods and our Windows nodes scaling up and down very seamlessly and efficiently. That was a really great turning point for us. We enjoyed that spectacle. This is a huge shift from the old model. Beyond the scalability itself, we noticed that by running the EC2 and the Kubernetes equivalent side by side, we actually got a 50% improvement boost that was completely unexpected.

It wasn't just that running and scaling worked better; it was actually more efficient in how it was operating. From an operational perspective, that was a real game changer because instead of debugging hundreds of processes, we had a single pod with its own process, and we could see logs, traces, and metrics that were just beautiful, super simple, and very straightforward.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2170.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2170)

Karpenter is AWS native. It understands things like spot pricing and commitments, so it can help you not just scale up but also scale down, consolidate workloads, and shut down nodes. If we are speaking of costs, let's talk about it because I think this is something that will interest a lot of people.  Going to a full-blown Kubernetes cluster running Windows must have its financial trade-offs. Definitely. I'll be honest about it because our infrastructure and our costs spiked up a little bit, but not by much. On a good note, it was actually very worth it.

How did you justify such additional costs? Well, the operational improvements actually weigh out the spend. We got a 50% boost, scalability, and ease of deployment. When you factor all these things together, they justify this extra spend. While our server bill did spike a little bit, when factoring in our total cost and all the pain and suffering we've alleviated from our developers and customers, it was just worth it.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2230.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2240.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2250.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2250)

Still, every smart company looks for optimization opportunities. What strategies are you exploring to keep the costs in check?  Well, we tried spot instances, but unfortunately our application couldn't handle the shutdown event with a two-minute notice for spot instances, so that was out the window. Instead, we relied on savings plans and  reserved instances for right sizing. We used EC2 Compute Optimizer and finally traced our resource usage using  AWS Container Insights to right size our actual pod usage.

As you know, the cost efficiency battle and the performance battle never ends. It's an ongoing session. You're right, it never ends. Just another insight: it's very important to use Cost Explorer. There is a feature, an option to enable cost allocation split cost allocation data. You can also leverage Cloud Intelligence dashboards to monitor your container costs. You can set up budgets and alerts to let you know if you have a cost spike unexpectedly.

As Danny said, the key insight is that cost optimization is a journey. It's not a one-time activity, and you need to always have it in the back of your mind. What's the long-term impact? Overall, it's an absolute win. We were able to cut down on manual operations, response times, and debugging. Best of all, we were actually able to eliminate the infrastructure bottleneck we had for scalability, and that kind of thing is priceless no matter how you try to go around it.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2340.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2340)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2370.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2370)

### Performance Optimization: Reducing Node Creation and Image Pull Times

Now everything is stable again.  What area did you focus on improving next? We decided to focus on two key areas: node creation time and image pull duration. If you break it down, node provisioning actually takes around seven minutes from EC2 creation to joining the Kubernetes cluster, and images were about four minutes just to pull.  Let's focus on the nodes first. Seven minutes is noticeable. Yes, definitely noticeable. We dug in and saw that EBS configuration was actually the culprit for the entire thing.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2390.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2390)

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2410.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2410)

As you know, the baseline when you create an EC2  machine, you get 125 megabytes of throughput with IOPS around 3,000. That's great for general workloads and normal EC2 machines running, that's fine. But when you're talking about Windows and you want to go faster, that's not enough because Windows needs more bootstrapping time and processing. So we did the natural thing and doubled all the values .

This shaved off around a minute from our bootstrap time. We cut down from 7 minutes to 6 minutes, and sometimes even lower than that. We tried to explore different values in other directions, but that proved to be in vain unless you were willing to spend extra and try using IO1 volumes, which are very expensive and blazing fast. That's a fair trade-off eventually.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2440.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2440)

The second bottleneck was container images . Our actual container images weigh around 4.5 gigabytes when you take in the baseline image and the application itself. When you start pulling it out, it takes a long time. If you compare it to Linux, it's massiveâ€”4.5 gigabytes compared to around 100 megabytes on a Linux machine, which is fairly massive.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2480.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2480)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2500.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2500)

It turned out to be a bit of both. We noticed that at some point AWS started preloading base images onto their EKS optimized AMIs. After a few AMI version changes, we noticed this quiet change and it dropped down from 4 minutes to around 50 seconds. That was great, but we were not satisfied . We introduced an internal image registry , and by doing so we were able to cut down from 50 seconds to 20 seconds. They're just very, very fast. That's a very dramatic improvement.

By combining both results, we were able to slash the deployment times from 11 to 11.5 minutes down to around 6 minutes, sometimes even less, with 6 minutes 30 seconds being the average. It actually made all our deployments feel very responsive and snappy. It was one of those moments when you invest all this time fine-tuning and it actually pays off.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2580.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2580)

We were pretty stable at that point. We were running with 4 Windows nodes, scaling up and down with 35 Windows pods, with more services being added every day. It was great to look at, and we're actually still running with a 50% improved performance boost and our deployments are around 6.5 minutes. But the best of all was the dramatic reduction in incident response time. All the incidents were just blowing us out of the water .

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2600.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2600)

### Battling Zombie Pods: A Six-Week Hunt for a Platform-Level Networking Bug

Our team's operations shifted from actually firefighting all the time to innovating, writing new features, rewriting, and refactoring even a bit of the code. But it wasn't the end . It was a bad ending, I have to say. After the containerization saga, we were finally kind of stable, but at some point we noticed that when we started rolling new versions, the pods really refused to die. You try to force delete them, they'd still be there. No termination grace period could help. Nothing worked. At some point we started calling them zombies. I don't like zombies.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2660.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2660)

We spent 6 weeks debugging with the AWS support team on this. Every time we got a new theory, we'd understand it, see if it could work, and test it out. Of course not, and we were just really pulling our hair at this point. We were hunting it down and nothing worked. Six weeks to hunt is a long time .

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2670.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2670)

It turned out to be a critical regression bug at the platform level. It was one of those Windows services, the host networking service , which is actually responsible for networking that we have zero visibility into. So a black box bug, definitely a black box bug. It turned out to be a nasty race condition. When one pod was starting and one was actually shutting down, it was causing the networking component to actually create two dual endpoints. At one point you had a living pod and a dead pod, kind of SchrÃ¶dinger's pods, if you say. That was very difficult to understand how it was working.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2740.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2740)

We were able to identify and find it. When AWS worked with us, they decided to examine the networking components. They tore them out completely and traced the bug to a recent platform change, then fixed it at the AMI level while coordinating with us. However, a platform-level fix is not immediateâ€”it takes time. 

So in the meantime, while we waited for the actual release, we got creative. We used a Karpenter feature to introduce a time to live on our nodes. We essentially paved over the problem by refreshing the nodes every three hours, which cleared the bad networking state entirely. We also introduced manual and automated procedures to exercise those demons, and finally we added alerts to catch them just in time or even before they actually happened so we could predict what was going to occur.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2800.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2800)

The final resolution came when AWS released the AMI fix. We immediately implemented it and it fixed everything, so we were back to reliable production. Everything was stable and smooth sailing from then on. It was also a challenging time for us, and working hard with AWS strengthened our partnership even deeper. 

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2820.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2820)

After all this work, our results were significant. We achieved a 50% performance boost consistently across the board when comparing to Kubernetes. Our scalability changed completelyâ€”from six hours to create a machine and install everything to just a pod running everything in six minutes. That's incredible on its own. 

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2870.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2870)

The installation and running of the pod and the service itself improved from a four-minute installation to just 30 seconds on a Helm deployment. That's a massive improvement for us. For the team, life changed for the better. We went from 12-hour debugging sessions to just two hours because we had clear observable telemetry for logs, traces, and metrics from a single pod process instead of hundreds of millions of multiple subprocesses. 

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2890.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2890)

From the business side, we achieved something that was pretty impossible for us early on. We had no horizontal scaling, and we were actually able to solve that using this transition. The business impact was incredible for us. It opened the door for many modern practices so we can employ them and continue innovating our procedures. 

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2920.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2920)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2940.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2940)

### Key Lessons Learned and What's Next: Building a Foundation for Continuous Modernization

When looking back on all these journeys, zombie pods and all, the key lessons learned that I would share with people thinking about a similar transformation break down into a few key areas that we learned the hard way. From a technical perspective, you have to start with small changes.  You have to test it out first and understand if you can do that. By doing so, you can preserve your existing .NET Framework and get modern benefits all the time. When you're working with Windows, just expect some unexpected things. It's kind of a cliche to say, but Windows is always behaving like it does. 

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2960.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2960)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/2970.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=2970)

From an operations perspective, engage AWS proactively. It saved us so much time, and we wish we had done that a lot earlier. Most of all, document everything. We documented every single process and every single execution that we did just so we have enough information to go on.  Strategically, incremental modernization worksâ€”it works 100% of the time. You don't need to rewrite your application from scratch. You can just start small. From a crisis management point, the Schrodinger's pods actually taught us to be a lot more proactive, and by doing so we're actually able to overcome all these issues that we faced. 

Comprehensive monitoring is definitely a must, especially for these types of things. You should also have mitigation strategies. Don't just blindly go into something. You have to think of some sort of way for the unexpected issues. There are four critical factors that usually contribute to a successful transformation. The first one is executive support.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3010.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3010)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3050.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3050)

I don't just mean someone writing off  on the check. Executive support means management understanding that this is not just an IT project. It's a business transformation, and one of the biggest transformation killers is organizational silos. Solutions that work perfectly in isolation often fail in organizational reality. So it's very important to create transformation teams with developers, operations, security, and business stakeholdersâ€”people who have  real decision-making authority.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3070.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3070)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3090.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3090)

I'm guessing that if you want to containerize, you want to do it yesterday, but this is not a race. It's like you don't want to migrate during peak hours. You need  to take it easy and start with non-critical workloads. Learn from experience. Leave enough time for testing, troubleshooting, and for the unexpected. The last success factor is that container technology is always evolving.  Successful teams build learning into their operationsâ€”not just initial training, but an ongoing education program with feedback loops. The best teams leverage support proactively, using expertise to optimize architecture and always stay ahead of best practices, not just fix problems.

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3120.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3120)

[![Thumbnail 3130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3130.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3130)

What's next for Tipalti?  The improvement game never stops. We're going to continue exploring how we can make  everything much faster, from node provisioning to image pulls, because we want to be as close to Linux speed as possible. Next, we're going to work on more cost optimizations because, as I mentioned, this never ends and you want to save as much as you can. We're also going to be tuning our scaling policies so they're better for us as wellâ€”more proactive scaling.

All this endeavor actually created a great foundation for our microservices, so we can port legacy applications onto microservices without the pressure of a complete rewrite. What about your broader technology strategy? GitOps is the best approach. We're going to implement more GitOps workflows, and by doing so, we can work with better and more modern DevOps applications and practices. That's not going to end anytime soon.

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3230.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3230)

[![Thumbnail 3250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3250.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3250)

That's a great example of what Tipalti didâ€”a successful transformation can enable broader modernization efforts. Containerization is not the end goal, but it helps set the foundations for continuous modernization. That's the real value of this approach. If you want to continue your containerization journey,  there are some resources we've put here to help. You have Windows containers on Amazon, best practices for Windows on Amazon, and check out Windows containers in Kubernetes. Take a picture, scan the QR code, and go check it out. 

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3260.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3260)

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/96298a893d260b5f/3290.jpg)](https://www.youtube.com/watch?v=RkjMcQ4Avw0&t=3290)

In addition to that, if you don't know, we have AWS SkillBuilder, which is your gateway to  mastering all things AWS cloud technology. There are over 1,000 free, self-paced, expert-led courses. You can follow guided journeys or just jump directly to the topics that interest you. With that, thank you very much for being here today. Feel free to reach out and stay tuned because  the blog post on this journey is in the making. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
