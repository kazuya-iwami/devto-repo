---
title: 'AWS re:Invent 2025 - Automating Amazon Fulfillment Center Operations with Generative AI (IND393)'
published: true
description: 'In this video, AWS and Amazon teams demonstrate how AI transforms manufacturing operations through IORA (Intelligent Orchestration and Recognition Architecture), a solution that automates operational readiness testing in Amazon fulfillment centers. The presentation covers AI systems engineering frameworks using automation, assistants, and agents, then details how IORA uses computer vision and Amazon Bedrock to verify over 200,000 components across 10,000 workstations. The solution reduced testing time from 8 to 3 days, cut required testers from 11 to 3, achieved 92% precision, and delivers $6 million annual savings across fulfillment center launchesâ€”demonstrating practical AI implementation that empowers employees rather than replacing them.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/0.jpg'
series: ''
canonical_url: null
id: 3093088
date: '2025-12-08T20:04:05Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Automating Amazon Fulfillment Center Operations with Generative AI (IND393)**

> In this video, AWS and Amazon teams demonstrate how AI transforms manufacturing operations through IORA (Intelligent Orchestration and Recognition Architecture), a solution that automates operational readiness testing in Amazon fulfillment centers. The presentation covers AI systems engineering frameworks using automation, assistants, and agents, then details how IORA uses computer vision and Amazon Bedrock to verify over 200,000 components across 10,000 workstations. The solution reduced testing time from 8 to 3 days, cut required testers from 11 to 3, achieved 92% precision, and delivers $6 million annual savings across fulfillment center launchesâ€”demonstrating practical AI implementation that empowers employees rather than replacing them.

{% youtube https://www.youtube.com/watch?v=si9STrMHG6A %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/0.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=0)

### Introduction: AI Transformation in Manufacturing and Supply Chain at AWS

 Good afternoon. Among the excitement around the rapid development and increasing accessibility of artificial intelligence is a great sense of optimism in our industry for how AI is going to transform manufacturing and supply chain operations. As the industry grapples with how we deliver on that transformation, I'm often asked for real practical examples of where we're seeing business value and where we're seeing increased productivity by redefining how work gets done with AI.

Today I'm going to be joined by our AWS prototyping team as well as our Amazon engineering operations team to look at a clear example of just that, not only how we're seeing transformation drive real business value, but also learn from Amazon and how we're implementing AI in our own operations to drive continuous improvement. My name is Joe Rosing, and I lead the Center of Excellence for Manufacturing and Supply Chain at AWS.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/80.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=80)

 Today, what I'm going to start with is an overview of how we're addressing use cases in manufacturing and supply chain, and then we'll give an overview of the Amazon fulfillment network and then dive deeper into the solution itself and how we've been using AI to really accelerate the way that we're onboarding new work cells or new work centers into the Amazon fulfillment centers.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/100.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=100)

### The Case for AI-Driven Reindustrialization: Three Durable Needs

 Much of that optimism really stems from the sense that there's a wave of reindustrialization that's facing our industry, and that reindustrialization really comes from some unique challenges that our industry is facing, everything from continuing to build supply chain resilience to thinking about the impact that an aging workforce or demographics is having on business continuity, to also then thinking about the risks related to geopolitics and trade. The reason why reindustrialization must be AI-driven is because in order for us to build competitive advantage through manufacturing and supply chain operations, it's really going to come down to how quickly we can respond to those changing conditions and respond by perceiving what's happening and re-optimizing at a scale and a speed that simply surpasses human capability.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/150.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=150)

 The winners in reindustrialization are not going to be those that build their technical strategy around implementing technology for technology's sake or just trying to latch on to the latest model that got released. Instead, we want to build that strategy for manufacturing and supply chain in a way that we're delivering a financial ROI but that we're also serving the durable needs of the operation, and I put those durable needs really into three buckets.

The first is reducing variation. How do we become predictably boring at the key performance indicators that matter for our business? But also, how do we think about the role that AI can play in helping us solve problems with human-like reasoning but machine scale consistency?

The second durable need comes down to empowering our employees, and this is a need that we've been working on in the industry for decades. It's been part of lean manufacturing, Toyota production system, but it's never more important than now because fundamentally the role of artificial intelligence is to help empower employees to make better decisions faster. It also creates an opportunity for us to remove the cognitive load on employees to make their job easier, more enjoyable, but also help them do higher order tasks in the operation.

The third durable need really comes down to executing at speed and the speed at which we can perceive something changing or changing conditions on the ground and re-optimize capacity quickly and do that in a way that is scalable and really automated and such that it surpasses human capabilities or our ability to have the team react manually in the plants or across the supply chain.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/250.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=250)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/260.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=260)

  Much of the industry recognizes or is optimistic about the role AI is going to play in driving this transformation. We see probably actually over 80% of manufacturers that are now thinking about using generative AI or AI broadly to develop closed loop manufacturing systems, provide real-time production adjustments, as well as thinking about how we're reducing variation in a continuously improving way.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/280.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=280)

### Building the Foundation: From Data Silos to Software Defined Factory

 But the reality is that only about a third of manufacturers or industrial customers that we work with have the data environment in a place that they're able to innovate quickly and innovate at scale. Much of the role that AWS has been playing and really helping to accelerate that transformation is helping put the data together in an environment that prepares it to do innovation and implement everything from automation to agents to assistants at scale.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/310.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=310)

 The way we do that is by first understanding where the operation is at today, and that typically starts with the traditional ISA-95 Purdue model that inherently has built data silos across each of these layers. It just fundamentally is not going to work for a future agentic world of how we're going to need to use data and drive actions across those different layers of those different systems.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/350.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=350)

So really, the desired future state, or how we paint a picture for what smart manufacturing or a Software Defined Factory would look like, is kind of a different picture. A few of the ways that we would characterize that picture is really doubling down on IT-OT  integration and making sure that we can contextualize the data as close to the source as possible so that we can make sense of the OT data and the IT data and use both in service of these AI workflows. We also need to be able to enable any-to-any communication. This becomes really key when we think about agentic workflows because we not only need to pull data, but eventually we're going to want to create those closed feedback loops, which means we're going to need to send an action or send a recommendation back into a device or back down to the machine or back down to the shop floor. And so we need to think about what is going to be the structure of how we have these systems set up to enable that.

And that really brings us to the last characterization, which is we're no longer only talking about systems of record like the ERP system, but really thinking what is the role of all of these systems and all of these screens, if you will, across our operations. Some of them are still going to be systems of record, but there's also systems of transaction, and now there are systems of action. And those systems of action really become kind of the observability layer of which we can see what is the automation doing or what are the agents doing, what transactions are taking place by that agent versus by the human.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/430.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=430)

And so at AWS, our approach is really  to help simplify that transformation for customers by breaking it down into three key layers. The first layer is really thinking about how we're going to deliver on the KPIs that matter most to you and really have these what we call digitally composed outcomes that are the output of a redefined process. So no longer are we talking about predictive maintenance or just doing computer vision for defect detection. Instead, what we want to think about is how are we going to redefine the way we do quality management such that we can deliver on the KPI of improving our quality yields out of the operation.

And when we think about it that way, what we'll get to in a minute is it opens up the door for us to have an AI approach to systems engineering that really kind of redefines what's possible. But any of those workflows, if they're going to be AI-driven, they're going to need access to data that is contextualized, and that is really a data product that allows us to scale success across the organization. And that's the role of this middle layer, the industrial data fabric. That's where we do the industrial data ops to contextualize the data off the shop floor. It's also where we build out the knowledge graph so we understand the relationships between all the different data sets.

And then finally, the last piece of transformation and one that's really key for this reindustrialization effort is modernizing the manufacturing applications on the shop floor and our approach to OT security. And the reason this is so important is because oftentimes I'll go into a factory and we're looking at systems like the SCADA or the Historian or the MES, and they've been in place there for 20 to 30 years. And the plant and those applications just fundamentally are not architected in a way that's going to enable us to do these AI-driven workflows and work towards that future state of wanting systems of record, systems of transaction, and systems of action. And so we look at ways that we can modernize these applications in a hybrid architecture and in a very nimble way.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/550.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=550)

### AI Systems Engineering: Choosing the Right Tool for Each Task

 Now, as I talked earlier about redefining those workflows, whether it's maintenance management or quality management, what we want to think about is traditional systems engineering but the new tools that are in our toolbox to go about redesigning those processes and defining what that future state process map looks like. And so we've been referring to this as AI systems engineering. And when we think about those different tools, I have some of the examples here. It could be automation, it could be assistants, it could be agents. But what's important is that this is not a progression. This is not where we start at automation and we take every task out to fully autonomous multi-agentic systems.

These are individual tools that we can pull from the toolbox on a task-by-task level to make sure that we're picking the right tool for the job and most cost-efficiently executing on that workflow. And so how do we think about which tool do I use when? Well, our framework for that first starts with a couple of tenants.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/610.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=610)

 First, we don't simply want to just automate the existing processes. I was working with a leader in operations about a month ago, and they said, look, the reason why we're working on this process is because it's not working today. It's chaotic, and I don't want you to automate chaos because that would be even worse than what I'm doing today. So what we really want to do is recognize the current state process map, but let's really map out what is that desired future state process map and think about what is best for the organization and how we're going to again serve those durable needs of reducing variation, empowering employees, and moving at the speed of business.

The second tenant is there's no tool to rule them all. Now we've been talking a lot about agentic AI and agents this week at re:Invent, but agents are not the hammer for every nail. As we think about some of those different workflows and break them down to a task level, again we're going to find that the most cost-efficient way to execute some of those tasks might be RPA or automation, or it might be building an assistant, but it's not always going to be an agent. And then the last point is really we want to think about establishing a new baseline for continuous improvement. While theories like Kaizen or approaches like Kaizen and continuous improvement are still very valid, this is really a time for us to take a step function leap in our capabilities and start driving continuous improvement at a higher baseline to reach new levels of transformation or new levels of results.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/700.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=700)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/720.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=720)

 And so the way we can think about this, of what tool to use when, is building out that future state process map, thinking about it at a task level, and then looking at the risk and the complexity profile of each task. And if it's relatively low risk and low complexity, these are probably deterministic workflows.  There are rules that we have in place. There are if-then statements that we can write. And so the most cost-efficient way to go execute that task is going to typically be automation.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/730.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=730)

 Then there are going to be other tasks that are higher complexity and higher risk, where we're going to want to still have the human do the work. We want to leverage the human creativity, but also the human capability. An agent is never going to be able to turn the wrench on the machine, so to speak, when we're doing maintenance. But what we want to do is make sure that that employee is empowered to do the task at a low variation and empower them to do it at a high performance based on best practices or best documentation. And so that becomes the role of not necessarily a chatbot but maturing that to an assistant who, using generative AI, can really help guide the operator or guide the technician along in doing that operation well.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/770.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=770)

 And then the role that agents play really becomes twofold. Where there's a lower risk profile on the task, we can delegate that task to an agent, meaning we have a high enough degree of confidence, or again the risk is low. In Amazon terms, we would say it's a two-way door such that we're going to have the agent just go do that work, and that removes the cognitive load on our employees and frees them up to do more of those higher-order tasks. And then the other role that agents are going to play is on those higher risk but maybe lower complexity tasks, but here we're going to put a human in the loop, and that's okay. It doesn't again always have to be fully autonomous, and in some cases this might be a predictive model that's providing a recommendation, and we're still putting the human in the loop to review that recommendation before we execute against it. Or it might be an agent, and we observe what the agent wants to go do, but before giving the agent the green light, we have someone look at it and make sure that that truly is what we want to do.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/850.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=850)

So again, when we think about systems engineering as AI systems engineering, we really want to leverage these new tools in our toolbox to effectively redesign the workflows, effectively redesign the processes to fundamentally transform how we do manufacturing and supply chain operations. Now my last comment before I hand it over to our other teams  is that the speed at which you're going to transform, the speed at which you're going to be able to build competitive advantage through manufacturing and supply chain operations, and the speed at which you're going to be able to drive continuous improvement through AI systems engineering really comes down to a leadership decision. The tools are there, but it's a leadership decision to really challenge your teams to dive in and start transforming.

### Inside Amazon's Fulfillment Network: Scale and Engineering Challenges

And so to provide an example of how we at Amazon are moving at speed to use AI to reduce variation, empower employees, and really optimize our operations at the speed that our customers demand, we're going to have a look at this example that we've been executing in Amazon fulfillment centers. And so to start us off by looking at that fulfillment network, I'm going to hand it off to Elad. Thank you, Joe.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/910.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=910)

Hello everyone. Before we jump into an example of some of the concepts that Joe just covered, specifically with Amazon's operations, I'd like  to start with a question. By a show of hands, how many of you have had the experience of ordering something on Amazon and getting it delivered right to your doorstep?

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/920.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=920)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/940.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=940)

 Amazing. As an Amazon shareholder, I'm very pleased with the result. You're not alone. In the US, 17.2 million people would have the same experience today alone. I'm not sure how many of you are of that 17.2 million people would pause for a second before you tear that open to see what's inside and have a look at the shipping label. This actually  tells you the story of what the package went through within our fulfillment network to get to you.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/960.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=960)

If I'll highlight just one piece of information, it will be those three letters and a digit. This tells you the site code or the last facility in Amazon's network that this package has left to get to you. Now, as you probably are aware,  Amazon has a global presence. We have more than 3,000 facilities worldwide, ranging from robotic centers to delivery stations. We aim to increase the throughput of that network by 10% year over year by either building new facilities or retrofitting existing ones.

I'm Elad. I'm a Senior AI Business Development Developer within Amazon's Global Engineering Services. We are responsible for this activity. We're responsible for expanding our network's capacity. Behind every such project, there's immense engineering effort with a clear purpose. We want to reduce the cost to serve, increase the speed to serve, and increase the selection for our customers worldwide so we can deliver our commitment to our customers as fast and with zero issues as possible.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1010.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1010)

### Operational Readiness Testing: The Manual Process and Its Limitations

But how do we do that? If we follow the process of delivering a  new Amazon fulfillment center, it starts off with a template. The template gives us the basis of design. It has the latest and greatest technology that Amazon would like to implement as a global standard. It allows us to keep a unified approach and our operations team to maximize the throughput through our network.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1030.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1040.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1040)

Our real estate team would then locate a plot that can accommodate for that  need, both from a network throughput perspective and from engineering capacity. And these two would come together after a site selection into what we call a tender package. This would  be a site-specific design that we will tender out to contractors, vendors, and internal Amazon teams that will aim to put it all together on site.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1050.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1050)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1070.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1070)

 Then we go on site. Execution is when all those teams will meet together in a highly orchestrated process that will put all of these together, meeting Amazon's highest quality and highest standards, so that eventually this could become an operational site. Operations needs to provide the  speed and complexity of delivering customers their packages as per our commitment. This requires the highest levels of quality, and to make sure that this is meeting our Amazon requirements, we have a very rigorous handover process.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1100.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1100)

That handover piece is where we, Global Engineering Services, will hand over to operations, and I'd like to spend another minute on this one. This is literally a day or maybe a couple of weeks where we go from a construction site to something that needs to deliver more than half a million  packages per week. We have a process called Operational Readiness Testing, or in short, ORT.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1110.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1110)

The way we go about it today,  we will bring testers from existing sites to support a new launch. In many cases, this will be more than 100 testers and sometimes even more than 200. Each and every one of them will be given a Kindle device, a tablet, that will have the site layout and a dedicated application to run the specific test.

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1130.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1140.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1140)

Within this application, each and every one of the testers will be allocated  to an area within the facility and with every module that they can find within this area. Within the application, they will be able to pull up the bill of materials or the BOM, which has a list of all the components of  that module. For every component, they will need to go through a checklist just to make sure that everything is there, anywhere from simply deployed, powered, or even configured, so that eventually they can determine whether this module is ready for operations, meaning that an associate can come in the next day and start operating this to the full capacity we expect them to get.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1180.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1180)

As you can probably infer, in a site that its footprint is 60,000 square meters, or 645,000 square feet, the numbers ramp up. We have more than 10,000 modules within a single site that translate into more than 200,000 items that need to be checked.  This takes time, a lot of time. And if you multiply this by 150 new launches that we have every year, the numbers only ramp up and you start getting the complexity of what we're trying to solve here.

The process is effective, but it is manual. And beyond that, anything that we will miss during the ORT will be the main root cause for day one issues within operations. So if we miss something, an associate cannot operate the site, an area manager cannot deliver the throughput that they have committed within their section of the building.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1210.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1210)

### Introducing IORA: Computer Vision for Automated Component Verification

 On top of that, it's highly dependent on people's experience. If a tester has tested a couple of sites before, they would know to spot items that someone doing it for the first time simply is not able to do. Before we jump into how we went about solving this, I'd like to run a quick exercise with you. But before that, you need to know a bit more about what is a module and what is a component.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1250.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1250)

On screen right now, you see what we call a module, an operational unit. This is a very common workstation that we have in an Amazon facility. As a whole, this is what we call a module. A module could also be a water fountain or a health and safety board. Anything that goes into the facility is a module. Those modules are made out of UINs  or Unique Identification Numbers. This number is the identifier of that component from the early stages of design, all the way through procurement, supply chain, until we get to verify this specific item on site. We'll refer to this as either UIN item or component in the next presentation.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1290.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1290)

And now to the exercise. In the next slide, I'm going to show you a workstation very similar to the one you see right now on the screen. I'm going to ask you during five seconds to try and identify and count as many UINs as you can. A UIN, again, can be anything other than the person you will see standing there. It could be a rack, a sign, a label on the floor, whatever you can identify and count within the five seconds. Everyone ready?  You're right.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1300.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1300)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1320.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1320)

Alright, by a show of hands, how many of you were able to count up to five items?  Up to ten. Anyone got to more than ten? Alright, cool. Great. You're helping me prove the point. Computer vision is able to spot 21 items within the same runtime. With our modules having on average 40 UINs  and sometimes even ramping up to 80 UINs per single module, we understand that computer vision provides us with immense value with everything that has to do with visual inspections.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1330.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1340.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1340)

 And this is exactly what IORA, our solution, is all about. IORA stands for Intelligent Orchestration and Recognition Architecture. It's a holistic  approach to how we reimagine our supply chain system. Today, we're going to focus on the end part of this, which is basically the visual inspection to make sure that everything is delivered and built on site and fit for purpose for operations. It is built on top of the existing system, so the onboarding is super simple. Basically, the testers could use the same Kindle devices they were using before, but instead of going through different screens and checklists, all they need to do is snap a picture and IORA will do the rest.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1380.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1380)

Now, instead of me just rambling on about it, let me just show you how it works. This is a video of a tester using a Kindle device to snap a picture of a workstation on site.  They upload it into IORA, and after a short analysis time, they get a fully populated checklist. They can then obviously manually review it and edit, snap pictures from other angles to complete the picture, and make sure that this module is fit for purpose. I'll now hand it over to Zakaria to tell you more about how the solution works and what's under the hood.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1410.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1410)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1420.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1420)

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1430.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1430)

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1440.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1440)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1450.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1450)

### Technical Deep Dive: Architecture, Prompt Engineering, and Optimization Strategies

Thank you, Elad. Hi everybody. Before jumping to the technical details and the challenges, let's start with a  simple demo to show you how in detail the solution works. So here basically the user selects a module, then will upload  an image or take a snap to send it to the service which will analyze the image  and provide the results. You have all the UIN lists in the table, and  the image is being analyzed, and at the end we get all the results of the AI analysis. In this UI,  the user has the capability to get all the details of the specific UIN, the reference image, and also manually override the AI decisions based on the outcome.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1470.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1470)

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1480.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1480)

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1500.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1500)

And the solution also provides all the metrics for the  specific project, so that we will see in this video. So all the detected, all the modules that have been completed, and also all the comments that have been added  by the AI for each specific defect or the confidence score. So I'm Zakaria Fanna. I'm the AI Prototyping Engineer for Amazon Operations, and today I will guide you through the technical details of the solution.  So let's see how we build the solution. So here we have the architecture overview of the solution. So we have two pipelines here.

The purple one is for the technical description and the rules detection for the image analysis process, and the green one is the pipeline for the image analysis. Let's dive into the first pipeline, the purple one.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1530.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1530)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1550.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1550)

So here  the user will upload an image, and that will go through Amazon S3, then AWS Lambda, which will trigger the image description generation, which will be a detailed description and the rules for the image analysis.  And all is restored to Amazon DynamoDB. Then this will be used in this pipeline, which is the image analysis pipeline, where the user uploads the image as we have seen in the demo. That will be uploaded to Amazon S3, and with the Amazon API Gateway, it will trigger the AWS Lambda that will trigger the image analysis.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1590.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1590)

So it will retrieve the user uploaded image, then send it to Amazon Bedrock to get all the descriptions and the rules, and then store the result in Amazon DynamoDB for UI display and also get all the metrics.  So let's dive into the developments, how we developed the solution and which solutions we explored. We started by using Amazon Nova Lite to analyze the image and compare the UINs. This provided really fast inference, which was also cost effective, but we got some challenges with the image noise, like you know in the warehouse environment with all the colors, the layers, which was resulting in inconsistent accuracy, so this was a challenge.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1630.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1630)

 Then we also moved to SAM, the Segment Anything Model, where you use all the masks identified by SAM and compare them with the embeddings of the images. So here we use multiple images with a specific embedding and then compare them with the embeddings found by SAM. So here we got zero shot object detection, which was a good point, but we need multiple images for each UIN, which is generating for us infrastructure and management overhead for this solution.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1680.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1680)

 So then we moved to this updated solution which generates technical descriptions for each UIN. So the image is uploaded, then sent to Claude Sonnet, and then generates all the detailed descriptions for each UIN that are stored in Amazon DynamoDB. So this is a one-off process for each UIN. We just upload it once. This is a one-off cost. For each UIN, we just pay once for each description that can be used for multiple modules and multiple projects.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1720.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1720)

So here is the updated approach that  we have developed. We take the descriptions that have been generated along with the user uploaded image, and then we put them all into the prompts. And here we analyze each description along with the rules, which are providing this detailed result with the confidence score and all the attributes that we need to identify the UIN results for each module.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1750.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1750)

 Now let's look at prompt engineering, what we use. The best practices like how to build the message for the Converse API with the image that is placed before the prompt, which is a best practice for increasing accuracy of the image analysis, and also the output instruction. Here we use JSON with all the component attributes to provide all the details into the Amazon DynamoDB for each specific module.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1790.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1790)

 We also have the rules and guidelines. So here we use three specific flags to identify the UINs based on the confidence scoring threshold.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1830.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1830)

For each confidence scoring, we have multiple rules based on the color, matching description, and checklists for the model to provide the confidence score and to then flag which has been detected or not on the image. One of the challenges was the  false positive. So here what we created is a rule-based approach where we created a false positive trap. This is how we call it, where we generated this distinguishing feature for each component. Like here we have the triangle, which is a component that we can see everywhere in the warehouse, but we have multiple triangles, different triangles in the warehouse.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1880.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1880)

So for this specific one with the number and the gate and the specific red arrow, we wanted to reduce the false positive. So we have the distinguished fixture for this triangle, and we have the trap detection where we provide to the LLM a condition to evaluate if it's the right one or not.  Then we have the challenge with the small components or the cables, for example, which cannot be detected correctly by the LLM. So here we built a hierarchical relationship between the items where, for example, here the bottle holder has a hierarchical relationship with the screw.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1930.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1930)

So if the LLM is able to detect the bottle holder, then we evaluate that the screw is present on the image. It's the same for the cables, for example, with the laptop where we got some struggles to identify the cables. We built the same relationship. Now in terms of optimization,  Bedrock offers the possibility to automatically manage the throttling, the timeouts, and the endpoint connection to reduce the risk for the service to hang on.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/1980.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=1980)

And we also have the optimized latency that we use with Amazon Nova to improve the performance, as well as the cross-region inference to optimize the usage of the token across multiple regions. And prompt caching to have subsequent requests that are  processed faster, but also helps reduce the cost. As presented before, we have the manual overwrites. So as we don't trust AI 100%, we need to have this solution for the user to be able to manually overwrite the results of the AI.

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2010.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2010)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2030.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2030)

So in case we have a false negative, for example, where the component is not detected but the human is able to see it,  the user is able to quickly change the result of the AI. In terms of user experience, we have the analysis polling where the user can see what is the status of the image analysis, how much time it took,  and for example, here which module is being analyzed.

In terms of processing, we have both the parallel processing or the sequential processing. So we started with parallel processing, but we had some issues with throttling because of the quota in AWS where we were reaching the limits very fast. So we moved to sequential processing where each image was processed one after one, and this also helped us because we wanted each wire to not be processed multiple times into the pipeline.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2080.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2080)

So for example, if a component is identified in the first image, then with this sequential processing they are not integrated into the prompt of the next  iteration. Now in terms of improvement for the solution, we have multiple options like cost optimization, storage optimization, S3 lifecycle policy where we can remove any images that are not touched for several days or months. We also have batch inference which can be used if we need asset processing, which also helps reduce the cost by 50% of the inference.

And also we are looking at using agentic AI to move further into this automation where we can leverage the tools to automatically, for example, test the printers, test the laptops, test the scanners which will enhance and improve this solution.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2170.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2170)

### From Hackathon to Production: Rapid Prototyping and Pilot Results

Okay, so thank you all, and now I will pass to Hin Yi, who will guide you through how we move from concept to a final solution. Can you guys hear me okay? Okay, perfect. So let's discuss how we deployed the solution into production and demonstrated that we can reduce manual testing by at least 60%.  My name is Hin Yi, and I'm a senior engagement manager with the AWS prototyping team.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2180.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2180)

Now earlier in the session we talked about the significant  challenge required to manually verify over 200,000 components across 10,000 workstations. Now, this was a problem statement that started us off on our journey and we first presented this idea at an internal Amazon innovation hackathon where we outlined the vision and potential of using generative AI to automate fulfillment center operations and address operational readiness testing. And our idea and submission won the hackathon, which was great, but the next step really was proving out the technical feasibility of the solution and prove that it would actually work in a fulfillment center.

So the next step, which was probably the hardest part for us, was to put together a team to dedicate themselves to building this. So this probably took us the longest in this timeline, but it was critical, right? And because we had won the hackathon, we had the sponsorship that was required, and we were able to ring fence together a two pizza team, meaning two to three developers and one project manager, to dedicate themselves for several weeks on this prototype. And once we got the team together, we started very quickly on building the prototype, and it took us about five weeks to build a full stack solution and then start deploying and testing in a few fulfillment centers across Europe where our team is based.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2260.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2260)

 And I'd like to highlight as well our approach to cross-functional collaboration because first of all it's very easy to get caught up in the technology, but the magic really happens when you put these few groups together. So starting with our end users, they are our customers and they know the ins and outs of the process. They face these challenges on a daily basis and we need them involved to give us feedback throughout the prototype process. We then have our process owners, so they own the solution in production. They know what it takes to deploy it into production and integrate it with the existing technology stack, which typically has a lot of legacy systems.

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2310.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2310)

And last but not least, we have our technical team, so our engineers. Without them we would not be able to stand here and talk to you about what we built, and they are the ones bringing our ideas  to life. So let's talk about our approach to rapid prototyping. So the way that we approach prototyping at AWS is to start working backwards from the customer pain point and then focus on time boxed experiments and deliver vertical slices of user value and not just focus on individual layers of infrastructure.

So we started off by setting up the infrastructure on an AWS sandbox account. We deployed access for our developers in Amazon SageMaker, and we didn't want them to work locally, obviously. And then we started systematically evaluating the different models. So as Zach mentioned, we used Amazon Bedrock. We started with Claude Sonnet. We worked with Amazon Nova models, some open source. We even looked at some traditional machine learning approaches as well. And we time boxed all these spikes because we didn't want to spend too long on one approach until we finally landed on what worked for us.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2400.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2400)

And in parallel we also developed the UI so we wanted to show our end users very early on what the final solution would look like and we didn't want to just show them output on a SageMaker notebook because then they could actually give us some constructive feedback during our weekly reviews. And the result was an end to end prototype in just three weeks, and we also gave ourselves two weeks to actually test, make sure that it was scalable and prepare for pilot deployment. And of course the real test came during our pilot deployment live  in the fulfillment centers because you encounter challenges that you simply wouldn't see in a lab environment.

For example, on the first day our tablets were not connecting to the network infrastructure in the fulfillment center, and we had to root cause what was going on, whether it was related to our Kindle devices or if it related to networking issues in the fulfillment center. It took us a few hours to secure more devices, but once we did that, we got started and we onboarded our users and assigned them to specific workstations across the fulfillment center.

We observed them very carefully, watching how they were using the IORA solution, and we also timed how long they were taking with the current process versus IORA so that we could actually measure the business impact. Then we recorded any feedback they had, big or small, to see if there were any quick wins that we could implement before the next test at a different fulfillment center. One observation that we had was for workstations that had a significant number of UINs, our testers had to take a lot of images, upwards of 40, and once we hit that 40 mark, that was when we saw the performance degradation and significant latency increases, as well as a decreased quality when it came to maintaining the correct counts across specific components, such as a laptop monitor on a workstation.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2500.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2510.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2520.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2520)

So that led us to thinking about what are some ways that we can improve how we ingest the data and improve the actual process for our testers. That brings me to our next innovation opportunity, which is actually moving from multiple 2D static images to 360 degree continuous  imaging, and we're currently looking at using OpenSpace to do that, which is a visual intelligence platform that's purpose built for the construction  industry. It uses 360 degree reality capture to document job sites and track progress, and we believe that this will really enable us to move away from a point  in time static analysis right up before fulfillment center launch to actually continuously monitoring the build life cycle and actually mitigating any issues proactively.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2550.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2550)

We've also gotten requests from other teams to expand this use case beyond focusing on workstations to monitoring more types of equipment like conveyor belts and heavy machinery, and we can ingest other types of data for this. So we've got CAD diagrams, we've got construction blueprints as well to do this.  Now let's talk briefly about costs. The first pipeline and the most significant cost driver is going to be the single image processing pipeline, and we selected Amazon Nova Pro to do this, and that's because it's purpose built for object detection and it's very cost effective for the scale that we're looking for in production.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2600.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2600)

It only costs about 0.13 cents per image, and our current estimates are that we'll need about 1.89 million images, and that's going to come up to about over $13,000 a year. We've also got a very small cost of just generating all the text descriptions, and we can reuse that across our sites, and we've actually already generated about 4,000 plus descriptions during our prototype phase as well. However, I don't think we should just focus on these costs in isolation. We actually want to compare  how much it costs currently to do operational readiness testing, and today it costs about $652,000 per fulfillment center in labor costs.

So for estimating out how many fulfillment center launches we need to do in a year, we're looking at at least $6 million in savings per year, which is quite a compelling use case. We've also demonstrated that we've reduced testing time from 8 days down to 3 days for a fulfillment center and reduced the number of testers required from 11 down to 3 testers, and we've also earned trust from our stakeholders because in the pilot testing we've achieved 92% precision and under 5 seconds of latency. So this has helped us build trust across our business stakeholders to take this further into the next stage.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2650.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2650)

### Impact and Future Vision: Empowering Humans Through AI Automation

 I want to share a quote from Wayne, who is one of our senior program managers. He's been with Amazon for over 16 years and has been quite highly involved during the process. So in his words, the precision results directly translate to time savings. 40% coverage equals 40% time reduction for our field teams. When the solution detects the UIN, our fulfillment center teams can confidently focus on only finding the missing components.

So although we are working on continuing to further automate and improve the accuracy of the solution as we scale up, the main message that we want to convey is that we're not automating the entire process and replacing humans completely. We want to just make them more efficient so that they can trust the results and really focus on things that our solution can't address today because we're very much focused on the deployment and installation of the equipment. Our solution, IORA, currently doesn't cover things like whether the machinery or workstation functions well, such as whether a laptop turns on or whether heavy machinery is working.

So we want them to actually move away from some of the heavy lifting of covering all of these components to actually going towards a more in-depth human verification, things like looking at transportation equipment and testing out other heavy machinery.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2730.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2730)

 So let's go back to this image of all the UINs in a sample workstation. Now that you've seen our IORA solution in action, can you guys guess how many UINs a human would have to manually verify now? I'll give you guys a few seconds. So how many people think a human has to verify tens out of 21? Okay, five. A few raised hands, one. Okay, so those of you who said one or two, that's how many UINs that with the current IORA solution a human would have to verify. So as Zach mentioned earlier, we looked at the different confidence levels. So this would be IORA saying that I have less than 70% confidence, please double check this item.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2780.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2780)

 So our journey from hackathon all the way to production solution demonstrates how we can use AI to enable innovation at scale. And so we hope that you can take these architectural patterns, prompt engineering best practices, and pilot learnings to apply to your own operational readiness solutions. We're continuing to expand our solution and we're very excited about where it's going and also the rapid innovation in this space. I also want to mention that the prototype team that we put together, we're still working together and we're building out more use cases. A couple of weeks ago we already wrapped up another use case where we delivered an agentic AI procurement solution for supply chain operations, and it took us about 2.5 weeks to do that. So we're getting faster and faster. It's a great repeatable innovation mechanism and collaboration effort between Amazon and AWS, and I think it also demonstrates the value of rapid prototyping as well.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2840.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2840)

 So thank you guys for your attention. That's everything that we have today. So thank you, thank you for being here. We do have a few more sessions coming up tomorrow and Friday in the automotive and manufacturing track. So please have a look, make sure that you flag these in your app. And if you haven't already visited our expo, we've got our Industries Pavilion where you can speak to our specialists, bring them your use cases and ideas, and we've got a few fun demos as well in automotive and manufacturing, so please do check that out. And also, these are our emails and LinkedIn if anyone wants to contact us. We've got a QR code up there if you want to email any of us as well.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1fe50eecb4821099/2880.jpg)](https://www.youtube.com/watch?v=si9STrMHG6A&t=2880)

 And we'd appreciate if you complete the session survey in the app. So thank you everyone for your time.


----

; This article is entirely auto-generated using Amazon Bedrock.
