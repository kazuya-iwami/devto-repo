---
title: 'AWS re:Invent 2025 - Derisk your mainframe exit with agentic AI-powered refactoring ft BMW & Fiserv'
published: true
description: 'In this video, Meena Vembusubramanian from AWS Transform presents how agentic AI de-risks mainframe modernization through refactoring, featuring real-world cases from BMW and Fiserv. The session covers AWS Transform''s multi-agent approach combining deterministic processes with LLM-based agents for code analysis, transformation, and testing. BMW achieved 75% reduction in testing time and improved code quality from 2.8 to 4.1 stars, while Fiserv reduced their 30-month migration timeline to 18 months using the Reforge capability. The Reforge feature transforms COBOL-like Java into modern, maintainable code with improved readability. Key innovations include automated test plan generation, test data capture scripts, and functional testing within the reforging loop. Both companies process massive transaction volumesâ€”Fiserv handles 10+ billion dollars daily across 150-200 million transactions, while BMW modernizes applications supporting their Neue Klasse electric vehicle platform. The session demonstrates over 70% total time reduction in modernization projects, bringing multi-year efforts down to months while maintaining functional equivalence and improving code maintainability by 40%.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/0.jpg'
series: ''
canonical_url: null
id: 3088679
date: '2025-12-06T10:53:25Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Derisk your mainframe exit with agentic AI-powered refactoring ft BMW & Fiserv**

> In this video, Meena Vembusubramanian from AWS Transform presents how agentic AI de-risks mainframe modernization through refactoring, featuring real-world cases from BMW and Fiserv. The session covers AWS Transform's multi-agent approach combining deterministic processes with LLM-based agents for code analysis, transformation, and testing. BMW achieved 75% reduction in testing time and improved code quality from 2.8 to 4.1 stars, while Fiserv reduced their 30-month migration timeline to 18 months using the Reforge capability. The Reforge feature transforms COBOL-like Java into modern, maintainable code with improved readability. Key innovations include automated test plan generation, test data capture scripts, and functional testing within the reforging loop. Both companies process massive transaction volumesâ€”Fiserv handles 10+ billion dollars daily across 150-200 million transactions, while BMW modernizes applications supporting their Neue Klasse electric vehicle platform. The session demonstrates over 70% total time reduction in modernization projects, bringing multi-year efforts down to months while maintaining functional equivalence and improving code maintainability by 40%.

{% youtube https://www.youtube.com/watch?v=AoJPmwwezVk %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/0.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=0)

### Introduction: The Challenge of Mainframe Modernization

 My name is Meena Vembusubramanian, and I lead product management for AWS Transform for mainframe. We're here today to talk about de-risking your mainframe exit through refactoring by leveraging agentic AI. Joining me this afternoon, you'll hear directly from Frank Uslaub, who's the Vice President of Transformation Programs at BMW, and Vimal Brahmbhatt, who's the Senior Vice President of Technology at Fiserv.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/40.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=40)

We'll start today with some context that, if you found your way here this afternoon, might be a little bit familiar to you  on the challenges associated with mainframe modernization. We'll talk about AWS Transform's agentic AI capabilities with that context in mind. Next, we'll get to the really interesting stuff and hear directly from Frank and Vimal on their organization's transformation journeys. We'll end our conversation today with the key success patterns that we're seeing from both customers here today, as well as across the board on the refactor pattern and beyond.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/70.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=70)

 Standing here towards the end of 2025 today, there are still over 220 billion lines of COBOL in production, and these lines of COBOL actually power 70% of critical enterprise applications. We're hearing from customers constantly that their teams that operate these applications are restricted from their ability to be agile and really adapt these applications to the changing business requirements as quickly as they would like to. And when they do address the transformation process, what they find is that they're encumbered by multi-year efforts and a lot of risk in terms of validating these applications and getting them across the line.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/110.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=110)

 These teams well understand the benefits associated with modernization. They know that they'll see better costs, often particularly as their applications scale. They'll have access to modern as well as open source technologies to innovate quickly. They also know that the talent conversation becomes a little bit easier, and they have more flexibility as they start looking at a variety of different stacks moving forward. And with this combined, they're able to keep up with the business changes and innovate quickly with their technology stack.

However, despite all of these benefits, teams often find that the balance is outweighed by the risks associated with the modernization process. Modernization requires investing in not just the transformation process and validation, but in parallel, continuing to invest in the day-to-day operations of running your existing workloads. Teams know that there are a lot of risks given the criticality of the business applications we're talking about here in doing that validation and switching over to that modernized application. And this process requires a unique skill set. You're looking for individuals that understand the legacy application, understand what the architecture should look like in the modernized world, as well as people at that intersection who understand both and can really drive that migration process.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/200.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=200)

### Agentic AI: Combining Robots and Humans in Modernization Workflows

 My background personally before AWS is in robotics and automation in the context of factories and Amazon's fulfillment centers. The thing I loved in that context was that ability to leverage robots for what they do best: high precision tasks, lifting heavy loads, and highly repetitive tasks, while combining humans into that process to do what human beings do best, the high judgment tasks and the tasks that require fine dexterity and judgment at every step along the way. So in the factory context, you might have a robot that is able to maneuver a heavy car door, but you bring the human being in in order to install a minute wiring panel so that your end product is at a high quality and the process end to end is efficient.

The thing that's been resonating with me in the past year is this ability with agentic AI to combine different approaches fluidly in one workflow. So what we're able to do now, which is starting to change the game, is leverage agents that have deterministic processes for specific tasks like extracting dependencies from source code or doing like-for-like transformation of code from COBOL to Java in order to achieve functional equivalence. We can combine those with LLM-based agents for understanding the source code and producing natural language documentation for teams to move forward with. All of this can happen fluidly in one workflow, bringing in humans in the loop to make the high judgment decisions, things like figuring out how to draw the lines between different business domains to decompose your monolith into distinct applications that you want to move forward with in your modernization journey.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/320.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=320)

With this, we're starting to see the balance shift over the past year. Connecting the ability to understand legacy  systems, to understand the business logic associated with them, and communicate that in natural language to both your business users as well as your technical users now really reduces the time that teams have had to spend in this understand phase of figuring out what that application does to begin with. This also really helps the experts that are working on this get a lot further and be a lot more effective in the amount of code and the number of applications they're able to process in the same amount of time.

Enabling agents to interact with each other and constantly learn and improve and iterate on the process through an understand, transform, test loop that then again iterates on the process is helping us get a lot more efficient and reduce the risk associated with the output and the risk that's handed over to that final validation team before you approach the cutover process. In combination, this AI-first approach has really helped us transform modernization into something that is tractable for many more teams across organizations.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/390.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=390)

### AWS Transform for Mainframe: Understanding Legacy Applications Through AI

I want to talk a little and tell you a little bit today about what we've been investing in with AWS  Transform for mainframe. Our hypothesis here is to bring this hybrid approach that leverages the right technology for every step along the way. With that in mind, we're going to automate the entire transformation process from initial analysis and planning to that refactoring and that heavily intensive testing and validation phase as well. Our goal here really is to bring those multi-year efforts down to the order of months, and that's what you'll hear from Vimal and Frank about today in terms of how that's translating in their organizations.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/460.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=460)

In addition to this like-for-like functionally equivalent refactoring that we'll hear from these teams on, we're bringing LLMs into the picture to improve the quality of that refactored code that forward-looking teams are going to be managing. We do this by leveraging a multi-agent process to what we're calling reforge refactored code to make it more readable and more easy to maintain for your teams that will be operating these modernized applications. 

Let me tell you a little bit about what the AWS Transform journey looks like at a high level. There are three phases. In the first phase, you're really looking to understand the legacy application. This begins with code analysis to get a static view of the system. Next, we recently introduced an activity analysis agent which looks at the SMF records in an organization to get a dynamic view of the system, to understand things like the frequency at which applications are run, what the CPU utilization for applications looks like, and so on.

Our goal here with this activity analysis is to give teams the information they need to prioritize workloads as well as to make architecture decisions. For example, if you have a medium frequency application that's very short-lived or a process that's very short-lived, that might be an ideal candidate for an event-driven architecture moving forward. The final part of analysis involves data analysis where we look at, for every data source in your legacy system, what the interdependencies look like and what different programs might act on the same data sources.

In addition, we provide a data dictionary that helps you at a field level understand what each data type is and what the function of each data source and fields looks like. Next, we take all of this and leverage our AI-based agents to extract business logic as well as technical documentation based on the user persona that's working on the project on your team. All of this information together goes into an automated decomposition process that your teams can bring their own business context and input into to help decompose into distinct domains and create a migration plan.

### Transformation Patterns: From Replatforming to Reimagining

Following this phase, we get into the transformation journey. AWS Transform supports multiple patterns for transformation. Some teams might have time constraints or particular data center exit timelines and are looking to move very quickly. They might choose to replatform workloads. Next, what we're going to hear about today and spend more time on is the refactoring process where teams are working on highly business-critical workloads and have a very keen need for functional equivalence of that modernized application as compared to the legacy system.

This refactor process is augmented by a Reforge capability that I'll talk about in more detail. Finally, we also support reimagining applications where your starting point is that business rules extraction, which can go into a spectrum and forward engineering process leveraging smart IDEs like Hero.

The last leg of the journey is often the leg of the journey that teams see the most risk and most effort and cost go into, which is in that testing and validation before you're finally able to deploy into your cloud or final destination. For testing, we've introduced a few capabilities this year that we're going to continue to add to. The first thing we've introduced is automatic test plan generation, which early in the process lets you get an understanding of, for every given application, what that testing scope and what the effort associated with it might be.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/670.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=670)

### Reforge and Test Acceleration: Making Refactored Code More Maintainable

I want to talk about a couple of the capabilities that Vimal and Frank's teams have been using extensively  to provide the context while you see the results from these organizations. The first piece of it is this Reforge functionality. After code is refactored, we run it through a multi-agent process called Reforge. Much like melting metal and reforging it makes it stronger and improves its properties, our goal here is to analyze that refactored code to identify any COBOL semantics and structure that have been carried forward.

We then improve on this code using LLM-based agents to restructure it into more native-feeling Java classes and improve on things like variable names that might in the past have COBOL semantics to reflect more business context terms for these variable names. The end result is code that looks much more intuitive and much more familiar to a Java developer. They're able to read the code and make connections to what the business logic being executed by that code actually looks like.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/760.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=760)

The other thing we added recently is functional testing within this loop of reforging code. Our goal there is to ensure that when you have reforged code, we're not introducing any additional functionality changes that might encumber your path to getting to that functional equivalence proof point and finally getting your application in production.  The next piece of what we introduced is around accelerating functional testing itself.

We talked about that test plan generation up front, which helps teams very early in the process understand what the costs associated with testing might be and helps them also make prioritization decisions based on how many test cases and how detailed these test cases associated with specific applications might look like. The other two pieces we introduced today are scripts that help accelerate specific steps in the process. The first one addresses something that we know takes teams a lot of time, which is gathering the test data from the mainframe.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/830.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=830)

When you have a test plan that was automatically generated, we also help you create test data capture scripts that correspond to this test plan. What that looks like is JCL scripts that you can run on your mainframe to capture the very specific form of test data that you need to correspond to the test cases you've identified. Finally, we also provide fully automated test scripts to execute these tests on the mainframe and on your modernized environment. 

Today, so far we've talked about the discovery process and the acceleration we're bringing there to extract the broadest set of insights in the deepest manner possible. Next, we talked about Reforge, which is our capability to combine the deterministic refactor process with LLM-based agents in order to produce more readable, highly maintainable Java code. Finally, we also talked about our test acceleration capabilities that we've seen real results from our customers on thus far, and it's something we'll continue to invest in.

### Fiserv's Scale: Processing 350-400 Million Daily Transactions

Now let's hear about some real world examples on how these features are being used in the context of real organizations. I'm going to call up Vimal to come tell us the story at Fiserv. Thank you, Meena. Good afternoon, everyone. Thanks for joining us. Before I start, let me ask a quick poll. Raise your hand if you have finished 10,000 steps to make it to this conference. Awesome. Love it. If you can navigate this re:Invent sessions, I guess we can get through the modernization of mainframe.

Quick introduction, Vimal Brahmbhatt, I work for Fiserv. How many of my colleagues here are from financial institutions, if you don't mind? Awesome, awesome. So this will be more relatable to you all. I'm joined by my colleague Amir Mohammad from AWS and HCL here in this room, and Amir is running this modernization for all of us. So let's get into the detail right away.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/940.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=940)

Let me talk about Fiserv, what we are, who we are. I'm sure some of you have heard about this company. Maybe some of you may not know. So let's talk about who we are.  Fiserv is a financial payments processing organization. Unbeknownst to some of you, we process pretty much three-quarters of the economy in the United States. We are also a global company. We have a lot of APAC and EMEA presence where we help out our customers.

Let me draw your attention to some of these numbers. We support 6 million plus merchants, and when I talk about merchants, think about them as starting from Amazons all the way down to the mom and pop shop at the corner, and anything in between. We support about 1.6 billion issued cards. This is through our partnership with the banks. We also have 10,000 banking institutions who use our software to run their banking operations. And at our peak we process close to about 25,000 transactions per second.

I want you all to pause for a moment and think about the scale I just talked about. At this scale, transforming mainframe into AWS or any cloud processing is not a small feat. It's complicated, and I'm sure all of you are here because somewhere you have mainframe in your world and you understand the complexity. I have coded on some of these platforms myself many years back, so I have utmost respect and understanding for this. But for those of you who do not know about what I'm talking about from the number perspective, let's talk about what it means in our day to day life.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1030.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1030)

 So here I have, these are just illustrative examples. Of course, we all have been here at AWS re:Invent for three or four days, so this does not depict our re:Invent journey, but this depicts whenever you go back, this kind of shows our journey of everyday life. You get up, you're on your phone, you need to buy something, you make a purchase at an online retailer. And then you're ready to go to the office. We are five days in office, so I have to go to the office every day. You get your coffee on the way, you get your car charged or you get your gas. You make a payment through either your mobile phone, tap your card, insert your card, whatever it is.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1070.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1080.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1090.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1090)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1100.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1100)

 And then after that you are in the office, that subscription pops up that I have to pay for my subscription. You pay in-app purchase,  then the lunchtime comes, you order your food, it gets delivered. Of course your kids wanted the new PlayStation 5 or 6. You order that before you go home,  spouse calls you that, hey, get some groceries from the grocery store. So you order ahead or you go to the grocery store and get your grocery. You reach home,  now you're so tired, you decided that I want to treat family with a good dinner. So you take them out, you go to the restaurant.

There's the Clover device, the white box which is shown there. We also have a point of sale solution which a lot of businesses use. If you are at a coffee shop, one of the famous coffee shops which was on the previous slide, they use this device, so you will know what I'm talking about. In general, I'm just explaining what we all do every single day in our life, and that happens about 350 to 400 million times every single day, 350 to 400 million. And during the peak season, we just are in the peak season as we call in the financial industry, we had Black Friday, we had the day before Thanksgiving, we had Cyber Monday. These are the days where we considered it as a peak season for all of us. So I hope this kind of gets the idea about what I'm talking about payments processing.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1170.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1170)

### Host Data Capture: Modernizing a $10 Billion Daily Payment Processing Application

With that as a context, let's talk about how we got here in our journey with Amazon.  So as a company we decided to kind of pivot and we said that we want to be on cloud and we partnered with AWS a couple of years back. We started thinking about how do we, net new buildouts we will do all on cloud. There are certain applications we'll start migrating onto the cloud, and there are some which are kind of lift and shift as well.

So we have done all of these flavors of this, so think about this. We have a lot of applications running in AWS as we speak. A lot of them are net new. A lot of them are migration, and this last couple of years' journey has given us an opportunity to now think about doing this at a scale which is, as I explained, unparalleled to anything. I used to work for another bank before I joined here, and some of these big institutions have huge mainframe presence with a lot of big workload processes on mainframe.

People do innovation around the mainframe, but not on the mainframe. What you typically do is take your DB2 data and VM data, replicate it somewhere, and build the services off of that. But nobody really touches the mainframe because it's hard, it's complicated, there's a lot of institutional knowledge there, and we don't have people who understand this. So that's the journey.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1250.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1250)

Now let's talk about the particular application we are transforming here called HDC, Host Data Capture. I explained how payment works, but let me draw your attention to whenever you use your credit card, debit card, in-app  tap. These are like four pillars of how transaction flows through. All of us who are customers are in the first box where you initiate your payments, either for purchasing online or card present, which is in-store. Then of course, wherever you are buying your stuff from, that's called the Merchant. Then Acquirer is where we are focused today. Fiserv actually plays in acquiring, network, and issuing, but we are focused on acquirer, core switching, and pre-settlement processing. Then comes the Visa, MasterCard, Amex, Discover, Star, and Accel. These are all the credit and debit networks. At the end, the card you have in your pocket, if it's a JPMorgan Chase issued card, Bank of America issued card, or Goldman Apple issued card, that's where the final decision happens.

In this entity, we are focused on the middle box where it says pre-settlement processing. Think about this: when you buy something online and you go to your bank, it shows that you have a pending transaction. You would have noticed that on your mobile app or your statement which says pending transaction, and the next time when you look at it, the pending goes away and it shows up on your statement. We are talking about that journey only. We are focused on just a small journey between pending and how it gets cleared, so that's the focus we have today.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1340.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1340)

Let's look at the application itself and what it does. The application name is Host Data Capture, and as the name might suggest,  this is a pre-settlement processing application. It powers the ingestion and the pre-settlement processing for high volume data. This application runs 24/7. Let me also give you perspective. When you buy something online before your order ships, the merchant needs to get paid before they will ship your order. When you make the purchase, there is an offline processing which happens where your order came in and now the merchant is asking for money. We have to match and flag your original purchase with the merchant asking for money, flag them, and make sure that it is all reconciled. So that's what the reconciliation means here, 24/7.

Just to give you an idea of the numbers you see on the right-hand side, this application is responsible for processing close to ten billion dollars plus in payment processing every single day. That's B with a capital letter. Ten billion dollars, between 150 to 200 million transactions every day. It goes without saying it's 24/7. This is the scale of the application we are talking about where we have embarked on the journey of modernization.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1420.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1420)

Given those stakes, we also have to appreciate and understand where it's running today. It is running on mainframe,  20 to 30 years of hardened mainframe with millions of lines of COBOL code. I'm sure some of you are actually developing on mainframe today, so you appreciate and understand what these applications have done for us. Non-functional requirements like response time are utmost critical for us because those merchants want to get paid on time. There is no delay we can tolerate there. The capacity and the availability of this platform has served us really well. Before we really migrate away from this and go on to the cloud journey, our standards and the table stakes are we need to make sure that at a minimum this functionality remains as it is and we do not break anything.

Keeping pace is critical. There's an interesting dynamic. I've been in an industry where there are a lot of mainframe applications and there are a lot of new builds. The culture friction between those who are building new versus those who are on these mainframe type applications is a very stark difference, because everyone is a developer and everyone thinks that I'm the best and I know what I'm talking about. Where we are missing out in a leadership position, I personally feel, is if we do not really innovate and if we do not transform, we are going to miss out on this whole agentic AI and what AI has to promise. This is the critical aspect of why we are embarking on this journey.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1510.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1510)

### Fiserv's Journey: Reducing 30 Months to 18 Months with Reforge

Quality has to remain as well, so the conventional path helps, but at scale  it really creates bottlenecks. What we did was, as Meena actually described, I won't go into a lot of details, but we started working with AWS about seven or eight months back where AWS has this product now called Refactor. We called it Blue Age to convert your COBOL code into Java code. One of my goals is to make sure that I have those who are on a COBOL mainframe application, I want to make sure that I keep them around and keep them interested in what we are doing. I also want their expertise because a lot of their knowledge is in their head.

So what we did was we said, okay, we are going to convert COBOL into Java, which kind of looks and feels like COBOL with four divisions: identification, procedure, data, and whatnot. So when you read Java code, they understand what it's doing. Soon we realized that once we do that, we would have to spend another twelve plus months to really make it a modern application, the API-based application, event-driven architecture, framework-based Java code. This is where Meena and team actually came to us and said that, well, we have this product before it becomes generally available. We would like you to see if you are interested in trying this, which is called Reforge, which is one of the agentic AI processes where it takes the refactored Java code and then it builds the modern Java application by using this AI.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1610.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1610)

Because we were embarking on this journey, we decided that we believe this is the right step. We decided to use Reforge to really change it from COBOL-looking Java to real Java, so we combined those two steps. This is where again I think Meena covered a lot of these things.  Our original journey would have been about thirty months, so we took this back. We said, okay, well, let's decide how we want to do this. Army and team actually got together. We said we will take a piece of application. Of course there are multiple modules, many, many lines of code, so we took one piece of application, we took that refactored code, ran it through the Reforge.

We basically made sure that the current team also understands, and automation, the testing piece, the automation piece, continue to make sure that we can test the functionality and not drift from the functionality. So all the results look promising so far. We are in about two or three months of the journey here. But we feel confident that this thirty plus month journey we might be able to finish it in about eighteen months, and we are three or four months into this, so we have about, we are a quarter of the way there, some ways to go, but we are very, very excited about this journey.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1680.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1680)

So before I show you, and I think, did you show the code? No, you didn't show the code. So before I show you the code, how it looks like, it's maybe an eye chart, let's talk about a little bit of the tech portion of it. All of you know that  we have COBOL, VSAM, DB2, flat files or database. We are converting that into Spring Boot. CICS, if your online application goes into Apache Tomcat, JCL gets into Groovy. Again, I think our principle was the business logic must be preserved, functional equivalence guaranteed. So that is like table stakes. We cannot change what it does.

We cannot change from ten billion dollars to eight billion dollars processing every night. We have to keep up ten billion and make sure that we can process twenty billion tomorrow after this application is migrated. And of course leverage AWS's services once we are on the other side of the fence, hoping to remove the legacy tech dependencies there. Another thing, so we have some operational users who use the output of these applications to do the chargeback to reconcile some of the data, so they have screens and they have standard operating procedures. We don't want to change that. We don't want to retrain the entire organization just because we are modernizing our app, so that's another aspect that we are keeping up with this transformation.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1750.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1750)

So let's look at the code quickly. I know it's an eye chart, may not be as visible, but what I'll say  is on the left-hand side is what you see, the refactored code. This is using the Blue Age. This is where we started. And if I show you the whole code, it basically feels like COBOL written in Java. It was good when we started the journey. We did not have Reforge at that time, so Reforge is there now. Now the right-hand side of the code is the Reforged code, and the beauty of that Reforged code is that if you're a Java developer who has never coded in COBOL and start writing the code here, they would kind of understand the right side of the code much better because I think it does a tremendous job of building the modern Java with the variable names that are meaningful.

Unless on the left side you really need to know what you're looking at. So idioms are there, the data classes are there as well. So this is what it looks like.

I think we really want to go to the Reforge code because here's one of the other challenges we have: the talent challenge. I'm not getting any college grad coming out of college to come and code on a mainframe application. I hope that's the same challenge all of you are facing as well, so we have to spend time on training them. There's less interest, but if I get a college grad to come and work on the code which is on the right-hand side, they're all familiar with it. This is what schools are teaching them today, so I think there are a lot of benefits of refactoring this whole code.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/1850.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=1850)

With that, I'll conclude the presentation.  Our expectations when we are done with this whole transformation: as I said, our expectation is that we want to continue to do 10 billion transactions. Well, our goal is to do 20 billion because now I can put in multi-regions. I can now spin up all the lambdas whenever I need. I don't have to rely on utilization, so we can continue to make sure that there are zero disruptions.

The tech stack will be a modern tech stack. Now we can leverage all the services, all the tooling. We can also be innovative in using AI as well in payments processing, which, by the way, in payment processing, there are some use cases for AI in fraud and risk, not in actual payment transaction processing. I think we are looking at some of those as well here. We'll have a modern developer experience. You can use an IDE, for example, versus having to code on the mainframe. So there are a lot of good uplifts as part of the transformation.

Of course, the number of months we will be looking at is reduced because we are using Reforge. And again, I think I cannot thank Meena and team enough for introducing this Reforge to us. That cuts down our journey by almost 12 plus months. And when it's all said and done, had we stayed with the refactored code versus going to Reforge, Reforge is going to uplift our maintainability by 40%, which is another huge thing, right? So we had a choice of either staying with refactor or going to Reforge code. 40% is a huge, humongous uplift.

That's where we are. I'm sure no transformation is easy. I've done a lot of this in my career, of course, not at this scale. I've worked on these platforms, but this is the first time we are embarking on this journey, so I don't expect this to be flawless. There will be bumps. There will be challenges. We'll have a lot of lessons learned. We'll continue to share that with our partners as well. So in short, I think we had a car. We changed everything around that car. Now we are changing the engine while the car is running and making sure that nobody spills their coffee at the checkout while the car is running.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2020.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2020)

### BMW's Neue Klasse Era: Driving Mainframe Migration for Digital Transformation

Speaking of car, I have Frank here who is going to talk about a journey for a car manufacturing company, BMW. So I'll hand it over to Frank. Thanks everyone. Yeah, thanks a lot. Quite an impressive number we've seen from Fiserv. So now let's jump from finance to automotive. So first of all, I would like to introduce myself.  I'm Frank Uslaub, leading BMW Group IT's cloud and modernization journey. So before I jump to our challenges on the mainframe, I would like to give you some insights on what makes us focus on these topics, and I would like to start with some insights from a product perspective.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2030.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2030)

So I think most of you know BMW and our fantastic products.  But now with our Neue Klasse, we are not just taking a step ahead, we are entering a new dimension. The new design is clean, modern, and instantly BMW. The fully electric performance enables up to 400 miles range. And inside, the panoramic iDrive and the completely new electronic architecture creates a smart and more intuitive digital experience. And with around 30% less life cycle CO2 and a higher share of secondary materials, the Neue Klasse also sets new benchmarks on sustainability and circularity.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2100.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2100)

As the first model of the Neue Klasse, the BMW iX3 showcases the innovations that will shape the entire BMW lineup, marking the beginning of a new era that will expand to 40 models by 2027. So if you look at that change on the product side, it's quite clear that  that also means a massive change on the IT side.

That new era on the product side is enabled by further digitalization of our end-to-end value chain. We integrate AI into our core processes on the engineering side, on the production side, on the sales side, and on financial services. That's based on a cloud-first strategy where we have built new applications in the cloud and modernized and migrated our existing applications to move them into the cloud.

We started a transformation program in order to do it, and we try to be as fast and efficient as possible. In order to do that, we copied a lot of approaches which we have from our factories and started actually a transformation factory. Our clear ambition is to use automation and especially AI as an accelerator.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2170.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2170)

We have been pretty successful so far.  The majority of our applications is running already in the cloud, and we also modernized our SAP landscape and put it into the cloud. We have more than 600 productive AI use cases running on a daily basis.

But in addition to our modern IT landscape, we also have some legacy applications. Typically, these are in areas where there was no process change in the last couple of years, so there was no need to update or to migrate these applications. We just keep them stable. But now, basically, if you want to optimize your end-to-end processes and you want to embed AI, at some stage you also touch these legacy systems, and that's where the challenge comes up.

It's really difficult to keep the process and IT knowledge for these legacy applications if things have not been changed in the last couple of years. That's why we made the clear decision to modernize our application landscape and to migrate away from our mainframe. What is quite important for us is that with this topic, we also want to master one of the challenges we see on the talent side.

It's quite easy actually to get COBOL engineers, but it's not so easy to get people who also have the process knowledge combined with the technology. So that's one of the big motivations for us to drive the change. When we started looking at how we are going to do the migration, we first analyzed our applications and found out they are quite complex, and we have a lot of dependencies.

Also, documentation was not up to date, and we did not have proper test cases. So we came to the conclusion that we will only be able to master that modernization and to master the migration if we can do it really in a smarter way with benefiting from AI capabilities. So what does it look like?

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2300.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2300)

 We have explored a couple of ideas, and at the end we defined our modernization approach. In general, we try not to combine changes, technical changes with functional changes, since that increases complexity and also slows you down. But there are a couple of areas where it's just the need to do so.

In those cases, we used AWS Transform together with COBOL, taking over the existing business logic as a basis and then adjusting it to be future-ready. In the other areas, we focused on the technical change with refactoring. Of course, we did some code improvements and also performance optimization, but we kept the opportunity to compare the actual test results of the refactored application with the original application because we've seen a huge advantage in safeguarding the refactoring approach based on that technology.

In general, getting testing on a new level was needed to really speed up. Doing like a code conversion in a very short time and then ending up with needing months for proper testing is nothing which looked for us to be the right way forward, and it was not an option. So we really focused on speeding up the testing activities from test plan generation to test data generation.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2400.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2400)

### BMW's Results: 75% Testing Time Reduction and 60% Improved Test Coverage

 So how does that look like in detail? Let's look at one of our applications in detail.

It's actually a pretty small one. In the past, only the process and IT experts have been able to tell us what's really going on in the application and what kind of variants we need to test. For that application, it took the teams about 10 days to prepare 137 test cases and an additional 2 days to prepare the corresponding test data. The experts even missed some of the variants, which later caused issues in production.

With AWS Transform, the business logic could be extracted from the actual source code, and the test cases have been created in just a couple of hours, including validation. The creation of the test data could also be done in an automated way. That helps us to speed up the migration and safeguard the go-live of our migrated applications.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2470.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2470)

Looking at our landing zones, it was quite important for us to get  rid of all the dependencies we had previously on the mainframe. That was already a huge benefit for us, but in addition, we found a good way forward to run these applications on our standard cloud platforms based on the template architecture. We only needed to use a couple of specific topics, like AWS Glue jobs and Step Functions, to take over some of the batch functionality. Like Fiserv, we also made use of the opportunity to optimize some of our coding with Reforge.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2520.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2520)

For us now, after refactoring the applications, that's really a good starting point for further improvements and functional enhancements, including embedding AI in these kinds of processes. So  what have been our results if you're looking at facts? We've been able to significantly speed up everything around testing, saving us about 75% on the testing side due to reduced time for test case creation. The test coverage could be increased by up to 60%, which is a significant number. Of course, that leads in total to a shortened timeline for all of your migrations.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2570.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2570)

On the other hand, we've seen that we've been able to improve the code quality, and that also leads to better maintainability of our refactored applications, from a 2.8 star rating to a 4.1 rating, which is a significant improvement.  So what do you learn from it, or what did we learn from it? Although AI did a lot of heavy lifting and groundwork, we think it's still important that you have a good plan when you start doing your migrations.

We think it's important that you choose your migration patterns wisely and align that with your business owners and your stakeholders. If possible, I would recommend going with technical conversion first and then doing functional enhancements. Use the AI to first understand where your dependencies are and use it also to get proper documentation that helps you to make informed decisions.

Taking over elements for us from our production factories with a high focus on continuous improvement on standards and efficiency has been a key success factor for our migration. The team learned very fast and improved the approach and the tools with every single step within the migration. The close collaboration with AWS helped us a lot to speed up, and we've seen that the agents are developing very fast, so there is even huge potential for further improvement.

To sum it up, we are now able to solve one of the biggest challenges in IT: to master a mainframe migration. That's really exciting, and I think what we should not forget is that at some stage, when we've mastered all of the big migrations, we should also celebrate the success. Thanks a lot, and back to you, Meena.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2690.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2690)

### Key Success Patterns: Achieving 70% Time Reduction in Modernization

Thank you, Frank. Well, I'm going to close today by connecting some of what we heard from Frank and Vimal with what patterns we're seeing across multiple customers. The first piece of it: yes, we want to embrace automation and embrace intelligence where we can.  But using the right technology for the right task is critical to success. This is true day to day within AWS Transform and each agent we're introducing, each problem we're looking to solve.

But in your organizations, as Frank just mentioned, depending on what you're trying to achieve with each phase of your modernization journey, it's important to select the right pattern that maps to what that business, what that team needs at a given point in time. The next piece of it is testing is not something that comes at the end as one big bang. It's something that needs to happen through the process.

Now, at this re:Invent, we've all heard the word agent a couple of times a day at least. What to me agentic AI means in this context really comes to light in the context of Reforge. Last year, Reforge was one step that took your refactored code and improved it using AI. This year, Reforge is truly an agentic process that not only does that uplift of the refactored code but also tests that code to confirm functional equivalence. When it notices errors, it goes back and modifies that initial uplifted code to test again and continuously gets you to a point where the human being is three cycles ahead. The ball goes as far down the field as possible before you need to bring a human being into the process. That to me is what the acceleration we're seeing with agentic AI.

Finally, we know that modernization is as much of a technology problem and perhaps more so of a people and process problem. Bringing the team along and ensuring that you're bringing humans into the loop and making the right decisions with the combination of technical input as well as business context is what sets teams up for success.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2800.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2800)

 Through all of this, we're optimistic and we're seeing early signs that we can see over a 70% reduction in what that total time looks like for teams. And again, this is now bringing the conversation from multi-year scoping down to the order of months where you can see results within a calendar year. And what I'm hearing from teams is that this is actually making the challenge of bringing this up to the CIO's office and bringing this to decision makers a lot more tractable across the board.

And like we say at AWS, it is definitely day one. Even in the modernization journey, you'll see continuous improvements as your teams are working on things and agents are able to bring in that learning loop to improve along the way. Similarly, what these agents can do is also going to continue to improve month over month, week over week at the rate that technology is advancing right now.

Finally, our goal is, we're still going to need experts, but we're going to reduce the dependency in the process as a whole on these experts. So previously, you might have needed experts for 60% of the journey to really understand the code and to elicit details from the code and the legacy systems along the way. Now AI can get them two-thirds of the way there, so your same team of experts can do a lot more across projects than they've been able to in the past. So our goal here really is to leverage the AI to accelerate what these experts on your teams can accomplish.

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ae0fff7807404594/2890.jpg)](https://www.youtube.com/watch?v=AoJPmwwezVk&t=2890)

 Thank you again for joining us today. I want to close with thanking Vimal and Frank again for telling their stories and leave you with a couple of pointers on how you can learn more about AWS Transform. If you feel that your teams are ready for the mobilization to start exploring modernization projects, I recommend that you get in touch with teams to schedule an experience-based acceleration process, which is a one to two day workshop that we do where we work with your teams to do an initial assessment, pick an application, and run through this Transform process with you. Thanks again for your time and for joining us at re:Invent.


----

; This article is entirely auto-generated using Amazon Bedrock.
