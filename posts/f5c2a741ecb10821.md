---
title: 'AWS re:Invent 2025 - Dive deep into Amazon DynamoDB (DAT435)'
published: true
description: 'In this video, Amrith, a Senior Principal Engineer at DynamoDB, explores two real customer cases to explain DynamoDB''s architecture. The first case involves unexpected write throttling at 800 TPS during a nationwide poll application. The issue stemmed from DynamoDB''s deterministic hash function: scanning a RandomID table and writing to another table with the same key caused sequential partition hammering instead of distributed writes. The solution was prefixing keys to leverage the hash''s avalanche property. The second case shows counterintuitive behavior where high traffic (20,000 TPS) had low latency while low traffic (10 TPS) had high latency. This occurred because thousands of containers couldn''t benefit from request router caching during low traffic. The talk emphasizes hash-based partitioning, connection pooling, and introduces new features like composite keys (up to 4 attributes) and enhanced CloudWatch Contributor Insights for throttling diagnosis.'
tags: ''
series: ''
canonical_url: null
id: 3085321
date: '2025-12-05T04:56:03Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Dive deep into Amazon DynamoDB (DAT435)**

> In this video, Amrith, a Senior Principal Engineer at DynamoDB, explores two real customer cases to explain DynamoDB's architecture. The first case involves unexpected write throttling at 800 TPS during a nationwide poll application. The issue stemmed from DynamoDB's deterministic hash function: scanning a RandomID table and writing to another table with the same key caused sequential partition hammering instead of distributed writes. The solution was prefixing keys to leverage the hash's avalanche property. The second case shows counterintuitive behavior where high traffic (20,000 TPS) had low latency while low traffic (10 TPS) had high latency. This occurred because thousands of containers couldn't benefit from request router caching during low traffic. The talk emphasizes hash-based partitioning, connection pooling, and introduces new features like composite keys (up to 4 attributes) and enhanced CloudWatch Contributor Insights for throttling diagnosis.

{% youtube https://www.youtube.com/watch?v=B05YpQ089w8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/0.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=0)

### Introduction: A Deep Dive into DynamoDB Through Customer Stories

 This talk is a deep dive into DynamoDB. Let me start by saying I realize you have many choices on where you could be. There are a whole bunch of talks happening right now. Thank you for coming to this one, and if you're watching this online, likewise.

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/10.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=10)

 Every year when I do this talk, I think about why we do these talks. The simple answer is that it's for you, our customers. The thing I like the most about what I do is work with all of you and see the things you build using DynamoDB and, in the past, with other databases I've worked on.

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/20.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=20)

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/30.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=30)

  This year, I wanted to change things up a little bit. In years past when I did this presentation, it was block diagrams and architecture and that sort of thing. This year I figured we would change it. We're going to do the same things. We're going to do the deep dive, but we're going to do it from the point of view of a customer. We're going to take two actual situations with real customers, talk about an issue they reported, and talk about the architecture through that lens.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/50.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=50)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/80.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=80)

  A quick word about myself: my name is Amrith. I'm a Senior Principal Engineer. I've been on the DynamoDB team for about six years now, and I've been in this business for about thirty-five years. The thing I like the most about the roles I've had is working with customers. So if we can, after this talk, if you have questions, please feel free to reach out. Let's dive right into it.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/90.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=90)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/110.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=110)

  So what is DynamoDB? It is a document and key-value store, and the objective is that we give you predictable low latency at any scale. Think about those two things: predictable, low latency and any scale. We typically see DynamoDB used in foundational services.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/130.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=130)

 What are foundational services? These are things like your login, the ability for you to connect to a system, inventory management, like most of amazon.com. AWS runs on DynamoDB. The control plane of most services runs on DynamoDB. These are applications where latency and availability are paramount. We're talking about predictable, low latency and very high availabilityâ€”well over five nines is what we achieve, and most requests are done in single-digit milliseconds.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/160.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=160)

 The important thing is that you as a customer don't have to worry about all the things we do in order to deliver these capabilities. You get data, you put data, and that's about it. It's fully managed for you at any scale. Whether you're doing ten TPS or you're doing ten thousand TPS, or we have hundreds of customers who do half a million RPS sustained, you'll get the same latency. We aim to give you the same predictable latency.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/180.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=180)

 I've talked about scale several times. Let me give you a sense of what that means. This is an Olympic swimming pool. Apparently this one is in Barcelona. It is fifty meters by twenty-five meters by two meters. Now, most people in most parts of the world understand two thousand five hundred cubic meters, but in the US we don't do things that way. We have to tell you how big things are in terms of football fields or jumbo jets. So for all of us here, this is seventeen thousand bathtubs, metric bathtubs ostensibly.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/200.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=200)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/230.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=230)

  Let's assume you take all the water out of the pool and you fill your seventeen thousand bathtubs with grains of rice. Everyone knows how big that is. I'm not going to tell you how much that is. For every request DynamoDB serves, place one grain of rice in the now empty swimming pool. How many days do you think it's going to take to fill this swimming pool?

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/240.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=240)

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/270.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=270)

  Less than one day. Okay, brave person there. In less than the time it takes for us to talk through this slide, you would have filled that swimming pool. That's an idea of the scale at which we operate. Every time you go to amazon.com, put something in a cart, search, or use any of the many applications which use DynamoDB, one grain of rice, sixty-one seconds, you've got a full swimming pool. So keep that in mind when we talk about scale.

If you have any concern about whether your application will be able to scale or whether we'll be able to serve it, the answer is yes.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/320.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=320)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/330.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=330)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/350.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=350)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/380.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=380)

### Case Study One: The Mystery of Unexpected Write Throttling at 800 TPS

Remember 17,000 bathtubs in one minute.  So we'll talk about the first of these two case studies. I'm firmly of the opinion that life is too short  not to be having fun. So I hope you have some fun and learn something along the way as well. This is an actual customer incident.  I'm not going to name the customer, but it is something which literally happened. The first time this problem happened, we were as puzzled as you probably are going to be when you see this, but it's an interesting way of understanding the architecture of DynamoDB. 

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/390.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=390)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/410.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=410)

So this customer came to us and said their application was receiving unexpected write throttling. Now I should probably pause and say, how many of you here have not used DynamoDB? Okay, so most of you have, so good. So this customer complained about unexpected write throttling. This was a national crisis. If this application didn't work, this application had not yet gone into production,  but it would literally have been a national crisis. They were conducting a nationwide poll, and they needed this application to work. 

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/430.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=430)

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/440.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=440)

There had been an older system that used a legacy RDBMS, and that system was fully rewritten now to use DynamoDB. The old system needed only to do 600 TPS. We'll get into why in a second. But COVID came along  and this old system which involved people going to a counter and so on could no longer happen, so they said we're going to have a self-service mobile application and the mobile application has to serve 45,000 TPS. So it was going from about 600 up to 45,000, which meant they had to do a full rewrite. 

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/450.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=450)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/460.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=460)

So the customer said they were getting write throttling. So the first question we asked them was, what throughput are you getting write throttle at? 800 TPS.  Now, 17,000 bathtubs in a minute, 800 TPS. There's something wrong with this picture. So we dug into it, we asked them, what's your application? It's a very straightforward application. And to understand this application, I'm going to describe the application almost exactly the way in which it is. The most important thing to understand is that anonymity was really important in this application. 

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/470.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=470)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/500.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=500)

So keep that in mind. So the workflow itself is very straightforward. They're going to read a table and they're going to write three tables. That's it. All right, what are these tables?  First one, a national ID table. The one on the left. This is not real data, this is just synthetic data. I said anonymity was very important. There's a table which has random IDs, and the survey itself consists of two tables, a survey table and the personally identifiable information. Now, they're very careful about keeping the data separate. So they came up with these two randomized IDs. 

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/510.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=510)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/550.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=550)

One's called a PID and one's called a SID. And the basic idea is that the PID ties  to the PII table and the SID ties to the survey table. Now, if somebody has access to one of these, let's see if this worked here, if you have access to the survey data, you can't directly get to the personally identifiable information because you would need this randomized ID looker. And they made sure that not many people have access to that. So if you need to, you can look at PII, you can look at survey but you can't ever put the two together. But they went one step further and they did not tie it to the national ID. What they did was they did a hash  of this combination and made a low cardinality hash, which means that if you did go from the random ID to the national ID, you would get some number of people but you could never go in this direction. You could never say what did Jack, what was Jack's choice in the survey? So this was the whole premise of the application. So a random ID, personally identifiable information and the survey information and they're throttling at 800 TPS.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/590.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=590)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/600.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=600)

So what did the application actually do? How does the whole flow work? They mail something out to everybody, to all registered voters and so on. And in the old system people would go  into a physical location and prove who they were with, you know, a driver's license  or whatever, answer a bunch of questions and they'd be done. But because of COVID they didn't have the ability for people to go into a physical location.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/610.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=610)

So they built this mobile application. What does the app itself do? It generates a random ID, records the personally identifiable information in the survey, and it puts that one-way hash in the national ID table.  Straightforward. One read, three writes. That's it. Everybody good so far?

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/630.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/640.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=640)

 So here's some information about how they got a random ID and how that maps to a particular record.  It's very straightforward. How can this application, so simple, possibly have write throttling? We were not able to see any way this could happen. This is a sophisticated customer. They did all the right things. They've scaled their tables with high cardinality keys and all of that good stuff. So we went one step further and asked them to tell us about their schema.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/670.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/680.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=680)

We got a little bit more detail. The national ID table and the random ID table also have GSIs.  GSIs are global secondary indexes.  The basic idea of a GSI is that it gives you another access pattern. In this case, if you have an AID, which is an anonymous ID, and you want to get the national ID, you'd use the GSI for that. Or if I have an SID and I want to get the PID, I would use this GSI. On the other hand, the primary key on the table will allow you to go from the PID to the SID as an example.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/720.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/730.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=730)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/740.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=740)

This is a simple application with one read and three writes. This can't possibly be happening. So the next thing we asked them was how big are your items?  Maybe they're writing one megabyte items and that's the reason they had a problem.  Nope, small items. 500 bytes, two kilobytes, 400 bytes.  The next question was whether the ID is really random, because that's the only thing we could think of. So with small items and properly scaled up tables, they didn't have LSIs or any other things. Why would they possibly throttle?

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/760.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=760)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/780.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=780)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/810.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/820.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=820)

So we asked if the ID is random and told them to tell us more about their random number generator for this random ID. They said it's not a random number generator.  For various statutory reasons, this table is populated in advance with random IDs. So they populated this table in advance, and then at runtime all they did was scan this table. This is the mechanism to come up with random IDs. This is the random number generator.  This table is populated in advance. They verified that it's high cardinality and truly random. There's some microservice which generates the random IDs. What it does is it goes and reads a whole bunch of rows at once. Maybe it reads a batch of a thousand rows and it records the last evaluated key. Then when you want an ID, it'll give you one. That's basically how this application works. They verified high cardinality, and by the way, GSIs are non-unique.  We don't enforce uniqueness. So they verified that the SID is also unique. That is their statutory requirement. 

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/850.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=850)

Something is funky with this random number generator, but it looked right to us. So we said okay fine. But we still had some nagging suspicions about it. So then the next question is where is this write throttling? Obviously we thought the write throttling must be on the table with the two kilobytes. Nope, it's on the smaller write, the 500 byte write.  Here's what we know so far: they're throttling at 800 TPS. Sixty days out, they need to get to 45,000 TPS. So does anybody out there have an idea what's going wrong? Raise your hands if you think you know what's wrong. They're throttling on the PII table at about 800 TPS. I don't see any hands up yet. Good. We were in exactly the same boat.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/890.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=890)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/900.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=900)

### Understanding Hash-Based Partitioning and the Deterministic Hash Problem

We were puzzled. We did not understand why this was happening.  So here's how we scale. We scale horizontally. We partition your table. You give us a partition key, we partition it.  It's basically hash-based partitioning.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/910.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=910)

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/920.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/930.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=930)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/950.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=950)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/960.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=960)

When you create a table, we ask you for the name of the table and what the primary key for the table is . That's all. The same applies to a Global Secondary Index (GSI)â€”we need to know how you're going to pay for it. But notice here, this is not unique . That's the only difference between a GSI and a table. So here's the NationalID table. When they wanted to populate this data, what would we do?  We would compute a hash. Throughout this presentation I show you hashes. These are not actual DynamoDB hashes; this is just MD5. So we compute a hash, and then we sort that data based on the hash value. And having sorted it on the hash value , we create demarcations and we call those partitions. That's pretty much how DynamoDB worksâ€”horizontal scale with hash-based partitioning, fairly straightforward. 

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/980.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=980)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1000.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1000)

A partition is a contiguous range of hashes. We compute the hash of the partition key, sort based on the hash, and contiguous ranges become a partition that we store on storage nodes. That's pretty much how we work. The hash which we use has a couple of properties.  Of course it has to be a fast hash. We do this at a very high frequency, so the hash has to be computed efficiently. The hash also has to be even, meaning you can't have a hash that produces the same value at a higher frequency than others. 

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1010.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1010)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1050.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1050)

Two other properties of our hash are that it has to be deterministic and it has to have what's called the avalanche property.  The deterministic hash basically says that irrespective of which table or who the user is, if X equals Y, then hash of X equals hash of Y. It doesn't have to be the same for every table or every user; that's just how we implemented it. The other property I mentioned is avalanche, which basically says that if you have some value and you make a small change to that value, you should see a large and unpredictable change in the hash. 

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1060.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1060)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1080.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1090.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1090)

Notice that I make a very small change to the value, and you get this very large change to the hash. So those are the characteristics of the hash function we use.  It's worked great over a decade; we've never had a problem with this hash function. Here's a look at how the RandomID table actually looks. There's a PiD and a SiD. We compute the hash values, we sort the data based on the hash value , and we store it in partitions. That's pretty much what DynamoDB does day in and day out. 

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1140.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1150.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1150)

When the customer has a request, they do a scan, and a scan walks the table in the order of the hashes. We then return the last evaluated key, which contains the actual attributes in the table. So the next time you come back and make another request and specify the start point, we'll continue the scan from the same place. That's basically how a scan works. Anybody have ideas now about what may be going wrong? So here's how the random number generator appears to work. They're scanning down the table. What they appear to see is this data.  Let's look at that data. This is what the random number generator is returning. It looks perfectly random to me. 

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1160.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1160)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1180.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1180)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1200.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1200)

What isn't apparent is that it's actually not very random because it is in the order of the hash. So now they go and write the Survey table. This is not the table which was throttling.  As they're reading data from the RandomID table and they're writing to the SiD, it's going to go all over the place, which is really good because this is the right pattern which they're going to see. This is how you get horizontal scale.  You've partitioned your table into, in this case I think eight partitions, and you continuously write, but the traffic is distributed across multiple partitions. Let's look at the other one.  I'm reading on the value of PiD, implicitly I'm getting it in the hash order.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1230.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1230)

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1250.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1250)

Therefore, the data is being written into the PII table, which is also on the same key. Therefore, the write is not randomly distributing across the entire table. For the first block of some number of random IDs, you hammer one partition, then you switch to the next partition and hammer that partition.  So you're not getting the benefit of horizontal scale. The reason for this, of course, is the unintended consequence of our deterministic hash. No matter which table, no matter who the user, no matter which region, the hash value is the same. 

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1280.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1290.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1290)

### Solving the Throttling Issue: The Two-Character Prefix Solution and New DynamoDB Features

Now, let me also say we intend to fix this. So don't take a dependency on that, because you will probably be hurt later. When you are scanning a table and you perform a query operation and you're getting multiple items back in an item collection, remember that the data may appear random to you, but it's coming back to you in sorted hash order.  So how do we fix this? The first thing we told the customer is to get yourself a real random number generator. Their answer was, yeah, no, can't do that. There's a statutory requirement that random numbers have to be populated upfront. 

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1310.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1310)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1320.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1330.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1330)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1350.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1360.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1360)

Sixty days out before this thing had to work, they were also not willing to make huge changes to the application. So one of my colleagues came up with this clever idea. He said what we really need to do is that you are going to continue to read down this table. This is your random number generator. We're not going to change that, but we want to generate and distribute the write traffic on the PII table.  We don't want it to go hammer one partition, then hammer the next partition and so on.  So what we came up with was we just said, when you're writing to the other table, how about you prefix the ID with some two characters? Like we said, use TX in front of it.  The reason for this is you make a small change, it changes the hash value a lot. So now as you read down the table, the traffic is getting distributed all over on the PII table.  So now we're back to distributing the load across multiple partitions and we're in a good place. 

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1370.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1370)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1390.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1390)

This team was asked to benchmark the system for 45,000 TPS.  I think they happily benchmarked it at 90,000, and the survey went off without a hitch. So the thing to remember is the way we scale is through hash partitioning. If your traffic has items which are like on a GSI, as an example, if you have a large number of items in a table with the same value for the GSI, you're going to get poor performance on that GSI.  High cardinality and uniqueness are really important. If you're ever in a situation where you're reading one table and writing to another table which shares the same key, remember you could be susceptible to this problem until we fix it. So it's going to take a while for us to fix this, but we do hope to fix it pretty soon.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1420.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1420)

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1430.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1430)

The benchmark was a success. The survey was conducted and life was good. So that was the first one.  When this happened, we literally had never realized this problem. But now that we know this problem, this is the way Murphy works. Once you know there's a problem, you run into it. Every now and again, every couple of weeks we hear of some customer who says, oh my god, here on fire crisis, my application is throttling.  Turns out to be the same problem. So if you are building an application, keep this in mind. It's a pattern which can give you sleepless nights.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1460.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1460)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1490.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1490)

So to you, the key takeaways are we have consistent hashing. This could create a problem for you. This is the way DynamoDB works today.  Hopefully we'll fix it soon, and probably, I'd say certainly a couple of times a week we hear, or a couple of weeks we hear this. It is easy for you to fix if you're able to use the power of the avalanche and make a small change.  So if you're reading one table, as you write to the other table, don't choose the same key value and you won't have this problem. So debugging throttling used to be very hard.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1510.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1510)

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1520.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1520)

When we ran into this problem for the first time, it was about four years ago.  CloudWatch Contributor Insights is literally the only way to do it, but it used to be very expensive. When we told customers we enable CloudWatch Contributor Insights, they would say  it's too expensive, we can't do that. They can't enable it once the problem happens. You need to have it when the problem is happening.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1540.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1540)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1570.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1580.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1580)

So we introduced throttled keys only, which means we only log when there's throttling, and you pay substantially less. The other thing we did  was when you do get throttling, we return enhanced error codes. These are backward-compatible, so you don't have to make any change to your application. The exception just identifies the exact resource and why. In the case of write throttling, you sometimes will get throttled if your GSI is generating back pressure, and it will identify which GSI for you as well. So if you do face throttling, keep that in mind.  There are 10 new metrics which we now have in CloudWatch Contributor Insights with this. 

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1590.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1600.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1600)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1610.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1620.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1620)

One thing I will mention very briefly: I said partition key and sort key in the past.  Until a week ago or so, a partition key and a sort key were a single attribute, which meant that if you had a complex schema, it became a little bit hard for you to model that.  So we had customers who needed to do these workarounds, like putting a customer ID and an order ID  together and then using begins with and contains on the sort key, all very hacky. 

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1630.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1630)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1640.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1640)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1660.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1660)

Also, if you want to have the customer ID in the key and you want to store it as an attribute unto itself, there is a problem: you are paying twice for the storage.  The two of them can get out of sync, and the problem with redundancy is always that you could have conflict between them. There are correctness issues.  If you did not want to pay the price and have redundancy, it became hard for you to evolve your schema over time. Because if you need to build another GSI, for example, and you wanted to use order ID, now you have to go backfill it and pull it out from the middle of this key. It became a pain. 

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1670.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1670)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1720.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1720)

So as of a week ago, we launched this: you can now have up to four attributes in your partition key  and your sort key, which makes for substantially easier data modeling. Right now this is supported for GSIs only, and in the future we will see whether we do this for tables. The links there are to our documentation. Of course, if you want to see Alex's LinkedIn post, you can see that as well. We also now have MCP support for schema modeling. So if you are so inclined, we have a very good schema modeling advisor, which will take advantage of these new capabilities. With that, let's move on to the next one. Again, this is similar, and this too is a customer-reported problem. 

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1730.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1730)

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1760.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1760)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1770.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1770)

### Case Study Two: High Traffic with Low Latency, Low Traffic with High Latency

So here is a view of a tool we use internally.  If you call support, one of the things they ask you for is your subscriber ID, and with your subscriber ID, we can look at some of this kind of performance metrics about your application. So here is data for seven days for an actual real customer. That is seven days worth of data showing the traffic history with reads and writes on reads on the left and writes on the right.  The peak to trough is about 2,000 X. 

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1790.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1790)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1800.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1800)

Now, this particular application runs some job once a day which drives some fairly high rate relative to the rest of the day.  So this customer application is perfectly how the application is supposed to work. This is not an anomaly; this is literally what they want. This is the continuous interactive workload, which is the low traffic period.  And then these are the spikes, which is the batch part of their workload.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1810.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1810)

Now, this is the peculiar thing about this application.  When the traffic is low, their latency is high and variable. When the traffic is high, the latency is low and very acceptable, just a flat line with predictable low latency. This is completely counterintuitive. Normally you would assume that high traffic means high latency, but no. High traffic delivers very good latency, while low traffic results in horrible latency. Has anyone here run into this before? Has anyone seen this pattern?

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1870.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1880.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1880)

I know there's one customer who has experienced this. They're here, and this is literally data from their application. This is the average latency. In case you're interested, we also store other metrics. We store P90 and P99 percentiles.   This is the 90th percentile, and it shows the same exact pattern. To understand what was happening, we built a simulator so we could reproduce this in the lab, and we were able to do that. High traffic resulted in low acceptable latency, while low traffic resulted in horrible latency. It's a peculiar situation.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1910.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1920.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1920)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1940.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1940)

To understand what this application was doing, these are the request types: get item and put item.   The next day showed the same behavior. Does anyone have ideas on what's probably going wrong? Does anyone understand what it is? We asked them what their application looks like.  This is more of a description of the simulator we built, but they have continuous traffic and batch traffic. The batch is what drives the high traffic, and they have a fleet of instances. In our simulation, we had 150 hosts driving traffic through DynamoDB behind it, which is very straightforward. What could possibly be going wrong? How can you have a situation where you have high traffic with good latency and low traffic with horrible latency?

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1970.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1970)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1980.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1980)

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/1990.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=1990)

### DynamoDB Architecture: Request Routers, Caching, and Connection Management

To understand why this happens, let's dig into the architecture of DynamoDB.   The application connects to us through an SDK, resolves the public endpoint, and gets pointed to a load balancer.  Behind the load balancer, and load balancers are in multiple availability zones, we have request routers. Every request you make needs to be individually authenticated and authorized. We verify that you are who you claim to be, that your SigV4 signature is correct, and that you're permitted to do what you're trying to do. Every single request requires this verification.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2030.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2030)

To do that, we look up some metadata and rate-limit you if you're going over your provisioned limit or if your table is not able to scale. Your data is actually stored on storage nodes, and the request router's job is to figure out which storage node it should send the request to.  Down on the storage nodes, all data is encrypted at rest, so we need to go to KMS to get the decryption keys. There's also rate limiting on the storage nodes as well. These are the throttling limits we discussed earlier. We have to do all this at what scale? 17,000 requests a minute, right? So we use extensive caching in all of these components.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2100.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2100)

The request router maintains caches for metadata and identity information. Your TCP connection from your application goes to a request router. As long as you use the same connection over and over again, you get the benefit of this cache. If you send one request here and one request there, you may not get the cache benefits.  We do this hundreds of millions of times a second, and it is physically not a good thing to not optimize this.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2110.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2110)

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2120.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2120)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2130.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2130)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2140.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2140)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2150.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2150)

Reconnecting on every request is not physically good for you because you have to go through the entire TCP socket setup and TLS.   Of course, there is authentication and caching for authentication, which you have to keep in mind. All of the metadata that we are going to use is on the storage nodes.  We have encryption keys to deal with, and in your application, you have connection pools.  Going back to where we were, if you managed to get to the same request router over and over again, you get the benefit of these caches. If not, you do not. 

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2160.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2160)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2170.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2170)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2190.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2190)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2200.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2200)

So wherever possible, try to use long-lived connections.  In the case of this particular application, the issue they were having was they were not getting the benefit of the cache.  Their large traffic used a large fleet of hosts, and when they were doing their continuous traffic, when the batch was not running, that was almost 2,000 times less traffic.  Our hypothesis was that they were not getting the benefit of the cache because of the number of hosts involved on their side. 

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2210.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2210)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2220.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2220)

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2230.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2230)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2250.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2250)

So we asked them how many hosts they had. For the purpose of my simulation, I had 150. The customer said they had a few thousand containers.  The bulk ingestion was 20,000 TPS and continuous was 10. This was the numbers which gave us the 2,000 times difference.  These thousands of hosts or thousands of containers were provisioned this way because they are statically stable.  Statically stable basically means that you want to be able to continue to serve whatever traffic without needing to make a control plane change. If you are not able to scale up in time, you are not going to be able to serve the traffic. So they are scaled to peak all the time. 

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2260.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2270.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2270)

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2280.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2280)

### Resolution and Key Takeaways: Optimizing Host Configuration for Cache Benefits

That is the requirement. Maybe they want to accept bulk injection at any time, not just once a day.  One option is to reduce the hosts during continuous interactive time. That is not something which they were willing to do, and also they are in a regulated industry, so it will take a while for them to actually try it out.  Our simulation was this: high traffic with low latency, and low traffic with high and variable latency. We were able to recreate this in the lab perfectly. 

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2300.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2300)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2310.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2310)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2320.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2320)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2340.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2340)

What we did was reconfigure it so that the continuous interactive traffic only went to a small number of hosts.  That is the point where we reconfigured it to a flat line, because now they are getting the benefit of all the caches they need.   So that is the configuration which gives them good performance. Now, these are two of the things which they could do if they wanted to save money: run that at low traffic and scale up when they have higher traffic. But this is not statically stable. 

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2360.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2360)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2380.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2380)

While auto-scaling would save them money, it does leave them with an application which is bimodal. I believe that the configuration they have is the right one: run always with the same number of hosts.  But the recommendation we made to them was that they should send the low traffic to a small number of hosts and not to all the hosts. In our simulation, we showed that that would be beneficial. 

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2400.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2400)

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5c2a741ecb10821/2410.jpg)](https://www.youtube.com/watch?v=B05YpQ089w8&t=2410)

I am going to wrap up by saying I have built applications with databases for several years, and it is always important to understand how the database actually works. Not knowing how the database works means your ability to build an application is substantially less. Understanding what goes on inside is really important.  Equally, anybody can go to a hardware store and buy paint, but it takes a certain amount of thought and some human creativity to come up with art. What our customers build, what you are building, are some really hard, complicated applications which take a fair amount of thought. So we are here to help you build these applications, and please keep the feedback coming. 


----

; This article is entirely auto-generated using Amazon Bedrock.
