---
title: 'AWS re:Invent 2025 - Transforming Apache Kafka into a Scalable Message Queue (OPN413)'
published: true
description: 'In this video, AWS Senior Streaming Solution Architects Subham Rakshit and Masudur Rahaman Sayem explore KIP-932, which introduces queue-like capabilities to Apache Kafka through share groups. They explain how traditional Kafka''s partition-to-consumer binding creates limitations like head-of-line blocking and scaling challenges. Share groups enable multiple consumers to read from the same partition concurrently, supporting per-message acknowledgment and eliminating the need for over-partitioning. The session covers record delivery states (archived, acquired, available, acknowledged), lock renewal semantics, failure handling, and use cases including task worker processing, microservice job queues, and asynchronous request execution. A live demo demonstrates three consumers reading from a two-partition topic. They discuss the share group state topic, new metrics, API changes, and current limitations including preview status in Kafka 4.1 and lack of dead letter queue support.'
tags: ''
series: ''
canonical_url: null
id: 3085116
date: '2025-12-05T03:11:19Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Transforming Apache Kafka into a Scalable Message Queue (OPN413)**

> In this video, AWS Senior Streaming Solution Architects Subham Rakshit and Masudur Rahaman Sayem explore KIP-932, which introduces queue-like capabilities to Apache Kafka through share groups. They explain how traditional Kafka's partition-to-consumer binding creates limitations like head-of-line blocking and scaling challenges. Share groups enable multiple consumers to read from the same partition concurrently, supporting per-message acknowledgment and eliminating the need for over-partitioning. The session covers record delivery states (archived, acquired, available, acknowledged), lock renewal semantics, failure handling, and use cases including task worker processing, microservice job queues, and asynchronous request execution. A live demo demonstrates three consumers reading from a two-partition topic. They discuss the share group state topic, new metrics, API changes, and current limitations including preview status in Kafka 4.1 and lack of dead letter queue support.

{% youtube https://www.youtube.com/watch?v=zbN1VeenxY4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Extending Apache Kafka Beyond Traditional Streaming

Good afternoon, everyone. I hope you're having a good day so far. Let's start with a quick poll. How many of you are familiar with Apache Kafka? All of you, right? Great. And how many of you have hands-on experience using Apache Kafka? Well, the majority of you, that's great to see. This is a 400 level talk, so we will deep dive into the internals, and any hands-on experience will be helpful.

[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/0.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=0)



Today we're going to talk about how Kafka is going to be used in modern architectures, extending Apache Kafka to support messaging at scale along with its traditional streaming benefits. While Kafka has been the go-to technology for handling high throughput event streaming, many engineering teams still need patterns that require independent work item processing, workload distribution, and decoupling of applications in a queue-like manner. In this session, we will set the context for why queue semantics is increasingly relevant for Apache Kafka and the challenges this particular approach is aiming to solve.

We'll talk about KIP-932, which discusses queue-like capabilities. We'll deep dive into the internals and learn about the mechanics of queues for Kafka. My name is Subham Rakshit. I'm a Senior Streaming Solution Architect at AWS and I'm based in the UK. Today I'm joined with my colleague Masudur Rahaman Sayem, another Senior Streaming Solution Architect at AWS who has come all the way from Sydney.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/100.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=100)



[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/110.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=110)

We will start with the motivation, talk about queue semantics and queue capabilities, deep dive into the mechanics,  and then show you a demo and wrap up with some limitations, roadblocks, and guidance for the next steps. So let's get started. What is Apache Kafka?

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/120.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=120)

### Apache Kafka Fundamentals: Architecture and Consumer Groups



[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/130.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=130)

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/140.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=140)

Apache Kafka is a distributed, partitioned, replicated commit log service. Think of it as a fault-tolerant,  ordered journal of events to which producers append and consumers read from. Each topic is implemented as one or more partitions. Each partition is like an immutable append-only log, which is addressed by a sequential offset. Here, the customer topic has three partitions: P1, P2, and P3.  Two different producer applications are writing events to this topic by sending the events over the network to the topic's partitions. Each producer is independent in nature. What happens is that events with the same key, denoted by the color, end up in the same partition. Note that both publishers can write to the same partition if appropriate.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/160.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=160)



[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/190.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=190)

The individual consumer clients form a group called a consumer group.  It reads data from the same partition in a sequential manner, the same way the data was written in the partition. Each partition is consumed by one and only one consumer. However, one consumer can consume multiple partitions.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/210.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=210)



[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/230.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=230)

### Kafka's Limitations: Scaling Constraints, Head-of-Line Blocking, and Queue Pattern Challenges

While Kafka can handle high throughput of data and can scale massively for hundreds of applications, let's understand some of the scenarios where it hits real limitations.  In most real-time systems, ingress traffic isn't constant. It spikes during business hours, campaign launches, or any kind of seasonal events. This can impact the processing of data by the consumers, as the consumers may not be able to keep up with the high ingress traffic. So the consumer lag keeps rising.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/280.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=280)

Additionally, in typical Kafka deployments, what you see is that the same data is being consumed by many applications. So your ingress throughput is often higher than the egress throughput. To ensure that outbound consumption can scale along with your increased throughput is critical to have good system performance. One way to solve the problem is  to add more consumers to the same group. But with Kafka, consumer parallelism is bound to the number of partitions in the topic. That means if you want to scale the number of consumers, you also need to increase the number of partitions in the topic. This becomes a challenge.

The problem is that partitioning for peak load when you are deploying the system for the first time and you do not know what the peak could be means you are probably over-partitioning for that particular scenario. This leads to having more partitions than you actually need at that point, which creates operational overhead and cost. Additionally, if you try to repartition the topic later on, it can pose a challenge because changing the partition count can disrupt the consistent hashing within the topic partition, which breaks the per-key ordering guarantee within the partition. This depends on whether you are using a key when writing the data to the topic partition.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/390.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=390)

One way to solve that is that teams find it better to create a new topic with a higher number of partitions. However, you then have to migrate all your applications to the new topic, which significantly increases the complexity and overhead. The second problem is head-of-line blocking. What is that? Well, Kafka guarantees ordering within a partition. The consumer reading from the partition reads the data in the same order that the data is written to. 

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/410.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=410)

Now if a single message is slow to process, maybe it requires a downstream API call, maybe it is doing some kind of extra validation check, or maybe it is some malformed data, then all the records after that particular record will be blocked, even if those records could have been processed instantaneously.  So teams typically try to work around this with three different approaches. One is by increasing the number of partitions. Well, it does not solve the head-of-line blocking problem, but it reduces the pain point because initially if you had three partitions, now that you have increased to twelve partitions, your data is distributed across more partitions. However, the operational complexity of handling more partitions that we discussed still exists.

The second approach is asynchronous or parallel processing by the consumer, which leads to some kind of state management you have to do, and your consumers end up writing a lot of complex code to handle this process. The third approach is you can skip the problematic message, but then again you have to write additional code in your consumer application to handle that. Imagine if you are writing hundreds of applications and you need to handle it in a consumer application, it is an extra burden to the consumer applications.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/480.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=480)

The third problem is that Kafka's native model does not behave like a queue.  Traditional queue systems are optimized for parallel consumption, where ordering is less important than processing speed. With Kafka, however, parallelism is bound to the partition to consumer mapping. Kafka restricts a partition to a single consumer in a consumer group. So achieving a queue-like pattern for which you need per-message acknowledgement, retries, solving the head-of-line blocking problem, or dynamically scaling the workers all goes beyond Kafka's default design. Teams end up building a lot of custom framework and logic to force Kafka to act like a queue, which increases the complexity.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/550.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/560.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/570.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=570)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/580.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=580)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/590.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=590)

### KIP-932 and Share Groups: Introducing Queue Capabilities to Kafka

These problems act as a foundational motivation for the open source community to think about how Kafka can be extended beyond streaming. What if we could use Kafka as a queue? That leads to KIP-932 Queues for Kafka.  The KIP-932 implementation has evolved Kafka from a data stream platform to also handle work queues.  Queues for Kafka introduces share groups, a new type of consumer group that allows multiple consumers to read data from the same partition in parallel.  This allows us to parallelize the processing without adding more partitions. It also brings in per-message  acknowledgement and retry, giving fine-grained control on message processing. And finally, by removing the  need for strict in-order processing, it eliminates the head-of-line blocking problem, and this makes Kafka more suitable for queue-style workloads.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/610.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=610)

So how does share group work? Well, share groups enable multiple consumers to read data from the  same partition concurrently,

and it enables higher parallelism than the traditional consumer group model. This is particularly useful when you have more consumers than the number of partitions in the topic, so you can scale and distribute your workload across the available number of consumers without needing to over-partition the topic. Share groups form a key enabler in ensuring that Kafka can handle queue-like consumption, allowing application teams to efficiently use it and maintain controlled message processing.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/680.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=680)

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/690.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=690)

A question that comes up is whether we can use a consumer group and a share group together. The answer is yesâ€”a Kafka broker can support both a consumer group and a share group at the same time. Let's say you have a topic with a single partition. In a real production scenario, I wouldn't recommend using a single partition topic, as that's an anti-pattern. But let's take this example.   On the right-hand side, we have a share group with three consumers, and all three consumers start consuming from the same partition. On the left, you can see the consumer group which also has three consumers. Only one consumer can process the data from that single partition. It is important to note that the share group ID and the consumer group ID must be different because these group IDs belong and exist in the same namespace.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/730.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=730)

Let's look into how record delivery works. A partition consumed by a share group is called a shared partition. In the regular Kafka consumer, we have a single offset which is managed by the consumer.  However, in the share group, we have two different types of offsets. One is the share partition start offset, which refers to the starting offset of records that are eligible for consumption. The second is the share partition end offset, which refers to the last offset of the record that is eligible for consumption. The records between the start offset and the end offset are called the in-flight records.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/760.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=760)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/770.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=770)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/780.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=780)

Consumers fetch records from the start offset and end offset range and acknowledge once they have processed the data.    If message processing fails, it can also be retried, and another consumer can pick it up and reprocess it. While multiple consumers can process data from the same partition, it can lead to concurrency issues and cause overload. To manage concurrency and prevent overload, the share partition leader controls the number of in-flight records at any point in time. This is done by using a configuration called group.share.partition.max.record.locks, which allows safe parallel consumption without any kind of conflict.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/820.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=820)

With this understanding of the shared window between the start offset and the end offset, let's learn the key states that records go through.  We can see in a shared partition with ten records. In each of these cells, there are three different rows. The first one refers to the offset, which starts with zero, one, two, three, and so on. The second one talks about the state of the record. The third one, which is only seen from offset two to four, is called the delivery count. This indicates how many times the record has been delivered to the consumer application for processing.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/870.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/880.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=880)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/890.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=890)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/900.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=900)

The first state is the archived state, which indicates that the data is no longer eligible for consumption.  Perhaps the data has already been processed or it has been skipped because of some kind of error. The second is the acquired state,  which indicates that the data is locked by a consumer for processing so that any other consumer doesn't pick up the same data.  The third is the available state, which indicates that the data is available for consumption and ready to be assigned by any consumer.  The fourth state indicates that the data has been successfully processed and has been acknowledged.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/990.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=990)

What does the states mean in this diagram? If you look at it, all the records earlier than the offset 2 are in archived state. That means they are not in flight and they're no longer going to be processed anymore. Records 2 and 4 are acquired for consumption and their delivery count is increased to 1, so that no other consumer is going to pick up those records. 

Record 3 has been previously acquired twice. It was not only acquired once, but it was previously acquired twice and it couldn't be processed, so it's been given back and is now in available state, which can be picked up by another consumer. Record 5 has been processed successfully and is acknowledged. Record 6 has previously been acquired for consumption by a consumer, but it couldn't be processed, so it was skipped and rejected, and it's marked as archived so that no other consumer picks up that same record again. Records 7 and 8 are in the available state, so any consumer can start reading those data. Any records from position 9, offset 9, and beyond are in available state.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1010.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1020.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1030.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1030)

### Share Group Mechanics: Record Ordering, Offset Management, and Error Handling

How does the ordering of data in a shared group look like? Here you have 9 records with start offset starting at 0 and the end offset at 9. We have the first consumer, which is reading the data from 0 to 4 inclusive,  and the second consumer reads data from 5 to 8. Consumer one crashes, but consumer two finished processing the data.  The output that comes out from the application, you can see on the right hand side, is 5678. 

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1050.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1060.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1060)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1100.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1100)

Now consumer two again refreshes, and this time consumer two is assigned 0 to 4, which consumer two could actually process. Consumer one had initially failed to process that set of data. So when consumer two finishes processing the  data, the output comes as 0 to 4 in the output order you can see on the right hand side of the screen.  If you notice it, the whole order of the data coming out of the application is in an unordered state. There's no ordering on the overall batch of records. However, if you look at each consumer batch, the data is ordered within that batch. You can see 5 to 8 are ordered and 0 to 4 is ordered within each batch. 

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1110.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1120.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1120)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1150.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1150)

Across multiple batches, records may arrive out of order, especially if they are retried. The model is mostly designed for scalability and not for strict sequencing. How are the offsets managed? Currently you can see the start offset is at 1 and the end offset is at 6. We have a consumer that reads data from  1 to 3 inclusive, and once this consumer has processed the data and acknowledged it back, the start offset moves to 4 and  end offset moves to 9. Records 1 to 3 are marked as archived. The start offset and the end offset form a sliding window where the start offset moves when the records are acknowledged, whereas the upper bound of the end offset is determined based on the configuration we talked about, the group.share.partition.max.record.locks, which basically says how many records can be locked at any given point in time. 

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1170.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1170)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1180.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1180)

By default, the start offset starts with the latest offset. However, we can also use the admin tool to point it back to the earliest offset so that we can reprocess the data.  Now let's look into how share groups work with Kafka log retention and compaction. Log retention in Kafka says that how long the data will be stored within the Kafka brokers before either the time is elapsed or the size exceeds a particular configured setting. For log retention, the start offset is  bounded by the log start offset, which is basically managed by the retention policy in Kafka.

If the log segments are being retained based on time and the inactive segments of the log exceed the configured time, the log start offset moves to the next log segment, and the old segment is going to be deleted. If the share partition start offset is pointing to a deleted segment, it will automatically advance to the next log segment. This behavior is very similar to how other queue systems handle message-based expiration. The behavior looks exactly the same if you still use retention based on size.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1270.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1270)

Next is log compaction.  Log compaction is Kafka's way to keep only the latest version of each key in the topic, rather than deleting all the data from the topic itself after the time or size threshold is exceeded. Share groups can read data from a compacted topic, and there is no problem with that. In most real-world scenarios, your consumers can read data from the compacted topic and the compaction process will not have any kind of problem because the compaction process runs periodically and by the time the compaction process runs, the consumer has processed all the data.

However, there could be a scenario where a record is cleaned while the consumer is reading from it. The consumer will finish reading the data and process it without any issue, but the next time when the consumer asks for a new set of data, it will see the offset gap. This process is exactly the same as how it works with the consumer group model. You might get batches with missing offsets, which is totally fine, but these are holes in the numbers rather than any data loss.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1350.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1350)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1370.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1370)

 So how do you handle bad records? Share groups can deal with bad records so that consumer groups can keep processing without being stalled. There are three different scenarios for how these bad records are handled. First, if a record is delivered  successfully but fails during processing, you get an application or a transient error. The consumer can explicitly acknowledge the record and mark it as released if the error is transient in nature, or it can reject it if the error is permanent.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1390.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1390)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1420.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1420)

 Second, if the record cannot be deserialized, it will throw a RecordDeserializationException. Kafka will automatically release the record and allow the processing to continue. However, consumers can provide an explicit acknowledge call if required. Third, if the record batch fails the cyclic redundancy check,  it throws a corrupt record exception. This happens when the record is corrupted at the byte level. Kafka at this point rejects the entire batch and ensures no other consumers pick up the same batch, ensuring that it can move forward.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1470.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1470)

### Real-World Use Cases: Task Workers, Microservice Job Queues, and Asynchronous Request Execution

Share groups provide this kind of built-in mechanism so that consumers can avoid stalling on problematic records and still provide good throughput and resilience. Now that we have talked about the queue capabilities for Kafka, let's look into some of the use cases for why this feature would be relevant.  There are three types of patterns where this particular capability will be useful. The first is a task worker processing pattern. This pattern is a classic worker pool pattern where every message is a discrete piece of work. This can be an image transformation, an ML inference job, or some sort of data enrichment job.

Let's take an example of image and video processing. When a user uploads an image using the UI, rather than the application processing the image, it simply puts a message in the Kafka topic saying that the image needs to be processed.

Another microservice picks up that particular image file and then does some transformation. It can do resizing, transcoding, generating thumbnails, and so on. If that worker crashes in the middle of the operation, the same task could also be distributed to other worker pools, making sure no image file gets stuck in the whole process. Therefore, it decouples the upload latency of the image from the heavy message processing and enables you to scale the application horizontally.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1550.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1550)

The second use case is microservice job queues. These are typical asynchronous  workloads inside microservices like sending emails, generating PDFs, or billing and invoice generation. These workloads don't require any strict auditing of data, but they rely on parallelism and retries. Let's take an example of an email and notification service. Instead of sending emails from the main microservice application, it just writes to the Kafka topic with a message saying that the email needs to be sent. The notification microservice picks it up, formats the email, sends the email via SMTP or SES, and acknowledges the process.

If that notification service fails to send an email, perhaps due to some network delay, rate limiting, or other issues, another service can pick it up and process it. This ensures a very fault-tolerant email delivery process without slowing down the user-facing API. The third use case is asynchronous request execution. This category covers operations that should run asynchronously while still keeping the application responsive in nature. You do not need to rely on the ordering part, but the focus is given on per-message acknowledgment and worker rebalancing.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1650.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1650)

Let's take an example of fraud detection during checkout. Rather than  blocking the whole UI of the user while running the fraud detection models, it emits a fraud check request to a Kafka topic. A fraud detection microservice picks it up, uses an ML inference model or any kind of rule-based approach to figure out if that particular transaction is fraudulent or not, and then publishes the result. Here also, if that particular microservice dies, the task is assigned to some other workers. Therefore, it eliminates the latency from the checkout path and ensures that the process remains fault-tolerant.

With that, I'll hand it over to my colleague Sam to take you to the next part of the presentation. Thank you. Hello, everyone. My name is Masudur Rahaman Sayem. You can call me Sam as well. I'm a streaming data architect. I've been working with Amazon for more than 10 years, heavily working on distributed systems like Kafka.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1720.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1720)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1730.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1730)

### Deep Dive into Implementation: Internal Architecture, Failure Scenarios, and Configuration

In the second half of this discussion, I will dive deep into the internal implementation of Kafka, specifically Kafka Share Groups.  For traditional Kafka consumers, there's an internal topic called the consumer offset topic for consumer state management. However, with this new capability in Kafka Share Groups, there is a new internal topic called the Share Group Offset topic.  This is a highly partitioned topic which is 50 by default. It uses a cleanup policy of delete and is not a compacted topic. The share coordinator periodically deletes records to optimize the space of this topic.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1790.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1790)

When a share coordinator becomes a leader of this internal topic and leader of the shared group partition, it can scan the share group state, build the in-memory state, and then start serving requests.  Like Kafka Consumer Groups, we also have the concept of partition leaders. When producer and consumer requests come in, partition leaders usually handle those requests. When I say usually, it's because from the consumer side, you can specify that you want to read from the nearest replica as well with the settings called the nearest replica feature.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1860.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1870.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1870)

In KIP-932, there is a new component introduced called the shared partition leader. The shared partition leader is a component in the broker that resides with the topic partition leader, which means it follows the leadership where your topic partition is on the same broker. The shared partition leader manages the shared group state, interacts with the topic partition, and fetches the record from the local replica where the topic partition leader is. It also tracks the message state, whether it is available , which means it is ready to send to a consumer, whether it is acquired , which means the consumer has acquired the record, or acknowledged, which means the consumer has processed that record. It can also be in an archived state, which means it has been delivered multiple times and has reached the delivery count limit, so it will not be sent to another consumer again.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1900.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1900)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1920.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1920)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1930.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1930)

The shared partition leader manages the record state. Here is how the delivery semantic flow works.  A consumer pulls the record, and when the share coordinator sends the record to the broker, the broker marks the record as a locked record, marking it as an in-flight record that has been given to a consumer. The consumer then processes the record.  If the consumer is acknowledging it, that is a successful acknowledgment, and the broker will mark the record as acknowledged.  During that time, there could be a scenario where there is a transient failure or a long duration expiration because the consumer has timed out. In that case, the message will become available again. There could also be a scenario where there is a problematic message or a poison message. In that case, Kafka keeps track of how many times it has been sent to a consumer, and there is a delivery count limit as well. Based on that, it will decide whether to send it again or not. This approach balances reliability and resiliency, guaranteeing that each record will be delivered at least once, no matter whether something is failing along the way.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/1980.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=1980)

Now let's look at the lock renewal semantic.  What I mean by lock is that the record has been locked for a specific consumer and is not available to any other consumer. When the consumer fetches a record, the broker applies the acquisition lock for that record, which prevents other consumers from reading the same message at the same time. Once it is locked, the record is held for a certain period of time, typically 30 seconds by default. During that time, the consumer has to process that record and send the acknowledgment back to the broker. If it is a successful acknowledgment, the broker marks it as an acknowledged record. The consumer can also decide to release the record, in which case it will be available for another consumer as well.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2050.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2050)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2070.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2080.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2080)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2100.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2100)

However, there could be a scenario where the consumer does not respond during that lock duration time.  In that case, the broker will release the lock automatically. When this happens, the record state changes back to available, allowing it for redelivery to another consumer. That is how the lock renewal semantic works. Things can fail, and there will be situations when your consumer is failing or your broker is failing as well. Let's understand what happens when a consumer crashes.  When a consumer crashes, the broker is unaware of that consumer crash and has no clue. This means the lock remains active during that time as well.  There could be a situation where a timeout is happening from the consumer. In that case, the group coordinator will remove the member from the group when the consumer stops sending heartbeats. There is a heartbeat interval setting on the consumer side, and during that time, the consumer needs to send a heartbeat to the group coordinator.  If the consumer is failing for a long time, the broker will automatically release the lock based on the lock duration settings.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2110.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2110)

Also, a consumer in a share group can  use a heartbeat mechanism to join or declare that it's still available, or it can leave from the group as well. When a consumer failure happens, it could be a transient failure, which means the consumer can rejoin again and start processing the records during that time.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2140.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2140)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2160.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2160)

How about broker failure?  As you know, the share coordinator is responsible for managing the state.  The state of the in-flight records persists on an internal topic, which is the share group state topic, as I mentioned earlier. If the broker acting as a coordinator fails, a new election will happen, similar to how a leader election occurs. This ensures that if the broker restarts or fails, another broker can take over the responsibility of the coordinator and continue the work.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2190.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2190)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2210.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2210)

During that time, the new coordinator will read from the highly replicated internal topic. Of course, during that time there will be a process called state recovery.  The new leader will read the state from that internal topic and build the memory state to understand which records are in which state and which consumer has them. Once it finishes building that state, it can serve the requests.  While there might be a minor pause of data processing during the leader election, the system can guarantee there will be no data loss.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2240.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2240)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2260.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2270.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2270)

Let's now take a look at a code example to understand how the API works. In this example, I first create a properties object to configure my consumer.  I set the bootstrap servers to localhost where I'm running the Kafka cluster on my local machine as an example. Then I give a group ID, which is my share group ID. Next, I create a KafkaShareConsumer specifying a string deserializer for my key and the value.  Then I subscribe to my Kafka topic.  The topic name is foo. Inside the infinite loop, I call the consumer.poll method, specifying a duration of 100 milliseconds. This means the consumer will wait for at least 100 milliseconds if there are no records available during that time.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2320.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2320)

Then I run an infinite loop to process my records the way I want to process them. I apply some business logic as well. In this example, I'm also specifying the way I want to acknowledge it. You can explicitly acknowledge every record when you are processing it. For example, whether you want to accept it, release it, or reject it. Once I finish processing that record, I commit the entire batch by calling the commit.sync method.  You can also call the commit async method, which means the commit will happen in the background without blocking the process.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2350.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2350)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2360.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2370.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2370)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2390.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2390)

In terms of security and ACL, there is no change actually. The way you manage Kafka ACL remains the same.  For example, if you're providing some granular access, you continue to do the same.  In that case, keep in mind that if you're providing granular access, the broker needs read and write access to that internal topic called the shared group state topic, so you have to provide that as well.  In KIP-932, there are some new API operations that have been added.  If you are providing explicit granular access, you need to know which operations require which type of access. You can find those in the Kafka documentation as well.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2400.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2400)

There are some additional metrics that have been added for the broker and the client. I will highlight some of them.  For example, for the broker, you can monitor the group status through various metrics.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2410.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2420.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2420)

The group count, group count by state, and shared group count metrics are available for monitoring.  You can also monitor the rebalancing rate and rebalancing count to track the activity between the broker and the internal topic.  To track partition load time, you can measure how long it takes to load the partition from the internal topic and what the latency is between the broker and that internal topic.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2440.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2450.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2450)

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2460.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2460)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2470.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2470)

For client monitoring, there are additional metrics available.  To check consumer health, you can monitor the heartbeat rate and the last poll seconds ago metric. The heartbeat rate will show the average number of heartbeats per second.  To monitor client throughput, metrics like records fetched rate and fetch size are exposed.  To measure consumer latency, you can monitor fetch latency and heartbeat response time metrics. 

Overall, with this new capability, Kafka scaling is no longer tightly bound to the number of partitions. You can have more consumers than the number of partitions. From the consumption side, capacity planning will focus on whether you have enough compute capacity on the broker and on the client, rather than just overprovisioning your topic. This means peak load handling, especially from the consumption side, will be much easier without doing partition assignment in Kafka.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2520.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2520)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2540.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2550.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2550)

As mentioned earlier about some of the key characteristics when using regular consumers and Kafka consumers, I will highlight that again.  You can run regular Kafka consumers and share group consumers on the same cluster, but they reside in the same namespace. This means that when specifying a group ID, that group ID needs to be unique.  You cannot use the same group ID for different types, such as regular consumers and share group consumers.  The group ID set by the first client cannot be changed later, so a clear naming convention will be very important to enforce the group type and prevent any conflicts in your environment.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2570.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2570)

As of now, the share group capability is disabled by default.  You have to explicitly enable it by using the configuration called unstable API version enable set to true, and then you need to add a new rebalancing protocol in your configuration called share before you run your Kafka. All of these need to be added in the server property file before you start your Kafka process.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2600.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2600)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2620.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2620)

### Live Demo and Conclusion: Testing Share Groups and Future Roadmap

Let's now switch to the demo to show you how this capability works.  I'm running Kafka on my local machine in Docker, so just to show you the list of available topics on my cluster, I have four topics here.  As you can see, there are two internal topics: one is the consumer offset topic, and another one is for the share group state topic. In the demo, what I will do is run a producer and then run three consumers. Before doing that, I will create a topic with only two partitions.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2670.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2670)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2690.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2700.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2700)

As we have discussed so far, if we create a topic with two partitions and I have three consumers, with regular consumers, we will see that only two consumers are reading and one consumer is sitting idle. But with share group consumers, we will see that all three consumers can read from these two partitions. Let's see if that happens. Let me create a new topic with two partitions only.  I've created the topic, and what I will do right now is start my producer. As we have discussed, there is no change on the producer side.  If you see my example code, I'm just using the Kafka Python library to build my producer application.  There is no change here.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2710.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2710)

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2720.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2720)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2730.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2730)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2740.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2740)

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2750.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2750)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2760.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2760)

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2770.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2780.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2780)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2790.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2800.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2810.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2810)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2820.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2820)

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2840.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2850.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2850)

Let me start my producer. It will send some random data to this new topic.  Awesome. So it is now sending random data. What I will do right now is I will start my consumer as well, and this will be the share group consumer.  And you see, I have consumer 1, consumer 2, and consumer 3. I will start my consumer in every console.  It has started. The second one should start processing as well. I can see it is also processing records.  And based on our discussion, consumer 3 should also start processing records. And you see in this example, consumer 2 and consumer 3 are  reading from the same partition. See partition 0, and this is also reading from partition 0.  That is the whole point of using this capability in Kafka Share Group.  So let me now run a regular Kafka consumer and see how it works with a regular Kafka consumer.   So again, I'm running a regular Kafka consumer on consumer one.  It is reading, and I started my consumer 2 as well. It should start to read as well. Nice.  And I will start my consumer 3 and see what happens. Somebody should say that I cannot read any longer. So you see, consumer 2 has stopped  reading. There is no message coming in consumer 2 any longer. It is just consumer 1 and consumer 3, because in a regular consumer, as you know, it is a one to one mapping, which Shubham has shown you earlier as well. So that is exactly the case we see. Awesome.  Let me stop my producer and consumer, and let us switch back to the presentation. 

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2860.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2860)

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2870.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2870)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2880.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2880)

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2900.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2900)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2910.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2910)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2920.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2920)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2940.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2940)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2960.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2960)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/2980.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=2980)

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/3020.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=3020)

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c9c520f3fc121d2b/3030.jpg)](https://www.youtube.com/watch?v=zbN1VeenxY4&t=3030)

So though KIP-932 added some interesting queue-like capability as well, you might not use it yet.  There are some reasons for it. The feature is still in preview in Kafka 4.1 version, and of course this is a preview version. It is not recommended to run in production yet.  Dead letter queues are still not supported. And you cannot monitor the share group lag when you are using a share group consumer.  And if you have a workload where you need exactly once processing and also you need ordering guarantee of your records, then this capability is not for you. Also, here are some roadmap items to overcome some of the limitations we have discussed earlier. The target is making it production-ready in Kafka 4.2 version.  And with KIP-1191, there is still discussion. There is a discussion to introduce dead letter queues for shared groups as well.  With KIP-1206, it will allow strict record fetch so that if you have a slow consumer, you can avoid overloading that consumer.  With KIP-1222, you can have a configuration where a consumer can extend the acquisition lock. So when there is a slow consumer, instead of redelivering the message again and again, the consumer can extend that acquisition lock with that.  With KIP-1226, it introduced monitoring the shared group lag.  And with KIP-1240, it introduced some additional group level settings for shared groups that give finer control like delivery limit or lock time without changing cluster-wise config.  And for those who are not aware, Amazon MSK provides a fully managed Kafka offering, and if you want to test your queue capability, you can also try it with Amazon MSK as well on version 4.1. Awesome. So here is the key takeaway. KIP-932 is essentially queuing done in the Kafka way. No more overpartitioning, built on the same scalable backbone of Apache Kafka.  It breaks the one partition per consumer behavior, enabling parallel consumption for truly parallel workloads all within Kafka. It is not replacing regular Kafka consumer, but expanding what Kafka can be. If you want to learn more about this new capability called Kafka Share Group, I would highly recommend checking KIP-932 using this URL.  And thank you for your time today. We hope this session on the new capability introduced in Kafka, Kafka Share Group, has been useful for you. Before you leave today, please share your feedback using the re:Invent mobile app and let us know how we did. Each year we try to create content that you find interesting. So give us five stars if you think we have made it today. And thank you again. Have a great rest of your day.


----

; This article is entirely auto-generated using Amazon Bedrock.
