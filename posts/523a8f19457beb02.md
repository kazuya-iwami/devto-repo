---
title: 'AWS re:Invent 2025 - iTTi''s Cross-Company Data Mesh Blueprint with Amazon SageMaker (ANT342)'
published: true
description: 'In this video, Diego Ortiz from AWS and Julieta Ansola from itti discuss how itti Grupo VÃ¡zquez built a hybrid data mesh architecture for their multi-industry holding in Paraguay managing 300+ terabytes across 20 companies. They explain their evolution from centralized to federated architecture using Dream Corp platform, migrating from traditional EMR clusters to EMR on EKS with Karpenter, achieving 80% utilization (up from 35%) and 45% cost reduction. The implementation of Amazon SageMaker Unified Studio and SageMaker Catalog enabled 75% faster onboarding and reduced data access from weeks to instant through Lake Formation tags, replacing ticket-based approvals with declarative governance.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/0.jpg'
series: ''
canonical_url: null
id: 3093335
date: '2025-12-08T22:56:07Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - iTTi's Cross-Company Data Mesh Blueprint with Amazon SageMaker (ANT342)**

> In this video, Diego Ortiz from AWS and Julieta Ansola from itti discuss how itti Grupo VÃ¡zquez built a hybrid data mesh architecture for their multi-industry holding in Paraguay managing 300+ terabytes across 20 companies. They explain their evolution from centralized to federated architecture using Dream Corp platform, migrating from traditional EMR clusters to EMR on EKS with Karpenter, achieving 80% utilization (up from 35%) and 45% cost reduction. The implementation of Amazon SageMaker Unified Studio and SageMaker Catalog enabled 75% faster onboarding and reduced data access from weeks to instant through Lake Formation tags, replacing ticket-based approvals with declarative governance.

{% youtube https://www.youtube.com/watch?v=2t1X2Wp5qZM %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/0.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=0)

### From Data Silos to Hybrid Data Mesh: itti's Architectural Evolution with EMR on EKS

 Hello everyone, thank you for joining our session and welcome everybody. My name is Diego Ortiz. I'm a Senior Data Strategy Solutions Architect at AWS, and today I have the pleasure to be speaking with Julieta from itti. I'm Julieta Ansola, Senior Cloud Engineer at itti. So today we'll be talking about how itti, a company that Julieta will describe better later on, built a hybrid data mesh by using Amazon SageMaker. So let's get started.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/50.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=50)

Before we dive deep into how they built this solution, let me put a little bit of context here. Across the globe we've seen customers that are trying to become data-driven organizations, and in that process they also face several challenges.  Business areas will always try to use the data in the way they believe it is best for their business. However, when you have several of them doing the same thing in their own way, you can end up with a fragmented data landscape and data silos, and this can hinder data sharing and data innovation.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/70.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=70)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/80.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=80)

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/100.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=100)

 This can also pose security and compliance risks because there is no unified data governance strategy in place.  Another challenge that we've seen in our customers is that in these complex data environments there is no unified mechanism for discovery of data assets, so business areas end up looking for the central data governance team to provide them access so that they can discover and consume the data.  So the combination of all these challenges in the end makes the goal of becoming a data-driven organization a far-fetched one.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/120.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=120)

So here is where our customers have turned to see the data mesh concept as a solution for all these challenges.  But first of all, what is a data mesh? Well, this is a data architecture pattern where data domains are treated as one specific area of the business. So these business areas now are treated as independent domains and they can consume and be interconnected with the rest of the organization through a mesh.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/160.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=160)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/170.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/180.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=180)

A data mesh comes with also some benefits. For example, those business areas can also leverage their existing investments and integrate them into the mesh.  Another one is that it improves data governance by pushing all those policies down to the specific needs of those business areas.  It comes also with a business data catalog which is a centralized mechanism for data discovery and data sharing, and this in turn enhances  self-service data sharing among all those business areas.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/190.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=190)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/210.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=210)

In the end,  a data mesh allows those business areas to be empowered to use the data and also start innovating with it. So now let's see how itti built their own version of the data mesh on AWS.  At itti Grupo VÃ¡zquez, we operate a multi-industry holding in Paraguay with over 20 companies across five verticals, generating more than 300 terabytes of data across different systems.

Our challenge was clear from the beginning. How do we enable autonomous data product development while maintaining enterprise-grade governance? Each company moves at a different speed with different compliance frameworks and different innovation cycles. Some need real-time streaming analytics, and others demand audit-compliant batch processing with full audit trails. So traditional hub-and-spoke architecture wouldn't scale for us, but also implementing full mesh topology would explode our total cost of ownership. So we needed something different, a hybrid approach.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/270.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=270)

So let's examine the architectural trade-offs.  In a centralized pattern like this, all producer and consumer teams roll through a central data platform team. This data platform team manages all the infrastructure, and while this creates a secure environment for data governance, it also creates a single point of failure in the middle of the pattern. Our sprint velocity was many times blocked by data platform team dependencies.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/330.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=330)

When decentralizing this pattern with shared data sharing across different domains, we face immediate challenges like data duplication ratios, inconsistent schema evolution, and of course overhead in infrastructure. Our solution was adapting Dream Corp, our self-service  ingestion data platform built entirely on AWS. It started as a centralized platform but evolved into a federated architecture. We also maintain our production layer centralized using EMR on EKS. We migrated from self-managed clusters to EMR on EKS, and we are adopting SageMaker Unified Studio for our decentralized consumption layer for democratized ML analytics capabilities. This approach allowed us to transform our operational model without affecting all 20 companies across the company.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/380.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=380)

So this is our taxonomy.  We created this diagram encoded in our IAM policies and network topology. We have three main domains. The central data domain is our nervous system, and we also have a digital ecosystem domain and a financial ecosystem domain. These domains are isolated through dedicated AWS accounts, and they have their own street prefixes in our central data lake living in the central data domain. We use Lake Formation for sharing data across different accounts. That means, for example, if someone from the payment team needs data from the central domain, they simply subscribe to it in the business domain catalog. This allows us to implement autonomy without anarchy.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/440.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=440)

 On the technical side, we moved from traditional EMR clusters to EMR on EKS for flexibility. In our before state, we ran one massive persistent EMR cluster provisioned for Spark job submission, which happened probably four hours a day. So we had a 35% average utilization, meaning we were paying for 100% of the infrastructure to use barely a third of it. Talking about job isolation, it was process level only, meaning one runaway Spark job would bring down others. Also, our painful point was scaling time. In the traditional cluster, scaling time took around 15 to 20 minutes to join the cluster.

Now each business domain runs in isolated EKS namespaces. Job isolation is enforced at two levels now. EKS namespaces provide hard boundaries, and pod level security provides secure workloads. EMR maps one to one with these business domains with these namespaces isolating them. Also, we implemented Spot instances for reduced cost. We use 70% Spot instances and 30% On-Demand instances for optimized compute. And also we are migrating some instances to Graviton 3 for memory intensive workloads. But our game changer was Karpenter. Karpenter provisions in under 60 seconds, so we achieved the elasticity that we needed.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/560.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/570.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=570)

 Talking about numbers, we went from 35% average utilization in our old cluster to 80% utilization in  our current cluster. That means 45% cost reduction using shared infrastructure and optimized compute. Also, through Karpenter, we can handle traffic spikes in near real time with pre-warmed node pools for predictable workloads. So at the bottom line, we are processing the same data at half the cost. It's a huge improvement for us. So this is our production layer.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/630.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=630)

### Accelerating Data Consumption with Amazon SageMaker Unified Studio

Before we continue with the consumption layer, I'd like Diego to give some context on SageMaker. Thank you, Julieta. So Amazon SageMaker is the center for your data analytics and AI on AWS. It comes with three main layers. The first one on the top is the Unified Studio, which is basically an interface where you can access different types of data functionality  from SQL analytics, data processing, machine learning model development, and even generative AI application development. We also have two other layers here, the data and AI governance, which we will dive into later on, and the open layout.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/660.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=660)

So let's dive deep into this layer about data governance. There is one key component here which is the SageMaker Catalog,  and this is a business catalog. It's a central repository for both technical and business metadata for different types of data products ranging from traditional datasets to machine learning models, generative AI applications, and so on. The service comes with a set of integrations to have full visibility about your data products. For example, if you want to see the data quality of those data products, it comes integrated into the catalog. You can also use metadata forms and business glossaries to classify your data products, and you can also see the full data lineage of all those data products from end to end. There is a bunch of different functionalities that are being added to this catalog too.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/730.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=730)

So now let's see how itti built on top of this SageMaker Catalog to achieve their data mesh platform. Okay, as you say, we see how we actually implemented this.  If we say EMR on EKS gives us elasticity, we can say SageMaker Unified Studio gave us velocity. We kept Dream Corp as our backbone, the place where data pipelines, ingestion, and transformation happens, but we needed something for our business users and data teams, a unified way to access and exploit that data that they produce. So that is where SageMaker Unified Studio became the missing piece in our hybrid mesh.

I say SageMaker Unified Studio gave us velocity. Our users from a single entry point now can use their services for exploiting analytics tools, and they can run simple Athena queries. They can spin up SageMaker notebooks and even build better-governed apps using foundation models and knowledge bases and publish in the SageMaker Lakehouse Catalog, which each project from different domains. SageMaker Lakehouse Catalog organizes the different assets under the right governance. And also we implemented Lake Formation tags. We replaced our old ticket-based approval for Lake Formation tags. We created this taxonomy in our central data catalog living in the central account and shared through different services like RAM in AWS, so our users can access all their machine learning, analytics, and generative AI tools from a single entry portal.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/850.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=850)

So I say SageMaker Unified Studio gave us velocity.  This is the number, 75% faster onboarding for our new hires. Before, when probably you know the things when someone joins the teams, we needed to open tickets for VPN access, package installation, security, and so on. But now in just five minutes they can spin up their environment through the blueprint in SageMaker Unified Studio. We created this template with CloudFormation, and they simply create an environment, click create notebook, and start coding, not waiting for IT. Also, data access, probably our biggest bottleneck in the past, went from weeks to instant.

Taxonomy and data federation tags provide instant access. For example, if you are on the payment team and the data is tagged as payment, you can start querying immediately with no waiting. You can simply start using it. So I will share some lessons learned in this journey.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/930.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=930)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/950.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=950)

The first one,  governance is not optional for sure, but it must be declarative, not imperative. We killed our old ticket-based approval process and replaced it with data federation tags, and as a result we achieved governance at scale with autonomy without human bottlenecks.  The second one, balance is dynamic. There is no perfect formula for centralization versus decentralization, and that is okay. We started fully centralized because we needed to establish some standards, and then when the domains proven maturity, we released the control, and that worked for us.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/523a8f19457beb02/980.jpg)](https://www.youtube.com/watch?v=2t1X2Wp5qZM&t=980)

And the last one, probably one of the most important, culture  matters as much as technology. You can build the most efficient and even fancy architecture, but if your people don't trust it, it fails. So our advice is keep it simple and build adoption first. All right, this is all. If anyone has some questions, we are here. Please complete the survey. Thank you so much for listening.


----

; This article is entirely auto-generated using Amazon Bedrock.
