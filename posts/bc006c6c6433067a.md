---
title: 'AWS re:Invent 2025 - Migrating your VMware workloads with on-premises dependencies (HMC309)'
published: true
description: 'In this video, AWS presents strategies for migrating VMware workloads to AWS hybrid infrastructure, featuring Caesars Entertainment''s real-world implementation. The session covers AWS''s "Where You Need It" strategy using Outposts and Local Zones to address data sovereignty and on-premises requirements. Scott Rose introduces AWS Transform, an AI-powered tool that accelerates migration assessment and optimization by analyzing VMs, networking, and wave planning. Travis McVey from Caesars Entertainment shares their journey migrating 34+ jurisdictions from VMware to AWS Outposts Gen 2, achieving 3-4x performance improvements through 1:1 CPU allocation and containerized workloads on EKS. Key lessons include proper capacity slotting, backup power requirements, and service link resilience planning. The presentation emphasizes AWS migration funding programs and the importance of right-sizing workloads when transitioning from VMware''s overcommitted resources to Outposts'' dedicated compute model.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/0.jpg'
series: ''
canonical_url: null
id: 3093138
date: '2025-12-08T20:34:29Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Migrating your VMware workloads with on-premises dependencies (HMC309)**

> In this video, AWS presents strategies for migrating VMware workloads to AWS hybrid infrastructure, featuring Caesars Entertainment's real-world implementation. The session covers AWS's "Where You Need It" strategy using Outposts and Local Zones to address data sovereignty and on-premises requirements. Scott Rose introduces AWS Transform, an AI-powered tool that accelerates migration assessment and optimization by analyzing VMs, networking, and wave planning. Travis McVey from Caesars Entertainment shares their journey migrating 34+ jurisdictions from VMware to AWS Outposts Gen 2, achieving 3-4x performance improvements through 1:1 CPU allocation and containerized workloads on EKS. Key lessons include proper capacity slotting, backup power requirements, and service link resilience planning. The presentation emphasizes AWS migration funding programs and the importance of right-sizing workloads when transitioning from VMware's overcommitted resources to Outposts' dedicated compute model.

{% youtube https://www.youtube.com/watch?v=DRZXYhyzczw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/0.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=0)

### Introduction: The Challenge of On-Premises Infrastructure and Data Sovereignty

 Welcome everyone. This is the session AMC 309, Migrating your VMware workloads with on-premises dependencies. If you came here today to learn how AWS designed a set of services to help you migrate and modernize from VMware through the cloud at your own pace, you're in the right place. Today I'm sharing the stage with my good friend Scott Rose and our guest Travis McVey from Caesars Entertainment.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/50.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=50)

 All right, this is our agenda for today. And without any further ado, let's get into the good stuff.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/60.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=60)

 I've been in this situation before, and if you are a professional working on traditional infrastructure on-premises, you can probably relate to those challenges. And if you do, I can feel your pain. I've been there for several years and plenty of time, and I can tell you I know it's very, very hard and very difficult whenever we are talking about administering and running on-premises infrastructure.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/120.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=120)

I would like to ask you guys a question. Is there still value to run on-premises infrastructure? Is that still a good thing? Is there still a valid approach? Well, as it turns out, it's very, very difficult to actually get a good grasp  of the TCO of running on-premises infrastructure, particularly on some aspects like personnel, the cost of running infrastructure, the cost of the facilities, and the technical debt that we accumulate over time. It's very difficult to get a good estimate, and if you don't know how much it actually costs, it's very difficult to make informed decisions about whether it's still worthy to run on on-premises infrastructure or should I take a different path.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/160.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=160)

 Over time, we have this compound effect of the business drivers and the technical drivers to actually push the idea of migrating our infrastructure through the cloud. And with that compound force, we can actually make an informed decision on should I go or not, is it worthwhile, should I go all in, what is the best strategy for me to actually go and start this journey.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/220.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=220)

Whenever you think of this, of course, there are some challenges that you encounter, and of course that will come to your mind. The first one is how long does it take to migrate?  Is it easy? Should I do it at a fast pace or should I take it slow? And if I take it slow, how can I demonstrate that I am generating value out of it? How can I demonstrate to our leadership that we are already taking the low-hanging fruit and that we are making good progress and this is good?

The next challenge is that lately we are having an ever-growing pressure about sovereignty. We still have to keep our data on-premises. We have regulatory requirements. We have government mandates, and they say I want to control the facility where my data lives. This is one more argument because there's a huge chance that you don't have a region nearby and you cannot say the region is in my country or eventually have a local zone in your country. So you need a strategy whenever you are being audited by one of these government bodies.

That strategy is to say, "How are you handling my data sovereignty requirements?" Not to mention, and I bet that you heard a lot these last days about Generative AI. A lot of customers that I deal with are always asking us how can they run a Gen AI augmented application on-premises. How can they run inference on-premises from their data sets and still meet their data sovereignty requirements? These challenges are imposing great pressure on the movement to migrate from on-premises to the cloud.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/380.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=380)

### AWS Where You Need It: A Migration Strategy from On-Premises to Cloud

And now you're probably thinking, how can AWS help me with that intent? AWS has designed a strategy  to help our customers migrate at their own pace from on-premises to, ideally, the cloud region. But if not possible at first, we can give you steps along the way. We can give you a journey so you can migrate, meet your requirements, and tackle those challenges that we just saw. You can quickly demonstrate that you are leaving that traditional world and then you are stepping into the cloud. This strategy is what we call AWS Where You Need It, and it's a set of services that allow you to migrate from your on-premises environment to either AWS Outposts, which is very, very close to you.

AWS Outposts can be placed in your data center or in a colocation facility of your choice, and that's the closest that we can get with AWS services. Or you can take the step of migrating to a Local Zone. A Local Zone is our infrastructure placed in metropolitan areas, selected metropolitan areas, and it's also a valid path for you to address sovereignty requirements. AWS can guarantee to you that our infrastructure is running in that metropolitan area, and this might be enough for a lot of governments and entities that require data residency.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/550.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=550)

With that, we can create a journey. You should always be aiming to get to the region whenever possible, but if not, we can give you steps along the way where you are effectively running on AWS, on a cloud infrastructure with cloud assets that are future-proof and that can pave the way for you to finally migrate as much as possible from your on-premises environment to the cloud.  There are several ways for you, several paths that you can take to actually embark on this journey.

Of course, the most common one is the lift and shift approach, in which our customers want to just show very quick results. They just take the workloads as they are and migrate either to Outposts or Local Zones, and this is totally fine. This is absolutely fine. What you could do is, along the way, try to modernize and try to shift gears a little bit, experiencing some of our managed services like managed databases and managed containers.

You don't need to install your database on top of EC2 anymore. You don't need to manage your Kubernetes or your Docker. You can leverage our offerings that will certainly alleviate what we call the undifferentiated heavy lifting, which is those activities that are fundamental but do not bring too much added value. Whenever you are choosing which route to take, it's important to factor in the timelines of your migration. What kind of requirements do you have? If you are running out of contract with facilities, if you have outdated hardware, if you are expiring contracts and warranties, this is really important for you to decide which one to take. But the best course of action is to take action now. Don't wait. That's what we are here for, to help you finally make that decision, and we are here to help you make that happen.

### Migration Funding Programs and Commercial Considerations

I'll hand it over to you, Scott, my good friend. All right, thanks so much, Frankie. Hi folks, my name is Scott Rose, colleague with Frankie. I've been with AWS for a little over five years, and I'm really excited to be speaking about how we've helped many customers such as yourselves assess, optimize, and migrate their workloads over to AWS, and specifically how to do that into our hybrid edge portfolio on-premises, Outposts and Local Zones. We've been doing it for the better part of five years, almost since Outposts was originally launched, and we're really excited to have Travis here from Caesars to talk about his journey. Caesars was one of the original customers that deployed Outposts, so you'll have some great context coming from him as well.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/730.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=730)

 Now here at AWS, and you guys have all been in a bunch of sessions, we throw a lot of tech at you, and we're going to do that here in a moment as well. But we also want to talk about the commercials because when we deal with migration situations, we're typically dealing with environments that are ten, twenty, sometimes thirty years old. We're starting to see some interesting licensing cost changes. Obviously VMware is one of them, but also the cost in operating and maintaining extremely old legacy environments such as mainframe environments has become extremely difficult for these organizations to continue to fund and continue to budget operations that were designed essentially decades ago, not to mention most of the expertise for those workloads themselves are just nowhere near optimized for current product and application requirements. So we're seeing the cost elements become a very important consideration, of course, in addition to the technical one.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/790.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=790)

 So with that said, AWS over the last six to nine months has really spent a lot of time helping out the funding situation, the migration compensation situation I'll call it, in that we want to make sure when you're analyzing this, don't worry about any kind of duplicate licensing costs, double bubble type situations. We will have a migration program and a suitable funding program to help alleviate most if not all of those costs. Both Frankie and myself have been involved in some fairly large customers, and when we calculated up the size of those funding programs, they are quite significant. So don't be afraid to bring those commercial concerns to us well in advance of the technical and the architectural analysis that obviously is going to be happening in parallel for your migration assessment.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/840.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=840)

 So there's a lot of ways to approach this from a financial standpoint. We can do this in the form of credits towards our ProServe organization, working to scope those projects again in a sequence and in a dollar amount that makes sense. We have a lot of partners that are involved in this as well. Most of our customers are engaged with multiple system integrators that have been contracted to do application management and operations for many years, so our funding programs are designed to also work directly from a commercial standpoint into those partners themselves. And then again, looking at this from a going forward standpoint, obviously AWS is cloud computing. It's based on a consumption model. A lot of your on-premises infrastructure might still be purchased and budgeted from a capital expenditure standpoint, so we try to bridge those two.

The net result is that if you're doing a migration program with AWS, we're looking forward from a standpoint of how that estate is going to look running on cloud native infrastructure, in this case cloud native infrastructure on-premises, such as with Outposts and Local Zones, and to negate out any kind of duplicate costs that you might be calculating.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/920.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=920)

### Migration Pathways: From Lift-and-Shift to Serverless Modernization

So you can be assured you can move forward, and it's a single layer cost, not multiple layers, because you're going from one platform, say VMware or an older set of infrastructure, over to AWS. As we move forward, Frankie had a bit of a complicated or just,  you know, fairly text-heavy slide on the various migration steps, and it really just consolidates down to this. It's a fairly simple staging or sequencing as far as how we want to look at your state, what we want to do from a standpoint of moving legacy applications or legacy VMs over onto the proper AWS footprint.

Obviously, the bottom of the layer is the very fast but also very useful EC2 lift and shift, and then simply moving up the scale. Some customers want to modernize those virtual applications, those virtual machines into containers. Maybe we want to move a step forward, add managed database services such as RDS, or take the final leap where we're completely refactoring some of those applications and going, say, with a fully serverless deployment. Of course, we can do all this. We've been doing all this for quite some time.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1000.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1000)

My experience with migration customers, and we've been doing many over the past three to four years and that speed has been picking up, it's not a one-stop shop. Many of these environments we're doing, say, a canvas of the environment first with the lift and shift, and then there's a fraction that gets containerized and a further fraction where we're deploying database managed services. So it's not just a one blob, it's several different layers of this cake. And then just to strike  the on-premises versus the regional element with Outposts, with Local Zones, we can typically take it all the way up into the managed database services, and then region is of course where you'll find our scalable serverless platform such as Lambda.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1020.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1020)

### AWS Outposts Gen 2: High-Density Infrastructure for Large-Scale VMware Migrations

So again, just a little bit of context and cut-through from the standpoint of the  slide that Frankie showed a bit earlier. Now, earlier last year we launched our second generation Outposts, and we're talking about this because a lot of our VMware migration opportunities are coming at us with hard requirements to stay on-premises. They want to deploy with cloud-native infrastructure, obviously leveraging AWS, but not into the region. It has to stay on-premises for a variety of reasons: data residency, latency, proximity to other on-premises assets, say, other storage or other compute assets. And so our infrastructure basically has to sit adjacent to those existing solutions.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1070.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1070)

That's what Outposts was specifically built for. The second generation Outposts came out mid-early last year, 2024, and we've been scaling it up over the last six to nine months. And this has been working out amazingly for our migration  opportunities, our customers with migration use cases. Why? Because many of those environments that we are analyzing, we're going in and we're seeing tens and tens of thousands of these CPUs, hundreds of terabytes, sometimes into the petabyte-type storage range. So the kind of scale that's needed to migrate those workloads over to an AWS Outposts footprint can be quite significant.

So with the new high compute density EC2 instances that were launched with Outposts Gen 2, our parallel networking substructure, which doesn't just help, say, high I/O data center applications and databases, but also some of our customers want to multiplex other workloads on them, network-intensive workloads. Outposts Gen 2 serves as a very solid footprint for both the standard IT applications coming over from VMware as well as perhaps some high-performance databases and other suitable applications and categories of applications, and those all can be multiplexed on the same rack infrastructure.

And so what we're talking about here is we're talking about customers who are ripping up extremely large fleets of legacy infrastructure. A lot of it running VMware, a lot of it running throughout multiple sites, and consolidating a lot of that down onto Outposts rack footprints, sometimes in the dozens and sometimes in the hundreds of rack size. But that footprint turns out to be a lot smaller and a lot more efficient than what that legacy infrastructure footprint looked like prior to getting on with the project itself.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1170.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1170)

So as I mentioned, the new instances that came with Outposts 2,  from a standpoint, most importantly from a standpoint of the CPU and the memory density, gave us very good metrics to go and calculate and create much more dense configurations than was currently deployed with the previous edge infrastructure. We also have a set of other instances in this Outposts, such as bare metal, which was designed for some of our very high-performance application customers, such as in our financial services segment as well as in our telco segment. And then coming soon, we'll have a next generation GPU instance, not just for graphics instances, of course.

This will be a GPU instance specifically designed for edge inference and perhaps even training as well, depending on how you're setting up your models. So this rack has been built future-proof from a standpoint of the type of workloads that our customers need for the next several years, and some of these are just in the design phases. So we want to make sure we're skating to where the puck needs to be from an aspect of your on-premises requirements.

And the migration situation, as we're specifically talking about in this session, has really pushed the limits of the next generation densities that are on these Outposts. And what's not even being shown here is in about a couple of months we'll have yet another wave of brand new, even higher performing instances that AWS will be launching. So stick around for details on that.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1260.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1260)

### Accelerating Assessment with AI: Introducing AWS Transform

Coming back to the migration and coming back to the VMware story in a second, what we've seen  with customers who are actively doing this today is that we've all been in these enterprise IT environments for quite some time throughout our careers, and you think we got it. You think that this should take maybe a couple of weeks to assess. It's not. This is taking months. Some of our customers are taking years because what we're discovering is a lot of these environments have basically sat there and they haven't been optimized. They haven't really been updated other than operating system and firmware, so the architecture doesn't make sense.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1310.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1310)

Like a specific example, we'll go into the VM disk manifest and we'll see VMs where there might be one CPU and 700 gigabytes, 760 gigabytes of memory, a very asymmetrical VM. And we try to find out what was that used for and do we need to find a suitable EC2 instance to house that, and no one in the organization seems to know where that comes  from. So when you do that at scale and you go through this with hundreds and thousands and tens of thousands of VMs, really the time to do all that really creeps up.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1320.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1320)

 So how are we solving that? How are we correcting that? How are we creating a better assess and optimize plan? Well, of course we have been analyzing this, and our solution is going to be a set of agentic-based optimizations and assessment tools to be able to go in, analyze the environment, and not just find out on a chunky basis how many CPUs, how much memory, how much storage the existing environment is. You probably already know that, to actually go in and determine based on the usage of these VMs, we think that this class of VMs deserves a higher priority or an advanced schedule of assessment versus some other VMs that haven't been used and have been idle for months. So that's one fairly easy optimization path that we can take with our AI-based tools.

The other key one is how do we analyze the networking elements. So we see a lot of customers that have not just the compute and storage but a fairly custom networking shell that exists around those VMware environments. What do we do with that? How do we take what might be a very flat Layer 2 environment and convert that over to a more scalable, routable Layer 3? Our AI tools will help analyze and assess the networking aspects, not just from the aggregation layer, but all the way down into the VVC layer which will interact with the EC2 instances.

And I think most importantly, again, if you have an environment where it's maybe a couple dozen or a couple hundred VMs, we can do that all fairly manually, old school spreadsheets. But if we're dealing with tens and tens of thousands of VMs across multiple sites, that's not all going to happen at once. So what becomes really handy is how do we do proper wave analysis? How do we analyze it and then determine how to reload into your environment in waves that make sense from a standpoint of the criticality of those applications being migrated from a VM over to EC2 or perhaps being containerized at the same time.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1450.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1450)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1490.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1490)

So all this is basically being done by AWS Transform. This is a new tool that we launched earlier this year and specifically  designed to basically analyze the variety of migration workloads, VMware, mainframe, also virtualized but non-VMware based legacy virtualized applications, and determine not just the infrastructure path but perhaps to also suggest and optimize the coding of the application itself. So as an example, taking mainframe legacy COBOL applications and translating those over into modern Java applications. So AWS Transform is both an infrastructure analyzer and modeler, but also a discrete  application optimization tool.

### Network Migration Automation and Key Benefits of AWS Transform

And there are several modules or several capabilities that Transform will take care of, and whether again we're specifically talking VMware or most of our customers when they come to us, it's first a VMware problem.

But then it changes into a multi-application problem including legacy non-virtualized databases, mainframe applications, very old Java applications, and so on. So there are multiple interfaces that Transform is going to help with, and we can go into our own breakout session specifically on this. I want to make sure we get to Travis's section, but just as an example, just this one segment around network conversion and migration.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1540.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1540)

This is some detail as far as how Transform would take care of  from an aspect of analyzing the source of virtual NSX switches, of course analyzing the VMDK files, as well as any third-party firewall or load balancers or application-centric infrastructure such as Cisco. It determines the appropriate new topology going forward and then creates the translated AWS network entities. In this case, the AWS network entities. Again, this is just the networking aspect. There's a similar compute and storage aspect. There's a wave analysis that goes into this, so it's a fairly comprehensive AI tool that helps take that multi-month migration problem statement and helps us accelerate that by at least two, if not three, orders of magnitude.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1590.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1590)

So obviously the key benefits that we can realize from this, one from the aspect of cost and time.  We analyze the environment not just from a standpoint of consolidation but optimization moving forward, with the additional variable of doing this on-premises. We know we can do this in the region. A lot of you probably do leverage resources in the region, but we are talking about doing all of this onto on-premises cloud-native infrastructure such as Outposts, such as Local Zones, that have slightly different nuances because that physical infrastructure is sitting inside your environment.

So we analyze your use costs from the standpoint of your existing infrastructure, your existing licensing, and try to figure out how we net all that out so that it's a single-layer cost structure from where you are right now to where you'll be with AWS. No parallel, no redundant, no double-bubble costs. We want to speed up the migrations as much as possible. AI is great, but by no means is it perfect. It's going to help us squeeze down a lot of the analysis time, but there's still going to be obviously a lot of manual intervention. But it's going to be orders of magnitude quicker than probably doing this from scratch or doing this in fairly old-school ways.

As we do this, we're going to bring AWS security principles to the picture as well. You hear time and time again that security is our top priority. So when we look at how we're interfacing into our infrastructure and into your infrastructure, we'll be bringing in those security primitives and principles in addition to the obvious migration activity itself. So security kind of comes along for free, but it's a very important industrialized component in all of our modern, well-practiced, well-architected migration projects.

And then innovate at scale. We of course work at scale. I mean, of course we can do this in small environments, but we've already engineered this to work across hundreds, thousands, hundreds of thousands of VMs, and that typically is the operating size we're seeing for most mid to large-size customers coming to us across multiple sites, sometimes continents in scale. This is something AWS has become obviously extremely good at, and we've learned the patterns to go from those small POCs up into those large production environments.

Okay, so I hope this kind of gives you a little bit of whetting the appetite as far as how we do it, how we approach it, and what we use it for. And now I'm very happy to turn the presentation over to Travis and hear about his journey with Caesars on Outposts. Thank you very much, Travis.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1750.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1750)

### Caesars Entertainment: Business Overview and VMware Migration Drivers

Thank you all for having me. My name is Travis McVey. I've been with Caesars Entertainment for four years in August. I'm actually part of the digital part of the business that runs the sportsbook and  racebook and iGaming sector, not so much the hotel and the palace that you will see here. But if you go into the sportsbook, all those kiosks, if you want to go place a bet, that is all managed by my team.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1770.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1770)

So first, let's go into just a little bit of background of what Caesars Entertainment is.  You know, we all know what Caesars Palace is, but we're a bit more than just the palace, right? So we're the largest casino entertainment company in the United States and one of the world's most diversified casino entertainment providers. We began in Reno, Nevada, actually, in 1937, and we've grown through development of new resorts, expansions, and acquisitions. You may or may not know this, but we actually operate several brands here in Vegas and other places such as the Horseshoe, the Linq, the Flamingo, and other major properties such as Eldorado, and of course Caesars and Harrah's.

We offer a full suite of mobile and online gaming and sports betting experiences, all tied to our industry-leading Caesars Rewards loyalty program. We focus on building value with our guests through a unique combination of these services, and we're committed to our employees, suppliers, communities, and the environment through our People Planet Play framework.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1840.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1850.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1850)

So just a couple of stats.  Yes, we do have over 50,000 employees.  Again, I was explaining some of the different iconic brands that we manage and what Caesars is doing, especially here in local Vegas. But let's dig into my part of the business, which is online gambling. In 2018, PASPA was repealed, and it's been kind of like a rocket ship, a gold mine, whatever you want to call it. It didn't exist before, and now it's a 20 billion plus annual industry. We've grown 10 times since 2018, and almost 60 percent, more than half the US, is allowed to gamble now, including Missouri, which just went live on Monday.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/1890.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=1890)

But that's not why you're here. We're here about migration,  VMware to Outposts, why we did it, what's involved, right? So let's get into some of the bad before we get into some of the good of what we were running. We had complexity. I don't know about all of you, but our dev and production didn't exactly match. It's very easy to just scale and build apps on AWS, but we have a very interesting problem that we have to solve that not a lot of industries have to solve, and that is the Wire Act.

In order for us to take a bet in the US, the bet actually has to take place in the state. You're like, well, how do you do that? Well, it's up to the legal rules of what that is, but effectively the transaction has to occur on-premises. So I can never move my betting engine off of on-premises. It has to stay in the state. And what's really funny is sometimes it's in the back of a casino because that's the only place that I'm actually allowed to do it.

Because of that, I have a VMware workload that's in there, and I have my devs testing in AWS, and then you get some weird things like the container won't even run because one of the, we're running EVC on some of these things, and the processor options, the CPU options that the app actually needs to run, isn't available, and you have to fix that, right? It's working everywhere else, and in this one state the deployment fails. So lots of complexity around the data residency and then not matching.

We also have the cost, right? So here we have maybe HP software, here we have Dell, here you have this vendor XYZ. VMware is giving you that abstraction layer, but you still have to upgrade the firmware on these. You still have to make sure that things are all working, and not to mention if you have some software that's managing the VMware build in itself, kind of in its own bubble, how to manage the patching of all of those environments. We have over 30 jurisdictions where we have these environments at, and to upgrade all of those vCenters is challenging. Another part is how do you get your golden image out to all these environments? It's not easy.

User experience. If you go and you place a bet at Caesars Palace or the LINQ, do you know whether it's running on Outposts or VMware? You don't care. You just want to make sure your bet goes through. That's a big thing that we had to understand. We have all this complexity, we're trying to manage all of this, but by the end of the day, what actual customer value are we adding for all this headache? But it's a good question, and we had to ask ourselves that. Do we want to go build it ourselves and do bare metal? Do we want to continue with VMware, or do we want to run Outposts and just kind of obfuscate all that away so that we can focus on higher customer value items?

And the last piece is performance. Yes, you can make your VMware environment performant, but I mean, just show of hands, how many people have seen someone that says, hey, I'm going to make my database faster by putting 60 CPUs on it? Yeah, that's not going to work. And what really happens with Outposts is it forces you to rethink your deployment strategy, and we'll get into that in a little bit more.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2100.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2100)

### Caesars' Technical Architecture: From CAPI on VMware to EKS on Outposts

So what did our  VMware architecture effectively look like, and why is it going to be easy for us to migrate things? Because I'm going to explain how fast we do these, and you're going to think I'm lying, so let's get that out of the way. I'm not lying.

All the things I'm going to tell you are true. What we were running was CAPI, which is the open source version of Kubernetes on VMware. Almost all of our workloads are containerized running in Kubernetes, and the only things outside of that were our database VMs, just because we can't get the recovery time and the recovery point objective that we need in Kubernetes. We're close. I want to get those things in there so bad, but we're just not quite there. And then we have the storage gateway so that we can manage backups and get them off site to meet our regulatory requirements.

So for us it was a pretty simple stack, but we continue to drive this way to make sure that the dev experience is the same even with the hardware being different. We wanted the devs to be able to create a container, to create an app, and deploy it whether it was in AWS or within VMware. And all the Kubernetes stories are true. As long as my team managed that infrastructure, it was the same. But again, when I'm talking about what the difference is, my team had to deal with all of that difference. We had to manage CAPI. We had to manage our upgrades. We had to make sure that the golden image was there and would boot. We had to manage all the firewall rules to make sure all the communication would work. And we love hammering our vCenter for all of our CSI volumes that Kubernetes is constantly doing. So I don't know if you've tried that, but it can take a while to move a cloud native disk, as they call it, from one VM to another when you're doing your upgrades.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2210.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2210)

So what lives on our AWS Outposts racks now?  What we've done is we've moved to where we have EC2 nodes and managed nodes for the different applications. So here we have our EC2 nodes for our database. If you look at the previous slide, we've just moved the VM databases over into EC2 node VMs. We then have our EKS on Outposts, so that now I'm not upgrading CAPI, I'm not testing CAPI, I'm not trying to make sure all those things are working, and I don't have this single VMware instance that's in my dev that I have one engineer that can iterate on slowly. Now we have AWS with EKS and I can rapidly make sure that all of our daemon sets and Helm charts and things are working.

We do have a couple of extra resources that we do because now we're wanting to use Route 53, more cloud native services, but you have to think of things in a little bit of a different way. Outposts is anchored to a region. Well, what happens if that connection to the region goes down? It's a very serious thing that you have to think about. It's a different way to think about because you are dependent on the region, even though the instances are there, you're still dependent on it. So we wanted to put in as much resilience as we could that we can serve stale cache when things are down because our IPs aren't really changing. We have a pretty static item. We just need to make sure ingress gets into the cluster and then everything works. So that's why we have our Route 53.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2320.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2320)

Managed EC2 instances still have the storage gateway, so you can see ours was, you could say lift and shift, but the thing that made it lift and shift for us was effectively Kubernetes. The stories of we want to be able to move clouds easily with our apps, we were able to actually test that theory and go forward with it. In this case it's private cloud to Outposts, but it really worked. You're like, wait, I didn't see any S3. I love S3.  You're right. S3 is going to add a ton of nodes to your Outposts rack. It's also going to add to the cost, so you've got to really think, is that what I want to do? And like I said, we have 34 plus jurisdictions that we're doing in, so every single time I'm adding cost, I've got to multiply that by 30, and you can see the number gets up there very, very quickly. So it's how can we be adaptive to what we need but also cost conscious for the CFO.

So what we've also done is tried to, the dependency of a single Outposts is that it's tied to an availability zone. So you have to think of an Outposts rack as an availability zone. It is not a region, it is a zone. I don't have an extra SLA. I don't have an extra failover. Yes, if a server fails, I have redundancy built into that. If a network switch fails, I have redundancy into that, but if the whole rack goes down or my service link goes down, I'm tied to that availability zone. And some of you will probably have the question, were you impacted by the US East One region? Yes, we were. We were very, very fortunate in mitigating it because of our design of utilizing a shared VPC for most of our services to route over the local gateway. If you looked at Frankie's earlier slides, he had one little bullet that said networking via the local gateway, and we've really pushed that to try and make our service endpoints operate over that infrastructure.

This way, if our service link goes down, that single point of failure, we can still route back out and take a bet, because that's our transaction and we want to be able to take a bet. We then put some managed nodes in the other availability zones to make sure that any kind of services that are running in those zones remain operational. So if that availability zone goes down, we can still make sure that traffic is flowing and updating, such as Route 53 and other services.

What we do is we split our Outpost. It does open the option that if any availability zone fails, we would be impacted, but if an availability zone is impacted, not all of our sites are affected. So it's a design decision. It could be different for your business model. This is what we decided. Work with your account managers and your solutions architects to really try to figure out what works for you, because this is not one size fits all. You have to figure out what works for your business, what your resiliency requirements are, and what you're willing to have.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2470.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2470)

Slotting, does anybody know what slotting is? Yeah, I have one hand. Okay, so  VMware has made this so simple, right? If you think about before VMware, what did you have to do? You would go and you'd figure out how many servers I could fit in a rack, how much power I have, thank you for all your power guidance back in my early career, Brad, and you could only fit so much in a rack. By that, you could only fit so many apps in that rack. You do go back to that a little bit with Outpost, and you have to really rethink it because what I could do is I could overcommit my VMware resources. So if you want a VM, yeah, sure, as long as I hit my 6 to 1 ratio and as long as my critical apps are right-sized enough, I'm okay.

Well, that's great, but now I have all of these VMs sitting that have all of these massive resource requirements that probably aren't real. The other thing that you have to think about too is that for us we had Kubernetes, so think about the number of schedulers we have. We have the base host which has CPU, you have the VMware scheduler, we then have the virtual machine which is also scheduling CPUs, I then have Kubernetes, and then I have my container. Wow.

Since I don't have the VMware orchestration layer now and I'm running right on the Outpost server, I'm getting a 1 to 1 CPU, so I'm no longer having that CPU swap, the CPU wait time, that's gone. The CPU wait, the CPU ready that you've been sitting and looking at your graphs forever and you're like, why is my VM slow? That doesn't exist, but you have to put the work up front. Spend time right-sizing your stuff, otherwise you're going to buy way too much hardware and you're going to be looking at what do I do with this extra hardware.

We're seeing 4 times performance because we're no longer swapping with our database nodes. Our database nodes were, again, the bet has to take place on-premises. Well, what does that mean? That means that I have to have a transaction. Transaction hits a database. If that's swapping in and out while it's trying to replicate and be highly available, and then now I don't need any of that swapping. What we did is we realized that we didn't need our nodes to be as big. So now, in order to distribute it across, I get these extra slots that now I have extra compute Kubernetes nodes that are very optimized for CPU, and now I can put different database technologies that before I was super scared to put into Kubernetes, and I'm ready to go.

So the last thing, like I said, is we need to get our other database nodes off of these dedicated instance slots. I can move those into Kubernetes, and I'm actually going to be able to flow my environment much like a DRS or an HA item, but using Kubernetes to manage that for me and not all the underlying VMware stuff I'm paying for.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2650.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2650)

### Migration Timeline and Results: Rapid Deployment Across Multiple States

So what was our journey like? My team moves and they move fast,  so we tried to figure out everything that we could. We designed it, we took the time up front to slot it, right-size it. We worked with the account team to get a test environment up so we could make sure that it worked, and then we put the pedal to the metal. So in July we had 4 racks installed and we migrated 2 states. In August we got 1 more rack installed, and we migrated 2 states. In September, we installed 5 more racks and we migrated a state every single week.

We got our downtime that I couldn't serve customers down to 2 hours. We then figured out new ways to do our database migrations over the logical gateways so that we could replicate things so that when our cutover happened, it was almost instantaneous. And then through, if you're familiar with anti-affinity rules, you have to think about how some of the services work. So for us,

we don't like a single point of failure, and we had to identify what services we wanted to use. One of the key pieces is you do lose the hyperplane on AWS Outposts. So I can't use a network load balancer in the rack. I can use one in the region and forward it, or I'm given an ALB option. Depending on your slotting, ALB may work very well for you. For us it didn't. We had to have so much capacity for the upgrades and the managed service of the ALB because it's designed to run in region, not an AWS Outpost.

So again, it's depending on your solution, but we decided that that wouldn't work for us because we have a single rack. The point I'm trying to make is that while we're migrating, we're learning and we're improving, and it's an important thing that you do as you do waves. We're doing states because it's the same app over and over again. But when you stop and look at most applications, whether it's a web tier, an app tier, a database, it has dependencies. Once you get good and once you figure out what the dependencies are, you're just rinse and repeating. You get on that rotation, and that's what's important to really recognize.

I'll keep going through the timeline. October we installed another 6 racks. November, unfortunately, December I was hoping that we would have 20. We're hoping to restart that backup, but we did improve our DR design which we're really excited about. Effectively we're able to use our regional availability zone designs and just replicate it now to AWS Outposts, which again, if you're thinking that way, cloud native and you're thinking about multi availability or multi-regional, that is just going to help you with your overall AWS Outposts design.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2820.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2820)

So we go back to the challenges that we talked about and it's like, okay, so  what was it? We had experience and complexity. I now have a single AMI and I can share it with all the AWS Outposts because it's in the same account and I can deploy it fast. I talked about the 1 to 1 CPU. Sometimes, depending on how we configure the app, we've gotten 3 times performance. I know I said 4 earlier. We're trying to constantly tweak things and make things better, but again, because I don't have to have that 6 to 1 ratio, I have a 1 to 1 ratio to really think about that. Does my database actually need 24 CPUs? If it was running on bare metal, how many CPUs would you give it? Probably not 24. I'd start with 12, see how that works.

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/2870.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=2870)

So really think about that and what it works and then think about the slotting because again, every slot is going to consume that and again with  the new gen twos you get a lot more scale, a lot more. These are still the gen ones, but it's important to think about. And then for us it was guaranteed pricing. I don't know how many of you are interacting with Broadcom, but we were very nervous when that conversation was starting. It didn't seem to matter the number of things we changed in the equation. The number always ended up being the same at the end. I don't know how that works, but we wanted to make sure that our pricing was guaranteed so that we could commit to the business that this is what our cost is going to be year over year and this is what it's going to be to add a new state. That was really important.

And I think the last piece that's really important is it's not just VMware. You have your storage vendors. How many of you go in, you love the storage, you love all that, it gets bought by another company, all of a sudden the price goes through the roof, or they are decommissioning that now you're having to migrate it. How much effort, how much time are your team spending to move storage devices, and what value did that bring to your customers? Zero. So you spent all that engineering time, you spent all of that capital, you spent all that operational cost, all the planning to move it so that there was zero downtime, you celebrated, it was so great, and your customers saw that much value. That's the key piece. How do I put that in terms that your C-suite understands? That's the selling piece, and that was what we really wanted to do.

### Lessons Learned: Slotting, Backup Power, and Resilience Testing

And the last piece is hardware. You'll get into a hardware vendor. You fully know how to support it. You'll buy their management software to handle all the firmware upgrades, and then they get bought out or they no longer support that. And now you're going to their version two, and there's no upgrade path. It's constant cost that again, Frank and Scott were talking about the hidden costs that you just don't really think about unless you kind of take a step back and you're like, hold on, what am I actually delivering to my customer?

Is my customer the developer? Is my customer the actual end user? That's what we really sat and thought about before we went and did all this. We could have done bare metal Kubernetes. I have full faith in my team. We could have done it very well, and on paper it would have looked cheaper. But then when I think about all the vendor swaps and all that, what value does it really provide?

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/3030.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=3030)

So I've kind of hit on several of these, and I feel like I'm going a  little fast, but lessons learned. Plan your deployment slotting. I know I keep bringing this up, but slotting is so important. It's a whole new paradigm shift, and every single managed service that you want will also consume a slot. So it was a great slide that showed that you could go to EC2s and then you could go to a managed RDS. Well, managed RDS is still going to consume a slot. You don't get it for free. That's going to be used. And if you want it redundant, that's now two slots that you've taken up. You want an ALB, that's going to take slots.

They also have to do upgrades. Well, in order to do an upgrade with no downtime, you have to deploy that third one, decommission the other one, right? So you have to have that room, that elastic capacity that's not necessarily there. And then the storage gateways, they all consume slots. When we first went in there, we didn't think about that. We just went and were like, yeah, this is what we need. Fortunately, we got the performance increase that gave us that additional capacity. But if you're not that lucky, it could really put you into a bad position when you're trying to set up, and you never want your first one to go bad, right? So please learn that lesson. Work with slotting, work with the teams to really test the things that you want and make sure that it'll fit.

Backup power, I think that's so simple, right? One thing that hit us is, like I said, we put things in the back sometimes of a casino. So we would have our VMware rack there, and if the power went out, which sometimes does because the casino doesn't necessarily have a production data center grade UPS that just automatically flips on, we'd power it back on. VMware would come on, we'd right-click yes, you're not a read-only file system, everything is good, next, next, next, and then we're back up, right? And it would take, you know, 15 minutes.

This is an entire cloud that has to boot. It has to connect to the region. It has to provision. It has to make sure that it does all of its security checks, has to set up all the metadata services that we take for granted. You know, the metadata service that sends who it is and its IP and everything, all of that has to power up and come online, and that takes hours. It's not minutes. So make sure you have either separate redundant power or you purchase the UPS and you put it there so that it will keep the rack on. I just please learn that lesson that I've gone through and had to explain so that you won't have to.

I think the biggest thing that we did is my team identified some of the single points of failure of the service link like I talked about. So what we did is we worked with the account team and we were like, hey, this is whole new for us. What things could fail that we don't know? Let's put our brains together and think of everything that could fail. What about a server failing? What about the service link failing? What about Route 53 going down? What are all of these things that could do, and let's build runbooks and let's make sure our app can go. And we did, and we tested them all. We had runbooks, it was all great.

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/3260.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=3260)

One thing I will also tell you is if you're going to test service link failure, test it for longer than two hours. It's helpful. And then, you know, I've talked about the VMware HA DRS a lot, so I don't want to spend too much time on it, and it kind of goes back to the slotting. But it's again, what do you really need to run? You know, it's real simple. Like if you run it in region, just pick the version that you want to run on that and hammer it, right? And if you can't, you know, if you have a lower environment, you should be able to just shift it over there, test it, see what you get on those M5. An M5 in region is the same thing that's running on Outposts, so you should get the same CPU memory performance that you're expecting, same with a C7 and M7. But you know, really test it. Take the time to do it. 

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/3290.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=3290)

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bc006c6c6433067a/3300.jpg)](https://www.youtube.com/watch?v=DRZXYhyzczw&t=3300)

And that's all I have, so yeah, thanks, Travis, thank you. Thank you so much. Some closing thoughts before we go. Please come to visit us at the AWS village. There you have a kiosk talking about migration and modernization. You can see a real Outposts rack, and there is also a kiosk for the Local Zones.  Please don't forget to complete the post-event session survey, and we will be taking questions at the back of the session. Thank you all for coming.  Thank you, thank you. Thank you, thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
