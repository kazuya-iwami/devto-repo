---
title: 'AWS re:Invent 2025 - Agentic Analytics in Action (AIM295)'
published: true
description: 'In this video, the speaker discusses how classical AI, data science, and GenAI are converging into agentic AI, presenting four production-deployed examples from Fortune 500 companies: on-demand customer analytics, marketing automation for lead generation, segmentation workflows, and forecasting engines. Key enablers include reasoning capabilities, agentic workflows, domain infusion through knowledge graphs, MCP servers, and compliance layers. A Forrester survey of 400 analytics organizations reveals that most expect human-in-the-loop agents within a year and autonomous systems in 24 months, with 50% faster insights at half the cost. Over 100,000 questions have been processed in deployed systems, spanning US, Japan, and APAC regions, with applications integrated into Salesforce and Viva.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/0.jpg'
series: ''
canonical_url: null
id: 3086392
date: '2025-12-05T12:04:42Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Agentic Analytics in Action (AIM295)**

> In this video, the speaker discusses how classical AI, data science, and GenAI are converging into agentic AI, presenting four production-deployed examples from Fortune 500 companies: on-demand customer analytics, marketing automation for lead generation, segmentation workflows, and forecasting engines. Key enablers include reasoning capabilities, agentic workflows, domain infusion through knowledge graphs, MCP servers, and compliance layers. A Forrester survey of 400 analytics organizations reveals that most expect human-in-the-loop agents within a year and autonomous systems in 24 months, with 50% faster insights at half the cost. Over 100,000 questions have been processed in deployed systems, spanning US, Japan, and APAC regions, with applications integrated into Salesforce and Viva.

{% youtube https://www.youtube.com/watch?v=VZCGmhJACwU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/0.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=0)

### The Convergence of AI and Analytics: Four Production-Ready Agentic Applications Transforming Enterprise Decision-Making

 Hello everyone, thank you for joining this session. What I thought I'd do today is walk you through a set of examples that we have built and deployed across multiple Fortune 500 companies, specifically focused around agentic applications which are on data science and analytics. Roughly around late last year we started with this thesis on the fact that classical AI, data science, and GenAI are converging into agentic AI.

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/30.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=30)

 When we look at that from that standpoint of view, likely most of the decision engines that have been built over the last maybe around 15 years will probably be reimagined from an agentic AI standpoint of view. And largely this is going to be multi-modal, multi-agent systems that are capturing many of these enterprise workflows. So that's essentially the thesis that we started with last year. And over the last 12 months or this year we have seen a number of key developments that are accelerating that particular trend, especially from an analytics and data science standpoint of view.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/80.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=80)

 Specifically, I think 5 things that started around December last year to now have helped accelerate this particular process. One probably is more on reasoning. Today we are able to execute complex multi-step analysis. So you ask a question, then the application is able to think through saying that here are the 5, 2nd order or 3rd order underlying questions that I need to investigate, and then reason around those. And then the second thing that also came out was more around agentic workflows which became mainstream. If you look at last year, probably most of the workflows are relay around HITL and those are the ones that are going around, whereas now we have workflows on the mainstream side.

And then we have domain infusion, which is going through knowledge graphs and ontologies which is helping us put not only the domain part into the agentic application but also any enterprise specific information that we need to plug in to these agents from a context standpoint of view. This combined with MCP servers and Evals is giving us very clear deterministic performance where the LLMs are not hallucinating but we're able to drive specific questions and specific answers that are running through it. And then if you look at this from an analytic standpoint of view we need reasoning and then also specifically from a fine tuning standpoint of view where I can ask the question to the bigger model, but then it triggers a smaller fine tuned model which is answering some of the specific things that are necessary for that to perform.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/170.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=170)

 And then you have the workflows, the domain infusion that is there, and then I need to combine this with the compliance and the security layer because if you look at taking an application into production on the enterprise side, compliance and legal needs to approve these agents that are running around. For example, one of the applications that we have deployed was on the healthcare side, where the reps are using it, the medical service liaisons are using it, and the home office is also using the application. And then the compliance team needs to have configurable guard rails which they can control and see how the applications are doing. This combined with context engineering and also the LLM ops and the telemetry side is helping us drive the performance of the agentic apps that we're deploying at this juncture.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/250.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=250)

 So today I thought I'll just walk you guys through 4 specific examples. One is on on-demand customer analytics and many of you would have done customer analytics in the past. It always takes a lot of time for us to execute, but now with the agentic AI applications we're able to take complex questions that are coming from home office and then drive that analysis end to end and then bring it back to the table. And all these 4 applications I think are in my mind kind of increasing in sophistication. So I'm able to do the analysis. Then in the case of a marketing standpoint of view, you're able to generate the right content and the response and then execute it from a CRM side and then segmentation is something that we are using from a marketing analytics side. And then also forecasting probably is the most nuanced because there are multiple stakeholders that are involved. Everybody has an opinion on what the forecast needs to be or what are some of the drivers that are driving it.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/320.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=320)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/340.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=340)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/350.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=350)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/360.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=360)

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/370.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=370)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/380.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/390.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=390)

So those are the four examples I thought would be interesting to share with you. All of these are currently deployed in multiple Fortune 500 companies and are in production at this point.  They are driving impact. The first one I would like to walk you through is on customer analytics. In this case, you can see that I've asked a question and then the application is thinking through what are the different steps that I need to execute.  This includes determining whether this is a question that I can actually answer and whether it's approved, or perhaps I'm not allowed to answer this question. It thinks through that.  If there are secondary steps, it goes through those and then generates a summary from that point of view. If you look back two years, the best strategy we were generating was like a Python chart at best.  But in this case now, we're able to provide full-scale visualization and it's also integrated into the CRM system at this point.  So you have the agentic application which the reps can access directly from Salesforce or in this case, Viva on the healthcare side.  They're able to interact with those applications. Similarly, when we look at medical reps that are accessing it, they're combining both structured and unstructured data together and driving those insights.  It's not just structured data, but you can also access unstructured data which is sitting in a different application and then drive the analysis completely in this case.

That's where we are looking at from an on-demand customer analytics standpoint. This is something that we have deployed not only in the US but also in APAC, like Japan and other areas where you have language that needs to be taken into account. On the Japan side, the context is a little different. The reps behave a little differently. The home office behaves a little differently. In fact, one of the challenges that we had is that Japanese language is quite formal. When an LLM answers a question, it is not what they are used to, so you have to take that into consideration. At the same time, the complaints department needs to be able to look at all the questions everybody is asking. Over the last six to nine months, close to 100,000 questions have been asked by reps, medical reps, and home office. They need to be able to look into what answers are coming out, trace the answers on an ongoing basis, and see if there are any issues that we need to monitor. That's one of the applications that we're able to deploy into production.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/490.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/500.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=500)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/510.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=510)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/520.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/530.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/540.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=540)

The second one that's interesting is more on a marketing standpoint. This has been deployed with a very large real estate company. I think they're the second or third largest real estate company in the US. The way they're using it is they scan all the new information that is happening at any given point in time.  Then they go back and see what new leads they are able to generate.  If you're looking at it from a marketing standpoint, in the past, any context enrichment that was being done has always been a separate process.  So now you're able to identify the lead, take that particular information, and then check it with Apollo or ZoomInfo, depending upon what they're using, and then enrich that context for that application.  Then you're able to take your existing emails that you might have sent.  So if there is an MD or someone else who has written a lot of emails, the agent is able to learn that context on how and what style those emails are being written in.  Then it drafts the appropriate response. In this case, they're not just automated emails, but you're continuously learning based on what the lead is doing, enriching more context around it, and then writing the right response including images or text that is going to them. That improves the response rate quite a bit.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/590.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=590)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/600.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=600)

In this case, the real estate company that is using it is able to scale up the total number of marketing leads that they're generating, enrich them on a daily basis, and then send those responses at scale at this point. Now, if we shift this a little bit from marketing to segmentation, which most marketing analytics divisions look at.  This is a case where we are able to run a full segmentation end to end.  Depending upon what the questions are,

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/630.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/640.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/650.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=650)

this can be a question around what is based on the latest information that is coming in. Let's say your sales are down or there's a new segment that you believe is coming up based on demographics. You're able to look into a planning engine which looks at all the data that's coming in, create new personas, and reason through those personas.  You can look into what are some of the drivers that might be affecting that particular demographic segment and what are the product lines that are resonating with that segment.  We designed this segmentation workflow which captures how a data analyst or a data scientist  does segmentation using multiple models and then trigger that from an agent standpoint of view and refresh this segmentation on an ongoing basis.

In the past, if you look at it, it used to be probably a three to five member team which does this work for three or four months on an ongoing basis, and that always used to be a lot of work. You cannot run multiple segmentation personas at any given point in time, and the latency to do it has been enlarged at this juncture. Now that entire workflow of the segmentation can be executed within a day or two across multiple data sets. The data sets can be in your main data warehouse in AWS and also across CRM and other elements, and then we're able to bring all that context together and drive the full segmentation exercise.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/710.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=710)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/720.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/730.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=730)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/740.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=740)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/750.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=750)

A slightly more complex engine is forecasting.  So in this case, forecasting, let's say tomorrow a particular event has happened from a pricing standpoint of view or some  product has a stock out scenario. Then I need to review how the forecast is from last quarter that somebody might have done.  Look at the actuals that are going in. And then there are a lot of opinions when we do forecasting on an ongoing basis. So I want to go back and see if a  particular assumption is still accurate or not accurate. If it is not accurate, somebody might want to smooth that particular aspect. So that entire aspect is  now encoded into the workflow.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/760.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=760)

This is where we are looking at from a domain infusion standpoint of view, where a knowledge graph is capturing all the steps that today  a forecasting team is performing. And then take that context and plug that into the system. In this particular scenario, if the business has a question tomorrow around how would my forecast change next quarter or next month because of a disruption or a particular activity that has happened, we're able to run that scenario end to end at this juncture. Look at the risks and the opportunities that are there and how that would affect the performance for a given product or a business line, and this is being done with human in the loop.

The team is able to supervise what are the different algorithms that are running from an agenting standpoint of view and also adjust those across the different steps that are happening. This gives complete control and transparency to the teams which are doing forecasting. And in my mind, at least forecasting is probably the most nuanced and most opinionated if you're doing marketing analytics because the business has an opinion, the data scientist has an opinion, and the operations have an opinion, and all of this needs to be tallied again next quarter, next month on an ongoing basis.

### Analytics at an Inflection Point: Industry Survey Reveals the Two-Year Timeline to Autonomous AI Analysts

These are a couple of reasons why I believe that analytics and data science is at an inflection point from an agentic standpoint of view. Because if you look at all the analytics that we have done, and I've been in the industry for I think twenty years now, pre-2010, we used to do all of the analysis in SAS. Most of the work is logistic regression or linear regression, and I would say the first inflection point was around 2011 or 2012 when the first big data clusters came into play where we had access to much larger data. And then probably around 2016, 2017 is when the cloud and the ML side took off. So I think probably in the next few years is where we can relocate all the engines that have been created over the last fifteen to twenty years, which will be redone from an agentic standpoint of view.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/840.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=840)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/900.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=900)

So it's not just me saying it, but what we did is  in the last two months, we asked Forrester to run a survey across roughly four hundred different analytics organizations. 

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/940.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=940)

The survey respondents are primarily decision makers running analytics departments. Overwhelmingly, most of them are saying that they expect human-in-the-loop agents to come online over the next year, and then probably many of them will reach autonomous levels in the next 24 months. This is across 400 different organizations regarding what they expect to happen in the next one to two years. 

They're also expecting agentic analytics to play a significant role in the next two years. If you look at it, they're expecting agentic analytics to automate many of the routine analytics that the departments are doing today. They're also expecting many of these core workflows to be able to generate insights at a much faster pace. Roughly half of them are saying that the speed to get those insights should be half the time it takes today, and it should roughly cost 50 percent of what it costs today to do analytics. That's a significant chunk of people working on this.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/1000.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=1000)

One of the questions I would ask the audience is this: if every big tech organization is saying that they're going to have a mid-level or junior software developer come online in the next 12 months, how long will it be before you have fully functioning AI analysts doing analytics at scale?  My guess is probably another two or three years. I would say the next two years will be mostly human-in-the-loop systems like the ones you've seen, whether it's segmentation or forecasting. Most of the analytics we're doing can be redone using human-in-the-loop systems.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/373e02ec8dd47253/1080.jpg)](https://www.youtube.com/watch?v=VZCGmhJACwU&t=1080)

As we get more comfortable with these systems, I would expect them to become increasingly autonomous. You could have a system running pricing on an ongoing basis, with an analyst monitoring it maybe once every two or three days to ensure everything is going fine. But then the system is largely running that function on an ongoing basis. That's broadly what we're expecting from how analytics and data science will change over the next four or five years. Those are a couple of things I wanted to share today. If there are any questions, I'm happy to answer how we have developed and deployed some of these analytics systems over the last year. 


----

; This article is entirely auto-generated using Amazon Bedrock.
