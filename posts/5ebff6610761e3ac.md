---
title: 'AWS re:Invent 2025 - Using graphs over your data lake to power generative AI applications (DAT447)'
published: true
description: 'In this video, Brad Bebee and Dr. Ãœmit V. Ã‡atalyÃ¼rek from AWS Amazon Neptune demonstrate three strategies for using graphs with data lakes to enhance generative AI applications. They cover reading and federating data lake data using Neptune Analytics'' neptune.read algorithm with openCypher, running graph algorithms like PageRank and closeness centrality for fraud detection and network analysis, and combining vector search with graph traversals for hybrid search approaches. The session includes real-world examples from Wiz, Trend Micro, and Amazon''s fraud detection team, showing how GraphRAG with Amazon Bedrock Knowledge Bases improves accuracy from 70% to 90%. They also explore agent memory using graph structures with Strands SDK and Bedrock AgentCore, demonstrating 10x latency improvements and reduced token usage while maintaining context across interactions.'
tags: ''
series: ''
canonical_url: null
id: 3085288
date: '2025-12-05T04:40:10Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Using graphs over your data lake to power generative AI applications (DAT447)**

> In this video, Brad Bebee and Dr. Ãœmit V. Ã‡atalyÃ¼rek from AWS Amazon Neptune demonstrate three strategies for using graphs with data lakes to enhance generative AI applications. They cover reading and federating data lake data using Neptune Analytics' neptune.read algorithm with openCypher, running graph algorithms like PageRank and closeness centrality for fraud detection and network analysis, and combining vector search with graph traversals for hybrid search approaches. The session includes real-world examples from Wiz, Trend Micro, and Amazon's fraud detection team, showing how GraphRAG with Amazon Bedrock Knowledge Bases improves accuracy from 70% to 90%. They also explore agent memory using graph structures with Strands SDK and Bedrock AgentCore, demonstrating 10x latency improvements and reduced token usage while maintaining context across interactions.

{% youtube https://www.youtube.com/watch?v=pNOn9DHlArQ %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Graphs as a Tool for Solving Real-World Problems

Welcome, everybody, to DAT447, using graphs over your data lake to power generative AI applications. I want to give a quick shout-out to the tech team here who worked through a couple of last-minute presentation issues. I'm Brad Bebee, a director with AWS for Amazon Neptune, and I'll also be joined today by my colleague Dr. Ãœmit V. Ã‡atalyÃ¼rek, who is an Amazon Scholar and also with the Neptune team.

[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/0.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=0)



Before we get started, could I see a show of hands of folks who are using graph processing on your data lake, either graph algorithms or graph query pieces? Okay, a few. How about people who are using different techniques, including graphs, to improve the accuracy of generative AI applications? Okay, cool. Well, it should be a good talk.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/60.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=60)



Graphs are awesome, and the reason that they're awesome is because they allow you to use the relationships in your data to solve problems and ask questions. In a graph data model, relationships themselves are first-class entities. You can query them, you can traverse them, you can write algorithms and programs that iterate and loop over them.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/90.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=90)



This enables all kinds of super interesting and valuable graph applications, from finance and fraud detection to knowledge graphs and engineering models to Customer 360 and profile pieces. One of the things that I've seen, and I joined AWS almost ten years ago to start what is now the Neptune service, is that the most successful businesses, companies, and applications find a way to use a graph to do something remarkable for their customers, to provide some sort of unique and interesting function.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/110.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=110)



A great recent example of this is Wiz. Wiz is a large cloud security posture management company. They started with a vision of the world as a graph and context as king, and they used that to reimagine cloud security posture management. We had the benefit of being able to watch them along the way over a few years. One of the things that struck me about what Wiz has done that many other people have tried in different ways is the way that they use graphs not just to connect data, but also to help explain findings and give you a way that you can ask questions and understand quickly why this is the most important thing or why this is the right answer.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/140.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=140)



We often quote that there are many problems in the world that can be solved by graphs, but as practitioners, as people who are building applications, we need to keep in mind that graphs themselves are not an intrinsic benefit. If your end state is "now I have a graph," you haven't really accomplished anything in your application.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/190.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=190)



[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/200.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=200)



Over the last year and a half, we've seen a huge increase in the number of customers who want to improve the accuracy of generative AI applications, who want to use data on their data lakes and try to get more insights. So today, what we're going to go through is a couple of different strategies about how you can do that, how you can do useful things with your graphs where your data lives.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/240.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=240)

### Strategy One: Reading and Federating Data Lake Content with Neptune Analytics



I'm going to introduce Ãœmit to take you to the next part. Good afternoon, everyone. I'm going to walk through two of the strategies we have been using and you can use in your applications as well. The first one we are going to look at is taking your data from your data lake and using it in a graph. In these cases, you know that there is a graph, either you created the graph or you want to create the graph, and we'll see how we can do this today.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/270.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=270)



[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/280.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=280)



Neptune Analytics is built on top of a unified graph data model called the OneGraph model, and it's a logical model that unifies both property graphs and RDF so that you can actually load both RDF data and the property graph into Neptune Analytics.

You can import your data and start your graph that way, but after you start the graph, you can also use tabular data that sits in your data lake using your favorite graph processing language, openCypher. You can query the data sitting in your data lake with openCypher and you can import and export to your data lake as well.

What are the use cases to do this? Let's say I have an airline dataset with metadata information about airports and their connections, but I don't have the most recent airline flight information. Let's say that sits in my data lake. We can actually read that and process it. We can read that and federate it with our datasets.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/370.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=370)

 The first approach is reading from your data lake. We developed the Neptune read algorithm. What it does is, if you have a dataset in either Parquet format or CSV format, you can load your dataset. All you have to do is tell the formatâ€”this is Parquet formatâ€”and specify the S3 URI where it sits. Then it returns every single row of that data to you, and you have direct access to the columns and can use those columns. You can do the same thing with CSV files as well.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/420.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/430.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=430)

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/450.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=450)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/470.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=470)

 Let's look at an example of how to use this.  Let's say we have an S3 data file in Parquet format that consists of multiple columns like carrier, flight number, origin, and destination, and you want to read that.  You simply specify the URI and indicate it's a Parquet file, and it returns the data row by row.  It's in openCypher, so you can use those columns as you wish. You can take the carrier information as is, you can concatenate origin and destination with an arrow between them, and you can produce output like that. This is just reading your data in your graph language.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/480.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=480)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/500.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=500)

 Of course, we can do more.  We can federate the data. Let's say you already have the airport metadata information about airports and what you want to do is get the most up-to-date recent flight numbers by each carrier. You can start your openCypher query with a MATCH clause asking for airport one and airport two and specifying their codes to be JFK and Las Vegas. So you're trying to find all the flights between those two locations. We call neptune.read, giving the same Parquet file. It returns each row, but we want to filter that so that airport one and the origin and airport two and destination match. If they match, you want to collect the carrier information and all the flight numbers, but you want to combine all those flight numbers as a set. You can use the collect intrinsic for that and return the results, and you'll get output like this.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/590.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=590)

So we federated what we have in our graph and what we are reading from the data lake. We can also augment our graph with the data.  Let's say we want to create an edge between airports. We want to create a new edgeâ€”we already have the route edge between themâ€”and we want to create a new flight edge for every carrier that's flying between those two airports. For example, the first row shows

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/620.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=620)

that between JFK and Las Vegas, B6 carrier is flying. We can use a very similar Cypher query.  We specify the first node and second node as JFK and Las Vegas with the same filtering. This time, we want just the distinct carrier information because the same carrier may have multiple flights. We only want one of them. We ask with the distinct keyword, and we can actually create a new edge with this information and augment our existing data.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/650.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=650)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/670.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=670)

That was strategy one.  When you have just data and either have a graph or don't have a graph, you either get the data and process it like a graph or augment it, and so on. These are simple Cypher queries. What if you want to do something more? What if you want to run some graph algorithms on that data? 

### Running Graph Algorithms: PageRank, Centrality, and Fraud Detection

What are graph algorithms useful for? There are many graph algorithms that serve different purposes. In one scenario, suppose you have a graph that contains technical research information and you want to understand the importance of a research or researcher. You may have a graph of airports and want to understand the importance of an airport in terms of its closeness to other airports. Or you may have a customer graph and want to investigate the most influential campaigns you can run. You may have financial transactional data and be interested in identifying fraud. In these cases, we use graph algorithms. Graph algorithms run on the whole graph, not just on single agents or pairs.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/740.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=740)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/770.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=770)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/780.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=780)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/790.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=790)

Let's look at one graph algorithm.  One of the most commonly known algorithms is Google's famous PageRank algorithm. Many of you might already be familiar with it. How do we define the importance of a vertex? To find that out, we need to look at what other vertices are pointing to that vertex.  Who is following you, right? That's the first step. The next step is that for those vertices, we need to repeat the exact same step.  We need to figure out who is following those people. As you can imagine, this goes recursively through the whole graph. 

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/810.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=810)

The PageRank algorithm basically operates like this: for every single vertex, the importance of a vertex is the average of the importance of the vertices connected to that vertex. That's one way of describing it, and that's the formula shown here.  The second term is basically getting all the importance of the vertices that are pointing towards me. The first term is some randomization so that you can do a random jump and smooth the process. Of course, this algorithm can be implemented in many other ways. You can think of this from the source perspective, distributing your importance to your neighbors, or another way of looking at it: for every single edge, you take a ratio of the source's importance and give that to the destination. There are different ways to implement these algorithms, and how your data is laid out will define which one performs well.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/880.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=880)

You don't need to worry about all of these implementation details because we implement these algorithms as efficiently as possible in our data structures.  All you need to know is how to run these algorithms. This is the Cypher call to run PageRank. We have two versions of PageRank. This one is called mutate. What it does is call PageRank on the existing graph and ask that the result of the PageRank be written to the property called score.

The PageRank algorithm is not a one-pass algorithm. You need to go over the graph multiple times and do that iteration multiple times. Either the user decides how many times they want to do this, or you run this algorithm multiple times on your graph and determine that some number is sufficient. Generally, 10 to 20 iterations are more than sufficient for the PageRank algorithm. You tell which vertices you want to run this on and which edge labels you want to use, and then you run the algorithm.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/950.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=950)

After you run the algorithm,  you have the score in your graph. If you want to integrate this score with other processing systems you have or transfer this to your data lake, you can do this by using an export command. You can basically specify that for each airport you choose, you want the score you computed to be written. This will give you a table in table format with each of the PageRank scores.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/990.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=990)

 Another set of algorithms commonly used are centrality algorithms. Centrality algorithms are useful to identify the important vertices. In this toy graph, you may identify the source of a problem, the culprits, the person hiding themselves who sits in the middle of all the connections. Obviously, that person is very vital for passing information between those two sites and also sits in the middle of the graph and close to everyone.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1060.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1060)

There are different centrality metrics. Closeness centrality is one of them, and closeness centrality measures how far each of the vertices is from you. It normalizes the values. The challenge, obviously, is to run this algorithm on the whole graph fast, so let me give you a simple formulation and algorithm to explain  how this algorithm works in general. Given a graph, for every single vertex, you go over the distance to all other vertices and sum them up. You compute how far this vertex is from everyone else, and closeness centrality is the reciprocal of all the other vertices minus one divided by how far you are. This is one formulation of centrality, which basically normalizes the number to be between zero and one, making it easy to interpret.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1120.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1130.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1140.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1140)

If you run the closeness centrality on this toy graph, you will see that the closeness centrality of the person in the middle is very high, so you can identify that. The algorithm starts with computing closeness centrality for every single vertex.  For every single vertex, what you need to do  is run either a shortest path if your graph is weighted or a simple BFS if you are running an unweighted graph,  and then write the output by normalizing.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1200.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1200)

I want to highlight that this BFS is your standard BFS algorithm or single source shortest path. The best you can do is go over all of the edges at some point in time. Then an outer loop over all the vertices is going to take quite a bit of time. This is a very expensive algorithm, so obviously no one is running such an algorithm at large scale. What you usually do is sample a random set of vertices. In Neptune Analytics, we do much more than this. We do other HPC optimizations like vectorization and parallelization to make this algorithm run much faster.  But what is important for you is how to use this algorithm. All you have to do is tell

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1240.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1240)

the system, "I want to run this algorithm on airports." When you call it, you can specify how many source vertices you want to provide. If you give a maximum number, it will run on the entire graph. You specify which routes you want to run, and it will return the score. We are here printing just the description of the airport, which gives you a human-readable form showing that Charles de Gaulle  is the airport that has the closest central route.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1250.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1250)

We have a suite of algorithms  implemented in Neptune Analytics that you can use. I have shown you just two of them. We have different classes of algorithms. Pathfinding algorithms include simple breadth-first search, which looks at connectivity between a single vertex and how others are reachable. Shortest path algorithms start from a single vertex, using weights to determine how far other vertices are. If you are interested in going just two or three hops and getting a limited set, you can use topK hop-limited BFS or Ego network if you are only interested in one vertex and its neighborhood.

Centrality algorithms include the two I already mentioned, and degree is obviously the third one, which computes the degree of the vertex. Clustering algorithms help you find related vertices, which is great for fraud detection. You can run any of these algorithms depending on your graph. Starting from the simplest one, which looks at weakly connected components and how the works are connected, you can move to label propagation and Louvain. We also have a set of algorithms to look at the similarity of vertices.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1350.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1350)

Most interestingly, which Brad will talk about more, we have vector algorithms that you can use to search for vertices that are similar and use those vector distances. The vectors come from LLMs, GNNs, and many other sources. Whatever vectors you have, we can use them as vertex properties.  If you want to try this yourself and see how you can use the algorithms, this is one use case you can track. If you use the QR code, you will get to one of the notebooks that we set up with an example case.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1380.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1380)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1390.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1390)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1400.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1400)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1410.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1410)

The case study is about using PaySim, which contains mobile payment transactions, and we are trying to find fraud.  The data is in S3 Tables in this case. Following the notebook,  you can load the data from S3 Tables to a graph, then run the Louvain algorithm on that graph to identify the communities.  You can obviously export those communities back  to be used by other upstream processing you may have, or you can even use the visualization enabled with the Graph Explorer.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1450.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1450)

### Strategy Two: Combining Vector Search with Graph Traversals

I have described the cases where you know that there is a graph and you want to run some algorithms. Now Brad will take over to discuss vectors and scenarios where you do not even need to know that there is a graph. So now that we know about graph algorithms, let us look at a different strategy where we combine vector search and different kinds of graph traversals. When you think about searching in vector space,  when you find things that are similar, you are taking a distance measure from basically a high-dimensional matrix. It is a super powerful technique that lets you find images that are similar and understand that a zucchini and a courgette are most likely the same thing.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1500.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1500)

However, if you are trying to build an accurate application and explain the results to someone, vector similarity search is really a black box. You cannot tell why some things are related. Contrast that with a graph traversal or a graph search. If you ask something like, "Why are Wang Xiulan and John Doe related?" you can answer it with a very explicit traversal  and say they are related because they acted together in these movies.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1520.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1530.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1530)

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1540.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1550.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1550)

It may not be as flexible necessarily as vector search, but it's certainly very easy for you to explain. One of the things  that we've seen customers do is look at combining these different types of search. This is one example of hybrid search where a user asks a question  and you use a combination of different techniques, maybe a conventional inverted text index, a vector similarity search, a knowledge graph,  and you combine those results to be able to give the user a better answer. 

Building on what Umit showed you, it's very straightforward to create a graph with vector embeddings. You simply add this additional parameter at the bottom here, which is the vector search configuration, and you specify the dimensionality of the vectors that you want to store. Then you can load the vectors into your graph through a fairly basic serialization. We'll also take a look at a Parquet example in just a second.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1590.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1590)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1610.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1610)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1640.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1640)

So if we go back to the airport routes and suppose that we have in S3 vectors or we've made some LLM calls and we've created vectors for our different airports and we have those in a Parquet file, we can use the same kind of technique that we used before. In this case we're using the neptune.read function,  we're calling it to read the Parquet file, but rather than federate the results or use them in a query, we're actually then matching on the airport codes and storing that embedding as a property in the graph.  Then we can go on and do other kinds of vector search type operations. 

### Hybrid Search in Practice: Detecting AI-Generated Content Through Graph and Vector Integration

Now, another way that you can combine vectors and graph traversals is not necessarily starting with a vector search. This is an example that's a little bit abbreviated, but it's taken from one of the fraud teams at amazon.com. As you can imagine, on a site like amazon.com, it's quite a challenge to keep up with various different kinds of bad actor type behaviors. One of the things that this group is looking at is who are trying to sell books that were generated by generative AI.

The way the technique that they used was they started off by doing a graph traversal to match something that they had in their graph, so in this case, in their product graph. In this example we'll look at, I want to look at books about travel to Las Vegas. They took that graph search result and then they combined it with a vector search. So, show me books that are similar within some topK of the travel books that I specified in my graph.

Then they combine this with information that they had in their graph already related to various different scoring that they use for sellers, listers, or buyers that have been flagged as suspicious. So they can start with something they're interested in, expand out from the vector search to find other things that are similar to it, and then combine that with the relationships about who is known to be selling, listing, or otherwise producing this kind of content. This is a very interesting way to combine the vector search with the structured graph piece.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1750.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1750)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1770.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1770)

If you want to do more or learn more on this particular case, you can use this as also a Jupyter Notebook that you can use to try and combine vector search with graph search.  

### GraphRAG: Enhancing Retrieval Augmented Generation with Amazon Bedrock Knowledge Bases

What else can you do with vectors and graphs? Well, one of the techniques that many people have used and tried is to use retrieval augmented generation to improve the accuracy of responses by bringing in specific data that might be relevant to your organization or to your business and using that to try and provide grounding. With GraphRAG, you can take that sort of retrieval augmented generation architectural pattern and bring in some of the explicit information that you have from a graph. We'll show you briefly an example.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1830.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1830)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1850.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1860.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1860)

There are lots of different ways you can envision combining these techniques. You can do various different kinds of ranking or other pieces, so there's a lot of different opportunity to use different combinations of techniques.  With Amazon Bedrock Knowledge Bases, you can do this automatically if you create a Bedrock Knowledge Base and you choose GraphRAG as the option.  What will happen is that in the generation of the chunks from your RAG application, we will automatically build a graph.  The graph is what we call a lexical graph, and it conceptually looks something like this.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1900.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1900)

Typically, a RAG application takes the documents and chunks them in some way, then embeds storage vectors along with those chunks. In this case, when you use Bedrock Knowledge Base as GraphRAG, we also use an LLM to extract entities and facts that are occurring within those chunks. We automatically create a lexical graph behind the scenes, and you don't need to have any graph expertise to be able to do that.  When you ask questions, particularly multi-hop, multi-stage questions where you need to assimilate information from multiple different sources to achieve a result, you're able to get the benefit of the relationships and the connections in the graph during what's called the re-ranking phase.

Typically, in a RAG application, you'll do a vector search that will have a topK, and the topK results often go through a re-ranking phase where you might take other kinds of metadata. In this case, where you're using the graph, we're effectively boosting the results that we're presenting back to the user based on explicit connections in the graph. As a quick example, if this document through this chunk was the topK result or was one of the results returned in the vector search, these other documents may not have shown up. However, because we have this lexical graph with these connections where we can see that there's a fact connected to these three entities, the re-ranking phase of this GraphRAG integration will effectively boost the results, allowing you to get more accurate results.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/1970.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=1970)

 This is a case study from Trend Micro, a computer security company that built a user-facing security chatbot. They deployed it over time, combining various different techniques to build graphs, use GraphRAG, use Bedrock, and use Neptune. Over time, looking at about two months of data, they were able to increase the accuracy of their chatbot from 70% to 90%, where higher is better. They were also able to increase user satisfaction, with the thumbs down rate getting lower over time, producing more accurate answers which also improved their user satisfaction.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2030.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2030)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2040.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2040)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2060.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2060)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2070.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2070)

### Strategy Three: Building Agentic Applications with Graph-Based Memory

  This brings me to the last strategy, which is looking at different ways to build agentic applications. If you look at a simple agent dialogue, you might ask a question about something like your phone and provide the model number.  A couple of days later, you ask another question related to your phone, and the agent asks you something that you think it should have known.  This sort of lack of context or loss of context means that agents aren't as personalized and they're not able to respond as accurately to user interactions.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2100.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2100)

People often talk about this as the goldfish effect, where an agent without memory experiences every interaction as a brand new thing for the first time.  A way to solve this problem is to add memory to your agents.

This allows you to save and store the context of different interactions so that it can be reused at a future point in time. There are many different potential applications for this beyond the typical user experience app model. For example, you could think about multi-agent workflows where they are doing plans, what plans generated what outcomes, and storing those kinds of things as memory. There is significant potential here.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2140.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2140)



There are different types of memory. As a quick example, if you were to ask an agent to plan a trip to Tokyo with certain length and price parameters and say you are interested in cultural sites and experiences, you might conceptually break out the memories into these different types. You might have a working memory, which consists of things that were just asked in the current session. You might have some kind of short-term memory concept where these are things from other conversations or other working memories that this individual or user has had. Then you might have something more organized and long-term where you might know that they have certain travel seating preferences or that they have global entry. If you are able to store and manage this context, you can use it within your agentic workflows and agentic interactions for both accuracy and latency improvements.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2220.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2220)

There are many different ways to add agent memory. Fortunately, most people are building inside different kinds of frameworks. The Bedrock AgentCore is one that we are using quite a lot.  You can have different techniques to enable those in those frameworks. You can use MCP servers to help you construct knowledge graphs based on memories or organization that agents have interacted with. Mem0 is an open source library that helps you do personalized knowledge graphs as part of your memory. Cognee is more conversationally focused, and Zep Graphiti is focused on temporal graphs. There are many other different options as well.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2250.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2250)



[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2280.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2280)

This is another opportunity for you if you want to go a little deeper. This blog post was published last week and has code examples about how to use the Strands SDK, which is an agent framework that we have here at AWS, to be able to add and store persistent memory of different types. You can use that QR code to access it. To give you a quick walkthrough,  this is a Strands agent example. If you go to the blog post, you will see there are really just three lines of code here to create this very simple agent. It is asking a question about the Mem0 project and it gets a result.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2300.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2300)



[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2320.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2320)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2360.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2370.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2370)

You can enable graph memory, which involves adding a couple of lines of Python code to the Strands agent. You can ask a question where the top contributors to the Mem0 project receive the answer.  Then when you ask another question, "Who works on the Mem0 project?" you will get both a faster result and a result where you have to retrieve fewer tokens. In addition to providing a benefit from an accuracy and user perspective, there is also a cost dimension. These are some numbers that were presented by Zep and Graphiti, and you can see that using memories,  they are able to get some improvements in accuracy. Very significantly, they were able to see  almost 10x improvements in latency and the average number of context tokens used. You are able to improve the latency in the user experience and also reduce the cost.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2390.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2390)

### Conclusion: Leveraging Graphs to Maximize Data Value



[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2400.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2400)

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2410.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2410)

To do a quick wrap-up, when you think about graphs, think about graphs as a tool to get more out of your data and ways that you can use graphs to process your data where it is.  Combine vector search and graph traversal to get the power of vector similarity search with the benefits and explicit information that you can get from a graph.  Lastly, look at how you can store and organize agentic memories, particularly using graph structures with Strands and Bedrock AgentCore.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2430.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2430)



Thank you very much for spending your afternoon, or at least an hour of your afternoon with us. We greatly appreciate it. Umit and I will be around afterwards if anyone has other questions or wants to talk. Enjoy the rest of the conference. Thank you.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5ebff6610761e3ac/2450.jpg)](https://www.youtube.com/watch?v=pNOn9DHlArQ&t=2450)




----

; This article is entirely auto-generated using Amazon Bedrock.
