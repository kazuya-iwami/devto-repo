---
title: 'AWS re:Invent 2025 - Building on AWS resilience: Innovations for critical success (ARC207)'
published: true
description: 'In this video, AWS senior principal solutions architect Mike Aiken and senior principal engineer Gavin McCullough reveal behind-the-scenes innovations that enhance AWS resilience. They explain four availability axioms: region isolation, AZ resilience, rigorous testing, and overload protection. Key examples include transparently migrating global STS traffic to regional endpoints, reducing cross-region dependencies for thousands of accounts while improving latency from 230ms to 20ms; developing Fleet Health Service and Zonal Event Detector using CloudWatch Contributor Insights to automatically detect gray failures and power Zonal AutoShift; building a dedicated AWS test region for continuous game day testing to expand the competence envelope; and researching metastable failures using the open-sourced Metafor tool to prevent rare overload scenarios in queue-based systems.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/80.jpg'
series: ''
canonical_url: null
id: 3087990
date: '2025-12-06T02:21:54Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Building on AWS resilience: Innovations for critical success (ARC207)**

> In this video, AWS senior principal solutions architect Mike Aiken and senior principal engineer Gavin McCullough reveal behind-the-scenes innovations that enhance AWS resilience. They explain four availability axioms: region isolation, AZ resilience, rigorous testing, and overload protection. Key examples include transparently migrating global STS traffic to regional endpoints, reducing cross-region dependencies for thousands of accounts while improving latency from 230ms to 20ms; developing Fleet Health Service and Zonal Event Detector using CloudWatch Contributor Insights to automatically detect gray failures and power Zonal AutoShift; building a dedicated AWS test region for continuous game day testing to expand the competence envelope; and researching metastable failures using the open-sourced Metafor tool to prevent rare overload scenarios in queue-based systems.

{% youtube https://www.youtube.com/watch?v=gLL2eOPxclU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### AWS's Obsession with Resilience and Antifragility

Welcome to the session. At AWS, we frequently talk about the many benefits of cloud computing, such as on-demand capacity, automatic scaling, consistent security controls, and managed services that remove undifferentiated heavy lifting. However, one of the less visible benefits of running on AWS is the fact that our teams constantly obsess about resilience. We relentlessly pursue opportunities, both big and small, to improve the resilience of our services and your applications.

At AWS, we serve millions of customers across 38 regions, handling billions of requests daily. This scale provides us a unique opportunity to innovate and make investments that most companies couldn't justify in their own data centers. It also requires us to spend significant time diving deep on a myriad of intricate details that others may overlook or that many businesses wouldn't bother with.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/80.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=80)

Resilience is more critical than ever for you, our customers, to deliver essential services to every corner of the globe that power economies, governments, critical infrastructure, and more. After almost eight years here, I've come to learn that at AWS we think a little bit differently about resilience.  I think it's summed up pretty well by this quote about antifragility.

We're proud of AWS's operating record, and we hear feedback routinely from customers that when they moved to the AWS cloud, their reliability improved significantly. However, we still occasionally have incidents, and to some degree they're inevitable. Our obsession with resilience means that we are continuously trying to learn and improve from every incident, so that we'll have fewer future incidents and that they'll be shorter and smaller. We're aiming as an organization to be antifragile, so that after each incident, we're better than we were before.

There's a lot of innovation that goes on to achieve this. Some of that innovation includes delivering transparent improvements in our services that are mostly invisible to customers. Other innovations seek to reduce the work that customers have to do on their side of the shared responsibility model, letting AWS shoulder more of that burden for you so you have less to do.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/160.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=160)

My name is Mike Aiken. I'm a senior principal solutions architect supporting customers across AWS to build and operate resilient workloads in the cloud.  I'm Gavin McCullough. I'm a senior principal engineer in the AWS resilience organization. We own services like Route 53 and Application Recovery Controller, as well as working across AWS on improving our resilience. I also work as an AWS corps leader, which is our equivalent of an incident commander if you're familiar with that term.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/190.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=190)

### Operational Excellence Culture: Ownership, Deep Dives, and Availability Axioms

Today we want to pull back the curtain and share some of the behind-the-scenes innovation that helps make AWS the best place to run your mission-critical workloads.  Before we get too far into the details, let's talk a little bit about how we think about operations at AWS. We consider operational excellence to be a centrally important part of being an engineer, an engineering manager, or a leader at Amazon. Our role guidelines talk about it. It's part of all of our performance and promotion discussions. It's a key part of what we do.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/220.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=220)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/240.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=240)

Our developers don't just write code; they're on call for the code very deliberately.  They think carefully about how the service is going to operate in production. At its core, this is about ownership. We need our developer teams to truly own their customer experience, both in the way the service works, the semantics of the APIs, and the performance of the service. 

Service teams will each run their own regular weekly operations meetings where they review recent incidents, examine metrics dashboards for anomalies, and plan out how to improve the operational state of the services they own over the coming weeks and months. A common example conversation you'd hear at these meetings is: since Tuesday, the 99th percentile latency on that API call has gone up. Do we understand what went on there? It's gone up by about ten milliseconds. We're constantly obsessing about little details like that so that we're aware of all the changes we're making and what's happening.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/280.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=280)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/290.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=290)

We have really high expectations for engineers diving deep. It's not just about resolving issues when they come in, but understanding why they  occurred and how we prevent them next time or how we become more resilient to them. As with any software services, AWS  services do have incidents occasionally. Most are pretty innocuous, with customers not even noticing. However, we always have to take even the small issues seriously. In fact, the small issues are often the opportunity to learn. Obviously, our top priority is always to mitigate customer impact as quickly as possible.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/320.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=320)

Our next priority is to learn. We want to analyze these events, learn the right lessons, and take action items that don't just correct a specific issue or bug, but make us more resilient for future unexpected events.  Finally, we aim to share what we learn across teams. With thousands of developer teams, we want to share our learnings, mental models, and best practices widely so that all the teams can learn from the lessons we find. This is really where we achieve the antifragility that Mike mentioned.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/350.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=350)

As AWS and Amazon have grown, one of the challenges has been working out how to succinctly communicate the core operating ideals across a very large organization. To help with that in recent time,  we've been working on what we call availability axioms. These are a set of concise best practices that our service teams all follow and can expect of each other. Having these short axioms gives us a common language in which to talk about our systems with each other.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/370.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=370)

### Four Core Availability Axioms: Region Isolation, AZ Resilience, Testing, and Overload Protection

 The first is what we call region isolation. We expect AWS regions to operate highly independently. In the rare case that a problem occurs in region A, we know it will be limited to region A and will not affect any other region. This very simple principle has major availability benefits for customers. As we now have 38 regions and growing, it's critical that each region operates independently.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/400.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=400)

The second axiom relates to availability zones.  For those of you who might not be familiar, each AWS region is built out of three or more availability zones for redundancy. These are physically separated groups of data centers within typically one metropolitan area. Every AWS region is designed to gracefully handle one availability zone being impaired or in the rarest case even completely unavailable. We should be able to lose one or multiple data centers in a single availability zone and just keep going as normal.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/450.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=450)

With that in mind, our AZ resilience axiom says that every regional AWS service must be built across multiple availability zones and able to tolerate an impairment to one AZ without degrading their customer experience.  We push teams pretty hard to meet this standard, and we test it fairly frequently. An important third axiom is centered around testing. The code should be tested prior to going into production, which hopefully won't be new to anyone. But the axiom goes into quite specific details on best practices around unit, integration, and acceptance testing and what we consider best practice to be.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/480.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=480)

In addition, you'll see teams use chaos testing regularly and what we call game days. We'll talk more on that topic later.  One last example axiom is around overload protection. We've learned a lot over the years about protecting systems against overload and the many ways overload can arise. A lot of people, when you say this term, will immediately think of bad actors, and that kind of thing can happen. But in practice, we often find that overload is not so much caused by bad actors, but how our systems interact in unexpected ways.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/520.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/530.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=530)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/550.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=550)

All our systems have to be built to defend themselves against overload from all of their callers, not just outside callers.  As with people, it's critically important that our systems have mechanisms to quickly and cheaply say no when asked to do more work than they can successfully complete. And as with people, deciding who to say no to is also pretty important.  Let's explore a set of recent examples in which AWS has been innovating to deliver on these four availability axioms. We're first going to look at how we improved the regional isolation of AWS services and customer workloads by exploring the evolution  of the Security Token Service over the past 14 years.

### The Evolution of Security Token Service: From Global to Regional Architecture

STS allows a given IAM role or user to assume a different role temporarily to perform some task, usually with different privileges. It's kind of like the sudo command on a Unix system except with one important difference. STS is aimed at assuming purpose-built, least privileged IAM roles. The mechanics are that when you call STS with the assume role API, it returns you a temporary set of API credentials for the IAM role you're assuming. These credentials are short term, typically expiring within an hour.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/600.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=600)

STS is used in many different ways, from single sign-on systems or when you authorize an AWS service to take action on your behalf.  When we launched STS in 2011, long-term credentials and users were pretty standard across the industry. STS's use of short-term credentials and restricted roles was a significant innovation at the time.

This approach may seem obvious now, but back then, this was a genuine innovation. We were exploring new territory and we were still learning about how the service was going to be used. It quickly became apparent how powerful and important STS was going to be. As its popularity grew, so did its critical nature within the AWS ecosystem as more and more workflows started making use of it.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/690.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=690)

Because STS was distributing credentials for IAM roles, we decided to locate the STS endpoint in the same place as IAM in US East One. Both of these were considered global services, meaning that there was a single endpoint that users and services in all regions used to call its APIs. This seemed like a minor detail at the time, but as we learned more about how this new service would be used, we started to realize that this wasn't the optimal choice for STS. While we originally thought of STS and IAM being strongly coupled siblings, there's actually some very real difference in how the services are used operationally. 

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/710.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=710)

First, IAM is globally consistent. We replicate all of your roles and policies to every enabled region, so everything has the same view of your IAM resources. This requires a central source of truth managed by that global endpoint. In contrast, STS is local and stateless.  Tokens once produced aren't stored by AWS. The same token can't be retrieved twice, so there's no similar need for a central source of truth or global consistency.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/730.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=730)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/740.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=740)

Second, the IAM APIs are typically used during IAM resource creation and updates.  This normally happens at build time, when you're deploying CloudFormation stacks or releasing resources through a pipeline.  STS, on the other hand, is frequently used during runtime. Obtaining temporary credentials from STS was becoming part of the critical flow for applications and temporary access in AWS. A lot of services started depending on STS in ways that they wouldn't for IAM. This also means that STS is called much more frequently than IAM.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/780.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=780)

So as a TV show once said, one of these things isn't like the other. So the question we started to ask ourselves about STS was, given how important it's becoming for AWS services and real-time importance for our customers,  could we make STS regional and meet our region isolation axiom to give customers that strict region isolation we want? We started work in 2014 to deliver regional isolation for STS. This was a pretty big decision. It involved a lot of investment, but it was the right thing to do for AWS and customers.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/810.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=810)

By early 2015, STS had launched regional endpoints in every region globally, and every new region we build has its own dedicated STS.  We're encouraging customers to use the regional endpoints in preference to the global endpoints all the time. As time passes, we see most customers adopting the regional endpoints, and things are going reasonably well. But slowly it starts to become clear that not everything's migrating. There's a sizable tail that's not moving.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/840.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=840)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/850.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=850)

In September of last year, we did a deep dive into this question. Although regional STS has been available at this point for nearly a decade, a significant set of you, our customers, were still using that global STS endpoint and not getting the isolation that we wanted everyone to have.   So we realized that we needed to change our approach, that we had been thinking about it wrong. We'd made the regional STS APIs available, recommended them strongly in docs, blogged about them, but we were not getting the transition we'd hoped for. More importantly, you guys were not getting the regional independence we wanted you to have.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/890.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=890)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/900.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=900)

The fundamental problem was that the migration to regional APIs required customers to take action. At the scale that AWS's customer base is at this point, that was just unrealistic to expect everybody to move. So you can see the global STS API here running in US East 1 and the 16 other enabled by default regions around the world which people are calling it from.  If we zoom in on North America, we can see what's happening a little more clearly.  Even for workloads in these other regions, customers are regularly making calls back to US East One to get their short-lived credentials from the STS Global Endpoint.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/930.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=930)

Obtaining those credentials introduces a cross-region dependency, and it doesn't provide the regional isolation that we wanted your workloads to have. This is how we want things to work. Requests originating from each region  for STS credentials should be answered locally in that region. We didn't want requests to the global endpoint to actually go to US East 1 anymore. A workload, say in US West 2, when it calls the STS Global Endpoint, should be answered in US West 2, and the same for every other region.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/970.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=970)

### Achieving Near-Perfect Transparency: Rerouting Global STS Traffic to Regional Endpoints

This would ideally create that regional isolation that we want in STS without customers in those regions having to do any work to get it. But this giant kind of change can be complex. We really needed to understand the customer requirements and make that change with near perfect transparency. So here's what we did.  One of the biggest challenges in running cloud services, and any large service for a lot of you too, is figuring out how to make big changes to your service while no customers notice. Our goal here is near perfect transparency. So let's look at some of the requirements and what we did to meet them.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1000.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1000)

Our core goal here is for every customer to get their STS calls answered locally in region, without them needing to change anything. The first thing we did was to build out 16  perfect replicas of the global service, one in each additional region. So we have 17 global endpoints in total. The regional service looks slightly different to the global one. So from the perspective of clients, if we just rerouted them to the regional endpoint, some customers would have seen problems with that. So we decided that for now, the simplest thing to do was to actually run a separate global endpoint in each of those default regions.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1040.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1050.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1050)

It's worth noting that opt-in regions, the more recent regions that we've launched, already require use of regional STS out of the box. So we didn't really need to worry about them so much. Customers were not using the global endpoint from there.  When we spoke with our identity experts, they asked us to ensure quite carefully that the change we make was to answer clients in the local  region where they were only. So for example, global STS requests made in Sydney were handled in Virginia before we started. What we wanted to do was answer them locally in Sydney, but to ensure we never routed them to any other region. So requests that originated outside an AWS region would continue to go to US East 1. Nobody was routed to a new region that they weren't normally talking to.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1100.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1100)

To achieve this, we used an integration with the Route 53 resolver, the DNS resolver that you use in your VPC. So when you query sts.amazonaws.com, it now transparently returns you the IPs of the new in-region endpoints. We made this change very carefully, one Availability Zone at a time across each region and monitored to make sure the workloads were continuing to work as normal.  One of the other aspects that we had to think about was CloudTrail logs. Every time you make a call to STS, an audit log is delivered to CloudTrail. Most AWS customers configure global CloudTrail logs, so that all of their logs are globally delivered to a single region where they can process them.

But for customers who don't, which was a sizable minority, CloudTrail logs are generally emitted in the region where the service they're calling is running. Up to now, Global STS was running in US East 1, so those logs would have been emitted in US East 1. But we need to be careful that even while we now answer that STS request in Sydney, we want the log to continue to show up in US East 1, so no one's taken by surprise. To ensure we're being open and transparent about this, CloudTrail actually added an extra metadata line to tell you in which region we answered the request.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1160.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1160)

Over the longer term, our strategy is to eventually turn off the global endpoint.  We don't really want it to be global. This project is about accelerating that change and giving customers the benefits much earlier. The newer AWS SDKs were already all using the regional STS endpoints out of the box. But we realized that a sizable portion of customers were using older major versions which still defaulted to the global endpoint. So as part of the longer term strategy, we're updating, we have updated all the supported SDKs at this point to ensure they default to the regional STS endpoint.

It's going to take a few years as customers slowly move and deploy and so on for this to take effect. But over time we expect traffic to drain from the global endpoint. The last thing we said was that we wanted to achieve near-perfect transparency.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1210.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1210)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1230.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1230)

But there is at least one  notable exception where we figured it would be acceptable for this change to be visible to customers. If you answer a request that originated in Sydney locally in Sydney, it's going to be significantly faster because the request doesn't have to travel halfway around the world and back. We were pretty confident that customers, if they noticed this, would be okay with it.  So in April of this year, we publicly announced that this work had been completed. STS had deployed a dedicated STS global endpoint infrastructure in every enabled-by-default region and rerouted traffic to it within those regions from customer VPCs.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1250.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1250)

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1270.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1270)

Now that it's done, what was the effect?  As the work proceeded through this project, the STS team tracked the number of unique accounts they were seeing making cross-region calls back to the global endpoint in US East One. In order to show it here, we've normalized this as a percentage of the total callers on the Y axis.  As you can see, as the project starts, we're oscillating below 100 percent, and as the project proceeds from about mid-March to about mid-April, this number drops down to practically zero. Many thousands of accounts which were dependent on cross-region calls to STS are now being answered locally in-region with no changes made on their part.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1300.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1300)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1320.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1320)

A nice side effect of this work was in the performance and latency of these calls.  As obsessive operators, the STS team monitors the performance of calls to the global STS endpoint from every region. Calls had previously had to transit from Singapore or Sydney or wherever to North Virginia, so the latency of responses tended to be dominated by the network round trip time. You can see the STS team's measurements of  latency change as the project proceeds. It was really satisfying to see calls that previously took up to 230 milliseconds dropping to 20 milliseconds even at P99.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1330.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1330)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1350.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1360.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1360)

### Lambda's Challenge: Managing Fleet Health at Massive Scale

Our first availability axiom was ensuring that our regions operate independently.  Focusing on this axiom drove a major change to ensure that all customers of STS have their calls answered in-region, even those who continue calling the global STS endpoint.  This has significantly improved both the reliability and performance of STS as you experience it. The next axiom we want to talk about is  how we're making our services and customer workloads more resilient during single AZ impairments and gray failures.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1380.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1380)

To see what we've done here, we need to spend a little bit of time talking about a service called AWS Lambda. Like STS, Lambda has also become a critical service  to both our customers and our own services. I remember back when I was a customer and AWS Lambda had launched in 2014. Back then as a newcomer to AWS, I didn't really understand how or why to use Lambda. I was still running along on EC2 instances. But now after a decade later, AWS Lambda has become a core compute primitive for event-driven architectures. I use it in almost everything that I build now.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1430.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1430)

It's a core component of a number of AWS services, and it runs mission-critical workloads for customers. But to get to that point, we've had to innovate a lot to make it resilient. Lambda operates at a massive scale.  Their fleets often have tens of thousands of nodes in them. These are the fleets where we run your Lambda code, and like everywhere in AWS, we really care about your customer experience. It's our top priority. Managing the health of these fleets is critical to the experience Lambda delivers.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1470.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1470)

But occasionally, there can be a single instance that becomes impaired. It doesn't always outright fail, but something happens where it starts misbehaving, maybe becoming a little slower or producing more errors than the rest. When you have just one bad node in a fleet of 10,000, it can reduce availability to 99.99 percent.  Lambda's availability goals are actually more aggressive than this. These bad nodes that are limping along, not completely failed, can also be hard to find. So recovery can be difficult and require manual intervention. We wanted Lambda to be more resilient when these types of gray failures occur.

Automated detection and recovery are essential to maintaining really high availability numbers. But to do this, we needed to change our approach to fleet health observability. So especially with big services like Lambda,

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1510.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1510)

### Your 9s Are Not My 9s: Detecting and Remediating Gray Failures with Budgeted Health Checks

one of the things we've learned over time, as one customer put it in her blog, is  your 9s are not my 9s. A single node in a fleet of 10,000 experiencing faults might only drop the service availability by 0.001% if you consider it as a simple aggregate. But that doesn't always capture the real experience of customers. What may be happening in practice is that most customers have a perfect availability experience while a small number have noticeably worse experiences.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1540.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1540)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1560.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1560)

If you as a customer happen to be unlucky and  the single bad node is hosting many of your Lambda invocations, you could be having a significantly worse time. This is why we consider it so important to obsess over even that one instance in 10,000. We really need to detect and remediate this fast, as there may be a customer who's really noticing it. When these kinds of situations occur, it would  be natural to ask whether we should not just have better health checks and whether this is not that simple.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1570.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1570)

The thing is that when a host fails unambiguously, like when it loses power, a simple shallow health check works great.  When a host is down or stops responding, we can detect and remove it easily. In particular, taking out a host that's already down is not going to make the situation any worse because it's not serving anything. But the scenarios we're dealing with here are what we call gray failures. The host is still answering, but it's doing so slowly, intermittently, or with unusually high error rates. The health state of this host is a bit more ambiguous.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1620.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1620)

In particular, if we were to remove a lot of gray failing hosts at the same time, we could actually make the situation worse. So we have to be much more careful with gray failures. One approach is to employ what we call deep health checks.  These examine the host application much more stringently to identify these gray failures. But in doing this, we have to be careful not to set ourselves up for an overreaction. Suppose a significant portion of the fleet might degrade together due to a spike in load or a problem with a dependency. The hosts are all still doing some good work. Automatically removing all of the hosts that have just gone above some threshold would be a dangerous thing to do.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1650.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1660.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1660)

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1670.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1670)

This leaves us with a hard decision of how deep we can make our health checks without adding further risks.  When we're dealing with gray failures,  simple binary health checks are really hard to get right. What we've learned is that when you're dealing with gray failures,  health checks really need to be budgeted. We have to limit how much functioning capacity we would ever take out automatically. They have to be non-binary ideally. In practice, no host is ever truly perfect, so we need to stack rank the hosts and take out the worst ones while leaving the good ones within our budget.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1690.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1690)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1710.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1720.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1720)

Here's an example of health check budgets.  Suppose we're dealing with gray failures and we say any instance is unhealthy if its error rate exceeds some number. We need to ensure we won't overreact and take too much of the fleet out. We need to know how much we can afford to remove.  I mentioned that this is a non-binary comparison as well.  Health checks need to compare health among all available resources, not just against a static threshold. We can see pretty clearly that this instance is an outlier with a much higher error rate. But we're doing that not based on some known threshold, but by looking for the outlier among the large number.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1740.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1740)

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1760.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1760)

We're looking for the proverbial needle in a haystack. We want to find the single instance or two with problems when the others might have tiny error rates but are generally fine.  How can we compare metrics like errors among a pool of tens of thousands of nodes quickly? In AWS services, we tend to  send structured log files to CloudWatch Logs, typically using our embedded metric format. This allows us to combine logging and metric production into a single pane of glass. Because we have metric data in our log files, we can analyze that data using CloudWatch Contributor Insights.

Contributor Insights actually started as an internal AWS monitoring tool that we made available to customers as well. If you were to ask our service teams, you'd probably find that Contributor Insights is one of their favorite features of CloudWatch. That's because at our scale we have so many customers, each with their own unique individual experience, and it helps us understand those individual experiences without having a per-customer metric.

This approach is far more cost efficient and allows us to focus on just the top-end contributors. These are the customers that may be having a different experience than everyone else. The service allows us to look at very high cardinality data, meaning there are a lot of unique keys like instance IDs or customer IDs, and then graph the top-end contributors using some data points from those log files. This produces visualizations that help us visually detect when there's an outlier for error count in a fleet of thousands of nodes.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1850.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1860.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1870.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1880.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1880)

However, we want to do more than visualize this data. We want to take action on it using automation.  We built a fleet health service in Lambda. Instances send their log files to CloudWatch Logs,  and our log files include data like fault count and instance ID. We then analyze that log data with CloudWatch Contributor Insights rules,  looking for instance contributors to faults. Next, we create a CloudWatch metric math alarm on those rules.  We look for any contributor that's responsible for 66% of the total errors the service is seeing. Large percentages like this work pretty well because they're fairly conservative, which means they won't produce a lot of false positives. Even a large value like 60 to 70% will reliably find unhealthy instances.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1920.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1920)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1930.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1930)

Once the alarm is triggered, a worker node responds by first finding the top contributor's ID and then checking its token bucket. This is a really important step because we don't want automation like this to run away  due to a bug or some unforeseen condition. There's a velocity control on how many times the health service can do this within a defined time span.  If the instance is part of an Auto Scaling group, it uses the set instance health API to mark it as unhealthy and lets Auto Scaling do its thing. Otherwise, it terminates the instance and allows the service to control replacing it.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/1970.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=1970)

This service started as something that Lambda built, but it's actually become a general-purpose health service that lots of AWS services take advantage of.  It's an example of how a lot of innovation happens at AWS. An individual team or even a single engineer has a great idea and builds something that's useful for lots of different teams, and it ends up making them all more resilient.

### Zonal Event Detector and AutoShift: Automated Response to Availability Zone Impairments

Another challenge we saw in Lambda were gray failures affecting single availability zones, where some event impacts the latency or availability of resources in that zone. While we run your invocations across multiple AZs, even a small impairment in a single AZ could have significant impact to your resources. We realized that the same outlier detection approach would work equally well to find impaired availability zones. It's basically the same problem with a much lower cardinality.

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2010.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2010)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2020.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2020)

Lambda built Zonal Event Detector,  which looks at different metrics to detect what an AZ may be unhealthy. This allows us to shift away from that zone and only invoke your functions in the AZs that we observe to be healthy.  Like the Fleet Health Service, Zonal Event Detector works so well that other services wanted to use it too. We started to build more capabilities that could use this telemetry for detecting single AZ impairments.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2050.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2050)

One of the features that we launched in 2022 is Zonal Shift. Zonal Shift is an API that allows us to manually or automatically shift  traffic and resources away from a specified availability zone. Combined with the Zonal Event Detector, we now have common signals and standard tools that all our service teams can use to automatically shift away from an availability zone in the event that an impairment happens. We also put this capability in the hands of customers, so you too can use the same tools as AWS to shift away from an AZ. We even made Zonal Shift free to customers.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2090.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2090)

Zonal Shift supports all Network Load Balancers and Application Load Balancers as well as all the scaling groups and EKS clusters. When you trigger a zonal shift, we inform both the DNS and load balancing layers to stop,  so the traffic stops being sent to the AZ you've shifted away from. Auto Scaling sends all new instance scaling and replacements to the remaining AZs. EKS removes endpoint slices from the endpoint and stops scheduling new pods in that impaired AZ. For safety, we strictly limit zonal shifts to one Availability Zone at a time.

We don't want to take two Availability Zones out. We've yet to meet a customer who is sufficiently large-scale that they could lose two out of three Availability Zones and keep going.

We also recognize that for customers, detecting a zonal event can be quite difficult. So we saw another opportunity. Having given the API for free, we saw another opportunity to shift more of the responsibility to our side. We're in a position running the platform to look at a broader set of telemetry, including our infrastructure metrics, allowing us to better detect when these things happen and respond quickly on behalf of customers.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2150.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2150)

So we use the Zonal Event Detector service  that was born in Lambda as a system-wide source of telemetry about Availability Zone health. And it powers what we call Zonal AutoShift. With AutoShift, if you enable it, we manage all the observability and perform the shift for you if there's an issue in an Availability Zone. When one of our metrics like connection failures or network reachability or the quantity of missing metrics shows that one Availability Zone is an outlier, it goes into alarm, and we trigger a zonal shift. Again, we strictly limit this to one Availability Zone at a time.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2190.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2190)

 We use this capability for both AWS services and customer applications. So that means when you're interacting with a regional AWS service like SQS or Lambda or S3, we're automatically responding to the event, so those AWS services recover really quickly. For resources that you've enrolled in AutoShift, we're performing the same action for you at the same time. So this provides a single automated response driven by Zonal Event Detector and ensures that we're all able to make the best possible use of Availability Zones.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2230.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2240.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2240)

### Testing Rigorously: Game Days, Competence Envelopes, and the Dedicated AWS Test Region

So our second availability axiom is around resilience to Availability Zone impairments, and in particular to gray failures.  And this axiom inspired the Lambda's Fleet Health Service, the Zonal Event Detector, as well as zonal shift and AutoShift.  The next axiom we're going to look at is testing rigorously. At AWS we commonly say that if you haven't tested something in the past week, it's probably broken. Regular testing is part of our culture.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2260.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2260)

In order to use Zonal AutoShift, we also enroll you in practice mode.  Following a per-resource practice schedule, we perform a weekly AutoShift to make sure all of our service teams and customers are ready and can actually perform a zonal shift without impact to their services or applications. We regularly do this in our production fleets, and it helps build confidence that all of this is going to work when we need it.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2290.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2290)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2300.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2300)

So just like an extra point kicker, they practice over and over, going through the same mechanics every  time. And so when it's game day, nothing changes. They do the exact same thing they've been practicing all along. Extra point kickers  tend to make most of their kicks. It's like 95 to 96 percent. And it's because practice is so similar to the real thing. It's all muscle memory at that point. They have a high degree of confidence they're going to make that kick.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2320.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2320)

 As an undergrad I was a philosophy major, and I never thought that I would be quoting philosophy on stage, but I think our mental model is captured really well by this quote: We are what we repeatedly do. Excellence then is not an act but a habit. And so while Will Durant and Aristotle were actually talking about ethics, I do think this way of thinking is equally applicable to many different disciplines. By practicing over and over, we build up this idea of excellence. When the real thing happens, it's just that muscle memory kicking in.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2360.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2360)

So one of the ways that we do this is with what we call game days.  We want to know that our services are resilient to a variety of different failure modes. Individual teams will test their services in pre-production environments, obviously. But when we build new AWS regions, before those regions go live, we usually set aside a week or two to run a set of game day tests on the production services that are ready to go. So this gives us an opportunity to test in a real production environment with all the services there at scale without risking any impact to any customers.

We test a variety of different things like power outages, network partitions, and service failures. And they help us validate how the other services will behave during failures, as well as how they're going to recover when things go back to normal.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2430.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2430)

These also provide an opportunity for our on-call engineers to train for real events, exercise and refine their run books, and ensure the required observability is in place so that they can see and detect the event when it happens. It helps validate what we call our competence envelope. Professor David Woods at Ohio State University talks about socio-technical systems and having a competent competence envelope.  This is an analogy with an aircraft's flight envelope, which defines the safe boundaries in which an aircraft can operate in terms of altitude and speed. Pilots know to stay within the competence envelope as it is the safe tested space. If they go outside of that, it may be fine, but things become more uncertain, so they try not to do that.

Similarly here, a competence envelope is the space within which a system is known to function competently or robustly. When pushed beyond its competence envelope, systems may still function, but tend to be more prone to failure. This competence envelope we find is a useful mental model to think about test coverage. If you think about a system you own and all of the modes you expect it to be able to operate in, such as having a couple of fallback paths in the code where it will call a different way, or how it will deal with the failure of a host if that happens, when was the last time you actually tested all of those? You're assuming they're in your competence envelope, but when did we last test them?

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2490.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2490)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2510.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2510)

 As we think about a competence envelope applied to distributed systems, there's a set of inputs, parameters, and failure modes that our systems operate in with expected behavior. We're generally not near those competence envelope boundaries, so we're comfortable and safe.  But as we start to move to the edge of our competence envelope and even beyond it, by being exposed to modes that we haven't seen before, we enter the danger zone, where our system can start to exhibit unexpected behavior to unknown conditions. In our case, it's not flying outside of a max altitude or top speed, it's dealing with failure modes that we haven't practiced with. This is where surprises happen and where danger can occur.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2560.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2560)

Our competence envelope is defined by the failure modes we regularly test against. While I'm showing our operating mode expanding here, it can also be true that our competence envelope shrinks. If we're not regularly testing these operating modes, we might not get the expected behavior. So how do we maintain or even expand that competence envelope?  Well, in airplanes, you can't really change the characteristics of the flight envelope. You're kind of stuck with max altitude and top speed. But fortunately in distributed systems, our ability to produce expected behavior is dynamic.

By regularly testing and validating our expected behavior, we can maintain the size of our envelope and we can grow it by regularly exposing our services to a set of failure modes that we might encounter in the future. We want fewer surprises and we want predictable behavior. We do that by turning unknowns into knowns through constant testing. This is part of the test rigorously axiom. If we haven't tested it recently, it's broken. It won't work as expected, and we'll find ourselves in danger of not meeting our goals.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2640.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2640)

Doing game days for individual teams in pre-production environments in our new region builds is a great start. But we only have so many new region builds and a short window between when all of our services are ready and when we want to go GA and start onboarding customers. We want to do it at a much greater velocity with all of our services in production environments. The results from these new region launch  game days were so useful that we decided to make a more significant investment and build a dedicated full AWS region just for this kind of testing.

We wanted to be able to run game day tests in a safe space with no customers anytime, and not just in the run-up to a region launch. We recently finished building an entire test AWS region with three availability zones and a full complement of the AWS services that come at region launch. To be clear, it won't be open to you as customers. It's not going to show up on your console, and it won't be on the health status dashboards because we're breaking it all the time.

The game day region is helping us to further expand our understanding of how our systems function in rare conditions. The idea is that our service teams all treat the game day region as just another production region. They don't think of it as gamma. It just happens to be a region that doesn't have any customers in it. This allows us to continuously propose and schedule game days without waiting for the next region to launch and the short window we get then.

So what are the kinds of things we test? The game day tests tend to be a mix of

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2710.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2710)

some standard benchmark tests like powering down or disconnecting one AZ from the network.  It's pretty common for teams to use a common dependency service that we might break, or we might perform a cold restart of a service to see how they will recover and how the services using them will recover. We also commonly perform more specific tests to validate if we have an incident and we put a fix into some code to solve it. We will actually perform that, simulate that issue later to validate that in the test region that it really does work and does recover the way we think.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2760.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2760)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2780.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2780)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2800.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2800)

### Metastable Failures: Understanding and Preventing Sustained Overload States

From time to time we also look at novel rare events that we've just never seen before, but we want to know what will happen if that event ever happens. Our third axiom focuses on rigorous testing.  This has inspired not only a set of high standards in terms of continuous integration and deployment, but also the concept of running these region-wide game days and building a dedicated region for that purpose. Our final axiom is going to be about protecting against overload.  Overload is one of the most common sources of impact that we see. While we have a number of protection mechanisms like load shedding, rate limiting, adaptive retry strategies, and more, there is a type of overload situation that's quite rare but can be really impactful.  We've seen it occur in our services and it's something that we like to prevent. What I'm talking about are metastable failures.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2850.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2850)

At a high level, a metastable failure looks like this.  First, the system starts in a stable state and everything's good. But then over some period of time, load is going to increase and we cross the threshold and enter this vulnerable state. The system is still healthy and it's not in an overloaded state yet, and the system could run like this for months or even years. And then at some point, there's a trigger, and it creates a condition where the combination of existing load combined with that spike transitions the system into a metastable state.

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2860.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2860)

In the metastable state, a feedback loop sustains the overload effect and keeps the system in a stable down state, not recovering.  The system maintains and remains in that state until a big enough corrective action is applied, which is typically through some kind of manual intervention by an operator. So let's look at a little bit more of a concrete example. One common type of scenario where we see metastable failures happen are in queues. And it's important to know that queues are all around us, right? They might be embedded in libraries that you use or part of how networking equipment manages moving packets around. So there are lots of places where a queue could exist in your system, even if you're not using one directly like an SQS queue.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2910.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2910)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2920.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2920)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2930.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2930)

Under normal conditions, our server here can process messages in the queue and the queue depth stays manageable. It doesn't grow beyond what the system can successfully process. But when there's a sudden load spike, in this case that's that trigger effect, the queue depth grows.  And so the server continues to process messages unaware of the spike. But as it's working its way through the backlog, it can't get through them all.  Eventually the messages in the queue get older and older. And now things that we're waiting on those messages to be processed, maybe like a TCP client,  they've stopped waiting. So we start to have wasted work, as the server processes messages that don't have any chance at success.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2950.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2950)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2960.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2960)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/2970.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=2970)

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/3000.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=3000)

And because we're processing first in, first out, we're always working on the oldest messages in the queue.  And so then we get more new requests coming in, adding more work to that backlog. And of course those clients that got tired of waiting, well, they're going to retry the requests, continuing to add to this queue depth.  Meanwhile, our poor server is just continuing to process the oldest messages in the queue, and it's continuing to perform wasted work.  And so the combination of steady state requests and the work amplification of retries have created that sustaining effect. The queue depth at this point is insurmountable, so we can never get to the messages that could be successfully processed at the top. Even after that load spike has been removed, the system is in a metastable state.  It's doing lots of work, just none of it is useful.

So we've seen a pattern of these metastable failures across the industry. They can lie dormant in systems, especially those with queues, though not only queues.

Because metastable failures tend to have long recovery times, they are a high priority for us to find and prevent. At AWS, we brought together a group of scientists within the company to study them deeply and ask how we can efficiently find systems that have these vulnerabilities so we can test and fix them proactively.

One of the big challenges with so many services and microservices is figuring out which services might have a latent metastable failure. Direct testing is quite expensive. You do not want to do it in production, and for each single service, the parameter space of queue lengths, load, and retry rates is very large. The scientists built a multi-step strategy to efficiently search this parameter space and map out where we should test it. They also open sourced a lot of it and published it so that you could potentially use it if you liked.

Their strategy consists of a statistical model that can quickly explore the full space for a given service, which has been open sourced. There is a single process simulation that can also run on a laptop and cheaply simulate one single data point. There is an emulation with physical servers running dummy code that cheaply imitates the real service and clients. Finally, once we have narrowed down on something that we think is really a potentially potential problem area, we can run tests with the real application and validate the results. This strategy of quickly searching our way through the space saves us a huge amount of time and energy.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/3100.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=3100)

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/3110.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=3110)

The statistical modeling piece was built and open sourced by the team in a tool which they called Metafor.  This performs the calculation and creates  these visualizations. It is a really lightweight way to begin understanding where a metastable failure might occur in a system. In this visualization, you can see the queue length on the X axis and the number of outstanding retries on the Y axis. Each point on the graph represents a point in the state space. The arrows visualize the most probable transition in the next time unit.

Where you see the arrows moving left, we are likely draining the queue over time. Where the arrows are moving right, the queue is likely growing. The team released a paper recently with the details of the research and presented it at HotOS 2025. If you would like more details, the research paper and GitHub link are there in these QR codes.

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/73802661d19fdf5a/3180.jpg)](https://www.youtube.com/watch?v=gLL2eOPxclU&t=3180)

### Transparent Innovation: Making AWS the Best Place for Mission-Critical Workloads

Our fourth and final axiom for today was for our services to protect themselves against overload. This axiom has inspired us to fund work on tipping points and metastable failures. We have taken a look at some of the behind-the-scenes work  where AWS is innovating on your behalf in areas that may not be completely obvious to you as our customers.

We have helped migrate almost all of the existing global SDS traffic transparently to be served in each region, helping strengthen the regional isolation of customer workloads. We built a fleet health service and zonal event detector that powers zonal autoshift, allowing AWS services and customers to detect and respond quickly to single AZ impairments. We built the AWS test region so that we can run game day activities all of the time, accelerating our ability to continuously validate expected behaviors in our services and continue to expand our competence envelope for new operating modes.

Finally, we are actively pursuing research on metastable failures to help protect our services against rare but impactful scenarios that can create overload situations. We are always looking for ways to innovate that transparently improves the resilience of the cloud and reduces the burden for customers to achieve their resilience goals. Our obsession with resilience and our culture of innovation makes AWS the best place to run mission-critical workloads. Thank you so much for attending the session on building AWS resilience.


----

; This article is entirely auto-generated using Amazon Bedrock.
