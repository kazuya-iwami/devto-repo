---
title: 'AWS re:Invent 2025 - Secure AI Agents Delivering Efficiency in Regulated Industries (WPS317)'
published: true
description: 'In this video, Amanda Quinto from AWS Brazil discusses delivering secure AI agents in highly regulated environments, featuring case studies from Sicoob (Brazil) and Holland Casino (Netherlands). Sicoob''s CTO Edson Lisboa explains their Amazon EKS-based architecture running open-source LLMs (Llama, Mistral, DeepSeek-R1, Granite) with Karpenter and Keda for cost-efficient GPU scaling, achieving solutions like Sicoob Code AI for 10,500 developers and saving 400,000 human hours through automation. Andries Krijtenburg from Holland Casino demonstrates their Amazon Bedrock AgentCore implementation using Strands Agents for management insights on costs, security, and compliance. The session emphasizes regulatory compliance frameworks (NIST, OWASP, ISO 42001), AWS''s responsible AI practices, data sovereignty requirements, and practical lessons including fine-tuning system prompts, using Bedrock guardrails, and managing non-deterministic agent behavior in production environments.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/0.jpg'
series: ''
canonical_url: null
id: 3085359
date: '2025-12-05T05:17:22Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Secure AI Agents Delivering Efficiency in Regulated Industries (WPS317)**

> In this video, Amanda Quinto from AWS Brazil discusses delivering secure AI agents in highly regulated environments, featuring case studies from Sicoob (Brazil) and Holland Casino (Netherlands). Sicoob's CTO Edson Lisboa explains their Amazon EKS-based architecture running open-source LLMs (Llama, Mistral, DeepSeek-R1, Granite) with Karpenter and Keda for cost-efficient GPU scaling, achieving solutions like Sicoob Code AI for 10,500 developers and saving 400,000 human hours through automation. Andries Krijtenburg from Holland Casino demonstrates their Amazon Bedrock AgentCore implementation using Strands Agents for management insights on costs, security, and compliance. The session emphasizes regulatory compliance frameworks (NIST, OWASP, ISO 42001), AWS's responsible AI practices, data sovereignty requirements, and practical lessons including fine-tuning system prompts, using Bedrock guardrails, and managing non-deterministic agent behavior in production environments.

{% youtube https://www.youtube.com/watch?v=R6m1vICK7-8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/0.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=0)

### Introduction: Secure AI Agents in Highly Regulated Environments

 Hello, everyone. Thanks for joining us today. I am Amanda Quinto, and I am a Sr Solutions Architect at Amazon Web Services in Brazil, responsible for public finances for the past 6 years. Today, we're going to talk about secure AI agents delivering efficiency in high regulated environments. I'm here with two really nice customers, Edson and Andres. They are over there and they will be presenting with me today. So again, thanks for joining. It's really nice to be here with you guys.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/40.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=40)

 There's a little bit about our agenda. We are going to talk about high regulated environments and how we can operate AI workloads on top of those kinds of environments. Then we are going to talk about how we can use Amazon EKS to run AI agents and AI workloads on AWS. Sicoob will explain how they are using this kind of environment to run workloads for generative AI in high regulated environments. We are then going to move to Amazon Bedrock and understand how Bedrock can help us deliver AI agents and also stay inside of our regulations. And then Holland Casino, represented by Andries, will show you how they are using Bedrock AgentCore to run their workloads on AWS. So let's get started. We have about an hour here and I will try to keep you engaged. I know it's a really heavy event and you guys walked a long distance to be here with us today. So thank you again.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/100.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=100)

 When we are talking about high regulated environments, we are no longer just thinking about deploying models on AWS. We need to operate inside of dense amounts of laws, standards, and patterns to stay inside of the regulations of our current countries. Here we are representing two different countries, Brazil and the Netherlands, but we can see these movements all around the globe. Europe, for example, has the European AI Act, which is a movement with a strict risk-based framework that bans some uses completely from AI and also determines different obligations for high-risk systems to run both on cloud or in on-premises environments when we are talking about data and AI workloads.

Brazil is combining their emerging AI legislations with the legislation that we already have on top of data, something like LGPD, for example. We also have two different patterns. We have some obligations that are federal, mostly for data from the public sector. For example, we should keep this data inside of Brazil, and we also have some discussions around data sovereignty. We also have some determinations around specific industries. So for example, my industry, finances and public finances, have some obligations that are determined by the Central Bank. We have received the normatives from the Central Bank, and we should make our customers run inside AWS respecting these kinds of obligations.

This is our worldwide movement. We have today more than 1,000 different AI regulations around the world, and this is representing more than 69 countries. So it's not easy to stay inside of the regulations as providers such as AWS. We work specifically in each customer case to deliver the best architectures that fulfill these kinds of requirements. There is something in common when we are talking about these kinds of frameworks and obligations when we look into Brazil or South America or the United States or Europe.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/230.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=230)

 There are always some pillars that are similar in different scenarios. We have compliance and governance. We have legal and privacy controls and risk management. These four pillars always appear in different normatives and regulations around the globe, and that can make for us the bottom layer that we can look into and deliver different architectures that work for most cases around the globe. But again, there are different obligations in different countries and different industries that we need to always make sure that we are responding correctly for these kinds of environments and obligations.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/290.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/300.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=300)

### A Layered Approach to AI Regulation: From Responsible AI to the AWS Well-Architected Framework

So there are some patterns that we can follow so we can understand the customer needs, the regulation needs, and stay on top of these kinds of obligations. We can see this in a layered pattern that can help us here. So we have at the bottom, we  have the generative AI application, for example. On top of this, we have regulatory layers.  As I said, it's different from each country, it's different for Europe, it's different for Canada, it's different from Chile and China, for example, and we need to be specific here.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/310.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=310)

But there are other layers that we can understand in a broader way. When we're talking about responsible AI, AWS follows responsible AI as a really important pillar that we are implementing for our customers and our services to deliver generative AI solutions. When we talk about responsible AI, we have some pillars and patterns when we talk about data, when we talk about safeguarding our customers, when we're talking about security and prompt engineering structures that we can be aware of malicious users, for example.  All of these discussions are inside the responsible AI layer in AWS. This is really interesting content that you should read about if you like this kind of documentation. It's comprehensive documentation, and there is a lot of good instructions there. If you follow the responsible AI patterns, you're probably going to be inside the obligations in your countries.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/370.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=370)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/390.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=390)

We also have compliance and standards.  Here we are talking about ISOs. We are going to talk a little bit further about SOC 2, but it's important to know that there are specifically ISOs for generative AI workloads already. We are also looking at this and delivering architectures on top of it. Here we are talking more about the technical aspects.  It's a layered approach. We have the bottom layers on the regulatory, and on top of this, we can think about more technical perspectives. We can see the NIST framework, for example. The NIST 800-53 framework, for example, is a risk management framework for AI. NIST is just a pattern that we can follow. They don't really audit your environment for this, but we can look into this framework and see if we are architecting our systems on generative AI following these patterns.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/480.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=480)

When we are going to undergo some auditation from the responsible government or something, we are probably going to be inside these obligations. The AI framework is a really good framework. It's the main framework that the United States is following for their obligations and the laws and definitions that you are following here in the US. On top of this, there is OWASP. OWASPs are really common when we are talking about, for example, OWASP top 10 vulnerabilities for security. There is also the OWASP LLM AI security cybersecurity pattern.  You can see it's an improvement actually. You can see what are the most common vulnerabilities that can appear in generative AI workloads. You can see how we can achieve some techniques to avoid this kind of problems. OWASP is always a good idea. There are also the top 10 vulnerabilities for LLM applications. You should also be aware, and there are also changes because it's really new. We are always having new vulnerabilities. It's not a good thing, but it's a pattern in the market.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/550.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/560.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/570.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=570)

On top of all of this, we already have the AWS Well-Architected Framework layer for generative AI applications. You should know the AWS Well-Architected Framework, and we have a bunch of different layers for our six pillars. There is already a layer for generative AI workloads as well. It's a live document. It's been proven as well, but it's the right guide for our customers to follow. This layered regulation and controls should help us to build this kind of workloads on AWS following and maintaining ourselves inside the regulations from our countries. Regulatories should not stop AI adoption. We think regulatories require AI adoption. We can also improve regulatory obligations using AI. We can use AI to understand the regulatory landscape and then accelerate AI workloads on highly regulated environments.  We can see this in government in Brazil.  We see this in the financial sector. We see this in the health sector as well. We can make them work together. One should not stop the other. 

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/590.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=590)

### AWS's Three-Layer Model for Generative AI: Infrastructure, Models, and Inference

Now we have discussed a little bit about how high regulators work around the world and how AWS positions itself on these matters. Let's dive deep into the technical perspective now. When we are talking about generative AI in AWS, there are three layers, and you're probably going to see this slide a lot at re:Invent. I'm going to go quickly here because this is probably the most used slide for AWS solutions architects around the world.  This is the AWS three-layer model for generative AI technology.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/630.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=630)

From the top to the bottom, we have the applications family. Amazon Bedrock is our suite and solution for all generative AI workloads running on AWS. You can use different models and leverage everything on top of APIs. At the bottom, we have the infrastructure layer. We're going to  talk more about the infrastructure layer here because of the use cases, and then we will detail this for you.

In the infrastructure layer, there are two names that make a lot of noise in the marketing: Trainium and Inferentia. These are chips specifically developed by AWS for machine learning workloads. We also have SageMaker, and within SageMaker, there's a whole world of solutions where you can deliver different kinds of models using machine learning or generative AI models. We're going to talk specifically about GPU workloads here. In AWS, we have a bunch of different types of GPUs. There is a wide availability of many GPUs in our stack, and you're going to understand how we can use GPUs to deliver generative AI solutions for highly regulated environments.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/690.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=690)

At the heart of generative AI, there is a model. There are some commonly used models here,  but before you think about the model, you should understand your use case. What are you trying to solve? What's the pain that you need to solve with generative AI? Check if there is actually a generative AI solution that you should use for this pain, for this problem in your company. If it makes sense to use generative AI for this kind of workload, then you should find a model for it.

There's a bunch of them in the market. We have Amazon Nova and other options from AWS. After this, you're probably going to use two or three models for your workload, and you will probably change this after a while. Week after week and month after month, we are seeing different models arriving in the market, and one is better for one thing while another is better for something else. You should not be in love with your model. You should use the best model for your use case. Choose two or three that work better for your specific scenario and go ahead.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/750.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=750)

You can use a foundational model and  use RAG, for example, or prompt engineering tools to solve your problem. Just to make one quick comment here, this is a 300-level session, so we're not going to explain what RAG is or what prompt engineering is in detail. We are going to say that all of the solutions architects here at the events are available to help you if you have any doubts. You choose your model and have your problem solved. You need to understand if you're going to use, for example, prompt engineering or RAG to make your data work inside of this model or in parallel with this model. That should work for you too.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/790.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=790)

If that doesn't work, you can fine-tune your model.  You can take your specific data for your specific industry and fine-tune your model to fulfill your use case and solve your problem. That works too. It's not an easy task to do. You need a lot of data to do this properly, but it works well when we're talking about specific industries. For example, in Brazil, we are seeing a movement of financial sectors using this, the health sector using this, and also the public sector. The government is thinking about developing and fine-tuning large foundation models with specific data.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/840.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=840)

That should work for some large use cases, but it's not an easy task to do, and you need a lot of data. Or you can train your own model.  For this, it's a heavy task to do as well, but it should work. You should have a lot of data again. You should use a bunch of GPUs to train your models, but that works as well. You need to understand which approach is better for your workload, for your use case, and for the size of your company, and then move forward.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/860.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=860)

We have a model now, so what we can do is we need to do the inference.  At the heart, there is a model, but we need to deliver this model to our application in a way that we can do predictions. We can run prompt engineering on top of this model faster and easier with lower latency if you're talking about some production workload. For example, you can also do batch predictions, but you also need an inference endpoint that is generally available, that's resilient, and that's secure as well for your workload. This is important here.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/900.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=900)

### Why Amazon EKS for AI Workloads: The Power of the Open Source Ecosystem

In AWS, there are three different ways to do this. We can use Amazon Bedrock, and  we are going to talk about this a little bit further.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/930.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=930)

You can use SageMaker as well. There are specific models that you can deploy on SageMaker, and you can also deliver endpoints for making inference on SageMaker. You can also use Amazon EKS. There is a really interesting movement running right now, and we are seeing this growing on AWS with Amazon EKS. If you search for data on AWS, there's a bunch of interesting materials. We have  a bunch of customers using this around the globe.

To use Kubernetes as your main platform, you need to understand that this is overhead on top of your infrastructure team. If you already are using Kubernetes to run other kinds of workloads and to run your workloads in production on AWS, your team already knows the solution. That's probably a path to go. But if you don't know Kubernetes and you don't know Amazon EKS, and you're going to discover this while you are trying to implement generative AI workloads on AWS, that's probably going to be a hard task to do.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1010.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1010)

I am a really big fan of Kubernetes, and if you are doing this, you're probably going to get some really good outcomes after all the overhead on your team. When you are using Kubernetes as the main platform, we probably already have some main pillars for production workloads on AWS such as disaster recovery and cost visibility. We can have deployment automations as well on Kubernetes, and we can also deliver and deploy our inference endpoints and our generative AI workloads  on Kubernetes. Instead of deploying Amazon EC2, for example, with GPU and running everything inside of this EC2 instance and having problems with scalability and deployment automations and disaster recovery, you can put this inside of Kubernetes with Amazon EKS and then run your generative AI workloads in this kind of architecture.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1040.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1040)

I'm talking a little bit more than I used to, and it's really cold in here, so I'm going to take some water, but let's keep going.  Why do customers choose EKS? As we were discussing, there is a bunch of good things such as disaster recovery, scalability, and cost predictions as well. We can have some tools for this. But for me, the main reason is the open source ecosystem. The open source ecosystem for Kubernetes has been really good and really dense with material for more than ten years for different kinds of workloads, and for generative AI workloads, this isn't different.

There is really good material for Kubernetes generative AI workloads with the open source community. They are always developing new solutions for this. When this started, maybe a year and a half ago, we didn't have solutions for deploying clusters with GPU. We didn't have, for example, images that were GPU prepared. We didn't have infrastructure code solutions for this. But a year and a half later, we already have some projects that are really mature and an open source ecosystem that supports these kinds of workloads all around the globe.

If you guys like this kind of material, you should search for data on AWS. There is a bunch of interesting materials over there. We also have all these pillars here for the reasons to choose Kubernetes. For me, the open source ecosystem is the most interesting one. You can also run this everywhere. You can run this on AWS. You can run this in different providers. You can run this inside of your own infrastructure if you have, for some reason, GPUs over there. You can make some tests on your on-premises environment, for example. There's a bunch of different reasons here that we can use EKS.

### Sicoob's Journey: Running Generative AI at Scale in Brazil's Financial Sector

Now, Edson will share a little bit of the Sicoob case with us and how they are using this kind of architecture to fulfill their business needs. Edson, please. Thank you, Amanda. Hello everyone. As Amanda said, my name is Edson Lisboa. I am CTO at Sicoob. Sicoob is one of the largest cooperative financial systems in Brazil, the credit union in Brazil, and it's my pleasure to be here with you today during the next few minutes to share on the re:Invent stage a little bit more about our generative AI journey and how a highly regulated financial institution is successfully adopting generative AI.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1210.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1210)

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1220.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1220)

 At scale, keeping security, governance, and compliance as non-negotiable pillars.  Let me start with who we are. Sicoob is a cooperative financial institution in Brazil, and we are present throughout the country. As you know, Brazil is almost a continental country, and Sicoob is present there. Sicoob offers almost the same financial products and services as a traditional bank does. We are present in almost 2,000 Brazilian cities, and it is very interesting to emphasize that over 400 Brazilian cities have only Sicoob. We have more than 300 credit unions integrated with our cooperative system. We have 14 central cooperatives and 4,700 branches throughout the country with more than 60,000 employees. All this structure serves more than 9 million people in our country, and it is very important to emphasize that even considering this presence in our country, over 98% of our transactions are performed on our digital channels such as mobile app and internet banking.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1320.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1320)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1360.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1360)

As a financial institution in Brazil, we operate under very strict regulation from our Brazilian central bank.  For us, governance is not only a compliance checkbox, but it is truly a competitive differentiator. We decided to create a centralized structure to control decisions about governance with clear accountability, risk classification, and continuous monitoring. When we started our generative AI journey, we defined four unbreakable pillars.  The first one is security. As a financial institution, the security of our credit unions and members' data is our priority. The second one is scalability. Considering our current numbers and our goals for the next few years, we really needed an environment and a solution to support our size and growth needs.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1440.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1440)

The third pillar is efficient cost management. It was very important for us to run large models considering a pay-as-you-go strategy and considering open source LLM adoption. The fourth and final pillar is multiple LLM models. We wanted to use the right model for the right task. We really believe that only one model does not fit our needs.  These are the open source LLM models that we are using. We are running on AWS using Amazon EKS, and the models are Llama from Meta, Mistral 7B, the Chinese model DeepSeek-R1, and the model from IBM named Granite. Sometimes we are using multiple models side by side in the same use case, and in other cases we can choose a specific model considering the fastest or the most accurate for that subject.

Considering all this strategy and this information, Amanda will explain a little bit more about the technical information of our environment.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/1520.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=1520)

### Technical Deep Dive: Sicoob's EKS Architecture with Karpenter, Keda, LiteLLM, and VLLM

Thank you for sharing. So again, we have a model as Edson said. They choose their model, they choose the problem they are trying to solve with generative AI. They understand the regulated environment and what obligations they should achieve,  and this is the architecture that they are running on top of on AWS to solve these problems. We are going to discuss this architecture here. It is actually a simple architecture. At the core of this, we have an Amazon EKS cluster. In the Amazon EKS cluster, we are deploying EC2 instances with GPUs.

To do this, there is a bunch of best practices that we are discovering with our customers over the past year and a half or so, and we are trying to bring it here for you when you are going to try this on your specific cases. For example, you should think about using specific AMIs for AI workloads and GPUs. There are, for example, drivers from NVIDIA that you do not want to deploy by yourself. There is a bunch of different versions that can cause problems when you are deploying this. So use specific AMIs.

Another good idea is to download all the models to Amazon S3, for example. So instead of downloading the model every time you run a new pod inside of Kubernetes, you can use this on Amazon S3 and it is really quick to deploy your own EKS when you already have the model there. This is a good idea when you are deploying this kind of infrastructure. But actually this is not a Kubernetes presentation, so I am trying not to dive so deep into the Kubernetes perspective. However, there are two solutions, two open source solutions that are really helping and making this workload make sense for this workload on Kubernetes.

You know GPUs are expensive. They are not the cheapest infrastructure. So they are using Karpenter to scale Kubernetes with instances on demand. Instead of just deploying the instances based on what model needs what shape of GPUs, they deploy their cluster with these node groups, and that is all they are using Karpenter to do this using spot instances on Kubernetes. So instead of just leaving the GPUs there without utilization, they pay as they go, as Edson said, and use these GPU instances based on their needs.

But there is another solution on top of this that is also an open source solution called Keda. This is for scaling the pods inside of the cluster. So whenever they receive a request from the environment, if the final users are not using the environment, they do not run any pods, so they do not deploy any GPU instances, so they do not pay for it. Keda is used to scale pods based on the request. So we receive the request in this environment, then we will deploy it for the end user.

It is something that you should think about because it took some time to deploy it. But if there is not, I do not know, a transactional workload that is running inside of the cluster, maybe the customer or the end user can wait some seconds to deploy this and then start using the generative AI model or something like this. So this works better in scope, and that is why these environments make so much sense with this kind of architecture combining Karpenter and Keda using EKS clusters. They can deploy different models. They can achieve different business cases using this environment.

There is a bunch of different cases here and we will explain for you a little bit later inside of the same cluster. So the same cluster is running this. To operationalize this, they are using other two open source solutions. As I said before, that is one of the core reasons that I believe that Kubernetes makes so much sense for generative AI workloads. The open source solutions. I already talked about two: Karpenter, which is developed by AWS but is an open source solution, and Keda.

There are also another two. We have LiteLLM. So instead of managing these models inside of the cluster, I do not know, in command line, we can expose this for the final users using LiteLLM. This is also an open source solution. It is also, somehow, how to deploy this on AWS in our documentation. It is a really nice solution that you can use to manage all these models inside of the cluster.

We have a customer interface and end user interface that you can use to run the models. VLLM is a solution that is new in this environment, in the Sicoob environment. Before this, they were using aEA, but they achieved much better results using VLLM. It is much faster and more compute optimized to run the models. VLLM is a new open source tool and is receiving improvements from the community, so it is probably going to be even better. We started using this kind of engine to run LLM models on top of Kubernetes. These are four open source solutions that really make sense for them to run this workload and to run different kinds of workloads inside of the same cluster.

We can also use other AWS services around this. For example, we have Amazon ECR for the images, and we are using Ingress to expose the endpoints of the models for the solutions with AWS load balancers and other solutions. However, the main structure of the architecture is inside of the cluster. This kind of solution really makes sense for this workload for them. When deploying GPU instances on AWS using Karpenter, we do not need to do the heavy lifting of thinking about which different instances are available in this zone. For example, you can put three zones in Brazil. We can put those three different zones inside of the Karpenter manifests and say which shapes of GPUs should be used. Karpenter will determine which one is better, which one is available, and which one is cheaper, and then deploy it on the cluster for you.

You can use budgets on Kubernetes to arrange the way that you take off your instances. However, they only take off your instances or your spot instances after you have another one deployed. There are some ways to work with spot interruptions when we are talking about Kubernetes environments. There is a bunch of solutions, tricks, and tips here that we can understand to help us operate generative AI inside of an Amazon EKS cluster on AWS. The main idea here is that we can run different workloads inside of the same cluster with cost and management achieved. We are going to be able to run this with some low overhead on top of the team.

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2010.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2010)

### Sicoob's Business Outcomes: From Code AI to Investment Advisors

Let me now explain which are the outcomes and what are the business solutions that they are achieving.  First of all, I would like to share a little bit about our internal intelligence development assistant named Sicoob Code AI. Cisbe is the name of our core banking platform, and we decided not to pay for or buy a specific solution to integrate into our platform. We decided to create our own solution using the LLM models that I showed you, and we integrated this solution with the IDEs of our developers. Nowadays, we have almost 10.5 thousand developers in our company.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2100.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2100)

There are a lot of benefits of using Sicoob Code AI, like automated coding insights and automated code recommendations to the developers. It accelerates the process of onboarding new developers, which was another benefit. It saves time in delivering new features and new solutions to our credit unions and our members. Not only does it save time, but it also improves the quality of our solutions and provides continuous support throughout the development cycle.  The second one is our Bekov automation.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2140.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2140)

Nowadays we have almost 8 digital robots automating manual tasks, not only simple manual tasks but complex manual tasks as well. The reason we are using generative AI integrated with our robots and AI agents is that it was possible to save almost 400,000 human hours with this kind of solution. Another interesting real use case  is our investment advisor, which is specialized support for investments using generative AI. It is possible to create good recommendations about investments to our investors considering the member profile, considering the best market practices, and considering our investment products offered by our credit unions to our members. It was another very good solution for our business.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2180.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2180)

Every day  and last but not least, I would like to share about our core bank smart assistant named CISB AI. There are a lot of features in this solution. It is possible to interact with documents, not only normal documents but legal documents and contracts, and it is possible to interact using natural language and get good answers using generative AI. It is possible to do intelligent search and create analysis and generate automatically credit loan decision reports using generative AI with this solution as well.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2240.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2240)

Finally, I would like to emphasize that there  are a lot of other benefits and results for the business considering these use cases using generative AI and other use cases that we have. It is a good option for our business using this kind of solution. Thank you for your attention, and Amanda, the stage is yours again.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2300.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2300)

### Amazon Bedrock for Regulated Environments: Compliance, Data Security, and Guardrails

Thank you, Edson, thanks for sharing the SICOOB journey. For me, it is a pleasure to be here with SICOOB. We have been working together for the past 6 years on AWS with SICOOB, so it is a really nice case. Thank you, Ed, for your presence. Let us dive into the second part of the presentation. We were talking about high regulated environments on top of Amazon EKS and GPUs infrastructures, and now we are going to talk about how we can build AI agents on Amazon Bedrock,  also covering the regulations perspective.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2320.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2320)

Let us go again to the most used slides in the ring bands. We have our two layers here, and now we are going to talk about the middle layer. Amazon Bedrock is a tool focused on builders and developers.  We are using building blocks to deliver generative AI solutions for our customers. We started, I believe, in 2023, just with some models and some endpoints for our customers based on APIs, and now there is a fully completed solution that hosts models but also covers guardrails of security, prompt engineering, and prompt caching. There is a bunch of different solutions that you can use on top of Amazon Bedrock.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2350.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2350)

We are here to talk about high regulated environments again, and we started talking about ISOs and I said we are going to return to this matter further in the presentation. Now is the time. When we are talking about Amazon Bedrock compliance, there is a bunch of different regulations that we are already achieving. There are different ISOs as well. We have SOC 1, 2, and 3. We are HIPAA eligible, and there are different kinds of regulations for different kinds of countries. But there is one specific ISO that I would like to highlight here: ISO 42001. It is specifically for generative AI workloads, and AWS with Amazon Bedrock was the first provider to get this ISO. This is a really interesting thing when we are discussing this kind of highly regulated environments, and the ISO is a really important thing for the product and for Amazon Bedrock. 

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2410.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2410)

The most common question I receive from my customers  regarding data and AI concerns is about data security. I would like to highlight that customer data is not used by AWS to improve any foundation models. Customer data is not shared between other customers. Your data is isolated, and AWS does not use this data to retrain models or improve Bedrock. This is really important when we are talking about every environment, but especially for highly regulated environments.

Going forward, you can also choose which model you would like to use. There is no black box where you send your data to an endpoint without knowing which model you are using. You choose the model that you want to use. You also put the correct permissions for your developers and builders to use those models inside Amazon Bedrock for your specific region. This is also important. It is your choice which model you are going to use, the size of the model you are going to use, and which region you are going to use it in.

When you put your data in a specific region on AWS, your data remains in that specific region. For some countries like Brazil, this is really important. We have regulations around data that must remain inside Brazil. We cannot take this data from the public sector of Brazil. This is important for our customers, and we like to highlight it here. However, you can do cross-region inferences if you want to.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2510.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2510)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2560.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2560)

For example, if you are using  Amazon Bedrock in Europe and you would like more latency for your models, you can use different regions for this. But it is your choice which region you are going to use. If you are in Europe and you are based in Europe, you can use the data inside all of Europe, not only one specific country. You can create profiles inside Bedrock with specific countries that you can use, and then make cross-region inference for your data based on your regulations as well. This is your choice. This is not made by default on Amazon Bedrock, but you can use it to improve performance while staying in your regulated environment. 

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2570.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2570)

Let's talk about Amazon Bedrock Agent Core.  Over the last year, we have seen AI evolve from assistants to agents. Agents are the hype of the moment right now. The main difference is that when we are talking about agents, we are talking about a solution that works autonomously. It is not task-defined; it can work autonomously. You can create your own agents using open-source solutions, and I am going to let Andreas talk a little bit about how you can do this and what the heavy lift is on top of it.

AWS understood that this is a need from our customers, and that is why we deliver Amazon Bedrock Agent Core for customers. If you do not know Agent Core yet, you should take a look at it. It is a really nice solution and a really nice service. We are probably going to have a bunch of new things at re:Invent in the next month. It is a really new service. Let me highlight some parts of Amazon Bedrock Agent Core that work for highly regulated environments and that can help you stay within your regulations.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2640.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2640)

We have Agent Core runtime.  You can run and host your agents with a purpose-built, serverless solution. You do not need to deploy any infrastructure inside AWS. Each session of your agents is isolated. You define some tasks for specific customers. For example, in the financial sector, you might have a customer running an internet banking solution. You can make each session isolated from one to another. This is really important for regulated environments, and Amazon Agent Core does this by default when you are using Agent Core runtime.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2680.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2680)

The other important feature is Agent Core identity.  It helps you maintain secure authorization, authentication, and credentials for customers. By default, you can put the credentials from the customer or any kind of authentication flow inside Agent Core, and it remains isolated and keeps this secure for you.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2710.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2710)

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2720.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2720)

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2750.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2750)

Sandbox environments  are helpful when you are deploying and developing your own agents on AWS. You can also do this inside of Agent Core without needing to develop or deploy anything.  It's a service that is inside of Agent Core, and you can arrange which kind of code interpreter you want to use or which kind of browser you want to use inside of the whole mechanism of Bedrock with security and authentication inside of AWS. There are a bunch of other solutions in Agent Core, but I would like to highlight these three because they are useful for high regulated environments when developing your agents on AWS. 

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2770.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2770)

When we are talking about LLM models, there's also a discussion around which data was used by the model to train. How can I know if this data has bias or not, or if it follows what I believe is correct for my company?  I would like to highlight here the Anthropic System Card. If you haven't heard of it, you should read it. It's a really interesting read of about 140 pages, and it explains how it was trained and why it is trustworthy. Anthropic is doing a really good job with the transparency of how they train their models and which kind of data they are using.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2800.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2800)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2820.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2820)

We also have AWS Service Cards for  Amazon Nova, which you can see in the documentation of Bedrock and the Nova model. There's a bunch of information there about how they were trained, which kind of data they are using, and the AI responsible practices we discussed earlier. This information is also described for Amazon Nova in these service cards. 

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2750.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2750)

Even when we are talking about all these different perspectives of regulated AI, LLM models are a non-deterministic technology.  You can use guardrails from Amazon Bedrock to stay within what you believe is the correct answer for your end users. Don't believe that the models will always write the same thingâ€”they won't. You should prepare to define your architecture on top of this using solutions such as guardrails to stay within what is correct for your end users when you are using LLM solutions. This is really important.

You can also be aware of what the model is going to respond to and prepare for malicious users, which we discussed earlier as different kinds of users for generative AI and cyberattacks. You can also use AWS Guardrails to help you with these kinds of issues.

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2920.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2920)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2940.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2940)

### Holland Casino's Challenge: Rapid Regulatory Response in a Highly Regulated Gaming Industry

Now I will ask Andries to join me here. He will explain how Holland Casino is building their agents on AWS. Andries, please. Thank you, Amanda. We're running about 5 minutes late, but I'll share very useful tips at the end, so please stay. They are very important. My name is Andries Krijtenburg, and I do indeed work for Holland Casino. It might surprise you,  but Holland Casino was established by the Dutch government about 50 years ago with a clear mandate to offer a safe environment for players. Currently we run 13 casinos in Holland and 1 online casino. We operate under very strict gaming oversight through the Dutch Gaming Authorities. This centers around player safety  and detection of fraud and money laundering.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2950.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2950)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/2970.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=2970)

Now the stakes for us are very high. If we fail to meet these regulations, casinos  will close until compliance has been restored. We will suffer reputational damage. We have a very strong brand name, even among people that do not play. Financial penalties are seriously highâ€”it's more than a headache. And we face possible license suspension.  Apart from meeting these regulations, we have other problems as well. Regulations are subject to political decision making, change frequently and suddenly, and have fixed deadlines. Sometimes we have 3 months, 1 month, or even less to change things. Holland Casino depends on third-party applications for gaming concepts, gaming machines, jackpot concepts, and casino management systems. The problem is that these third parties cannot move fast enough as we would like them to. So Holland Casino needs to have an environment where we are in control and have rapid response capabilities.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3010.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3010)

We ended up in Amazon Cloud.  In Amazon Cloud, we build our regulatory flows, our regulatory reports, and our regulatory alerting. We have been doing this since 2017 with a group of very talented colleagues, including not only technical people like testers and developers, but also architects, product owners, and IT managers. We are assisted by colleagues from Easy2Cloud. Over the years, as a team, we have had to understand Amazon Cloud very well to implement our regulatory use cases. It has been a growth period for us.

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3050.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3050)

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3070.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3070)

Our success and internal belief in AWS has enabled us to grow in Amazon Cloud with more workloads.  Now we host our central casino management system, business intelligence, data analysis using SageMaker, and even our legacy enterprise bus in the cloud on Easy2Cloud because of its stability.  Why am I telling you this? Our business culture, and perhaps many of yours as well, is to start small with an architecture, gain confidence with it, and then build up the capacity to execute regulatory use cases. This is exactly what we want to do with our AI architecture. We want to start small, gain company-wide confidence in the technology, and then grow and eventually have agents assist us with regulatory use cases.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3100.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3100)

 Let me briefly walk you through our journey so far in generative AI. About a year ago, we started using code generation with the Anthropic models in Bedrock. Then we looked at Bedrock Agents. In Q2 and Q3, we started looking at Strands Agents, and we really like them because they are simple and they show you a clear path forward. Then we started evaluating Amazon Bedrock AgentCore to host these agents on. I will show you that as well. AgentCore just came out of preview and we are right now busy putting them into production.

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3140.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3140)

We have been looking for a small but important use case, and we found this in what we call the management insight gap.  Management always has a need for cost, security, and compliance oversight. They will not log into the Amazon console to get the data because it is not the right tool for them. So management, stakeholders, and especially the security department came to us, my team, to provide them with ad hoc reports. We created many Python scripts to provide these reports, but this creates an unnecessary dependency and it does not scale.

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3180.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3180)

The scripts that we write for these ad hoc reports come back later when we talk briefly about Strands Agents, so keep them in mind.  As a first use case, we give management and stakeholder agents that provide them with the information they need on a self-service basis. Because we are starting off, these are single agents and we are not yet busy with orchestration. These agents center around costs, security, and operations, and they can have a dialogue with users. That is very important.

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3200.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3200)

### Building AI Agents with Strands and Amazon Bedrock AgentCore: Holland Casino's Implementation

We use Strands Agents for several reasons.  They are very simple, and I will show you in the next slide. They are compact with tools that you can use, but it is very easy to integrate your own tools and MCP servers. They are Python-based, and we are a Python shop, so it is easy for us. They are open source, so you know exactly what is going on.

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3230.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3230)

This is a Strands Agent.  In the middle, I declare my agent. I give the model, in this case a Bedrock model, Claude Sonnet 4.5, and I give it a system prompt. Normally we craft a system prompt that is quite long and we spend considerable time creating our system prompts, but for space purposes, I have redacted it here. Then you give it a tool, in this case a billing tool. Remember the scripts I was talking about earlier, these ad hoc scripts. It turns out that it is fairly easy to transform them into tools for your agents. So do not throw the scripts away. Just make sure that you write your system specification or your tool specification really well. That is really important.

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3290.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3290)

I had an ad hoc report for billing and I transformed it into a tool for my Strands Agent. Once I have set it up, I can invoke my agent. I can say, for example, what are my costs so far this month.  Then it gives an answer. If you use session IDs, you can delve into it. You can have a dialogue and ask, for example, what were my costs and how did they compare to last month, or I can delve into a service. So you have a dialogue, and this is what we want to give our stakeholders. You have got your Strands Agent and it works on my machine.

[![Thumbnail 3310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3310.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3310)

I can guarantee you it works on every one of  yours machines. It's easy. However, we need to give it to our stakeholders, right? So we need to host it in Amazon Cloud. We want it to scale. We want it serverless. We needed to stream the responses, and security is non-negotiable. So how do we do that?

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3330.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3330)

We take our agent.  We have rapidly fast API for streaming. You package it in a Python project, create your Docker files, your Lambda stack. You build it for authentication and authorization. You make sure it versions very well and then you deploy it. And this will work. I've done it. We've done it. And if you want to go this route, you're fine. However, if you say to me like Andre, that is a lot of boilerplate code for a simple Strands agent. Is there another way?

[![Thumbnail 3360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3360.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3360)

Yes, there is. Bedrock AgentCore.  If you look at the right hand side of the slide, in its simplest form, what you can do is bring your agent into the agent core by wrapping it into an invoke Python method. And you decorate that method with an app entry point decorator. You test locally as you normally would, and at some point you say, agent core configure on the command line. And this builds all the boilerplate code I've just talked about in the previous slide. Once you're happy with that, you say geo launch, and it launches it into production into your Amazon environment. So it's very easy for CI/CD integration, even more brilliant for rapid prototyping. If you go this route, you benefit from all the other agent core services that it offers that Amanda spoke about earlier on.

[![Thumbnail 3420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3420.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3420)

So how does the architecture look like? Simple.  We take the services as native as possible, no magic sauce. So in the middle here we have our agent core agents. They have access to foundation models, guard rails, memories, knowledge bases. We use knowledge bases, RAG as a service, a Bedrock knowledge base to obtain our regulatory information, tools. And then we've got two sets of apps. We've got in-house apps we develop ourselves and third-party apps. The in-house apps we host on Amplify hosting with Cognito user pools, federated to Active Directory, Cognito identity pools, we obtain temporary STS access keys. And this gives you direct access to the Amazon APIs. In this case, invoke agent. That is really powerful.

[![Thumbnail 3480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3480.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3480)

Third party applications that do not have the capability to access STS tokens, we just put an API gateway in between. It's a bit more effort, but well worth the compromise.  If you look at the overall picture on how we do this, we basically host our AI architecture in two accounts, pre-production and production. Not different than any other application we host in our environment. And it is surrounded by what we normally in our normal multi-account architecture that takes care of security, compliance, monitoring, and networking.

[![Thumbnail 3510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3510.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3510)

OK, my last slide before I hand you back over to Amanda.  The lessons we learned in this path of Gen AI at Holland Casino is that AWS democratizes the use of Gen AI. You do not have to study four years at a university in order to create well crafted, stable, and scalable AI architectures. So you can keep your DevOps methods, you can keep on investing in your current AWS knowledge, especially Bedrock and Bedrock AgentCore. Now this is important. This is the tips I was speaking about. That's why we placed it also in the box, right? Expect non-deterministic behavior of your agents and mitigate the risks. And here are some of our tips.

Fine tune your system prompt of your agent. Really important. Give very clear instructions. Give the do's and the don'ts, and give a very clear output format. That is really important. You combine this with Bedrock guard rails. Then agents must focus on one thing and do this very well. When I had my first agent, I got very excited. I uploaded all the tools that I had. So one agent to rule them all. The agent did not perform. Why? Because my tool specs were not really specific. And the model struggled to select the correct tool. Right. So make sure that if you need to use a lot of tools, make sure that you're very clear in tool specification. Make sure that the tool specification don't overlap with each other so that the model can select the right tool.

If you set up knowledge bases, you must invest in very realistic evaluation jobs, and you can use Bedrock for that as well. They give you very nice metrics. Especially if your knowledge base is in flux when the data changes all the time, or where your agents start using different models, you need to rerun your evaluation jobs.

The last one is very simple but very important: make sure that your end users have a very easy way of giving feedback on your agents in production. Thank you. That's it for me. I'll hand you back over to Amanda.

[![Thumbnail 3660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3660.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3660)

### Final Thoughts: Accelerating Regulatory Compliance with AI on AWS

Thank you, Andrew. Thanks for sharing about Holland Casino. All right, everyone, some final thoughts and thank you for staying to the end. It's a lot of content here.  In summary, we can use AI to accelerate regulatory compliance. Sicoob demonstrated how we can do this on top of infrastructure using EKS, and Holland Casino with Andrew showed how they developed this using Strands Agents. There is a bunch of different paths to follow, but you can achieve your regulatory needs using generative AI on AWS.

[![Thumbnail 3690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3690.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3690)

So where do I start? That depends. That's the architecture answer for all the questions.  You can start with a compliant foundation, so you need to have a standardized foundation to run your workloads on AWS. Select your models as we discussed here, build your agents the way that works better for you, and then deploy it in production in AWS with your regulatory needs.

[![Thumbnail 3720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3720.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3720)

[![Thumbnail 3740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3740.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3740)

[![Thumbnail 3760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9ba37e37528cfe7f/3760.jpg)](https://www.youtube.com/watch?v=R6m1vICK7-8&t=3760)

Here are some resources. This is the first day of the event. I hope you're enjoying the event.  There are some resources here from both perspectives that we discussed, both with infrastructure and with Amazon Bedrock. I will wait for you to take the photo. Some sessions to watch at this event. Again, this is the first day of the event, so we're going to have a lot of different content here with our colleagues from AWS.  You can take pictures too. Thank you all for joining us and thank you for staying to the end. I would also like to ask you to go into the app and let us know if you like the presentation or not. If you have any questions, we will be outside of this stage for maybe five or ten minutes.  Thank you and have a great evening.


----

; This article is entirely auto-generated using Amazon Bedrock.
