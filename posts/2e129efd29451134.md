---
title: 'AWS re:Invent 2025 - Advanced agentic RAG Systems: Deep dive with Amazon Bedrock (AIM425)'
published: true
description: 'In this video, Danny Mitchell and Akarsha Sehwag demonstrate building an agentic RAG system for an event assistant using Amazon Bedrock Knowledge Bases, AgentCore Memory, and Strands agents framework. They create a personalized agent that retrieves re:Invent 2024 session information from a knowledge base with 583 sessions using Titan text embedding V2 and Claude Haiku 4.5. The implementation includes short-term and long-term memory strategies for user preference extraction, hooks for agent lifecycle management, and tools for semantic search. They show deploying to production using AgentCore Runtime for session-level isolation and AgentCore Identity with Cognito for authentication, transforming a notebook prototype into a scalable, secure enterprise solution with just four additional lines of code.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/0.jpg'
series: ''
canonical_url: null
id: 3085660
date: '2025-12-05T07:26:19Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Advanced agentic RAG Systems: Deep dive with Amazon Bedrock (AIM425)**

> In this video, Danny Mitchell and Akarsha Sehwag demonstrate building an agentic RAG system for an event assistant using Amazon Bedrock Knowledge Bases, AgentCore Memory, and Strands agents framework. They create a personalized agent that retrieves re:Invent 2024 session information from a knowledge base with 583 sessions using Titan text embedding V2 and Claude Haiku 4.5. The implementation includes short-term and long-term memory strategies for user preference extraction, hooks for agent lifecycle management, and tools for semantic search. They show deploying to production using AgentCore Runtime for session-level isolation and AgentCore Identity with Cognito for authentication, transforming a notebook prototype into a scalable, secure enterprise solution with just four additional lines of code.

{% youtube https://www.youtube.com/watch?v=bu2cD1pCFTs %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/0.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=0)

### Introduction to Agentic RAG: Beyond Traditional Retrieval-Augmented Generation

 Hi everyone. Thank you for joining us for this code talk after lunch. My name is Danny Mitchell, and I'm a Senior GenAI Solutions Architect at AWS. I'm focused on generative AI and work on the Amazon Bedrock team, specifically focusing on knowledge bases and AgentCore memory. I'm here today with Akarsha. Akarsha, would you like to introduce yourself?

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/30.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=30)

Hi everyone. My name is Akarsha Sehwag. I'm a GenAI Data Scientist with the AgentCore Memory Team.  So today, let me set the stage for what we're going to be talking about. We're going to be doing a code talk, which means we'll be going through code together. This is a level 400 session, so we'll dive deep into the details of different APIs and the code we'll be using today. We're going to be diving deep into agentic RAG systems. We'll start by setting the foundation and talking about what agentic RAG is, then we'll see what we're building today, and then I promise I'll get out of the slides and go into the code. I won't spend an hour on slides. At the end, we'll give you some resources that we've put together to help you get started.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/90.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=90)

Let's start with the basics: Retrieval-Augmented Generation. I hope everyone in the room has heard about this. We've been talking about this for the past two or three years.  I just want to go quickly over what Retrieval-Augmented Generation is and how the workflow goes at a very high level. We have our query that we send into our model. But before we send that into our model, we're going to augment that query by giving it additional information. This can come from our internal data sources, or we can provide external information to the model so it doesn't hallucinate. Then it will have the full context to generate an answer and give you back the response.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/120.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=120)

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/130.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=130)

 But Akarsha, is that correct?  It's partially correct. When we think about agents, we have the LLM that has a lot of capabilities, but agents themselves are fundamentally stateless. All we can provide from our side to make them better and smarter is in this retrieval component, this RAG component. So today we don't just consider RAG to be retrieving from a knowledge base or a data source. We talk about it in a more generalized way, which is called context. Today you must have heard this new term popping up everywhere, which is called context engineering.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/160.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=160)

 Essentially, what context engineering means is that we are curating our context in a way that provides very precise information that the agent needs to give smart and more accurate responses. This includes information from documents, which is the traditional RAG component. It also includes curating our tools, figuring out which tools are relevant for the current query or use case, and finalizing those while making sure that whatever information we've provided in those tools is very accurate for the agent to know when to use it. And then we have the memory component, where we provide the agent with more information about the user to make it more personalized and make the experience much better for the user. All of this combined is what defines the context.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/230.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=230)

Yeah, that sounds right.  So now we have all this context coming in and we have a new way of retrieving our information. We have our query, we'll have a retrieval that can import information from different sources as Akarsha just mentioned, then we'll augment that query. And then the model will decide: am I able to answer with the information I retrieved, or do I have to go back again and look for more information? If not, we can get back the response. This is agentic RAG at a high level.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/270.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/280.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=280)

### Building an Event Agent: Creating a Simple Agent with Strands

 So what are we building today?  We are going to be building an event agent. As I imagine you have your AWS event application right now, and at the bottom there was a small text box which said something like "ask me something," right? You'll have your common agent or chatbot implemented into all of these applications, but we want to

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/350.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=350)

deliver a truly hyper-personalized experience to our users. I can say, "Hey, what sessions do you recommend me?" and I expect, or we normally expect nowadays, these agents to have information about this, remember some things about it, and be able to recommend content based on our preferences. Or questions like "Which session did we talk about last time?" which requires having history, or "What sessions are there on Monday?" These are common event questions that show up, and if you don't ask the app, you ask someone who is there to help. We wanted to have it all in a single place. So we're going to be building this awesome event agent. 

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/380.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=380)

This agent is going to have two capabilities. Our users are going to be able to interact with their agent, and that agent is going to be able to do two things. One is, as I mentioned before, retrieve session information from a knowledge base. Secondly, it's going to be able to store and retrieve the conversation history that we have, and it's going to be able to learn from our conversation and our user preferences. 

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/430.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=430)

I think that's enough slides at the moment. Let me go and show you how this looks from a code perspective. If there are any questions, put your hand up. It's a small room today, so we'll try to answer the questions as we go. If not, we're happy to answer the questions also when we're outside. I've got the notebook ready with this event agent, and we'll make this available this week in our repo for reference samples. We'll give you the resources for that later, so just bear with me and let's figure out what's going on in this notebook. 

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/460.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=460)

Can everyone see this? Let me just go to the text and see if we need to make it a bit bigger. Yeah, okay, good. I don't see anything because I've got these big lights here. Anyway, this is the same slide as before. This is the high-level picture of what we are building today. We're going to go through what we call different chapters. We're going to be creating the agent, we create the simple agent, we create the knowledge base where the session information is, we create the memory for the agent to have persistence, and then we're going to see how we can move this to production and do it in a secure way using AgentCore Runtime or AgentCore Identity. 

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/470.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=470)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/490.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=490)

If I start here with my notebook, it's pretty simpleâ€”just installing some dependencies I need to work with.  I'll import some libraries that I need for the notebook to work. An important thing here is that I'm going to be using Claude Haiku 4.5. I'm going to be using Amazon Titan text embedding V2 for my knowledge base, and I'll talk about that a bit later. I just want you to understand what's going on here. I'm just setting those up and creating a unique ID to have unique resources. 

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/500.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=500)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/520.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/530.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=530)

For every code cell, I will show you the outputs because it makes much more sense.  Let's start with the agent. What is the database? Sorry, we'll get to that. Give me one chapter and I'll get to that. Let's start with the agent. Let's start with the most simple step. We're going to be building a simple agent, and that agent is going to have a context, and that context is just going to be "You are an intelligent event assistant."  

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/560.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/570.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=570)

I'm going to be using Strands agents. Akasha, would I use any other framework here? You can use any framework you want. You can use LangGraph, LangChain, Crew, any framework that you're currently using. We use Strands agents because it makes it very easy for us to build agents. Strands agents is a fairly new framework that has been out in the market. It's an open-source framework that allows us to leverage the capabilities of the model itself to be able to plan, to take actions, and also to revisit whatever has occurred to see if it actually achieves the goal or not, to be able to reflect on that.  

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/580.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=580)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/590.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=590)

Okay, then I'll go with Strands. So what do I need to do to create my agent with Strands? I'll just create and import my agent class and import my model class. I'm going to be using a model which is available in Amazon Bedrock. That's Claude Haiku 4.5.  To create an agent with Strands agents, it's as simple asâ€”let me just get my pointer out here. I'm going to create my simple agent. This is going to be my class that I wrote before. I'm going to be defining a model, the model that's underneath this agent, and I'm going to be giving it a system prompt. 

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/610.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/620.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/630.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/640.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=640)

It's as simple as saying, "You're  an intelligent event assistant." Creating your first agent takes just one line of code if you remove the whitespace.  When I interact with this agent, I'll say, "Hey, which session is good to learn about security and AI agents?"  The agent responds, "I don't have enough information about specific  sessions or events in your current context," which makes complete sense because I haven't told it about any event. I haven't given it any information about the sessions. It's just a blank agent using the model under the hood with the information it's been trained on. That's completely normal.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/660.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=660)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/670.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=670)

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/700.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=700)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/710.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=710)

### Implementing Knowledge Bases: Setting Up Amazon Bedrock for Session Data Retrieval

So how do we start  giving context to our agents? One way is through knowledge bases.  With knowledge bases, we're going to provide the re:Invent 2024 sessions that I found, and I'm going to provide that to my agent. For anyone who doesn't know, Amazon Bedrock Knowledge Bases is a fully managed service inside Amazon Bedrock that helps you create fully managed workflows for RAG.  We're going to be able to create a knowledge base where we define our data source layer  and our data storage layer. When I'm talking about the data source layer, we currently have two available data connectors, so I can bring in my data from S3 or I can directly send the information into the Bedrock knowledge base. I have four different chunking strategies. When we're talking about RAG, the flow is normally you have your documents, you parse them to understand what's inside those documents, and once you have that, you split those documents into smaller chunks. Then using an embedding model, we transform those chunks into vectors and those vectors end up in a vector database.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/810.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/820.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=820)

Bedrock Knowledge Bases helps you with this whole process in a managed way. You only have to configure where your data is coming from, what kind of chunking strategy you want to use, what kind of parsing strategy you want to use, and then define what embedding model you're using and the underlying vector store. All that information ends up in my knowledge base, and then I can use the Retrieve API to retrieve the chunks, basically doing a semantic search against the vector database, or I can use Retrieve and Generate, which combines the process of retrieving my chunks and automatically sending it to the foundation model and getting back a response. I have those two options. What I'm going to do is create a tool for my agent.  That tool is going to be able to call knowledge bases.  Here in my code, these are the steps I'm going to go through. I'm going to download my 2024 session data, create my S3 bucket where those documents are going to end up, and upload them. I'm going to create an S3 vector storage index. For this use case, I'm going to be using S3 Vectors, which is a fairly new capability from S3 that provides support for vector storage. Then I'm going to create my knowledge base and do the ingestion job. Finally, I will test that knowledge base and then create that tool for the agent to consume.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/880.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=880)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/890.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=890)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/900.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=900)

These are pretty simple steps. First, what I'm doing is downloading my data. It's basically a single file which contains information about 583  sessions, if I remember correctly. It downloads that file and I'm just understanding what's going on in that file. Then I prepare those documents.  Instead of having a single file, I'm going to split it across 583 different files which will go into the knowledge base.  That's basically what I'm doing here. I'm creating my directory and splitting that single file into multiple files.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/910.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=910)

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/920.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/930.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/940.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=940)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/950.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=950)

I'm preparing those documents as you can see here, creating 500 documents and putting them into a folder. Then I'm going to send those documents to my S3 bucket.   Here I'm just creating my S3 bucket and giving it a unique name. That's where I'm going to be sending my files to. Once I have created that bucket, I will upload the documents using Boto3 to send the documents to the bucket.   

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/960.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=960)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/970.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=970)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/980.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=980)

As you can see here, I'm creating my bucket and then uploading all the files. This took about one minute, which is acceptable. Next, I need to create my IAM role for my knowledge base.  My knowledge base needs access to an embeddings model and it needs access to the S3 bucket in order to do all of its tasks. I'm just creating a simple IAM role that I'll use later.   All of this we can do via code, which is what I'm doing right now, but you can also do this via the console, which might be a bit nicer. However, I have my notebook here, so I'm just creating my knowledge base role, which is pretty simple.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1000.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1010.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1020.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1030.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1030)

Then I go and create my S3 vector store and index.  This is where the knowledge base is going to do the whole ingestion job, and the resulting vectors from the chunks that we created are going to end up in that vector store.   It's as simple as saying I'm using the Boto3 client for S3 vectors to create the vector bucket and give it a name. Then I go and create a vector index.  In the vector index, I'm specifying where the index should be located in the vector bucket I created before, giving it an index name, and giving it a dimension of 1024, which is what Titan Embeddings V2 is going to give us back when we do that conversion from text to embeddings.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1060.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1070.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1070)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1100.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1100)

Then we set other topics like distance metric, data type, and some metadata configuration.  Here we're just creating our S3 vector store and index. As you can see here, we'll get an ARN for both of them. Now I can go ahead and create my knowledge base.  I'm creating my knowledge base with a simple API call. I'm giving it a name and a description, so it's my AWS re:Invent 2024 sessions knowledge base. I'm giving it the role that I created before and then I'm filling in the configuration for that knowledge base. 

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1110.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1120.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1120)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1140.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1140)

Basically, here what I'm doing is I'm saying you're going to be using the Titan text embeddings model that I defined at the start of the session, and this is the configuration for that embedding model.  Here's the 1024 that I used before for my index.  And then for my storage configuration, it's going to be using that index store that I set up before. I just created my knowledge base, and now I can go ahead and import and ingest the data from S3. 

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1150.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1150)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1180.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1180)

I create my knowledge source, which is an S3 type data source with my S3 configuration and giving it my bucket name.  Here I'm giving the chunking configuration. I'm using a fixed size strategy. We have fixed size, we have no chunking, we can do semantic chunking, we can do hierarchical chunking, and you can even bring your own Lambda to do your own chunking configuration. There's quite a lot of flexibility there depending on your use case.  In this case, I'm using a pretty simple fixed size with a maximum of 300 tokens and 20 percent overlap, which is a 20 token overlap percentage.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1190.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1190)

From there I can go and start ingesting my data.  I'll wait for that to finish. That took me around two minutes, which is good. It processed my 583 files and I can go ahead and test my knowledge base retrieval.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1210.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1210)

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1220.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1220)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1230.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1230)

If I ask a question like "What sessions are about generative AI and security?", I will use the retrieve API  which I mentioned before. I'm just retrieving the chunks by doing a semantic search, and it will give me back the three most relevant results for that query.  If I ask that question, it will say "Hey, I found these three results," and these are the three chunks it got back.  These three sessions that I have here are pretty simple. I just created a knowledge base, ingested my data, and I did that all programmatically.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1250.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1250)

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1270.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1270)

Now for the fun bit, I'm going to create my knowledge-based search tool, and this is what the agent is going to be using. How tools work with Strands is that you're going to take your Python code with  your typical function and put in this tool decorator. With that tool decorator, you can pass it on to the agent, and the agent will make use of that tool. The nice thing about this tool is that I'm going to pass in a description, and that's what the agent is going to  first read to see if it needs to use that tool or not for the task it needs to perform.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1290.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1300.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1300)

As you can see here, I'm saying this is a tool to search re:Invent sessions from a knowledge base using semantic search. Use this tool whenever a user asks for a recommendation about re:Invent sessions.  I can pass in a query and max results, and this is just a plain API call to Bedrock Knowledge Bases, which is retrieve.  I'm passing in my knowledge base ID, my retrieval query, and my retrieval configuration. With that, I have my knowledge-based search tool created.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1310.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1310)

We have one of the components of the agentic RAG that we were talking about ready for consumption because we have created it as a tool.  One key difference for why you would use something as a tool is when you want the agent to decide and have the flexibility to choose when it wants to call it and when it feels it's relevant to call the knowledge base. That's when we're using the tool. We could take another approach with hooks, which we're going to see a bit later. It's dependent on how dynamic or how deterministic you want it to be. You would choose one or the other.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1370.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1370)

### Understanding AgentCore Memory: Short-Term and Long-Term Memory Architecture

Now let's add the second component of RAG, which is context, into our agent. Let's look at the memory.  Memory persists our agent conversation. So far we added RAG with some generic information. Now we are adding memory to have more personalized context about the users. How that works is when users are interacting with the agent, they build up a conversation over time. That information goes into AgentCore Memory as events. We term those events as short-term memory. We send those events into short-term memory, which stores those events into a data store as raw conversational messages.

Now those messages, based on how you configure it, can be processed in different ways. You can choose to have more extracted insights from those events. For example, you might want to create a user preferences strategy where you extract user preferences from all the conversations you've had. Or you can choose something between semantic or summary. These are some predefined strategies that we have in AgentCore Memory, or you can customize those as well. You can either customize the prompts of those built-in strategies or create a completely new pipeline of your own. That's completely dependent on you.

Just to give you a use case, we have our agent which we are speaking with for a whole week. If I'm stacking up the conversation history of over a week and sending it to the agent for the LLM to process, whoever is paying for the bill is not going to be happy. You have this really big context, and it's going to be affecting the price because it's a huge context. It's going to be affecting the latency because the LLM has to process more. It can even have problems with accuracy. If I have this huge amount of information to process, it should be better if it was smaller. Instead of taking that big context and passing it on to the agent, it makes more sense to extract what's actually important from that conversation. That's where long-term memory comes in.

You define what you want to extract from all of the messages going between your users and your agent. You can extract user preferences, semantic facts, session summaries, or you can define your own workflow of what you want to extract.

For short-term memory, you store messages deterministically and send those messages to the agent core memory. If you have a conversation, ideally you send all of them. Agent core memory defines the separation based on the actor ID, the session ID, and the memory ID. When you're creating a memory, it takes in two identifiers from you: the actor ID and the session ID. The actor is usually the user, but it doesn't have to be. It takes in a string that defines who these memories belong to. It can be the agent, it can be a project, it can be a team, or it can be a combination of these. You have flexibility in what you want to define in that actor ID, and the same applies for session ID. Those are the identifiers based on which it will separate the memories from one another.

For example, you could take the feedback that a user has given, sessions that they have attended, or survey responses. If they gave a session a five-star rating, you could use that to give recommendations at the next stage when they ask. That would be stored in memory. We'll look at the code to clarify how to use it, but this is just the conceptual level right now.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1740.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1740)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/1750.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=1750)

You're storing everything into DynamoDB, and when you need to retrieve information, you're getting back everything from DynamoDB. DynamoDB corresponds to the short-term memory. This is a managed service, so you don't have to focus on building a memory storage solution. We provide that for you, taking care of infrastructure, scale, and all the other things that normally come with managed services from AWS. We don't stop at DynamoDB. Based on how you've configured the memory when you created the resource, you can define one or more strategies for smart insight extraction.  You can define one or more of these strategies: summary, semantic, or user preferences,  or create your own.

It goes through an extraction module, which consists of two steps. The first is the extraction step, which looks at all the conversation and extracts only the relevant pieces of information. For example, if you're talking about user preferences, not everything the user discusses is about user preferences. From all of that context, it only retrieves where a user mentions something they prefer, like, dislike, or any other preference. It extracts only that information. The second step in this extraction module is the consolidation step. It looks at the existing vector store, at whatever memories it has existing about that user, retrieves the most similar ones, and compares them.

Based on that comparison, it consolidates the information and takes one of three steps: it can add a new memory into the vector store, update an existing memory with more complete information, or skip it if it's redundant and not adding any new information. This way, we do not have any duplication in the memory store that we create. Once those memories are ready, we save them into the long-term memory store, which is essentially a vector store from which you can semantically retrieve similar memories when the user asks something similar from the agent.

You can retrieve the long-term memories, which are more extracted insights, but you can also retrieve the raw events. You can list, for example, the last five messages from that particular session plus the summary of that session to compress the context so that we do not have to deal with the entire context window. This makes our agent more aware of the conversation history.

### Integrating Memory with Hooks: Personalizing Agent Responses Through User Preferences

So how do we do that with Strands? First, we create a memory manager, which provides us with the control plane API where we can call get or create memory. We provide a name of the memory and the strategy that we want to create. These strategies are optional, but we can provide one or more strategies that we can add here. It takes in a list of dictionaries which specify what strategy type you want to define and what namespace you want to have.

The namespace in this case takes in a path-like structure which allows you to create separation of concern in the long term. In short-term memory, we did that by actor ID and session ID. In long-term memory, we do that by namespaces. Based on how granular or how global you want your memories to be, you can define a longer path with more variables. You can add zero or more of these variables, such as actor ID, strategy ID, or session ID. Based on that, it creates a path for you which you can then retrieve from.

Instead of just having a very big vector database where you throw in the new memories being generated automatically, you can create your own separation and structure for how you save those memories. This way, when you have to retrieve those memories, you have them in a logical way. You can also define an event expiry date, which defines how long you want the short-term memory to stay. In this case, it can be anything between seven days to 365 days. When you call that API, it returns you back with a memory ID.

You can store this memory ID somewhere and use it every time you want to store memory information for your users. It takes about one to two minutes. You have your agent and your memory resource. The agent will have to write those events to that memory resource. You have that memory ID, you have your actor ID, and you have your session ID, and you just have to send the information over to AgentCore Memory.

So how do we add that to the agent? We have created a memory resource. Now, how do we write to that? The first approach was a tool approach that we saw for RAG. The second approach that I was talking about was hooks. Here we are taking control of different parts of the agent lifecycle. It enables us to write or take an action at different points of the agentic lifecycle. I can define that whenever the agent is initialized, for example, we run a certain action. I can hook something in that part of the agent lifecycle so I can say every time the agent is initialized, call the agent initialized event.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2130.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2130)

Similarly, whenever the user sends a request, call the message added event. This is another hook that you can add. We're going to use this capability to hook at different points in time, such as when the tool is being called or when the request ends, meaning when the agent responds back. We are going to use two of these hooks  to create and add our memory and to retrieve from our memory.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2140.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2140)

So how does it work? First of all, whenever the agent is being initialized,  we ask our agent to retrieve the user preferences from the long-term memory. Every time the agent is initialized, we essentially load a profile of the user to know what their preferences are. Then every time a new message is added, we store the message that the user sent and also the messages that the agent has sent to our short-term memory. Behind the scenes, it should automatically propagate to long-term memory, so you don't have to do anything there. The second aspect is that we already created a tool for the session data retrieval, which was a RAG aspect. That's what we're doing for retrieving the event information from knowledge bases.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2190.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2190)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2200.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2200)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2210.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2210)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2240.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2240)

Now let's see how this looks.  Here we are creating a class called MemoryHookProvider, which inherits from the HookProvider class provided by Strands agents.  Here we define mainly two functions. One is on_agent_initialized. So whenever the agent is initialized, we search the long-term memories  from this namespace, which was a namespace that I defined when I created the memory. With this query, it can find similar information. I hardcoded it to say: give me the user preferences, interests, and background so that I can personalize my agent based on that. Give me five of those preferences, and I will then load and append them into the messages.  This way, my agent now knows or has some context about what the user prefers.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2250.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2260.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2270.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2270)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2290.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2290)

The second event that I'm going to add is the on_message_added  event. Every time a new message is added, whether it's a tool message, a user message, or an agent message, any time a new message is added to the state,  it is going to load the last message and add that message  into our memory session manager. It's going to send it to AgentCore memory as a conversational message. It's going to take in a message content, which is the string, the question, or the content that the user provides, and along with the message role. That's what we are sending  to short-term memory for it to be processed for long-term use.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2300.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2300)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2310.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2310)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2320.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2320)

So you have to define this.  For example, here we are adding it into the system  prompt. The user preferences we are adding in the system prompt. You can add it as a new message as well, and you can write literally in a string message something like "these are my user preferences," so it cannot literally separate between what is memory or what is a user message. You have to define how you want to send it to the agent.  A nice thing about this is that we give you the tools and we give you the flexibility in using those tools for your use case. I'm pretty sure everyone here has a different use case, and you're going to be able to interact with memory in a different way than the person sitting next to you.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2350.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2350)

Regarding privacy, when you're sending messages to the short-term memory, you're defining an actor ID and a session ID.  With that actor ID, you can actually define who that event belongs to, so you have that logical separation. You also have IAM policies that you can apply, which contain conditions. This way, you can give permissions to an agent to only access a memory resource if the actor ID corresponds to whatever you specify.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2420.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2430.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2430)

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2440.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2450.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2450)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2470.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2470)

So you have multiple ways of securing this. I'll cover that later. Now we have created the hooks,  and the only thing we have to do is register those hooks with the hook registry so that the agent knows these are the callbacks it has to execute every time an agent is initialized or a message  is added. You add that into the hook registry. Now that is done, and we have a way to call  our memory. We've also already defined a way to call our knowledge base. Now we're going to initialize the class for the hook provider  and add that to the agent. We have defined the memory hooks, and we're going to add that to the agent along with the tool we created for searching the re:Invent sessions for the knowledge base. On top of that, we provide a state. This is where we  give it in a more deterministic way by providing the actor ID and the session ID. This should ideally come from the application. Once the user has been authenticated, you get the user information from that session and provide the actor ID based on that. Similarly, create a new session ID or get it from whichever environment it's running in, and pass that in as a state to make it more deterministic and ensure that the same information is propagated within the agent.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2510.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2510)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2530.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2540.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2540)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2560.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2560)

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2570.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2570)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2600.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2600)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2620.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2620)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2640.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2650.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2660.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2660)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2680.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2680)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2690.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2700.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2700)

You basically don't want to say to the agent, "Hey, this is the user, blah blah blah, and this is your system, blah blah blah," because it will probably hallucinate and evoke away. You want to pass that as part of the application code. Exactly. So now we have the agent ready.  Now we can test it. The first thing we do is give it some information because right now we haven't talked to the agent, so it doesn't know anything about me. I give some information about myself, saying my name is Alice and I'm a software engineer at TechCorp.   This should ideally be loaded in a more gradual way. When you're actually building an application, this will be built over time. But in this case, for demo purposes, we are loading in information at once.  We send that event and call the agent with this information. The agent responds back with, "OK, nice to meet you, Alice. Thank you for telling me about yourself."  Behind the scenes, using the hooks, it should be able to store that information in the short-term memory. Then, around about 30 seconds later, it should also be available in the long-term memory. So the next time we call the long-term memory records, we should be able to see what we have already fed in. If we list now the long-term memory records from that  certain prefix, it should be able to retrieve our preferences that we just sent in. So the interesting thing to note here is that it not only stores the preference but also stores the context of how it extracted that preference.  It says the context is that the user explicitly stated that they work as a software engineer at TechCorp, and that's why I extracted the preference that they work as a software engineer. So it defines a more explicit and proper reason for why this preference has been extracted.  Based on that, it creates a preference, and now you can use this information across sessions.  If you create a new agent with a new session ID  with the same actor but a different session ID, you are able to retrieve the same information about the actor and use it within the question. So if I ask this question, "Hey, which sessions should I attend at re:Invent?" and I already mentioned that I'm interested in AI and security,  it responds back with, "I'm happy to help you with the perfect re:Invent session. Based on your profile, I can see you're interested  in cloud architecture, AI, and security, and that you prefer hands-on technical sessions. Let me give you some sessions."  Based on that, it searches the re:Invent sessions and gives me recommendations.

It provides some GenAI, some cloud operations, and some security recommendations. It makes the experience more personalized based on what you're interested in and your profile, and it responds accordingly. So now we know that our memory is working. Let's move to runtime.

### Deploying to Production: Moving from Notebook to AgentCore Runtime

We now have the agent running in our local environment. How do we move this to production? We need something that provides us with session-level isolation because, as you said, we are dealing with user data here. We have to be highly vigilant about how we are deploying this. The session not only has to be isolated at the compute level but also at the memory and storage level. AgentCore Runtime provides us with true session isolation across compute, memory, and storage, and it provides us with a fully enterprise-ready production workload system that is automatically scalable. It's a serverless setup which allows us to run hundreds of sessions in parallel.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2820.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2820)

So how do we add this to the solution? We have the endpoint that we created, so everything else remains the same. We're going to deploy this to AgentCore Runtime. This is going to be our way to run this in production. So how does that work? First of all, what we have created right now is an agent with any model and any framework. You can bring in any model and any framework of your preference.  Then what we're going to do is create two endpoints: one for ping and one for invocations. We use ping for health checks and invocations to know where the entry point is and how to invoke the agent. We also add a requirements file which defines what dependencies we have to install. This is what defines our agent.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2900.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2900)

The first thing we have to do is deploy or push that in a more packaged way to the ECR, which is Amazon Elastic Container Registry. Once we have pushed the code to ECR, we will have our container image ready. We will then provide some more configurations like protocol settings, such as which protocol we want to useâ€”HTTP, A2A, or MCPâ€”and how we want to configure that agent. We also provide network settings, and that's what completes our configuration setup.  Now once we have that, we're going to deploy that to AgentCore Runtime, and then we have our agent ready for invocation.

Then we can invoke multiple sessions in a concurrent way from the application that we built. Each session is a microVM that spins up, loads that agent code, and you have a user that's going to be interacting with that specific agent with their session ID. When that session finishes, the microVM gets shut down and everything gets deleted. That's why we have to persist the information somewhere else, because when the AgentCore Runtime session finishes, the context that the agent has is gone.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2950.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2950)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2960.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2960)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2970.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2970)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2980.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2980)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/2990.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=2990)

So how do we prepare the agent that we created right now to deploy it on AgentCore Runtime? We literally have to add four extra lines of code. Except for the structuring of the code, we have to add four lines to make it ready for runtime.   The first thing we add is importing the AgentCore application. The second thing we do is initialize the app. The rest of the code is the same things that we already created in the notebook previously.  We created a hook provider, we created a tool for searching the re:Invent session, and we created an initialized agent which is doing the exact same thing as we did above.  We just package it into a function. Then we create an entry point.  This is the third line of the code that we have to add. We add a decorator to define that this is going to be the entry point of the application. Here we've taken in a payload of whatever the user passes in from the front end.

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3040.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3040)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3050.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3060.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3060)

So that takes in a prompt and the actor ID, which is determined after authentication. Then that initializes the agent, takes in the user input, and gives a response. Finally, you have to run the application. This is line number 4.  This is what you have to add to make it ready. Essentially, you're creating a FastAPI server that allows us to run this piece of code. Then we create an execution role  and configure and launch. These are the two things we have to do to get it ready  as an endpoint that is ready for consumption by different users.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3070.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3070)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3080.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3080)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3090.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3090)

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3110.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3110)

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3120.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3120)

[![Thumbnail 3130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3130.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3130)

The AgentCore starter toolkit provides us  with tools to do that. We have a dot configure function. First of all, we configure the application with an execution role.  We allow it to auto-create the ECR, provide a requirements file, and ask it to take in the agent.  Basically, it configures based on that. Once it has configured, it creates a Dockerfile for you. If you want to customize that, you can create your own Dockerfile or provide your own endpoint from a different registry. Then once you have that, you can  call launch, which is going to launch the application and put it for you to warm-start.  Here you can also provide environment variables, which take in your memory ID, knowledge base ID, model ID, and region.  Now we have it up and running, so this will take around a couple of minutes for you to get started.

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3140.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3140)

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3150.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3150)

[![Thumbnail 3160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3160.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3160)

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3170.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3170)

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3200.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3200)

So the only thing missing here is the identity.  We just passed from our local notebook where we have our proof of concept working without our managers, and now we need to get this into production.  That's where we use AgentCore runtime. We pass from a notebook to actually having our agents running at scale securely.  If you're not going to show how the invocation looks like,  you can see there it says saving AgentCore runtime invoke. I'm passing in my prompt, "What sessions would you recommend?" and in this case I'm passing my actor ID. Remember, right now this is hard coded. The goal is not to have this hard coded. It should actually know who the actor is based on our login.  We asked the same question, "What sessions would you recommend?" and we get more personalized recommendations to us. That's basically our agent running in AgentCore runtime.

[![Thumbnail 3210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3210.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3210)

### Securing Agent Access: Implementing AgentCore Identity with JWT Authorization

The last bit of the architecture is identity.  Here's where we use AgentCore Identity. How is this going to help us? Initially, we have our user who interacts with our application. First step, our user logs into the application using our identity provider of choice. We support multiple identity providers. Our user logs in and gets back an identity provider token from that identity provider. Then I can go and invoke AgentCore runtime. So to that call that was made before, I will have a new variable that will say bearer token, and there I will include the identity provider token that I got back from my identity provider.

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3290.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3290)

Now thanks to AgentCore Identity inbound authentication, it determines who the user is and if it's allowed to call the agent. Every time we call AgentCore runtime before it actually does invocation, it will go call AgentCore Identity. Identity will go back to the identity provider and validate that identity provider token. If that's valid, it will say, "Hey, you're allowed to go ahead and do that invocation." Pretty simple steps here. So how does that change at the code level? When we're doing the AgentCore runtime  configuration, we're going to add two different things. One is the authorizer configuration. That's where we are configuring our JWT authorizer. In this case, I'm using Cognito. So I'm passing in my discovery URL and my allowed clients.

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3330.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3330)

The URL and my allow list for clients is very important. Also critical here is my request header allow list. What I'm doing is allowing AgentCore Runtime to propagate the authorization token to the agent code. When my user logs into the application, it will retrieve that identity token from Cognito. It will then invoke the agent, passing in that Cognito identity token, and from inside the agent, I'm calling Cognito with that token to retrieve the user information. 

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3350.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3350)

[![Thumbnail 3410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3410.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3410)

The code I showed before has the exact same functionality. The only difference is that instead of passing the actual ID directly, we are getting the user identity from the authorization header. This is what is being propagated from the AgentCore Runtime and authorized by AgentCore Identity through Cognito. So no more hard coding either sessions or actor IDs. The actor ID comes straight from the identity token from Cognito, and the session ID uses the same session that AgentCore Runtime is running. As you remember, it works with sessions where each session is a micro VM that spins up, loads the agent code, and runs. That session has an ID, and that's what we're using as the session ID inside AgentCore Memory.  

[![Thumbnail 3420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3420.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3420)

[![Thumbnail 3440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3440.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3440)

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3450.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3450)

[![Thumbnail 3460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3460.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3460)

[![Thumbnail 3480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/2e129efd29451134/3480.jpg)](https://www.youtube.com/watch?v=bu2cD1pCFTs&t=3480)

### Resources and Wrap-Up: Getting Started with Amazon Bedrock AgentCore

When we're configuring it, we added these two configurations for JWT authorizer and the header configurations.  This allows it to know what the discovery URL is for authorization and what the client ID would be, so what the allowed clients are for me.   We have some authentic AI resources to get started. At GitHub, we have Amazon Bedrock AgentCore samples. We'll be pushing the code that we saw today this week there. If you go to GitHub and search for Amazon Bedrock AgentCore samples, it should be there.  

Follow me on LinkedIn and I will probably post it there as well. Very importantly, please complete the session survey. Five stars is recommended, and others are optional. I'll take any questions now. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
