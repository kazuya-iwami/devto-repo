---
title: 'AWS re:Invent 2025 - Leverage observability to build responsible AI applications (COP360)'
published: true
description: 'In this video, Principal Technical Account Manager Sasi Kiran Malladi presents a practical framework for building responsible AI applications using Amazon Bedrock Guardrails and Amazon CloudWatch. He addresses three critical concerns: ensuring responsible AI usage, protecting sensitive information, and verifying business value. Through real-world banking and contact center examples, he demonstrates how guardrails prevent hallucinations (blocking 88% of harmful content and 75% of hallucinations), anonymize sensitive data like Social Security numbers, and filter inappropriate content. The session covers CloudWatch''s automatic telemetry collection, custom dashboard creation for monitoring model usage and guardrail interventions, CloudWatch Logs data protection for redacting sensitive information in logs, and CloudWatch alarms for real-time alerts. Malladi emphasizes the complete lifecycle: prevention through guardrails, detection via CloudWatch insights, and action through continuous monitoring and policy adjustments.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/0.jpg'
series: ''
canonical_url: null
id: 3093246
date: '2025-12-08T21:45:53Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Leverage observability to build responsible AI applications (COP360)**

> In this video, Principal Technical Account Manager Sasi Kiran Malladi presents a practical framework for building responsible AI applications using Amazon Bedrock Guardrails and Amazon CloudWatch. He addresses three critical concerns: ensuring responsible AI usage, protecting sensitive information, and verifying business value. Through real-world banking and contact center examples, he demonstrates how guardrails prevent hallucinations (blocking 88% of harmful content and 75% of hallucinations), anonymize sensitive data like Social Security numbers, and filter inappropriate content. The session covers CloudWatch's automatic telemetry collection, custom dashboard creation for monitoring model usage and guardrail interventions, CloudWatch Logs data protection for redacting sensitive information in logs, and CloudWatch alarms for real-time alerts. Malladi emphasizes the complete lifecycle: prevention through guardrails, detection via CloudWatch insights, and action through continuous monitoring and policy adjustments.

{% youtube https://www.youtube.com/watch?v=KjGAR_WCtWE %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/0.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=0)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/20.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=20)

### Three Critical Questions About AI Risk and Real-World Challenges

 All right. Hello, everyone. Let's get started. Can you all hear me okay? Yeah, thank you. So, whether you're an executive making AI investment decisions,  or you're a leader trying to make your investment decisions, or a builder simply trying to deploy your applications, I have three questions for you that could keep you up at night. You've invested a lot of time and resources in building your AI applications, right? So, how do you know that the users are using your AI applications responsibly, or are they exposing your organization to risk? The second question, how do you know whether the users or your AI systems are exposing sensitive information to unauthorized users? And the third question is, how do you know that the actual AI system is actually driving your business value, or is it just creating chaos, noise, and liability for your applications?

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/90.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=90)

These are the exact same questions I hear from my own customers through my direct interactions with them. And as per the analysts, 66% of executives identified data privacy and security as one of their top AI risks. And 77% of executives believe that you can build a true AI system only based on the foundation of trust.  So in this session today, I'll walk you through a practical framework for building responsible AI applications using Amazon Bedrock Guardrails and Amazon CloudWatch. I'll also show you how guardrails can help you ensure safety at runtime, how CloudWatch can get you the insights and meaningful actionable insights out of your data, and how building a powerful, purposeful dashboard can actually get you the insights so that you can act on that data available to you.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/140.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=140)

Hello everyone again, I'm Sasi Kiran Malladi. I'm a Principal Technical Account Manager and an observability specialist here at AWS. I work with some of the largest and most impactful customers globally, and I help them on their AWS cloud journey. Let's get started. So, in this session, I'm going to  take two real-world examples so that we have the full application context. I'm going to use the same examples throughout the presentation today. I'll show you first, without the guardrails, how in the real world those applications would look like, then with the guardrails, how that will look and what difference that would make.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/160.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=160)

So the first example,  it's a very common use case for the context. Let's say I've taken a banking example, right? So I've given there is a fee for opening a checking account, there's a fee for international transfers, and some other details. But the user, in this case, you would have seen that they prompted what's the fee for opening a savings account, which that information isn't available in your enterprise data. But the AI confidently responded that there's a fee for opening a savings account. This is called hallucination. It's fabricated information or inaccurate information. And when AI responds with hallucinations, that's when it creates business risk for you, and it can quickly erode trust for your own customers. And the difficult part is that the AI sounds so reasonable that it's difficult to detect these hallucinations without proper guardrails in place.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/220.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=220)

Let's look at the second example. In this example, all of your organizations will be having the same contact center. You'd  have a customer support agent and customers asking for a request and help. So in this example, I've given a simple context of a conversation where one of your customers, let's say they call customer support to reset the password, and during the process, they would have given the credentials like a full name, a username or email, phone number, and social security number. But when you're trying to do call center analytics, that sensitive information can be exposed to unauthorized users that can pose a significant risk for your businesses. So this is another challenge, and this is a very common real-world example as well.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/260.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=260)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/280.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=280)

So let's do a quick overview of what responsible AI is and what is that practical framework.  Your AI systems are incredibly powerful, but at the same time, they bring a new set of challenges, and those include, let's say for example, harmful content or irrelevant topics. Starting with  irrelevant topics, for example, you build a powerful chatbot for a trading application, but your users are using it to ask for political views or maybe vice versa. Let's say you build an application for a healthcare application, but your users are asking the model to respond back on trying to get stock information which is irrelevant, which is only misusing your system resources. And this can be harmful content.

These risks include either user prompting or the model responding in an inappropriate way which is against your company policies. The next is data privacy and protection, revealing sensitive information either to unauthorized users or externally as well. And the next is hallucinations, which is inaccurate information that is not as per enterprise data but is just fabricated information.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/330.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=330)

### Building Responsible AI with Amazon Bedrock Guardrails

 Building responsible AI applications means that they are safe, they are secure, and the data is protected. All the stakeholders have equitable information, and they all have visibility into end-to-end monitoring of what exactly is happening within your system. There is a full concrete governance framework for your entire AI supply chain, and they are also able to understand why the model made certain decisions, so they have full transparency into how those systems are built. All in all, this is what would actually help build a responsible AI system.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/370.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=370)

Now, to build a responsible AI application, all these three capabilities need to work  together. Amazon Bedrock Guardrails can help build that foundation for you, enforcing safety and policies at runtime. With that, it can block harmful content, it can filter sensitive information, and it can also prevent hallucinations. It will also ensure that the model is responding in the same voice and tone that your organization requires. With CloudWatch, it can help transform all of your telemetry into meaningful insights so that you can act on them, and you can also get insights like, for example, who is using the system, how are they using it, when the guardrail intervenes, why it intervened, and what types of topics are being blocked, so you get full visibility into that. So that's how we derive insights out of your guardrail telemetry.

Once you have the insights, the next step is to actually act on that. Once you know that you've breached certain thresholds, and you can define the thresholds like how many times the guardrails intervene and so on, when those thresholds are breached, you can act on that. For example, you can look at your policies or you can take remediation actions. So this completes the full loop, which is prevention, detection, and action. That's all required for your responsible AI applications.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/450.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=450)

Now with Amazon Bedrock Guardrails, it provides you with flexible configurations  across different categories. These can be based on the content. You can configure what type of content is accepted and what is blocked. You can work on your sensitive information, or you can anonymize that, or you can completely block any sensitive information either in the prompt or at the model response level. And you also get full configuration options. Either certain words, certain phrases, or certain topics that are against your company policy, the guardrails can also intervene on that. With the guardrails in place, it can block up to 88% of harmful content. It can also prevent up to 75% of hallucinations using advanced automated reasoning checks and advanced grounding checks as well.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/500.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=500)

Now, let's take a look at the previous example with the guardrails in action. On the left side, you could see without the guardrails  that using the hallucination scenario, it gave the information which is not available in your enterprise data. On the right side, you could see with the guardrails how the guardrails intervene. It identified that whatever the user is asking, that information isn't available in your enterprise data. So that's the standard response, but you can fully customize how you want the model to respond.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/530.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=530)

Let's take the second example, which is revealing the sensitive information. On the left side, you could see without the guardrails.  On the right side, you could see when the guardrails are applied. You could see the guardrails intervened. In this case, I chose to anonymize the data so that I still have the conversation in context, but the credentials or the sensitive information is anonymized. So that would also be a powerful way to understand conversations. If you choose, you could also block sensitive information as well.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/570.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=570)

### Deriving Meaningful Insights with Amazon CloudWatch Observability

So the first step is enforcing the guardrails. Let's take a look at the second step, which is actually to derive meaningful insights out of your guardrails telemetry. So Amazon CloudWatch helps you get your end-to-end full stack  observability for your application either on AWS, on-premises, or other clouds with powerful capabilities, including the generative AI observability for your end-to-end agentic AI workflows. And you also get AI-powered operations which can significantly reduce your incident detection and response times, and you get full powerful infrastructure capabilities, observability and monitoring, including for your core compute, storage, databases, networking, and security. It also comes with all your powerful capabilities including metrics, logs insights, and powerful dashboards for you to get the right insights all at one place.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/610.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=610)

 Now with Amazon CloudWatch, you get all of this guardrail telemetry automatically. You can get meaningful insights out of that, and there is no additional instrumentation required. All of the telemetry is available to you and you can use them to derive meaningful insights out of that. And under that guardrails namespace, you could use that information. For example, you can understand how many models are invoked, how many times those are invoked, or how many times the guardrails intervened against your total invocations. You can derive all of this powerful information both at the model level and also at the guardrail level.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/650.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=650)

 Now it also provides you with an automatic dashboard. This is out of the box and available to you. It's very powerful. With this, you can understand what are the total invocations of your AI systems, out of which how many times the guardrails actually intervened. Or you can also look at your model performance, latencies, so all of that is available to you in this out-of-the-box dashboard.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/680.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=680)

Now let's say you want, other than the automatic dashboard,  you'd want to customize more, you'd want to get more additional deep dives into that. You could build a purpose-built dashboard like this, fully customized. I know it's a busy slide with a lot of information. I'll break it down into each component and walk you through that as well. But with this one single view, you get the full picture of your entire AI application, both from the performance and also from the safety standpoint.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/710.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=710)

So let's dive deep into this a little bit. Starting with, in a lot of my customer  conversations, this is an important topic where you'd want to understand across all of your accounts, across the regions or across the models, you'd want to understand what's the model usage, what are the top models that are being used, or how many times those models have been invoked. This will also help you to understand what model usage is actually driving your AI spend. So it will give you these powerful insights into your models across accounts as well.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/740.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=740)

And the next set of insights is now, now that you know you've got guardrails in place, you'd  want to understand whether it's the user prompts that's causing the guardrails to intervene or is it the model responses. You could also understand by the categories, whether it's to do something with the content or is it with sensitive information, so you could get all of those insights from this view, and this is all available in your metrics as well in the guardrails telemetry.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/760.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=760)

 And the next set of information is more about understanding the performance itself, overall how many invocations you made to your system, and out of that how many times the guardrail has to intervene. That will also give you an understanding of how sensitive your guardrails are. Then the next information is on the latency itself, whether it's adding additional latency or what's the latency across your model invocation.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/790.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=790)

Going a little bit deeper into the content itself,  we talked about the sensitive information. Now we'd want to understand within the sensitive information what types of information is being leaked. For example, it can identify the names, emails, phone numbers, or Social Security numbers, bank account numbers, any other sensitive information. It can flag and tell you if that information is being leaked in this case. And also it can identify any denied topics. Let's say as per your company policies, it could deny certain topics like investment advice or security-related topics. It could also identify that. And finally, any blocked words, profane words, all of that it can identify.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/830.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=830)

 And the next is the important one. This is the view which would get you insights into the hallucinations. You could see that based on the grounding checks and irrelevant topics. If your users are prompting irrelevant topics or if the model is responding with irrelevant topics, you could see this view would give you that. And also within the content, another powerful feature is you could understand whether it's hateful speech or insulting, or another important one is a prompt attack, which is very similar to the DDoS attacks. The prompt attacks can quickly drain your resources and can increase your spend or AI spend. So that's also an important metric for you to get insights into.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/870.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=870)

 And finally, you'd want to understand what accounts are contributing to these guardrail interventions. And finally you'd want to understand which are the bad actors or who exactly are the users contributing to this guardrail intervention. So this is a powerful view to finally drill down to get into who exactly is breaching these violations.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/890.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=890)

 And all of this for the custom dashboard, you could also build using these metrics and also use capabilities of logs using CloudWatch Logs Insights. It has powerful capabilities, SQL-like queries as well, and you can also use the latest natural language style queries as well. You could prompt and it would generate the queries for you, and you could use them to build and make them as components in your dashboard as well.

### Taking Action: Multi-Layered Protection and Continuous Monitoring



[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/930.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=930)

The next powerful feature is that now that we have Amazon Bedrock Guardrails, they can help you protect at the prompt level. But another use case to consider is what would happen when model logging is enabled. Those logs also would be available in your CloudWatch Logs, or  it could be as simple as a developer doing a print statement, right? In that case, that sensitive information is logged as well. So you could use the CloudWatch Logs data protection feature with very few simple steps. You can configure this, and there are more than 100 managed data identifiers already available for you to use. You can also customize that with custom regex as well.

With those in place, what it can do is automatically detect sensitive information like names, emails, phone numbers, Social Security or driver's license, bank account numbers, and so on. It will intervene and you can see it completely redacted sensitive information on the right side with the asterisk. So that's two layers of protection. The guardrails can protect at the prompt level and the CloudWatch Logs data protection can intercept at the log level, so you get end-to-end protection with these two layered protections.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/990.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=990)

Once we have the insights, the next step  is what are the actions we're going to take out of those insights. That's when we use the CloudWatch alarms, which can continuously monitor your guardrail telemetry using the metrics and can alert you when the thresholds are breached. You can set the configuration. Let's say in this example, I set the threshold when there are 10 guardrails interventions, I'm going to get alerted. I'm going to take remediation actions in real time, and you could use that for something like when there is a prompt attack or when there is a sensitive information leak or anything that's relevant to your business. You can define alarms for that, you can get notified, and you can take actions in real time.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1030.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1040.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1050.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1050)

And finally,  once you're notified that there's something wrong, what are the different actions that you can take? As a leader, you'd want to assess your organizational  risk and come back with a prioritization plan. What are the next steps that you'd want to do? Those controls can be as simple as assigning responsible AI trainings to your employees,  or it can be adjusting a guardrail sensitivity. On a recurring basis, continuously monitor and audit so that these controls are working as expected or in place.

And finally, the threats keep changing and the bad actors would find new ways. That's when you can reiterate to look at and optimize, maybe update your trainings or update your guardrails. So all in all, this full lifecycle would give you the ability to understand and set up complete responsible AI applications.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1080.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1090.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1090)

As next steps, to summarize, we  looked at a powerful practical framework to build responsible AI applications using the Amazon Bedrock Guardrails and Amazon CloudWatch.  I've put together these resources for you. Please use this QR code. It has all of the information available to you, including expert discussions, hands-on workshops, and best practices guide. Using this information, you could build a custom dashboard like the one that I built, and that dashboard is really powerful. I would encourage you to try to build a dashboard and you'll be surprised to see how your AI systems are being used. Please give it a try.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1120.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/86d8b29f44fac1c3/1130.jpg)](https://www.youtube.com/watch?v=KjGAR_WCtWE&t=1130)

And please stop by at the CloudOps kiosk to  have expert one-on-one conversations and also to collect your swag. Thank you. Thanks, everyone. Thanks for your time. 


----

; This article is entirely auto-generated using Amazon Bedrock.
