---
title: 'AWS re:Invent 2025 - Secure in milliseconds: Visa''s AI-powered fraud defense on AWS (IND3313)'
published: true
description: 'In this video, Visa engineers Hema Zutshi and Aravinda Kumar Nagalla explain how they built Visa Protect for A2A, an AI-powered fraud defense system for account-to-account payments on AWS. They detail meeting stringent requirements including sub-250 millisecond latency, 99.99% uptime, and GDPR compliance. The architecture leverages AWS Nitro Enclaves for in-memory data protection, Amazon EKS with topology-aware routing, Amazon MemoryDB for fast durable reads, and multi-region active-active deployment across London and Ireland. The system handles 1,000 TPS bursts while securing the rapidly growing A2A payment market, projected to reach $195 trillion annually by 2030.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/60.jpg'
series: ''
canonical_url: null
id: 3085358
date: '2025-12-05T05:17:18Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Secure in milliseconds: Visa's AI-powered fraud defense on AWS (IND3313)**

> In this video, Visa engineers Hema Zutshi and Aravinda Kumar Nagalla explain how they built Visa Protect for A2A, an AI-powered fraud defense system for account-to-account payments on AWS. They detail meeting stringent requirements including sub-250 millisecond latency, 99.99% uptime, and GDPR compliance. The architecture leverages AWS Nitro Enclaves for in-memory data protection, Amazon EKS with topology-aware routing, Amazon MemoryDB for fast durable reads, and multi-region active-active deployment across London and Ireland. The system handles 1,000 TPS bursts while securing the rapidly growing A2A payment market, projected to reach $195 trillion annually by 2030.

{% youtube https://www.youtube.com/watch?v=Ms0lDo_19sM %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### The Rising Threat: Account-to-Account Payments and the $58 Billion Fraud Challenge

Thank you for joining us at Reinvent 2025. It is exciting to see a full room today. My name is Suraj Sajo. I'm an AWS customer solutions manager working with our largest global financial service customers on their enterprise transformation journey. Welcome to Secure in Milliseconds, where you will learn about how Visa built an AI-powered fraud defense on AWS to secure account-to-account payments. I'm joined today by Hema Zutshi, senior director of engineering at Visa, and Aravinda Kumar Nagalla, chief architect for the Visa Protect A2A product at Visa. It's a pleasure to have them here with me. We've had a phenomenal partnership over the years, and it is a testament to their team's hard work that we're able to share key learnings from their journey.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/60.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=60)

 Let's go ahead and get into the agenda. First, I will talk about the growing payments landscape, focusing specifically on account-to-account payments, its increased relevance, and the growth in risk. Then my colleagues from Visa will talk about how they secure this risk through Visa Protect for account-to-account payments, the high bar of requirements that they held themselves to, and the journey in building this on AWS. We will deep dive into how Visa leveraged AWS Nitro Enclaves to meet stringent security requirements, achieve low latency through Amazon EKS optimizations and Amazon MemoryDB, and built for a high bar of resilience. We will then wrap up by reflecting on the key takeaways Visa had from their journey.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/100.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=100)

 In today's day and age, consumers and businesses alike are looking for flexibility and convenience when it comes to paying for goods or services. For years, the swipe of the credit card had done the job. For decades, this had been the backbone of commerce, and it still continues to play a critical role. Then we saw the rise of the digital wallet. All of a sudden, with a tap of your phone, you can now pay for your goods and services in a seamless, contactless checkout manner. This was the next stage in the payments evolution.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/150.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=150)

But with the rise of digital technology, consumers and businesses are looking for even more streamlined, cost-optimal methods of payment. This is where account-to-account payments, or A2A payments, come into play.  A2A payments are a secure, reliable, streamlined, instant payment method that moves payments or funds from the sender's bank account directly to the recipient's bank account without the need for additional intermediaries like credit card networks or credit card instruments. There are various use cases for A2A payments, such as person-to-person, person-to-business, business-to-person, and beyond.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/180.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=180)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/200.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=200)

There are two types of A2A payments. First is the push payment.  The push payment is initiated by the sender and moves funds directly from the sender's bank account to the recipient's bank account. We've all probably used this. A great example is, let's say a buddy of mine covered me for lunch yesterday and I need to pay him back. I'll go ahead and send him a push payment.  The next is the pull payment. The pull payment is initiated by the recipient and moves funds from the sender's bank account to the recipient's bank account. Examples of this include a business that has subscriptions or recurring payments that they would like to receive from their clients at an agreed-upon time interval, and so they initiate a pull payment.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/220.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=220)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/230.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=230)

 Let's go ahead and look at how the A2A payment actually flows. First, the consumer or business requests the payment, and that request gets routed to the sending financial institution, who then processes it  and initiates the payment via a real-time payment network, or RTP network. A few examples of RTP networks include PIX in Brazil, UPI in India, faster payments in the UK, and beyond. The real-time payment network then forwards the payment to the receiving financial institution, who then processes it and posts it to the receiver.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/260.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=260)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/290.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=290)

This more streamlined payment method has seen increased adoption over the years, which is attributed primarily to the cost optimizations available through RTP networks. RTP networks are significantly more cost efficient than credit card networks.  Juniper Research is forecasting 83% growth of the annual number of transactions in the A2A landscape by 2029, reaching 1 trillion annual transactions.  By 2030, the annual transaction value is expected to grow by 113%, reaching $195 trillion of transactions annually.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/320.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=320)

That is tremendous scale and tremendous growth, but why does this matter? Well, as account-to-account payments grow, the transaction rates grow, the transaction value grows, so does the associated fraud. Juniper Research also forecasts that the annual value of fraud-related  losses is expected to grow by 153%, reaching $58 billion worth of losses in the banking segment. That is not a negligible amount.

This risk and this fraud is exactly what customers like Visa are solving for by leveraging AWS while meeting stringent requirements around security, latency, resilience, and even data localization requirements while enabling tremendous scale. I invite my colleague Hema to tell us how Visa secured account-to-account payments and built this on AWS.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/380.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=380)

### Visa Protect for A2A: Building Trust Through AI-Powered Fraud Prevention

My name is Hema Zutshi. I'm a Senior Director of Engineering at Visa. Visa is synonymous with trust, and to build that trust, Visa has invested  over the years in core principles around resilience, high uptime, high availability, stringent adherence to cybersecurity, and low latency. Based on these principles, as Suraj was talking about account-to-account payments and the growth of account-to-account payments, wherever there is growth in payments, unfortunately fraud follows.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/450.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=450)

Since these account-to-account payments do not use the existing fraud payment solutions of the card network, Visa built a solution specifically for non-card payments called Visa Protect for A2A, which is aimed at enhancing the security of non-card payments. It's designed to address the demand for greater security in non-card payments, and as Suraj was talking about, it caters to all kinds of non-card payments: B2B, B2C, C2C, and C2B. 

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/480.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=480)

To build this solution, we leveraged the AWS infrastructure. This infrastructure allowed us to build both our scoring engine, which gives the fraud score back to our customers so they can make a decision on this, as well as our model inferencing AI platform, which is our backbone for giving the score back to our customers based on account-to-account models. 

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/500.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=500)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/530.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=530)

In Suraj's diagram earlier, we talked about account-to-account payments and how when a sender sends a payment through its sending FI, which is the sender's Financial Institute, it goes to the RTP network.  To avoid fraud between the sender and receiver, we are introducing a call made to Visa Protect for A2A. This call lets the sending FI decide based on a score that Visa Protect for A2A is going to return back to make a decision. If the score is high, it can stop the payment from going to the RTP network, thus preventing the fraud from happening. 

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/550.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=550)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/590.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=590)

Now, let's say for whatever reason we are not able to make that call to Visa Protect for A2A through the sending FI or it did not get a high enough score. There's another opportunity for us at the receiving FI, which is the receiver's bank, to make a call  to Visa Protect for A2A and stop the fraud from fully happening. It can reject the payment from the RTP network. Visa builds solutions for its global audiences across various different regions. Within various regions like the UK, the liability of fraud lies on both the sender and receiver. So to protect both senders and receivers from their liability of fraud, Visa Protect for A2A is a solution that a lot of our customers are using to prevent fraud from happening. 

In the next few slides, I will be talking about the requirements that we had to build this solution and how we leveraged the AWS platform for building this solution. Visa has traditionally built its solutions within its on-premises system.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/630.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=630)

### Meeting Stringent Requirements: Security, Latency, and 99.99% Uptime

Many of our requirements were catered towards our on-premises data centers. We had to rethink some of that and make sure we are able to build it according to the same guidance as we are building to protect our customer data in the on-premises systems. The first requirement is adherence  to very stringent cybersecurity guidelines. This is not just a buzzword for Visa. In Visa, this is taken very seriously because our customers trust us with their personally identifiable information and their PII data. On top of your regular design security requirements and technical security requirements and everything that comes with data protection, we had a unique problem to solve as we were taking our solution into the cloud, which was to protect against in-memory data exposure.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/670.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=670)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/720.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=720)

Data is encrypted and protected when it is in transit or when it is stored. However, it is not protected when it is getting processed in memory, and it is during these times that various threat vectors and malicious agents try to steal the sensitive data.  Threat vectors like core dumps and swap files, exposed data endpoints, and memory scraping hardware are the chances where they can get into the cloud network and steal customer information. To protect against that, we not only built another layer of security, but we rethought the architecture. Our solution against this threat was to go with zero trust architecture  and use AWS Nitro Enclaves. Think about Nitro Enclaves as a secure vault within a main server, and nobody has access to that secure vault. All communication to that secure vault happens through the main server on a secure channel, and nobody, not even AWS or Visa accounts, have access to that. My colleague Aravinda is going to deep dive on this in the next few segments.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/750.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=750)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/760.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=760)

 The next very important requirement to build a solution that caters to all the regions that we support  is to adhere to the local regional requirements. Since we were building this solution for our European markets, one of the very important requirements for us was to build around GDPR requirements. Even though GDPR does not necessarily request you to have data localized, we are able to do better with their requirements if the data is localized. We also had requirements from our banks, who are our customers, to keep data locally. For that specific reason, our solution to build this platform was to use AWS regions for both transactional and model influencing engines to be hosted in the European regions in AWS.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/810.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=810)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/840.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=840)

 This not only helped us with the localization requirement, but it also helps us with the next requirement, which is around low latency. Low latency is not something we take lightly. It is a very serious requirement in the financial sector. Transactions happen within a few seconds, and any kind of latency can break the overall user experience. Take, for example, the example that Suraj took where two friends are splitting the bill. This is happening over the RTP networks. If we now want to add another layer to it of security by using Visa Protect account to account, we cannot afford to extend that time of end-to-end transaction between these two because that will break the overall experience.  To make sure that we are able to provide this fraud solution, we had to make sure the solution that we are building is able to give the end-to-end results in less than 250 milliseconds. That is a fraction of a second, which means that you have to accept the transaction, process it, run it through our models, and respond back within 250 milliseconds.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/920.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=920)

To address this requirement, we looked at two different approaches. One was co-localization, which I discussed in the previous slide. To avoid any network latency, whenever we try to create a TLS connection, it goes back and forth a few times, which takes time to establish.  To avoid that latency, we made sure to build our solutions within local regions. We also architected our solution based on Amazon MemoryDB with Valkey. This is a very interesting solution because Amazon MemoryDB not only gives us faster reads but also provides durability of data, which is the right combination for a financial solution. Essentially, it gives us the faster read time of a cache combined with the durability of a database. Aravinda will talk in detail about this. We also used TLS connection pooling mechanisms to help us achieve the 250 milliseconds latency that Aravinda will be covering as well.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1000.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1000)

The last business requirement I want to discuss is around resiliency and high availability.  One of the principles for our trust is high availability, resilience, and uptime. Within Visa, there is a categorization of tiers for our applications. Anything that is transactional is considered a tier zero. To be considered tier zero, an application needs to fulfill at least 99.99% uptime. Think about this: within an entire year, an application can only go down for a maximum of 52 minutes and 35 seconds. Beyond that, we will be breaking our SLAs.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1080.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1080)

There is another important requirement we had to cater to. Within Visa, we have internal availability and disaster recovery requirements, which we call ITDR. To cater to those ITDRs, we have to build redundancy in at least dual regions, and within that redundancy, the regions have to be more than 60 miles apart. To build our solution to cater to our tier zero uptime requirements of 99.99%, we built our application within two regions and gave them redundancy in both London and Ireland regions with a multi-AZ setup.  We are using three availability zones within our AWS setup. This covers the various requirements we had to build around.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1110.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1110)

### Functional Architecture: Real-Time Scoring and Model Inferencing Platform

I will spend some time talking about the functional architecture of Visa Protect Account to Account, and then Aravinda will go into the details of the architecture.  Visa Protect Account to Account is a scoring engine that takes a request, processes it, runs it through our models, and gives a score back to the end customers. To enable this, we have two important components. We have the real-time component, which includes a scoring API as well as a model inferencing API. The scoring API talks to our external clients, does all the data processing, performs data acceptance and security checks, and then decides through a decision service which model to call for inferencing.

Within model inferencing, we have an aggregation service, an orchestration service, and long-term profiles that run and provide a risk score for a transaction. Along with the real-time system, which is the core of our platform, we also have a near real-time offline system that uses a scoring API as well as a feature engineering platform. The scoring API is an important offline system that accepts daily files and daily status files. It has data consumers that accept data from our clients. When you are onboarding a new client, this system allows us to get historical data of the clients.

We are able to build our features based off that data. Our feature engineering components has both long-term profile generation and short-term profile generation features, along with your billing, reporting, and all other components that comprise a system. So with that, I'll hand it over to Aravinda, who'll be going deep on the architecture.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1260.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1260)

### Technical Deep Dive: Real-Time and Offline Pipeline Architecture

Thanks, Emma. Hello, everyone. My name is Aravinder Nagala. I'm a solution architect at Visa. Next, I'll walk through the technical architecture for the functional components that Hema just described. Later, we will dive deep into some of the AWS technologies we used in our architecture. 

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1270.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1270)

Let's start by looking closely at our real-time scoring API workload. This is the REST API that our clients use  to get their score from our model. Client requests land in our DMZ VPC. Think of this as a secure perimeter facing the internet where each incoming packet is inspected by Amazon Network Firewall, and only the traffic from whitelisted IP addresses is allowed inside. The request then lands at our load balancer where we authenticate the request using mutual TLS. Successfully authenticated requests travel via transit gateway, VPC endpoints, and then land at our Amazon API Gateway, which is hosted in a private VPC.

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1360.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1360)

API Gateway enforces usage plans, applies throttling per client, and then sends it to the A2A Gateway application running inside the Nitro Enclaves via a network load balancer. A Gateway application receives the request, terminates the TLS connection inside the enclave, and then performs some validations. It also does secondary authentication using token validation and encrypts the sensitive PII data using KMS encryption. Then it converts the REST request into gRPC protocol and sends it to our downstream Decision Service application. The Decision Service runs on top of Amazon EKS. 

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1380.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1380)

It transforms the incoming request and then performs duplicate checking by doing lookups into Amazon ElastiCache for Valkey. It also enriches the incoming payload by doing some lookups into ElastiCache as well. Then it calls our model inferencing platform.  Our model inferencing platform runs on Ray cluster, which is deployed on top of Amazon EKS. Model inferencing mainly consists of three different services. The LT profile service fetches the long-term profile features from Amazon MemoryDB. The aggregation service performs real-time aggregations by fetching short-term profile features from MemoryDB as well.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1470.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1470)

Once the features are ready, the AI service runs the model and generates the risk score. The score is then sent back to the Decision Service where we have a rule engine applying the rules and transforming the response by decisioning. Then it sends that response back to the gateway where we convert the gRPC response to REST, and then that response is sent back to the client. So that completes the real-time flow. Both the model inferencing service and the Decision Service and the gateway service are writing the requests and responses to Amazon Managed Streaming for Kafka, which is fed to our downstream applications such as reporting, billing, and near real-time feature engineering processes. 

So we chose ElastiCache for Valkey and MemoryDB with Valkey engine because they are about twenty percent cheaper than Redis, and they provide the same functionality and performance as well. Our applications didn't need to change because the Redis client libraries work as is with the Valkey engine. That's on the real-time side. Next, let's take a look at our offline flow pipeline. This flow enables our clients to upload daily files such as entitlements, fraud data, and PAN to bank account data and transaction status files.

These files are also used by our clients to upload one-time historical transactional data, which gets used for model training and baseline feature generation. Clients connect to AWS Transfer Family Service running inside the DMZ VPC, which is facing the internet, and they upload the PGP encrypted files using SFTP protocol. The files land in an S3 bucket from where we have a scan process picking up these files and running AWS GuardDuty, scanning for malware detection.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1570.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1570)

The successfully scanned files get moved to another S3 bucket, and an SNS notification is also generated. This SNS notification is consumed by our data ingestion process, which is also running inside the Nitro Enclaves. It consumes the notification and then picks up the file  that was successfully scanned and pulls that file into the Nitro Enclave memory and decrypts the file inside the Nitro Enclave. Then it parses the file and performs various checks and validations.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1610.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1610)

The data ingestion process starts encrypting the PII data which is in each record. The encrypted records are written to Amazon Managed Streaming for Kafka. The file metadata is also stored in Amazon Aurora DB for reporting purposes. The Kafka data is consumed by various applications. We use Amazon Data Firehose  for consuming the data from Kafka and writing into an S3 bucket.

From there we have EMR jobs picking up these files and then aggregating and then writing into a separate S3 bucket. This S3 data gets used by our downstream feature engineering and model validation and model retraining purposes. We also have another consumer application which reads from topics such as entitlement and branch code data, and then writes that data into Amazon ElastiCache for Valkey. This Valkey data is used by our real-time scoring API workload.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1670.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1670)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1680.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1680)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1690.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1690)

### AWS Nitro Enclaves and Performance Optimization: Achieving Sub-250ms Latency

The same data is written to Amazon Aurora DB for long-term storage. This ensures that both our real-time pipeline and offline pipeline use secure, validated, and enriched data and share that data across both flows.  Next, let's dive deeper into AWS Nitro Enclaves,  the security backbone for our sensitive data processing. The Nitro Enclaves are secure,  hardened, and isolated environments running inside the EC2 instances.

They have no external connectivity, no SSH access, and no persistent storage. Not even the root user from parent EC2 can log into the enclave. Only the parent EC2 can connect to the enclave using a secure local channel via VSOCK protocol. These are ideal for processing sensitive data such as bank account numbers, credit card numbers, and customer data. Our A2A Gateway application runs inside the Nitro Enclave.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/1730.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=1730)

This is the technical design for our  application. During the startup, the application connects to Secrets Manager and KMS proxy via proxies and fetches the envelope encrypted data encryption key into the enclave. Then it decrypts the encrypted key using KMS customer managed key, and then it caches that key in the enclave memory. When an input scoring request arrives, the NLB sends it to the gateway proxy running on the parent EC2.

The proxy converts the incoming TCP protocol request into VSOCK protocol and sends it into the enclave. Inside the enclave, the A2A Gateway application receives the request, terminates the TLS connection, and then validates the payload, and then starts encrypting the plain text PII data using the cached key that was cached during the startup. Then it converts the request from REST to gRPC and then sends that request to downstream Decision Service application, proxying via VSOCK channel again.

Application proxying via VSOCK channel ensures that sensitive data processing happens entirely inside the Nitro Enclave without exposing it to memory exposure threats that were discussed earlier. These are some of the best practices we follow for application deployment inside Nitro Enclaves. The enclave resource allocation is done using an allocator YAML file. We declare the vCPUs and memory inside this YAML configuration, and the best practice is to use user data scripts because Nitro Enclaves require contiguous huge pages memory. If allocation or reservation is not done during startup, the memory can become fragmented and may not be available for the enclave to reserve. To avoid this, we perform the allocation with user data scripts.

Careful tuning of proxies is essential because it provides better throughput. Otherwise, we may not achieve the throughput we require. We use SOCAT proxies for converting TCP to VSOCK and vice versa. SOCAT has options for concurrent connections such as reuse address and fork. By carefully tuning these options, we were able to achieve better throughput. Health check endpoints should check the health of both the application and proxies to ensure that the instance is healthy before sending traffic to a particular instance.

As we saw earlier, the application inside the enclave fetches encryption keys during startup and then caches them in memory to avoid latency in retrieving keys for every request. By using all these optimizations, we were able to achieve around 150 to 200 TPS throughput with a single m5a.2xlarge EC2 instance. Performance is just as critical as security for our application.

We use TLS 1.3 and HTTP 2.0 throughout our stack for faster handshakes because TLS 1.3 requires one less round trip, and HTTP 2.0 provides multiplexed connections. AWS load balancers support both of these protocols, and our applications support them as well. We recommend our clients also use TLS 1.3 and HTTP 2.0 because it provides better end-to-end latency and throughput. This approach is also highly secure.

For the Amazon API Gateway with increased traffic, we use VPC endpoints, and for backend integration with our A2A Gateway application, we use VPC Link. This ensures that traffic stays entirely on the Amazon backbone network, giving us consistent low latency performance. For the NLB client routing policy, Availability Zone affinity setting is enabled, which means traffic stays within the Availability Zone where the client is making the request from. This provides better latency. For the load balancer target selection policy, cross-zone load balancing is enabled to provide high availability.

The application gateway running inside the enclave fetches secrets and keys during startup and caches them in memory. We have background threads refreshing these keys as needed. This avoids repeated fetch latencies. We fine-tune Amazon EKS for both performance and resiliency. We use the AWS Load Balancer Controller to provision and manage load balancers. The Load Balancer Controller provisions ALB for Kubernetes ingress resources and NLB for Kubernetes load balancer resources. NLB can be provisioned with either instance or IP target type. The annotation we see here, aws-load-balancer-nlb-target-type, is set to IP.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2120.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2120)

This means the pods get IP addresses directly assigned from Amazon VPC subnets, avoiding any extra network hubs and providing better performance. 

Next, let's look at pod scheduling. There are mainly three ways we can control pod scheduling: node affinity, pod affinity and pod anti-affinity, and topology spread constraints. Node affinity addresses whether you want to place a particular pod on a node with a specific characteristic, for example, a hardware characteristic like enclave enablement. For instance, we are now migrating our gateway application from EC2 to Amazon EKS, and this application pod needs to run on the nodes where enclaves are enabled, so this is done by using node affinity.

Pod affinity and pod anti-affinity apply when the placement of a pod depends on the location of other pods. Pod affinity can be used to co-locate synergistic workloads. For example, if we have a web server and a corresponding caching pod, they can be co-located together using pod affinity. Pod anti-affinity distributes the pods across a failure domain, providing high availability. For example, if you declare a service with three replicas, you don't want all of them to stay within the same Availability Zone, so this can be controlled by using anti-affinity.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2220.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2220)

Next is topology spread constraints. When you want to ensure that the replicas from a pod for a single service are evenly balanced across the different failure domains, that's when we use topology spread constraints. This is a sample manifest definition for topology spread.  The section topology spread constraints ensures that the replicas are evenly distributed across the failure domain. Max skew is set to one, which is a crucial setting. This dictates that the number of pods for a given service in a given Availability Zone will not differ by more than one with the number of pods in other Availability Zones. The topology key is set to zone, which means that the failure domain we talked about earlier is at Availability Zone level.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2260.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2260)

Next is topology aware routing.  This keeps the network traffic within the Availability Zone where it originated. This helps with reliability, performance, and costs because cross-zone traffic is charged. This is a sample definition for topology aware routing. We have the service definition under which we have the metadata section, and we are setting the topology mode annotation with auto. This auto mode tells Kubernetes to keep the network traffic within the same Availability Zone where it originated. It also has a smart default: if there are no healthy pods available within that Availability Zone, it will automatically reroute to the available pods in other Availability Zones.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2340.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2340)

This feature works by adding hints to endpoint slices. In order for the routing algorithm to work effectively, it is recommended that we have at least three replicas running in a given Availability Zone. For example, if we have a service with nine replicas defined within three Availability Zones, this definition will make sure that there are three pods per each Availability Zone. 

So combining all these optimizations together and using Amazon ElastiCache Valkey and Amazon MemoryDB in our real-time scoring API workload, we were able to achieve our latency SLA of 250 milliseconds. In fact, our P99.5 latencies are much better, about 40 percent lower than that. We were able to successfully test our service with bursts of up to 1000 TPS.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2390.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2390)

### Multi-Region Active-Active Architecture and Global Expansion Blueprint

Next, let's take a look at our multi-region active-active architecture, which is essential for a tier-zero application such as Visa Protect for A2A for visa's ITDR requirements. We have Cloudflare and Amazon Route 53 DNS routing traffic to the nearest region.  In each region we have both compute and data services deployed across three Availability Zones. The load balancer is distributing the traffic across these three Availability Zones. We have the application layer writing to ElastiCache in both local and remote regions for real-time duplicate checks.

We also use Amazon Managed Streaming for Kafka, with MSK data replicated using MSK Replicator for our near real-time feature engineering flows. S3 cross-region replication is enabled for our offline flow pipeline buckets, and we use Amazon Aurora DB in our file processing pipelines with Aurora DB replication enabled from primary to secondary.

Using AWS MSK Replicator, S3 cross-region replication, and Aurora Global DB replication reduces the operational complexity on our side because AWS takes care of the replication overhead. This gives us high availability, disaster resilience, and consistent performance globally. That concludes the technical deep dive for today.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2510.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/384c9045a2fc757a/2520.jpg)](https://www.youtube.com/watch?v=Ms0lDo_19sM&t=2520)

Next, I invite Hema back to the stage to wrap up. Thank you very much. Thank you, Aravinda, for the detailed architecture overview. I would spend a few minutes discussing the key takeaways of our partnership with the AWS team and what we learned as part of building the Visa Protect account-to-account solution.  This is the first time we took something internally  and tried to bring it to the cloud.

Visa has spent considerable time and effort building its hybrid cloud solution, and our application was one of the very first applications to adopt that hybrid cloud solution and host an application on it. As part of this journey, we built a solution that caters to Visa's stringent operational and cybersecurity guidance, which we follow very carefully on our on-premises systems. We needed to see how that would work on a hybrid cloud while handling 1,000 transactions per second with 99.99% availability and sub-second latency. We consider this a blueprint for our application, and it is fully functional with our clients using the scoring service today in our UK region.

The next part of our journey was to see how we could use this blueprint and take it to various other regions. As I mentioned, Visa is a global company with many solutions built for our global markets to cater to their local requirements. We are using this blueprint we have built, along with Infrastructure as Code scripts, to move faster to other regions with slight tweaks needed for those specific regions. As an example, I will discuss Visa's expansion into South American markets, with Brazil being one of our key clients where we are taking this solution to the SÃ£o Paulo region to open it up for our markets and help protect and prevent account-to-account fraud there.

The third important aspect I want to highlight is our partnership with the AWS team, particularly the Suraj team. We have worked for the past two and a half years building this solution with various partners within Visa, including our operations team and cybersecurity team, and it has been a great partnership. We have all evolved and learned from our experiences, leveraging many different AWS patterns that have helped us scale our services and solution. With that, I would like to conclude this presentation. I would like to thank you all for coming and listening to us. If you have any questions, we are happy to have side conversations, but thank you all for your time. I would love for all of you to fill in the survey in the mobile apps. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
