---
title: 'AWS re:Invent 2025 - Generative AI, agents, MCP, and the future of AI-powered software development'
published: true
description: 'In this video, Derek, Pavitra, and Geetha explore generative AI agents and MCP for software development. Derek explains agent composition fundamentals including the agentic loop, LLM reasoning, context window management, and MCP as the standard for tool integration. Pavitra demonstrates building custom agents with Kiro CLI and the newly launched Kiro Powers, covering model selection, prompts, tools, resources, and guardrails, plus the distinction between local and global agents. Geetha shares FINRA''s journey implementing custom agents across their SDLC, including a tech upgrade agent that automates dependency upgrades using Amazon Bedrock AgentCore. The session emphasizes context engineering, human-in-the-loop design, and team collaboration through shared agent configurations, with FINRA achieving 30% code quality improvement and 80% positive engineer sentiment.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Generative AI, agents, MCP, and the future of AI-powered software development**

> In this video, Derek, Pavitra, and Geetha explore generative AI agents and MCP for software development. Derek explains agent composition fundamentals including the agentic loop, LLM reasoning, context window management, and MCP as the standard for tool integration. Pavitra demonstrates building custom agents with Kiro CLI and the newly launched Kiro Powers, covering model selection, prompts, tools, resources, and guardrails, plus the distinction between local and global agents. Geetha shares FINRA's journey implementing custom agents across their SDLC, including a tech upgrade agent that automates dependency upgrades using Amazon Bedrock AgentCore. The session emphasizes context engineering, human-in-the-loop design, and team collaboration through shared agent configurations, with FINRA achieving 30% code quality improvement and 80% positive engineer sentiment.

{% youtube https://www.youtube.com/watch?v=Kq0Se1nnT3U %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/0.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=0)

### Introduction: Generative AI Agents and MCP in Software Development

 Alright, welcome everyone. Thanks for joining us today. My name is Derek, and with me are Pavitra and Geetha on the side of the stage here. Today the three of us are going to be talking about generative AI agents and MCP and how we're seeing customers leverage those to get better results and higher velocity in their software development using custom agents. It's an exciting topic, and things are evolving quickly. Pavitra and I work with customers around the world that are adopting these agentic software development tools, and so we're happy to talk to you about the AI agents and how we're seeing those being used.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/60.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=60)

So for the agenda today, I'm going to talk first about how agents are composed and some of the fundamentals  that we as software developers need to be keeping in mind as we're getting ready to use agents to develop software. After we talk about those fundamentals, Pavitra will come up next and talk about how you can turn those into reality using Kiro to build custom agents and deploy those within your software development teams. And then finally, Geetha will come up and talk a little bit about how FINRA has deployed custom agents to help their developer teams move faster and get better results.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/100.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=100)

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/120.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=120)

So starting off with agent composition.  Simon Willison has a definition that I think is commonly accepted and is really nice for agents, which is an LLM agent runs tools in a loop to achieve a goal. It's simple. The concept is simple. I want to extend it a little bit for the purposes  of today's session and say that an LLM agent runs tools in a loop while building context to achieve a goal, and that context piece is very important, as I'll talk about in a minute. And as a software developer, it's something that we need to keep in mind from the beginning about managing that context depending on the task at hand.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/150.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=150)

### Understanding the Agentic Loop: How AI Agents Process Tasks

So here's a visualization of how we can think about the agentic loop  when we're using the agent for software development. And again, I think there's beauty in the simplicity here. As a software developer using a tool like Kiro, I have an input where I ask the agent to build some software or write some tests or help me plan or do spec-driven development. That input is going to go into our context window, which I'll talk about in just a minute. And then that context is going to be fed into the LLM, and the LLM is going to reason over that input about what I've asked and whatever else is in the context window, and then it's going to feed that into tool selection.

So the agent is going to decide what the next logical step should be, and it's going to go invoke a tool as that next step. We'll invoke the tool, we'll get a result back from it, and we'll put that result in the context window. And then we'll pass that back to the agent again and say, hey, here's the result of what just happened. What do you want to do next? We'll keep iterating until the agent decides that it's achieved the goal or for whatever reason it's done, and it'll generate the response. So nice and simple, but we really need to think about each of these components as we're getting ready to start software development. So I'll talk about each of these briefly.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/230.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=230)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/240.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=240)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/250.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=250)

Starting with the LLM. So for today's purposes, we're all going to wear our  software developer hats and think about applied AI, and we're not going to think about the details of how the LLMs work. And so I'm not going to talk about  n-dimensional vector space and how the relationships work. I'm going to say an LLM provides reasoning, and we could all debate  after the talk about what that means and how they reason. But as a practical matter, I can give a task to an LLM, and it will reason about it, and it will suggest the next step that it wants to do, which is great.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/270.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/280.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=280)

I'm also not going to talk about auto-regression and  attention patterns and all that stuff. I'm going to say the LLM provides knowledge. And when I say it provides knowledge, during the pre-training stage,  a whole bunch of information has been trained into the model weights, so we can call that latent knowledge or latent understandings about how software works, how to write code, how libraries work. It has that knowledge built into the LLM, and so it's able to draw on that reasoning capability and that kind of deep understanding about a lot of different aspects of software to help us achieve a goal.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/310.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/320.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=320)

### The Context Window: Providing Working Memory for Stateless LLMs

However, that language model is stateless. It doesn't come with any working memory,  and so we have to provide that working memory in the context. This is a simplified mental model  of the context window, but essentially it's a big long string of tokens. All of the context, all of the working memory about what we're asking this model to do, is in the context window. System prompts are going to be in there that are giving it overall guidance about how to work. Any context files that we want to load as developers, so we can include information about how our teams work together, how we want to do the software development process, how APIs work, and so on. It's up to us to provide those context files, which Pavitra will talk about in a minute.

The history of the current chat conversation is also included. We're going back and forth and iterating and building, and all of that chat history will be in there as well. Any source code that our tool has read as part of its understanding of what we're working on will be included. The output of all of these tool calls are going to be in there as well, and hopefully some free space. The context windows keep getting larger and larger, but it's always possible to fill them up. If that happens, we can do things like compression, but hopefully we can manage this context window well so that it has some room in it.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/390.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=390)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/410.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=410)

### Tool Selection and MCP: The USB Plug for AI Models

And finally, tool selection and tool execution.  So now that our agent has the ability to reason, it's able to draw on its latent knowledge that's trained into it. It has a working memory. It's able to use that in the loop. It needs to be able to do stuff, and that's where tool selection and tool execution  comes in. The standard that's emerged, that many folks in the audience will be familiar with, is MCP. That's become the de facto standard for plugging in tools.

I like to think of MCP as the USB plug for a model. I can take any MCP server, plug it in, and it'll just work. On the left side here we have MCP clients, so that could be the Kiro CLI, which we'll talk about, Kiro IDE, autonomous agent, or any other developer agent tool that we're using. Then we're able to plug in MCP servers on the right hand side here. MCP servers can provide skills and teach our model how to do in-context learning and pick up new information. They can provide resources if we want to do read-only from a database or some other data source, and they can also allow the agent to make changes, mutative actions, similar to an API on another system. So a very simple concept but very powerful.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/480.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=480)

As we're thinking about setting up an agent,  this is an important step as developers that we think about upfront. What language model am I going to use? How am I managing the context window? Do I have the right information for the agent to be able to achieve the task or to work in the style that I want to work in? And what tools do I want to provide to the agent? If we skip this step and we just get started into building, we're probably not going to get the output, the outcome that we want, so it's important to think about this upfront.

What we're seeing is an emerging pattern where development teams are creating agents for different tasks that their developers might do, and then they're sharing those amongst themselves. As they refine these what we call custom agents, that allows them to have tools that are better and better suited for different purposes, and they keep improving over time. So now that we've talked about what goes into an agent and what goes into a custom agent, Pavitra is going to come on the stage and talk about how you can create custom agents defining each of these aspects that we just talked about using Kiro and also tools like Bedrock AgentCore. So Pavitra, over to you.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/560.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=560)

### Introducing Kiro: Spec-Driven Development for Enterprise Software

Well, thank you, Derek. Hey everybody, I'm super excited to be talking to you today about Kiro, which is our  newest generative AI development tool, and also talk to you about how you can create custom agents using Kiro CLI. Basically, custom agents power them with tools and resources so that you can then get rid of all your repetitive and toil tasks as developers. My name is Pavitra Krishnan, and I lead the specialist solution architecture organization focusing on Kiro, autonomous agents, and developer experiences.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/600.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=600)

So with a quick show of hands, has anybody tried Kiro yet? Oh, quite a few people. All right, awesome. 

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/610.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=610)

So for folks who have not tried Kiro, I just want to take a step back and talk about what Kiro is all about.  Kiro is this new agentic AI development tool that we launched literally just over two weeks ago. It went GA. It was in preview for a while, but then it went GA just a little over two weeks ago. It's based on a fundamental design principle of software development called spec-driven development.

Now, before I even talk more about Kiro, let's take a step back and try and understand why spec-driven development is even so important and why we care about it. So think about this: you have a couple of developers, you put them in a room, and you give them a couple of bullet points and tell them, hey guys or girls, go ahead, write code for this, and I'll check in in a few hours, with not too much detail but just a few bullet points. What do you think is going to happen? Probably the first thing is confusion, chaos, trying to figure out what am I supposed to build here. And then, if you have smart developers, you'll probably get some product in the next few hours, but will that be the product that you wanted? Probably not, right?

So when vibe coding came along and everybody was super excited about this and started using it by giving it prompts and then having it generate a piece of code, you pretty much just go in there and you say, hey, I gave you this prompt, go build me this thing. The agent takes it and it's like, okay, I'll go take a look at this prompt. You probably have missed specifying a few things in the prompt and detailing it out, so it'll make decisions on your behalf, and then you'll come back and say, boom, here you go, I built this thing for you. Then you have to go in and look at the 500-line code that is generated and figure out, did it build what I wanted it to build in the pattern that I wanted it to use? And that's not a very good use of a developer's time, right? And that's not how we build software traditionally, if you think about it.

Yeah, well, if you're just one developer sitting in a corner of a room and trying to build like a rock, paper, scissors application, sure, that approach might work. But in an enterprise-grade application when you have complex systems that are distributed and you have multiple teams of developers working on it, it gets very tricky. How do you standardize it? So you go back to how software was originally developed, where we first started with the requirements. We started with the requirements, and then we sat with the stakeholders and aligned on the requirements, and then identified what the technical design would need to look like.

Once we decided, hey, this is a technical design that we are going to go ahead and implement, then we came up with a project plan where we identified those designs into tasks, what tasks can be performed sequentially, what tasks can be performed in parallel. And then we figured out, okay, these are the task backlogs. Let me go ahead and find the resources to now say, hey, take this, now go ahead and implement it. And at the end of it, you get an enterprise-grade software that gets delivered, right? That essentially is what all spec-driven development is based on.

So Kiro takes your prompts and converts them into clear requirements that are called specs, which then you are literally just looking at text and you're like, okay, I think this is right, this is what I wanted to build, instead of you looking at like 1,000 lines of code and figuring out what it built. You're looking at the requirements first, and then you convert that and say, okay, I think this is great, go ahead and generate a system design for me, Kiro. And then Kiro comes up with a system design for you. So, oh, that looks fine. Let's go ahead, create a task, break it down into multiple tasks so that I can start assigning you some tasks, and maybe I'll take some and start working on them.

So it gives you a very systematic approach to software development, and when you try and build, like I said, enterprise-grade software that's complex, distributed with multiple teams of developers, it adds a layer of governance to it. So that's what Kiro is all about. So it provides you a spec-driven development-based approach. Now that we spoke about Kiro, I actually want to go and talk about custom agents. How many times have you heard the word agent this week? One time, two times, probably plenty of times. Somebody's saying 15 times, 20 times, right?

### Kiro Powers and Custom Agents: Building Specialized AI Assistants

If you see the trend and where software development is heading, it's all heading towards agentic, where you are envisioning a world where you can have these agents do these tasks for you and how can you build that. So I don't know if you all caught this, but yesterday we actually launched what's called the Kiro Powers. And Kiro Powers is nothing, but it gives Kiro agents the specialized context and takes away the hassle

so you no longer have to deal with MCP context overloading and things like that. How do you use it? You basically go in, and if you go to kiro.gov right now, you actually have a power section where you can go and take a look at all these powers that are already available. You find a power and then you just install it with one click and you start using it. You don't have to manage any context around it. The tools are already preconfigured for you.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/960.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=960)

If there's a power that you want to have but it's not there, go ahead and create it. You have the power to create your own Kiro power as well. This gives you the ability to actually build these agents in the IDE, the Kiro IDE, but I actually want to dive a little bit deeper into the custom agents that you can build with Kiro CLI.  How many of you have heard about Kiro CLI? I love this audience. Awesome. So Kiro CLI gives you all the great things of Kiro, but in your command line interface. I used to be a software development engineer in my career, and I used to love working in the CLI because it's just so much faster and quicker.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/990.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=990)

But before we go down and talk in detail about what really goes into a custom agent, I want to talk about what is a custom agent.  So a custom agent, think of it as a customizable AI assistant that you can configure for your specific use cases and your domains and workflows. Instead of using a generic agent, you can actually use a custom agent. You can define a custom agent, provide it all the context that it needs, preconfigure it with the tools that it can access, preconfigure it with all the permissions that it has, and it can execute on your system and then be available for you to use when you want to call that particular custom agent.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1050.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1050)

An analogy that I like to use is imagine if you had an AI sidekick that you could literally mold into whatever you were doing. Custom agents are like digital clay in your hands. You can basically define these agents  to perform tasks that are specific to a particular workflow that you're working on or to a particular use case that you're working on, and then you can have it as another team member and delegate tasks to a custom agent and say, hey custom agent, go ahead and run this. A custom agent allows you to change the Kiro CLI behavior to make it specific to what you want it to do for your use case.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1090.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1090)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1110.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1110)

### Anatomy of a Custom Agent: Models, Prompts, Tools, and Guardrails

So what goes into a custom agent? Let's talk about that. In the middle you have the Kiro CLI custom agent, and as with anything else, it starts with a model selection. You start with the model, you specify  what is the model that this particular agent can use, whether it's a Haiku model, whether it's Opus or Claude Sonnet. You specify the model choice first. Once you specify the model choice, you then go ahead and specify what is the prompt.  Here the model prompt is not the prompt that you would give when you're doing vibe coding. This is essentially the prompt that describes the behavior of your custom agent. How is this custom agent going to behave? What's its personality?

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1150.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1150)

If I am creating a custom agent that is going to do a DevOps task for me, the prompt for me when I create the agent would be, hey, you are a custom agent who is a specialist in AWS infrastructure and understands all the CI/CD pipelines and is able to make decisions, intelligent decisions that are safe and secure and can be deployed on AWS infrastructure. So that's your model prompt. The third thing is pre-approved tools. So here is where you will define things like your MCP  servers. MCP is all the rage now. But this is where you'll identify this custom agent has got access to these tools. These are the tools that it can access.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1170.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1170)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1190.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1190)

How and in what capacity will it access? That's where your tool configuration comes into play. You tell it, okay, you have the ability to go read this particular tool,  or you have the ability to go write at this particular location. You don't have the ability to go and remove any files, things like that. So that's your tool configuration that comes into play. The third thing that goes into a custom agent is resources, which is nothing but your context  and your hooks. Context is nothing but your context that the agent will need to use when it's executing, but you preconfigure it. So that could be something like, say, a documentation.

It could also be access to your local repo files that you wanted to go take a look at. Any of those things that you want to provide that particular agent as context will go in the context. Then hooks actually allow the agent to do dynamic context evaluation.

There's also a concept called agent hooks, which are specific steps that the custom agent can take at certain points during execution. So in the execution workflow, for example, say you want the custom agent every time before it starts running or giving an answer to what you asked it to do, you want it to go ahead and log something in a particular location. That could be a start hook that you provide. This is an agent hook. I want you to always do this before you respond to me. Or there may be something else like after you're done responding to me, go ahead and list everything that I asked you to do. That could be another hook that you want to provide. So there are multiple predefined parameters that are available in Kiro CLI that you can use to define these agent hooks as you define these custom agents.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1280.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1280)

Last but not the least, definitely most important is guardrails.  You want your custom agent to operate in a very secure fashion so it adheres to your security requirements and your compliance requirements. These could be something as simple as what I was saying earlier, that you don't have any access to delete any files from my directory, as simple as that. If you're going to delete something, check with me. Something as simple as that, but it's very important.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1310.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1310)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1320.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1330.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1340.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1340)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1350.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1360.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1360)

### Creating Custom Agents with Kiro CLI: A Hands-On Demonstration

So I'm going to run a quick demo here.  There are two ways you can actually create custom agents. One is what I call like a wizard mode. Essentially what that means is that you start a prompt with Kiro  CLI and you tell it to go to slash agent and ask it to generate a custom agent, and it'll start prompting you and give you step by step  instructions. It'll start with, okay, tell me what is it that you want me to build? What is this agent name? So here I'm just building an AWS Rust agent. Okay, it's a development agent that I'm building.  Then it says, okay, great, what's my description? Tell me what my personality is going to be. That's where I'm going here and telling it that  you are going to be a specialized agent for AWS and Rust development tasks. So now I gave it a name. I gave it a description, and the next thing it asks me is, okay, what is my scope?  Am I going to be run locally? Am I a local agent or am I a global agent? And we'll talk a little bit more in detail on what a local agent is and what a global agent is. And with that, it actually gives you a straw man starting point of a config file.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1390.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1390)

So if you see behind a custom agent, it's a JSON config file. That is where you define all the things that I was talking about as to what goes into a custom agent. So here let's look at a complete example. This is something that I actually  built just recently. This is the AWS Rust agent. This is actually a full blown custom agent config file that's built, and you can see that this particular agent does not have access to specific MCP tools. So you'll see it has access to fetch and it has access to Git, but it also has access to some other tools, AWS tools like whether it is read, write, shell, AWS, calling AWS commands, and it has some agent hooks in it.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1440.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1440)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1450.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1450)

You also probably see tool aliases. Now tool aliases are if you have conflicting namespaces. As you continue to use multiple tools, if there are conflicting namespaces and you want to be able to give it a friendly name, it's easy to resolve and there's no confusion when the agent is running. That's essentially your tool aliases. And let's scroll down a little bit on this so you can see all the other sections that are folded here.  It gives you a list of what are the allowed tools. So again, going back to what I was saying, how you're preconfiguring the tools and giving it what it has access to and what it doesn't have access to. So this is telling you what are the tool settings,  what paths am I allowed to write? These are your guardrails. Yes, I'm telling it you can go and write only in source folder. You are allowed to go and use S3 and Lambda if you want to.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1500.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1500)

And I'm also giving it resources. This is my context. I'm telling it here I have this readme.md file that I've created. You can use that as a context, or I can also provide it a docs file that's there and say, guess what? Go to this folder, use all the MD files that are in there. That's your context. So that's another way. And then last is the agent hook. Here in this hook, the example that I have is that when the agent is spawned, run the command Git status so you know what the status of it is. And lastly, the model that it's using is Claude Sonnet 4. So earlier, 

### Local vs. Global Agents: Choosing the Right Scope for Your Workflows

when we were defining the agent, there was an agent scope section that came up. If you remember, it asked, do you want me to be a local agent or a global agent? This is really the definition of what a local and a global agent is. It's very simple if you think about it. Local agents are agents that are available only in specific project directories and their subdirectories. That's the scope of that particular agent. That's what they'll have access to.

Global agents are probably agents that you want to give access across your projects. You probably don't just have one project in your workspace. You have multiple projects that you're working on, and you want that agent to have access. I'll show you some examples of what are some scenarios where you would want to create a global agent versus a local agent.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1550.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1550)

The first scenario where you might want to consider  using a global agent is for general purpose workflows across projects. If, for example, across your projects you use infrastructure as code as a way to create your deployments for the products that you're building, you could create an infrastructure as code assistant that can basically have a unified way of how it writes infrastructure as code for all of your projects, not just one particular project or one particular subdirectory. That could be a general purpose workflow across projects.

The second one is common development tools. Say, for example, you do code reviews in a particular way and you want to have a code reviewer that reviews your code commonly across everything. Or you want a document writer which writes documentation identically across all your projects so that when you do a Git check-in and a good Git commit, then you can go and say, okay, I think this was created by X, Y, and Z. So that's another example.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1630.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1630)

The last one is tool-specific workflows. If you're using Docker workflows, for example, across your projects, or if you're using Git workflows, those are some of the tool-specific workflows that you can convert into custom agents and have them run globally. For local agents, the scope is opposite.  Local agents are project-specific configurations. For example, custom agents need access only to specific project files.

One example I can give you is, say you are writing microservices. You're developing microservices and you're working on multiple microservices, and every microservice has a different signature to it or a different requirement. You want to be able to create an agent for one particular microservice, just that one particular microservice. You'll create a local agent and call it, hey, you are my microservice developer agent. You can go ahead and do these tasks because you have access only to this particular microservice project and all the subdirectories within that particular project.

The second one, which is actually my favorite, which I really wish was there when I was an engineer, is personal productivity. A cool agent that I would have loved to build if this was available was when I was in SDE. We used Scrum. Are there folks in here who use Scrum? Okay, a few hands. So we used to use Scrum, and the fun part of Scrum is daily stand-ups and going in there and giving a status of what did you do and what's going on.

I would have loved to build a personal productivity agent. Honestly, I would call it a stand-up Scrum report agent where I would literally tell it, hey, can you just go and look at all my Git comments and all the changes that I did yesterday, generate a report, and give me like a file that I can just go to my Scrum meeting and say these are all the things I did, these are how many user stories I finished, and this is what I completed yesterday, something as simple as that. So something that's unique to your own development style. Think of it as anything that you would need to improve your personal productivity. You can define it as a local agent.

The next one is unique development environments. If you have custom development environments for your projects, say, for example, you are doing both front-end and back-end development, and you have two different environments for that, you want to have a test agent only for back-end, and you want to have a test agent only for front-end, correct? You can define those as local agents individually for each of those development environments.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1800.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1800)

And lastly, testing and experimentation. This one is also one of those things. When you do the demo at the end of a sprint, many times I've been in a situation where I've had to generate synthetic data just to show a demo so that it's working fine. I would actually go in and create a custom agent and say for my project, when I'm done with this project, go ahead and generate a demo, and you go ahead and you generate all the synthetic data so I don't have to worry about it. Come back and tell me when the demo is ready. Another example of a local agent. 

### Benefits of Custom Agents and Amazon Bedrock AgentCore for Cloud-Scale Deployment

So what are the benefits if they're not obvious yet? The number one thing that I see is workflow optimization. What do I mean when I say workflow optimization?

You're able to create these tailored agents that are customized for specific tasks like AWS infrastructure management, code reviews, debugging, and all of the examples that we were just talking about a few minutes ago. That gives you optimization in your overall development workflow.

The second one is reduced interruptions. Because you have already preconfigured the custom agent with all the context it needs and all the tools it can access, it's not going to come back and ask you every single time, "Hey, should I do this? Should I go ahead and access this or should I not access this?" Things like that. So that's reduced interruptions.

Third is context awareness. Because you are preloading it with all the context that it needs to operate, it reduces the repetitive explanation that you have to give for a custom agent. You're able to do better team collaboration. Say, for example, you build a custom agent that's super awesome at figuring out what are the best practices of writing code for your particular project, and you have ten other teammates in different teams who are also using the same tech stack who want to be able to do the same thing. You can go ahead and share the configuration file with them and say, "Hey, go ahead, you can use this." So it adds a little bit more uniformity.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1890.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1890)

And lastly, security control. Because you are predefining the tools beforehand, you're predefining what it can access and what it cannot access, you have a little bit more control on how secure your agents are and how they're running.  So we learned about local agents, we learned about global agents, but we're still talking about running agents on our laptop. What if I don't want to run agents on my laptop? What if I want to run them on AWS Cloud? That is where your Amazon Bedrock AgentCore comes into play.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1910.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1920.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1920)

 Amazon Bedrock AgentCore is your comprehensive agentic platform.  It basically gives you a mix and match of modular services that you can use to run agents at scale. We actually just launched two new services under AgentCore in the last two months: Policy and Evaluations. If you're really interested in learning more in depth about AgentCore, I would highly recommend you go and attend a lot of the other sessions that we have around AgentCore to give you in-depth knowledge.

But in a nutshell, think of these as blocks that you would need when you're running agents at scale. Things like, do I need long-term memory? Do I need short-term memory? Do I need Identity? Do I need to have observability tools like which agent did what and who did what? Do I need to have a Gateway if I have multiple agents? Things like that. You may not need all these services. You might need just one. Maybe you just need Runtime. Geetha's going to come a little bit later to talk about how they're using some of Bedrock AgentCore services here.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/1990.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=1990)

 So I want to leave you with this. These are some of the agentic use cases that we are seeing in key industry verticals. I mean, this space is growing so much and there are so many more intelligent agents that are being built. These are some of the use cases that we have seen and we've helped customers actually build. Now, to share a specific use case and a specific example as to how FINRA went ahead and is building their own custom agents so that they can improve their SDLC, I want to welcome Geetha Ramachandran from FINRA to share their custom agent journey.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2040.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2040)

### FINRA's AI Journey: From Exploration to Augmentation in the SDLC

Thank you, Pavitra. Pleasure to be with you all today.  In today's presentation, I'll walk you through a brief introduction to who we are, our AI journey within the Software Development Life Cycle, and the experiments we are conducting with AI agents, and a demo, finally, with lessons learned and next steps. I'm excited to share how we are advancing the Software Development Life Cycle at FINRA.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2070.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2070)

I'm Geetha Ramachandran, Senior Director at FINRA Technology, leading Infrastructure and Productivity Enablement. I'd like to begin with an introduction to who we are and what we do at FINRA.  FINRA stands for Financial Industry Regulatory Authority. We regulate and monitor broker dealers and their activities within the US. Our mission is to protect the investors and promote market integrity.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2090.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2090)

 So how do we do this? We oversee and analyze activities of over 630,000 broker dealers at 3,200 securities firms, and we collect and surveil equities and options market transactions. These events are then ingested, processed, and analyzed for fraud, insider trading, and market manipulation activities on a day-to-day basis.

That's a huge amount of data, so we process up to 1.45 trillion data events in a single day and manage up to 1,100 petabytes of data in storage.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2130.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2130)

Given the introduction to FINRA, let's look at our AI journey within the Software Development Life Cycle. When we began our AI journey, our  intent was to explore responsibly, scale deliberately, and find real value in how we build and deliver software. This evolution has been intentional, not a rush to adopt every shiny tool out there, but a methodical progression from exploration to integration by starting small, learning fast, and thoughtfully expanding.

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2160.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2160)

We started with a rigorous R&D experimentation effort. What made this phase unique was  the structure of our R&D program. It's a partnership with business, creating a safe space for testing transformational ideas and technologies without risking production stability. We began early on with focused use cases such as AI code completion, AI code generation, code documentation, and explainability. Through this process, we were able to establish the evaluation criteria we started with and confirmed the promise of AI-assisted coding, ensuring every dimension of legal, security, and enterprise readiness were met before moving forward.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2200.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2200)

Once we established trust and clarity in  the explore phase, we moved on to the scale phase. Today we have over 1,000 users actively using our AI code assistant tool across different personas, right from development, operations, product, and program management, and we are expanding to business users for their data analytics needs. The results speak for themselves: a 30% improvement in overall code quality and 80% positive engineer sentiment.

How did we scale this adoption? This wasn't a mass tool rollout across the organization. We ran about four waves, learning and adapting every single time, starting with a pilot wave. Every participant completed mandatory training and adhered to FINRA's acceptable AI use policies for software code. The teams remained accountable for their code outputs. We also introduced a measurement program to measure developer experience as a core outcome of this effort. This developer experience is measured on a quarterly basis to ensure that the AI adoption and the tool adoption created sustained value and not just early enthusiasm that we were tracking early on, because some of these AI tools tend to create a lot of hype around the initial adoption phase and it starts dying down.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2280.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2280)

 Now we are in the augment phase. As we enter the augment phase, our thinking is evolving from using AI as a code assistant tool within a developer's laptop and their environment into positioning it as a core participant within the software delivery process itself. We will talk about how this shift is unfolding throughout the rest of this presentation.

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2310.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2310)

As we progressed through the explore, scale, and augment phases,  one thing remained constant: our platform engineering approach. As I said, we didn't treat this as a tool rollout project. Instead, our focus has been to provide solutions and services to evaluate new capabilities in these tools, because some of these tools tend to have new features and new models that are shipped in weeks or months, so you have to constantly evaluate those features and new tools so that we are able to offer opinionated guidance to our users for using those features responsibly.

We enabled the enterprise to adopt these tools so that every user can use the tool with confidence. We built a secure, governed foundation and paired it with thoughtful enablement by building our own FINRA training curriculum for using these AI code assistant tools that we offer as part of the onboarding process. We also developed a FINRA-specific prompt catalog for software code for common use cases that every developer can leverage and built clear usage guidelines for our users.

We amplified learning during the adoption process by hosting monthly showcases, bringing those champion users and early adopters, and providing them with a platform to share their lessons learned and their experiences with the rest of the community, so that we are able to build that AI adoption that continues to gain momentum.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2400.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2400)

 So as discussed, we are in the augmentation phase.

We are right now exploring opportunities for AI augmentation within the SDLC, where the AI tool just doesn't respond to users' prompts within a chat interface, but it anticipates needs, learns from context, and operates within our SDLC workflows. Our AI-enabled SDLC keeps humans in control of every stage of the life cycle, starting from planning, development, testing, to deployment and operations. So the vision here is not end-to-end AI automation. It is about assistive intelligence and augmentation so that we are able to remove friction, accelerate training, and improve consistency.

We are building on our existing code assistant capabilities and are now experimenting with custom agents. Each agent, as Derek mentioned, has a specific role to play and has a specific task or goal at hand. What do I mean by that? For example, in the development stage, a code review agent that accelerates feedback and improves consistency to FINRA's architectural patterns and coding design standards. In the develop and testing stage, a unit test agent to strengthen coverage and bring confidence by integrating modern testing techniques to reduce hallucinations and generate test code. And in the operations stage, a tech upgrade agent to help keep our technology stacks modern and secure. All of this is grounded within the enterprise SDLC tooling, the systems we trust every single day, where we track all of our work.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2510.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2510)

 So to make this work at enterprise scale, the code assistants and the AI agents cannot work in isolation. They need secure, governed access to the tools where work truly happens. And where does software work happen? Those are in code repositories, build pipelines, artifact systems, ticketing platforms, and observability solutions. And that's where Model Context Protocol and tool use comes in. As Derek mentioned in his section, these MCPs and tools can connect to our enterprise SDLC platforms, the ones that I mentioned earlier. But it can also connect to our own custom-built tools that you might have rolled out into your organization for solving specific functions such as productivity, QA, SRE, DevOps. So you can bring that enterprise context awareness of those tools and the data stored in those tools by building these MCPs and tools usage.

### Tech Upgrade Agent in Action: Automating Dependency Management at Enterprise Scale

Now, this could be a combination of vendor-provided MCP servers or MCP servers from open source, which we will see today in the demo, and custom-built MCPs for your own interfaces as well. So let's look at an example of a specific agent in detail, our tech upgrade agent. This is a real use case that solves keeping our tech stack modern and secure without burning any developer cycles, right?

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2610.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2610)

So what's the opportunity here? Our opportunity came from a real challenge. As many of you know,  development teams have to do continuous upgrades on their technology stacks for their dependencies so that they are not using any end-of-life software, end-of-life dependencies in their technology stack, and so that they are also on the latest and greatest versions of those dependencies that are free from any security vulnerabilities and have those patching, right? Now, the challenge here is that this kind of upgrade work often comes with a competing cadence with business priorities, right?

In addition to this, for the upgrade process itself, the developers have to discover and triage the right dependency versions they need to upgrade to for all of their direct and transitive libraries in their software system. And they usually go through a manual discovery triage process by mapping those dependency versions and then get to prepare the code changes before taking it to the next step. This is a fairly manual, repetitive task. So the reason we are looking for an agent here is so that the agent is able to augment in this process so that we are able to reduce some of those manual steps and that we are not competing for business value work.

So how does this work? We developed our tech upgrade agent that starts by scanning the code repositories. When it scans the code repositories, it assesses an upgrade complexity for that repository. If the complexity is low, such that the agent is fairly confident in achieving a successful upgrade, mostly these are minor version upgrades, right, and if it goes through that low complexity upgrade process, it manages the mapping of the dependency versions, applies the code changes, and validates to make sure that after the code changes, the code still compiles and is able to build successfully.

It also ensures that the unit tests pass successfully, or at least at a minimum, the baseline status of the unit test execution before and after the code upgrade remains the same.

If the validation is successful, then it issues a pull request. This pull request is then reviewed by the developers and the development team just as they would for any other code change they would make in their code repository. Nothing bypasses governance there. If the code review gets approved, then it goes through the normal process of deploying into lower environments, running a battery of tests like performance regression, security scans, and security tests before deploying into higher environments.

The design principles we kept in mind when developing this agent are human in the loop approach, making sure that no AI action goes without a human review and approval process. We also focused on having the agent have enterprise context awareness so that the agent is not creating just slop. We do have a lot of agents creating just code that is ineffective. We want to make sure the agent understands how our code repositories are set up, what is the layout and formatting of our projects, and that it understands common libraries that are internal platforms within your enterprise so that it understands those versions. It also understands how these artifact systems work along with the policies and controls. Most importantly, the entire agent execution process is transparent and traceable.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2850.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2850)

So let's look at the agent in action in the demo. In the demo we will see the agent upgrading a project from one version to another version of technology, making the code changes, running the validation checks, and issuing a pull request. We will see this demo in two modalities. The first one is running the agent in a local CLI environment in my laptop, and the second one is where we run the agent in a remote virtual environment within Bedrock AgentCore. 

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/2860.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=2860)

Okay, so here is my local environment. Here I have checked out an open source Python project in my local. This is a CredStash  project, just a secret management solution using DynamoDB and KMS. I'm starting a chat session with the agent in the terminal window right in my IDE. So I've got CLI running in my IDE, and you can interact with these agents just like how you would interact with your CLI with Kiro CLI or other systems. You can start listing out what are the tools. Just as Pavitra showed, we are going to see what kind of tools and MCP servers does this agent have access to and what kind of context is set in the system. We'll see that now by invoking a command called MCP list.

This agent has two MCP tools. It has access to AWS Knowledge MCP Server, which is an open source AWS provided MCP server to understand some of the common libraries used in AWS services like Boto3. So if you use Boto3 and its dependencies, it's able to understand those. It also has a secondary MCP that is custom built to connect to our code repository that we have built ourselves. All right, so let's run the MCP list command here and it shows there are two MCP servers. We are now going to look at the context.

This agent has context written in markdown format. It also has examples for few shot prompts and has access to tools in terms of Python functions. Now we are issuing a command here that upgrades the CredStash project in my local to Python version 3.11. When we saw the context, what we saw was it has access to the agent instruction prompt in terms of the markdown format. It also has access to tools that we have built for agents to invoke within those instruction prompts. It also has access to some of the examples we have created with few shot prompts. It has a workspace where it will use to upgrade the project. It's basically a temporary workspace that it is going to use to upgrade the project.

Now as it upgrades, the agent has executed and completed the upgrade here, and we can see the execution log of every step.

The agent has also produced a well-formatted summary report of the upgrade process. It provides a summary of code changes, validation results from all the checks that it has run, syntax testing validation, and a risk assessment score along with next steps. In this case, it provides context to upcoming operational deadlines like Python deprecation and Lambda runtime for the development team.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3030.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3030)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3050.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3060.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3060)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3080.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3080)

 Now we saw the demo with the agent running locally. Let's move on to seeing the demo of the same agent running in a remote virtual environment. This is in Amazon Bedrock AgentCore. I have pulled up the agent sandbox within the Amazon Bedrock AgentCore for this demo.  The input here is a project key within my source control repository, and the output is a pull request. Let's look at the pull request.  The pull request contains the summary report of the changes that we saw in the local demo, and it also provides the validation results, risk assessment, and the next steps. Let's also look at the code changes that it has made.  Now the teams can look at the summary of the upgrade, review the changes that it has created, and go through the code review process and take it to the next step.

What this kind of environment provides is that instead of having the developers actually running the agent locally, which is going to occupy their terminal window and sessions and require issuing prompts, the developer can fire off this upgrade into a remote virtual environment and come back to review it after it is done. Instead of managing these agents locally, this is one opportunity for teams to manage multiple upgrades. If you have multiple repositories and multiple components, you can plan these upgrades to run them in a virtual environment and for the organization to orchestrate these upgrades at scale, not just within one team but across teams.

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3120.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3120)

[![Thumbnail 3130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3130.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3130)

  As seen in the demo, we are right now experimenting with the remote agent infrastructure for asynchronous executions. The remote agent architecture includes build, deploy, monitor, and evaluation stages for building the agent prompts, tools, and context, and deploying them into agent runtime with consistent monitoring on the logs and traces, which is then fed back into the evaluations pipeline. The governance workflows and the enterprise platforms provide the needed policy compliance and telemetry.

[![Thumbnail 3160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3160.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3160)

 As we are developing and piloting the tech upgrade agent, the metrics that we are tracking against this effort are speed and meaningful reduction in the time it takes to upgrade, so that we are able to shift the manual toil developers are going through to have them focus on a high-value oversight exercise. Adoption is measured by the number of pull requests that the agent has created that get merged without any rework. This directly measures the effectiveness of the agent itself, ensuring that we have a compliant, consistent upgrade process that we can run at scale.

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3200.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3200)

 As with any emerging capability, the real value comes from the lessons we learn along the way. Through this journey, we have learned that context engineering and context management is key. That's everything. Less context is always more, and making explicit instructions to the agent prompts are critical. As Pavitra mentioned, applying boundaries and constraints as guardrails for the agent is very critical to avoid context pollution. For example, not using large language models for deterministic tasks such as checking out from Git, and instead using them only for reasoning tasks. We had to build more tools to expose them for the agent so that we are able to efficiently use the context window, the number of tokens that we are able to use, and at the same time keep the agent executing and not in a hallucination mode.

We further refined our architecture with specialized sub-agent prompts, applied clear tool restrictions, and also codified some of our enterprise artifact controls. Early pilots and feedback were very critical because those real usage scenarios surfaced real insights. Those continuous refinements and the feedback signals from the evaluation pipeline helped us mature the agent more significantly. Perhaps the most important thing is that having a mature agent and tooling along with cultural readiness is very important. We cannot just deploy an agent. We have to prepare people and processes so that they are able to trust it, adopt it, and integrate it into their workflows.

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3300.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3300)

 With our pilot results and lessons learned, our next steps are to scale the tech upgrade agent across the enterprise for production use and explore connected agents.

Connected agents is a topic that we are right now thinking deeply about, where connected agents can work together. For example, a unit test agent goes and improves the coverage of the project before running a tech upgrade agent so that the tech upgrade agent can get much richer feedback signal on the whole upgrade process through those unit test results. And advancing our remote agent infrastructure. This will allow synchronous executions for larger workloads, not just for agent workloads, but even beyond that.

So each step builds on the same principles that guide us here, which are human-centered design, thoughtful rollout, and enterprise alignment. This is a journey for us. It is governed, it is iterative, and most importantly, it is grounded in value and effectiveness. Thanks for your time. I'm going to hand it back to Derek to close us out.

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a855f32b0b911e9d/3380.jpg)](https://www.youtube.com/watch?v=Kq0Se1nnT3U&t=3380)

### Closing Thoughts: Building and Sharing Custom Agents for Better Development Outcomes

Alright, thanks Geetha. So just to recap here at the end, we talked about what makes up an agent, right? The agentic loop and the concept of an agent is  simple but very powerful concept, and as software developers we should be thinking up front about how we're building those. We learned more about how we can use Kiro CLI. And also, I should mention that we launched Kiro Powers just yesterday as well, so we have a similar concept in the IDE that's similarly easy to set up, and so I'd encourage all of you, if you haven't yet, grab the free trial of Kiro.

Try the CLI and the IDE and start playing with building some of these custom agents for your personal use, but also think about committing those in as part of a project, so anybody that's working on the project gets the benefit of those custom agents to help. And then more general purpose agents that folks can use across projects. We're seeing again this pattern of teams that are sharing what's working for them and refining those together is a very powerful concept. So if you're not already doing that within your teams, I'd encourage you to think about starting to do that.

And then finally we heard from FINRA about some learnings about the custom agent work that they've been doing. I hope that was some valuable information for you all. So thank you very much, and please go out and build some custom agents. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
