---
title: 'AWS re:Invent 2025 - Deep Dive into Deloitte''s Amazon Neptune GenAI Security Intelligence Center'
published: true
description: 'In this video, Ian Robinson from Amazon Neptune and Evan Erwee from Deloitte discuss improving generative AI question answering using GraphRAG techniques. Robinson explains the GraphRAG Toolkit, an open-source Python library that builds hierarchical lexical graphs from unstructured data to retrieve semantically similar and structurally relevant content through hybrid RAG approaches using entity network contexts. Erwee demonstrates how Deloitte''s Cybersecurity Intelligence Center applies this toolkit to their AI for Triage system, which helps SecOps engineers prioritize and remediate thousands of security alerts from platforms like Wiz by maintaining an evolving organizational knowledge base combining short-term Document Graphs with long-term Lexical Graphs, enabling human-augmented decision-making with 99.999949% uptime on Amazon EKS infrastructure.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Deep Dive into Deloitte's Amazon Neptune GenAI Security Intelligence Center**

> In this video, Ian Robinson from Amazon Neptune and Evan Erwee from Deloitte discuss improving generative AI question answering using GraphRAG techniques. Robinson explains the GraphRAG Toolkit, an open-source Python library that builds hierarchical lexical graphs from unstructured data to retrieve semantically similar and structurally relevant content through hybrid RAG approaches using entity network contexts. Erwee demonstrates how Deloitte's Cybersecurity Intelligence Center applies this toolkit to their AI for Triage system, which helps SecOps engineers prioritize and remediate thousands of security alerts from platforms like Wiz by maintaining an evolving organizational knowledge base combining short-term Document Graphs with long-term Lexical Graphs, enabling human-augmented decision-making with 99.999949% uptime on Amazon EKS infrastructure.

{% youtube https://www.youtube.com/watch?v=KD5C93kwBMg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/0.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=0)

### Introduction: Addressing the Cybersecurity Alert Overload Challenge

 Thank you very much for coming along this afternoon. My name is Ian Robinson, and I'm a Graph Architect with the Amazon Neptune Service team. I'm very pleased to be joined today by Evan Erwee, who's an AVP for Cyber Operations at Deloitte. In this session, we're going to be talking about some of the work that the Neptune team have been doing to improve generative AI question answering solutions using GraphRAG techniques and how Deloitte have used the results of some of that work, which is an open source Python library called the GraphRAG Toolkit, to build their Cybersecurity Intelligence Center.

Our assumption here is that you're familiar a little bit with graphs and graph databases, with vector similarity search, and with RAG or Retrieval Augmented Generation techniques, so we're not going to go into a lot of explaining of those concepts or tools. Instead, we're going to be looking more at how Deloitte or the Cybersecurity Intelligence Center have applied them. But before we get into those details, I'm briefly just going to sketch the overall problem that Deloitte are trying to solve here with the Cybersecurity Intelligence Center.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/90.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=90)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/110.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=110)

So has anyone here used a cloud security platform such as Wiz or CrowdStrike?  Okay, we've got a few hands. If you use such a platform, something very interesting happens the first time you switch it on, the first time you enable it in your environment. As one of Evan's colleagues likes to say, the whole Christmas tree lights up.  Suddenly hundreds, possibly thousands of alerts or non-compliance notifications appear.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/130.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=130)

So now, having solved one problem, that of identifying potential security risks in your cloud infrastructure, your SecOps engineers and analysts  are faced with another. How can they remediate these issues in a safe and timely fashion? It's not simply a matter of automating fixes. I mean, platforms like Wiz already allow you to do that. Given the number of issues and the complexity of production environments and the possibly punishing timelines for fixing known issues, particularly if you're in a regulated industry, these SecOps engineers are going to have to triage and prioritize.

They're going to have to prioritize on at least two factors. The first is they need to understand each issue or the seriousness or the significance of an issue in the context of your organization's cybersecurity policies. And second, they're going to need to understand the likely impact of any remediation on existing production systems. There's no one size fits all solution here. Your cybersecurity policies and your production systems are particular to your organizations, so you're going to need a lot of organization specific knowledge in order to be able to make these informed decisions, to be able to prioritize and then remediate.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/220.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=220)

The SecOps engineers are the real experts here, but given the size of the  challenge, they'll likely benefit from some expert assistance. On the one hand, it's a simple problem of information overload, so automation can help. You can improve the flow of work by automating common tasks. But more importantly, it's a context problem. These engineers, before they can make an informed decision, they need to understand each issue in its organizational context.

That context can comprise lots and lots of different things, lots of formal knowledge, lots of tribal or informal knowledge, things such as the cybersecurity policies, runbooks, best practices, architectural guidance, and even the current state of those production systems. So automation and context. That sounds a lot like generative AI. Given the title of the talk, you probably won't be surprised to learn that we think that generative AI and RAG or Retrieval Augmented Generation has a large part to play in the solution.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/290.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=290)

So later in the session,  Evan is going to be discussing AI for Triage, which is an expert in the middle AI assistance for SecOps engineers. Effectively, it's closing the loop between a platform such as Wiz identifying an issue and an engineer being responsible for applying a remediation back to production systems.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/320.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=320)

In building AI for Triage,  Deloitte have used the GraphRAG Toolkit, which is an open source Python library developed by the Neptune team. AI for Triage uses the GraphRAG Toolkit to maintain this ever-evolving organizational knowledge base, which comprises things such as policy documents, runbooks, architectural guidance, and triage records.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/360.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=360)

### The Context Problem: Why Similar Content Isn't Enough for Quality Answers

Evan will be talking about AI for Triage a little later on, but before we talk about that, I'm just going to spend a little time talking in a bit more detail about some of the work that we've done on the Neptune side to improve GraphRAG techniques.  I want to start here by returning to this issue of context. RAG is all about context. We populate the context window with relevant content that we've retrieved from some backend data sources, and then the language model uses that context in order to generate a good response to a user query.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/400.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=400)

However, the kind of context that we retrieve and the ways in which we retrieve it can make a big difference to the quality of the response and the reliability of the response.  I want to illustrate this with a simple example. Imagine we're responsible for a large corpus of documents, many thousands of documents, things like press releases, news articles, and analyst reports. For this example, there are five in particular that are important to us.

The first is a news article that discusses Example Corp and its wonderful new Widget product. The second is an analyst report that discusses the fact that there's a potential large demand for Widgets in the run-up to Christmas in the UK. The third is a press release that says that Example Corp have partnered with AnyCompany Logistics in order to reduce shipping times from Asia, where those Widgets are manufactured, to Europe and the UK.

Then we've got another press release that says that AnyCompany Logistics are now using the Turquoise Canal, a fictitious canal, to further reduce those shipping times from weeks down to days. Finally, we have this recent news article that says that unfortunately the Turquoise Canal is currently shut because of an ongoing large-scale cybersecurity incident. So we've got five articles among many thousands of other documents.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/490.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=490)

Now imagine you're an analyst tasked with investigating the fortunes of Example Corp, and you've decided to use a RAG application to help with your research.  Here's one of the questions you might ask: what are the sales prospects for Example Corp in the UK? In order to answer this, the system is going to have to retrieve some context from that corpus of documents. One way of retrieving context is to use vector similarity search.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/520.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=520)

We can use similarity search to find content that is semantically similar to the question being asked. In this case, it's going to be content that likely mentions Example Corp  and Example Corp in relation to the UK, something like these top three articles. We retrieve that context, and given that evidence, the system can then reply that sales prospects are great and really looking up. On the surface, that looks like a good response given that evidence.

But we, as omniscient observers of that corpus of documents, know that it's not necessarily the whole story, that there's a more nuanced answer that we could give to that question. In order to produce a more reliable answer, we need to retrieve content that's not only semantically similar to the question, but we need to retrieve some additional structurally relevant, potentially dissimilar content.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/570.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=570)

 I say structurally relevant because we can perceive some structure here. There is a connection between Example Corp and AnyCompany Logistics, and between AnyCompany Logistics and the Turquoise Canal, and between the Turquoise Canal and this ongoing cybersecurity incident. If we can retrieve both of these kinds of content, then the system can give us a more nuanced answer: yes, sales are looking good, but they're likely to be negatively impacted by an ongoing cybersecurity incident.

As you can see, the kind of context we retrieve and the ways in which we retrieve it can make a big difference to the quality and the reliability of the response.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/620.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=620)

### Hybrid RAG and the Challenge of Graph Quality

 In RAG terms, this is all about improving recall. Recall is the percentage of relevant content that a system retrieves in response to a user query. Based on that example, we might say that in order to improve recall, we need to be able to retrieve content that is similar to the question being asked, and content that is structurally relevant but potentially dissimilar to the question being asked.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/650.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=650)

 One way of retrieving that content is to employ a hybrid RAG approach. With hybrid RAG, we use vector search to find that semantically similar content and graph search to find the structurally relevant but potentially dissimilar content. Hybrid RAG can be pretty effective, but once again we're in this situation where we've solved one problem but have potentially introduced another.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/680.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=680)

 The issue is that the quality of a graph search is very much dependent on the quality of the underlying graph. If the structure of the graph isn't aligned with our information goals, we can often end up retrieving misleading or irrelevant content, and that's going to generate a poor response. These are the kinds of approaches and some of the issues that we had in mind when we started designing the GraphRAG toolkit.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/710.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=710)

 From the outset, we set ourselves two high-level design goals. The first was we wanted to make it easy for you to be able to build a graph from your own unstructured or semi-structured data sources with very little information architecture overhead. If any of you have built a graph application in the past, you may know there's often a large information architecture component to designing a good graph model that is aligned with your information goals. We wanted to try and reduce that information architecture effort on your behalf.

The second goal was that we wanted to help you find all of that relevant but non-obvious or distant or potentially dissimilar content. We wanted to make it easy for you to find all of that content without you having to write complex graph queries. How do we go about addressing these two goals?

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/780.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=780)

### Building the Hierarchical Lexical Graph: A Repository of Statements

 The first was we wanted to make it easy for you to be able to build a graph from your own unstructured or semi-structured data sources. To be clear here, we're talking about things like text files, PDFs, markdown files, but also things like CSV files or Excel documents or even JSON documents, primarily text-based content.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/800.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=800)

 The first thing we decided to do was to eliminate that information architecture overhead. Instead of you having to build a graph, the toolkit builds a graph for you, and it builds a very specific type of graph with a very specific model, something we call a hierarchical lexical graph. You can think of this lexical graph as a repository of statements.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/840.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=840)

A statement is a short, well-formed, standalone proposition that we've extracted from your source data. These statements form the primary unit of context that we're going to pass  to the language model to generate a response. At retrieval time, our goal is to retrieve highly relevant statements, and then we group them thematically by topic and we attribute them back to their sources. It's the statements themselves that really comprise the primary unit of context, and that's what we're passing to the language model in order to generate a response.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/870.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=870)

The statements are really at the heart of this lexical graph model. All of the other parts of the model  really have a role or a responsibility to make it easy to find all of those relevant statements. Every node type here has a job to play at retrieval time in making it easy to find all of these relevant statements. At the top here, in blue, we have what we call the lineage tier. This comprises source nodes and chunk nodes. The source nodes represent source documents and they contain metadata, so at retrieval time we can apply metadata filtering.

We can also use that metadata for versioning. If you reindex a document that has changed since the last time you indexed it, we can add some metadata so that we can identify which version is current and which are the historical versions. Then you can query the graph and say I'm interested in the current state of the graph, or I'm interested in the state of the graph at a historical point in time.

So that's the source nodes. Below them are the chunk nodes, and they represent chunked content. This is exactly the same kind of chunk content you're going to get with a traditional vector RAG application. These chunks are also associated with embeddings in the vector store. So at retrieval time, they act as a vector-based entry point into the graph. If we do a similarity search in the vector store, we can use the results to then find the corresponding chunk nodes in the lexical graph, and from there we can begin traversals throughout the rest of the graph.

The middle tier here is the summarization tier. This comprises those statements, those primary units of context, statements grouped thematically by topic and supported by discrete facts. I'll talk about the role of topics and facts in a bit more detail in a moment. And then at the bottom, we have this entity relationship tier. This comprises entities and the relations between entities that we've extracted from your source data.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1020.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1020)

Now this entity relationship tier helps us infer further domain semantics of the dataset. But more importantly, at retrieval time, it helps us find all of that structurally relevant but potentially dissimilar information. And I'm going to explain how we do that in a lot more detail in a little while.  We're just briefly back to topics and facts. The topics act to thematically group statements that belong to the same source document. So there's a kind of source, chunk, topic, and statement subgraph that represents a single source document, and topics act to group statements thematically belonging to that single source document.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1060.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1060)

So in this way they provide what we call a degree of local connectivity. They allow a graph search or a graph traversal to do a deep investigation of thematically linked content belonging to a single source document.  Facts, on the other hand, can connect multiple statements, and multiple statements that are potentially drawn from different source documents. So in that respect, the facts provide for global connectivity. We can allow a search or a graph traversal to conduct a broad investigation of the corpus by way of navigating across that fact layer.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1090.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1090)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1110.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1110)

 So this is what the context window actually looks like at retrieval time. This is what we're passing to the LLM. As you can see, it's sets of statements grouped thematically by topic and attributed to an individual source.  So one of the key takeaways here, and this applies whether or not you're using the GraphRAG toolkit, whether or not you're actually doing GraphRAG, but this responsibility-based approach to graph modeling, I think, is a very powerful complement to the traditional representation-based approach that we have.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1170.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1170)

Normally we think of a graph data model as representing the things in our domain that we're interested in. But I'd suggest that there's a kind of complementary way of thinking about certain types of nodes. They're there to help us find the stuff that we're interested in. It's not so much a matter of saying to a node, what are you, it's more like, what can you do for me? So that's the lexical graph, a repository of statements. We'll return statements in the context windows to the LLM. Other parts of that graph are there really to help at retrieval time  find relevant sets of statements.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1190.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1190)

### Reframing Graph Search as Dual Similarity Searches

So our second design goal was how can we make it easy for you to be able to find all of that relevant, non-obvious, potentially dissimilar content, and all without you having to write graph queries.  Well, as I said earlier, with hybrid RAG, we use vector similarity search to find content that is similar to the question being asked,

and graph search to find content that is structurally relevant but potentially dissimilar. But he also said that there was a problem here in that the quality of a graph search is very much dependent on the quality of the underlying graph. So how can we mitigate this issue? Well, one way is to redescribe what it is we're trying to achieve at this point in time.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1230.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1230)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1260.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1260)

 So here's an alternative formulation. In order to improve recall, we need to be able to find content that is similar to the question being asked and content that is similar to something different from the question being asked. So that sounds a bit like fancy wordplay, it's a bit clunky, a bit abstract. But look what we've done.  We've taken what was previously a vector search and a graph search, and we've turned it into two separate similarity searches: one for content that is similar to the question being asked, and one for content that is similar to something else.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1280.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1290.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1290)

So what we need is some kind of proxy or representation for  something different from the question being asked. And that's exactly where that entity relationship tier can help us.  So at retrieval time, what we do is find a number of what we call entity network contexts. And an entity network context is a one or two hop network surrounding key entities and keywords that we've extracted from the question that's being asked.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1310.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1310)

 We then take these entity network contexts, we create textual transcriptions of them, and we use them throughout the rest of the querying process in order to find those relevant statements. So that's the high level approach. I'm now going to tell you exactly how we go about generating those entity network contexts and then how we use them throughout the rest of the querying process.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1340.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1340)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1360.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1360)

### Generating Entity Network Contexts Through Path Expansion and Filtering

So generating entity network contexts.  Well, we've got a user question: what are the sales prospects for Example Corp in the UK? The first thing we do is look up significant entities and keywords from that question in that entity relationship tier. So we end up with a set of candidate entity nodes.  We then rerank those nodes against the question.

So with that reranked set of nodes, we can now identify what we consider to be the most important entity. We're still interested in the entire candidate list, but we're also interested in the one that represents the most important or significant entity. And at this point we're going to make a note of the degree centrality of that most important entity. Make a note of it because we're going to use it in a little while.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1390.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1390)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1410.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1410)

 Now we do some path expansion, and this is configurable, but typically we expand each candidate node one or two hops. So we're beginning to generate those little entity network contexts, one or two hop networks surrounding each entity.  We then filter each entity along each of those paths, and we filter based on a threshold that's derived from the degree centrality of that most important entity.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1450.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1450)

Effectively what we're trying to do at this point is eliminate whales and minnows. We're trying to eliminate nodes that might completely dominate the conversation and nodes that are potentially completely irrelevant. So we filter along each entity path, eliminating the whales and minnows. We then rescore all the nodes along each path.  Calculate the mean score for each path, and then we can reorder the paths based on their mean scores.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1460.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1460)

So now we've got a list of paths ordered by their mean scores.  And then finally, again another configuration parameter, we limit that list. Perhaps we're interested in the top five, the top three. So at that point, we've got a list of paths derived from these entity network contexts, and we can create textual transcriptions of those paths and then use them throughout the rest of the querying process.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1490.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1490)

So here we've got three paths.  And what we'll find is that as we go down that list, we're introducing notes of dissimilarity. So the first path is the path that is most similar to the question being asked, but the paths that come below introduce more and more dissimilarity.

### Three Ways Entity Network Contexts Improve Retrieval and Response Quality

So we then use these paths, these entity network contexts, in three different ways throughout the rest of the querying process.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1520.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1520)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1560.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1560)

 The first way is that we use them to seed a series of dissimilarity searches. So this is the point at which we are looking for content that is similar to something different from the question being asked. We can do a vector similarity search for each of those entity network paths and use the results to then find corresponding chunk nodes in the lexical graph. From there, we can take advantage of that local and global connectivity in order to do deep and broad traversals. So we're doing a vector similarity search  that's then helping us find those vector-based entry points into the graph, those chunk nodes, and from there we're traversing the graph to try and find the statements along those paths. Again, all we're trying to do is accumulate sets of statements that we're going to return in the results set.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1580.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1580)

 The second way in which we use these entity network contexts is to actually re-rank the results. Remember, the results comprise sets of statements, so we've got a long list of statements. We re-rank them using weighted term frequency analysis. We don't use a re-ranking model, we use weighted term frequency analysis. So we've got a list of candidate statements that we've found through our graph traversals, and we create a set of match items.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1610.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1610)

 The match items comprise the original question plus all of those entity network contexts in descending order. We weight those match items so that the first two, the original question and the first path, have the strongest weight, and then the weighting descends from there on. Then we can score each statement against each match item, calculate the mean score for each statement, and then reorder the statements based on their mean score. What you'll see in this example is that we've got statements that mention AnyCompany Logistics and the Turquoise Canal, and they've been promoted to close to the top of the results set. They've been promoted even though they score very, very low for the original question, but the reason they've been promoted is because they score relatively highly for those last two entity network contexts, the ones that mention AnyCompany Logistics and the Turquoise Canal.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1680.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1680)

 And then finally, the third way in which we use these entity network contexts is to enrich the prompt that we use to generate a response. So besides supplying the user question and the search results, we also add in this additional context, which comprises those entity network contexts. We're effectively telling the LLM, hey, look, take advantage of all these search results, but pay attention to these bits of additional context. We're effectively guiding the LLM or instructing it to pay attention to statements that it might otherwise skip over or ignore. So does all this work?

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1730.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1730)

 Yes. If we use traditional vector similarity search, the kind of responses that we get, and remember we're also getting chunk-based context that's being supplied to the LLM, but the kind of response that we get is lots and lots of detail, but finally it's that overly optimistic response: sales prospects appear exceptionally strong. But if we run the exact same question with the GraphRAG toolkit, we get all of that detail, but then we get a more nuanced conclusion: potential supply chain challenges, global distribution may face disruption, all because of a large-scale cyberattack. So this could create product shortages and affect delivery times.

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1780.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1780)

 Another key takeaway here is that graph search and vector search are mutually beneficial for our kind of operation or our kind of use case. Graph search is great at finding all of that structurally relevant content, but vector search is really good at mitigating quality issues in the original question and in the underlying content.

The two work in concert during the retrieval process in order to smooth out any quality issues.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1820.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1820)

### GraphRAG Toolkit Resources and Backend Support

 So that's the GraphRAG Toolkit. A couple of resources here. The toolkit itself is an open source library. There's a link here, a QR code to the GitHub repository. There's also a link here and a QR code to a paper describing the hierarchical lexical graph in a lot more detail, and that includes some of the results that we generated towards the end of last year when we started some of this research. The GraphRAG Toolkit employs a hybrid RAG approach, but importantly, it supports multiple different backends. So today on the graph side, we support Amazon Neptune, Neptune Analytics, and Neo4j. On the vector store side, we support Neptune Analytics again, and there's Amazon OpenSearch Service and Postgres with the PG Vector extension.

The key point is it's a library. It's not a full-blown application. It's intended that you combine it with other tools, other libraries, in order to build out more fully featured applications. And that's exactly what Deloitte have done in employing it in their Cybersecurity Intelligence Center. So at this point, I'm going to hand over to Evan, who's going to describe in more detail the Cybersecurity Intelligence Center.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1910.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1910)

### Deloitte's Vision: Building Long-Term Memory for Cybersecurity Operations

Thank you very much. Now what I'm going to try, I'm not going to talk about Deloitte. I'm pretty sure you're not here about a partner.  I want each of you, I want you to ground in a couple of concepts. The best way to do that is to talk about us as a human because we all understand how we work. You've had the GraphRAG Toolkit for a moment in time. I imagine that is your long-term memory, and I'm going to focus on your working memory and your short-term memory. When we live and go through our daily lives, there is no segregation between those other than on paper. We keep on building a long-term memory. What I will demonstrate is a problem we see often is that the React, especially in cyber but with many other tools, only out of short-term memory, the here and now, and that becomes problematic because what we cannot establish is the true nature of the problem across your entire organization.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/1980.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=1980)

 What we have in many organizations is this dilemma, what we call the Zero Disconnect Dilemma. It's a dichotomy. On one end, you've got an organization that wants to proverbially and physically keep things connected, networked, or they want to be in the know as far as what they know about cyber threats. But at the exact same time, you've got perpetrators who are doing the exact same but opposite. Imagine for a second, you've all been in these rooms where instead of everybody working very, very hard to get your microphones to work, get them cleaned up, get everything to be perfect, imagine some of us are rogue agents. We seek out the best possible way to disrupt. We wear the same clothes, we wear the same badges, we don't know who we are. Keep that idea in your mind. That's the world of cybersecurity. There's no friend or foe. What we're dealing with is a continual dichotomy of Zero Disconnect, and that is how we in Deloitte end up talking to the AWS team about our relationship. We realize graph can help us on a journey of Zero Disconnect.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2060.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2060)

 Now, what we've seen in many instances for many organizations is this problem where organizations are somewhat disconnected, and they have defenses in order to help them to justify that. They have this idea of, well, maybe I've got a checkbox compliance. I mean, that's great, isn't it? I've done my best. Maybe it's optics driven. It looks good. I've got the best tools. I bought lots of them, and that tells me what's going on. For us, at the end of the day, it comes back to something which nobody wants to talk about, and that's plausible deniability. Surely if I've done enough, then nobody can tell me that what I've done isn't good enough. For us as an organization and for all of you here,

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2110.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2110)

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2120.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2120)

I'm a developer. I know when I go to my boss and say A and it's B, I know that. I've been a developer for many years.  The problem we had up till now in our organization and many of our clients was we have got these SOC analysts. When they triage things, no problem,  you guys all do that, and then they throw it over the proverbial fence to the business who's merely an observer, and they make their own notes. Honestly, if these notes are the same thing, that's going to be a miracle from above, because they're not.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2140.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2140)

What we wanted was a world  where, as our first step using the GraphRAG Toolkit, we could build long-term memory. We wanted to take our lessons learned in a triage record, that's our experiences, not what the tools are telling us, but our experiences, and feed that back into some form of factory to where that becomes a central repository for what's happening as far as cybersecurity in our organization or in our client's organization. We realized the GraphRAG Toolkit can do exactly that. It's really brilliant.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2180.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2180)

 We've also realized that we have to keep our triage record generated by AI separate and immutable. Human arbitration is very important. We have to be able to state what was AI generated and what was human generated. We can never, never break that rule, because if we break that rule and we make a decision, it's not defensible afterwards.

We also realized that we've got these folks that want to reason and reflect using these triage records. But we also realized that if we can do this properly using the toolkit, we can get closer to our vision of zero disconnect by creating collaboration between our analysts and our business, whoever that business might be. That was our vision. Can we do that?

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2230.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2230)

### Implementing the Document Graph and Cognitive Substrate Architecture

 The one thing we've learned is that we are missing short-term memory. When we get these signals and feeds from our systems as far as our tooling, those are not long-term memory. They get converted into a triage that has long-term memory, but at the moment in time, those are just noise, those are just signals. So we needed a Document Graph. We built a Document Graph and added that to the GraphRAG Toolkit.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2290.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2290)

We built it in such a way that we can build Document Graphs into logical domains, and that allows us to group document-centric information from, for example, tools. We're going to talk about this briefly in a second. These tools have got their own sort of derived vocabulary, so within that domain we can have a short-term derived vocabulary. We can have what we call moderate entity resolution because within the space, for example, of a Wiz or a Prisma, there's some form of relationships inherently that exists, and  it wouldn't be taking too much energy to do.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2300.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2300)

Now again, I reiterate that what we're aiming for is to build a triage record,  which is the conversion from our short-term memory into our long-term memory. It's a common mistake we see. People think AI is all about taking all my short-term memory and reacting on that. No, because it's going to miss the nuances of your common experience.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2320.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2320)

So again, what did we build? What did we  see? I think that's what I want all of you to take away. GraphRAG is your knowledge corpus. It is your experience, just like all of you as you get older have more and more experiences. You want to capture that experience in your organization. You want to retrieve it. You want to make sure that they're good experiences, but at the same time, you want to curate your short-term experiences, your short-term memory.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2350.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2350)

 Curate it in a language that's understandable. Otherwise you're going to get lots and lots and lots of information in your long-term memory that's completely meaningless. So what have we done? We've written a Document Graph, which is our hard data. We bring in hard data to our analyst in the form of an AI-generated triage record. We feed that back into the Document Graph and we use that for overarching contextual analysis.

That allows us to get a picture of the organization, not a picture of a lot of tools. We do that with a man in the middle, human in the loop, whatever term you want to use, because we want to make sure that we are accountable. And then the decision we make is what we call human augmented decision-making, which means we basically put an Iron Man suit on our people. We don't build robots and get rid of our people, because that wouldn't work.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2410.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2410)

There's a couple of things we learned that we had to do.  We had to make sure with the GraphRAG Toolkit, we worked very closely with AWS that we can read from more sources and we've added those more sources. JSON was great but we needed some other sources. For the short-term memory, long-term, short-term we needed videos, audio, and all of that. We need multi-modal embedding, so we worked a bit on that.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2430.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2430)

 And then we realized that the biggest struggle is how do we get this lots and lots of data from these various tools very quickly into a graph. None of our people want to become a graph expert, so we've written a very sophisticated pipeline. The pipeline allowed us, and in the pipeline it's embedded with Amazon Bedrock, to build a mechanism of turning any signal log, CloudWatch Alarms, whatever data we get into a JSONL and then turn that rapidly into our short-term memory.

Why? Let me give you one example. Let's say that we're interested in a short-term experience about a place we visit. Now, let's make it practical. We get information in about an IP address. An IP address is not a place that we visited directly, it's indirectly. So for example, this pipeline would take an IP address and automatically convert it into the Autonomous System Number in an actual location. Because that's what I'm interested in my short-term memory, that's what I want to know.

So we wrote this pipeline, we wrote it in such a way that the engine that reads and an engine that writes is two separate engines. Why? Because there's a new concept breeding inside of Document Graph, there's very recent writings, which is called graph pollution. And that is an injection of phantom data into a graph in order to derail. All of that we built in here to make sure we don't pollute our short-term memory.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2530.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2530)

 We also made a decision from the ground up that we cannot expose the experience of talking directly to this AI world into applications. It evolves too quickly. There's too many moving parts. So we created something called a cognitive substrate, where we have effectively, we call that a factory or an AI-enabled factory, it's not an AI factory, where the factory is your interface. Behind it, encapsulated, shielded, is the process where we have the triage protocol, that's the generative AI, the prompt set, that we can curate for that specific mining of your short-term memory.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2600.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2600)

The Document Graph, which effectively builds, which captures your experience from the particular tools, maybe the signal or the log file coming in, but we also always bring back the Lexical Graph because we always have to bring back our long-term experience because it's an interflow. We then take all of that and we let our experts  deal directly with our factory. That means we built a factory in our organization that is about the organizational cyber experience and encounters, as opposed to what a tool set. Now how do we build this?

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2620.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2620)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2640.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2640)

 It was very important for us first to build or find a platform which we deem is reliable, we chose Amazon EKS. It's a reliable platform, and we currently have 99.999949 percent uptime, and that's pretty stable. We decided that we want to make it narrow,  so we used Neptune and OpenSearch. Although the GraphRAG Toolkit does support other databases, we opted for those, it just makes it easier for us.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2650.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2650)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2670.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2670)

 We've standardized on Amazon Nova from the get-go just so in a proof of concept our world is not too broad. We've also brought SageMaker into the mix. Although we are not developing models yet, think about your long-term memory becoming a model one day, because it's curated data.  It's not tool data.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2680.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2680)

And then our factory is running on DynamoDB and S3.  S3 is where I've got journaling, so we can always play the factory back if anything happens. And then for any upload or download into our environment, we've used AWS Lambda and API Gateway, CloudFront as well, in order to bring in documents. There's a lot of other elements here that I haven't spoken about, but those are not relevant. Those are the high level that we've used.

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2700.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2700)

### AI for Triage in Action: From 50,000 Issues to Actionable Intelligence

 So let's look at it in action very quickly.

We already spoke about the long-term memory that stays there and serves as our reference point. In this case, we have Wiz, and we're going to go through it very quickly. With issues, we switch on what I call the Christmas tree phenomenon. That's our facts that get pumped into the factory, and the factory makes a decision. Have I seen this before? Is this a common class of an issue? For example, encryption of an EBS volume. We might get dozens of issues, and the system recognizes that it's seen this before and links it to an existing triage record. It's like when you go to a restaurant and you eat something you already had before. There's nothing new, been there, done that. Or now it's a new experience, something new, and the system will generate a new record. It then takes that record and pumps it back into our contextual artifacts, and it bolts onto our long-term memory all the time because that is our prized possession. That's what makes our teams powerful.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2760.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2760)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2780.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2780)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2800.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2810.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2810)

 So let me give you a real issue example. Taking into account seven AWS domains, we switched it on, and within just over a couple of weeks, four weeks, we had 50,000 issues. We ran our pipeline and distilled those 50,000 issues to just over 1,300  that were usable and that we wanted to investigate further. We rapidly converted that into just over 6,500 nodes, all automated, with just over 19,000 actual relationships. We pumped all of that data into what we call the Wiz playbook, which is an  instance of a triage record inside the factory. It generates for us evidence, remediation, and so forth in a JSON format  because ultimately we're going to pump that JSON format back. We've taken that information and run it through a report service so analysts can look at it, annotate it, and reject it. Based on the evidence we found, we also create automation from that. We'll talk about automation in a second.

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2830.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2830)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2860.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2860)

 Now, very important for automation, and this is a bit of a takeaway for you all. Don't create automation on code. Libraries change, there are vulnerabilities, all those kinds of things. Create recipes for automation. They are a lot more enduring and durable than code. Also very important, any short-term memory, whether you like it or not, is historical. So make sure you have this idea  of what I call check, do, check. Even if we find something, we're going to go and check it again because maybe what happened then, or what we think happened, is no longer happening. We built that into the recipe, so we get an interpreter. The interpreter for the recipe is written by a human, the recipe is written by AI. Very important. We trust AI with a recipe, not so much for code when it comes to large-scale remediation.

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2890.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2890)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2910.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2910)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2930.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2930)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2940.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2940)

 At the end of the day, what we have is a fully fledged recipe that we can store in a central repository that is human readable. The great part is this goes back into our long-term memory, goes back into the lexical graph, which means over a period of time we can make claims such as  what exactly is happening in our organization. What evidence have we collected? What is the trend? Why is that so important? It is a first step to go from operational reality into policy intent. Instead of writing policies that have nothing to do with your operational reality, now we  have a reality, and we can actually augment our policies based on what's really happening. These reports are effectively stored centrally, and they can contain compliance.  They are a class of report, but also every individual issue is linked to a report. Think about a report as a class of experience and every individual issue as an instantiation of that class. We can either traverse in our memory on a particular incident or on all incidents of a particular class in order to understand what is happening.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/2970.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=2970)

 So what does this mean? It means for us that we now have an ability to do truth and traceability short-term. We fit that into a lexical graph, and that becomes the meaning of our understanding for both our analysts and our business. At any point in time, we have an understanding in our organization. We no longer deal with just a Christmas tree. We no longer deal with an application-centric point of view. We have our point of view based on a GraphRAG toolkit.

That corpus of knowledge gets expanded, gets built, and that is our prized possession. So I'm going to hand back to Ian to conclude for us.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/3030.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=3030)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/3050.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/3060.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=3060)

Okay, so we've learned quite a bit about the AI for triage.  What I really like about this use case is that we are augmenting human expertise, we're not trying to replace it. We're augmenting it with, as Evan describes it, this long-lived, evolving memory. And in order to provide those experts with that memory, we need to have  improved recall within the systems that we're using to serve up that memory. And that's really where that GraphRAG toolkit comes into play.  And I've tried to describe some of the techniques that we've applied on the Neptune side in order to provide those capabilities.

The graph model itself is structured all around the statements, returning relevant statements which would effectively comprise my particular memory at this moment in time. And the query techniques that we've introduced, particularly around entity network contexts, can help us find all of that structurally relevant but potentially dissimilar information that is super important when you're trying to reconstruct a deep understanding of an issue.

So there's a lot that we haven't talked about here in terms of the GraphRAG toolkit, a lot of additional features. I mentioned earlier that it supports multiple different backends. It supports multi-tenancy, so this is the idea that you can have discrete or separate lexical graphs and the associated indexes in the same underlying infrastructure. I can have multiple different lexical graphs in the same graph database.

It supports document versioning, so I can reindex documents that have changed, and then I can query the graph either for the current state, the current documents, or the state of the graph and state of specific documents at a point in time. We also provide the ability to surface domain-specific agentic tools to an MCP server, for example. So we can take all of those discrete lexical graphs, we can automatically infer the domain semantics for each graph, we can use that to produce a tool description, which we can then advertise to an MCP server. So now we've created a capability for an agent to query across different domains and answer very complex questions.

And then finally, we have a separate module within the repository called BYOKG, Bring Your Own Knowledge Graph. So what I've described today is the ability to create a graph from unstructured data and to query it. BYOKG allows you to bring an existing graph in Neptune or Neptune Analytics and again perform question answering over it.

Things that we've got coming up next, well, we've always welcomed contributions from the community. Deloitte have contributed already quite a bit into the repository. Over the next few months, we plan to move more of some of the stuff that Evan has described, things in particular like the document graph, bringing that into the repository. We're always taking requests from customers if they discover new features or new needs. We try and add those into the thing. At the moment we're working on several different additional vector store backends that we'll support in the GraphRAG toolkit over the next couple of months.

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/04f79656fb9fde71/3240.jpg)](https://www.youtube.com/watch?v=KD5C93kwBMg&t=3240)

Okay, over the next couple of days,  a couple of other graph talks. Tomorrow we have the last of three builders' workshops, which are all about building an application, a GraphRAG application using the GraphRAG toolkit. We ran one at the beginning of the week, one this morning, and we've got our last hour-long one tomorrow in the Mandalay Bay. And then on Friday, our very distinguished colleague Ora Lassila is talking about symbolic AI in the age of LLMs, and he'll be here in Caesar's Forum.

Okay, well thank you very much for coming along today. Thank you for listening and I hope you have a great re:Invent, thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
