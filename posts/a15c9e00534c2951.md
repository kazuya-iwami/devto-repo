---
title: 'AWS re:Invent 2025 - Build more effective agents through model customization (AIM383)'
published: true
description: 'In this video, Davide Gallitelli and Sam Palani demonstrate building custom AI agents using Amazon Nova Foundation models. They explain AI agents as LLMs calling tools in a loop to accomplish goals, and discuss the inflection point driven by reasoning models and test-time compute. The session covers customization techniques ranging from prompt engineering and RAG to fine-tuning methods including supervised fine-tuning, Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), model distillation, and continued pre-training. They showcase Amazon Nova''s model family including Nova Micro, Lite, Pro, Premiere, Nova 2 Pro, Canvas, Reel, Sonic, and Nova Act for browser automation. The presenters demonstrate customization workflows using Amazon Bedrock for simplified fine-tuning and SageMaker AI with HyperPod recipes for advanced control. They emphasize evaluation strategies using LLM-as-a-judge and deployment options through Bedrock Custom Model Import or SageMaker endpoints, concluding with production deployment using frameworks like Strauss Agents and Amazon Bedrock Agent Corp for enterprise-scale agent management.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/0.jpg'
series: ''
canonical_url: null
id: 3093248
date: '2025-12-08T21:45:56Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Build more effective agents through model customization (AIM383)**

> In this video, Davide Gallitelli and Sam Palani demonstrate building custom AI agents using Amazon Nova Foundation models. They explain AI agents as LLMs calling tools in a loop to accomplish goals, and discuss the inflection point driven by reasoning models and test-time compute. The session covers customization techniques ranging from prompt engineering and RAG to fine-tuning methods including supervised fine-tuning, Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), model distillation, and continued pre-training. They showcase Amazon Nova's model family including Nova Micro, Lite, Pro, Premiere, Nova 2 Pro, Canvas, Reel, Sonic, and Nova Act for browser automation. The presenters demonstrate customization workflows using Amazon Bedrock for simplified fine-tuning and SageMaker AI with HyperPod recipes for advanced control. They emphasize evaluation strategies using LLM-as-a-judge and deployment options through Bedrock Custom Model Import or SageMaker endpoints, concluding with production deployment using frameworks like Strauss Agents and Amazon Bedrock Agent Corp for enterprise-scale agent management.

{% youtube https://www.youtube.com/watch?v=sL9EU_O9Mxw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/0.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=0)

### Introduction to AIM383: Building Effective Agents Through Model Customization

 Thank you for coming to AIM383, Build More Effective Agents Through Model Customization. I'm going to be one of the two hosts, Davide Gallitelli, Worldwide Senior Specialist Solutions Architect for the SageMaker AI team. I have the pleasure to have with me on stage today Sam. Sam, do you want to introduce yourself? Hey everyone, my name is Sam Palani, and I lead the Nova go-to-market team here at AWS. Awesome, thank you very much.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/40.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=40)

This is a silent session, so you guys are happy to come ask questions after the session. We're pretty packed on content, so there's a lot to go through. Without further waiting, we're just going to go ahead and get started.  The agenda, as I was saying, is pretty packed. There's a lot to talk about, lots of new things coming. A lot of new things were announced as of yesterday and Tuesday. Hope you guys checked out the keynotes.

Today we're going to go through how to build custom AI agents using the Amazon Nova Foundation models. How many Foundation models do we have available when we talk about the Amazon Nova family? And then because we know that you guys have your own use cases and you have your own peculiarities in the way that you solve your tasks, we will show you how to customize Amazon Nova models. There are different techniques, both from a point of view of the open source community as well as the specifics of what we allow you to do within Bedrock and SageMaker. So we're going to go through that.

Of course, this is an AWS tech talk. It doesn't come without a demo, so we're going to go through an example of a model customization using Amazon SageMaker as part of a demo. And then finally we'll wrap up. We'll give you some resources to get started and to learn more about how to use Nova for your use cases. All right, with that said, not going to take any more of your time. I'm going to let Sam do the rest of the talk. Thank you.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/120.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=120)

### Understanding AI Agents and the Inflection Point in Agentic AI

Thanks, Davide. So how many of you have attended a silent session before? Okay. For those who have not, I'm with you because this is also my first silent session. So let's see how this goes.  Let's start with the basics. What is an AI agent? Now there are quite a few definitions of what an AI agent is, but a simple one that most of the industry is also gravitating towards is to think of them as LLMs calling tools in a loop to accomplish a goal.

Let's break that down a bit. So an agent has access to a model, a set of tools, and is given a very specific goal. It then uses these tools to take certain actions within an environment. The environment can be something like your web browser, your computer terminal, or even your enterprise application. It then analyzes the results of these actions to verify if the goal has been met. If the goal is met, the loop ends. If not, the agent continues to take more actions until the goal is accomplished. So there you have it, a model calling tools in a loop to accomplish a goal.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/190.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=190)

But let's also look at what's causing this inflection point in agentic AI, starting with reasoning  models. Now sometime last year there was a perception that frontier model intelligence had hit a wall. It seemed that we had consumed all the data that was available to train these models, and most model releases seemed more like incremental updates rather than a fundamental shift in frontier intelligence. It also seemed that the scaling laws were no longer working.

Just a quick recap, the scaling laws famously stated that if you add more data, continue to train longer, keep increasing the model size, you will eventually get a more performant model. It seemed that that was no longer holding true. All this changed when reasoning models burst into the scene. Suddenly the focus shifted from pure training to test time compute and inference time compute. What happened was that these new models were able to think and act out of the box, unlocking new capabilities for agentic AI that was previously challenging.

The other benefit of reasoning models was that it enabled you to deploy agents at scale and in a cost-effective manner. For example, you could now break down complex long-running tasks into smaller tasks and run them in parallel at scale. You could also choose to route tasks based on complexity to a smaller and more efficient variant of the model, making it more cost-effective.

Now a model with all the intelligence and reasoning capabilities can only do so much if it doesn't have access to your specific context. So what we saw was during this time, organizations also invested in building a secure data infrastructure that enabled them to connect their proprietary data to these powerful models. And last but not least, the emergence of specialized tooling and technologies such as Model Context Protocol, the agent-to-agent protocol, model level skills, and even tool calling further streamline the process of building and deploying these agents.

### Why Model Customization Matters for Agentic AI

But this is not just us. This is what customers like you and builders like you are telling us.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/320.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=320)

 According to Gartner, one-third of all enterprise applications will be powered by generative AI or agentic AI by 2028, and 15% of all day-to-day business decisions will be taken autonomously by an AI agent. To put these things in perspective, at the same time last year, this number was close to 1%. They also predict that of all these AI interactions, at least one-third of them will involve invoking an action model or an AI agent for task completion. But this is not a roadmap of the future. This is happening right now.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/360.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=360)

 Although this session is around customization, it's also important to understand that customization is just one step in your journey to production, where you have other steps such as building, deployment, and testing. For now though, we'll focus on customization.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/380.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=380)

 But why talk about customization in the context of model customization for agents? While frontier model intelligence has increased, it also exhibits brittle behavior, specifically with agentic AI. You saw that an agent takes a series of steps to accomplish a goal. This introduces a unique risk that we sometimes call compounding mistakes, where an error from one step propagates and compounds to the other step. For instance, consider a scenario where your model achieves 95% accuracy on a single step. Over the course of 10 steps, there is a risk that this might actually reduce to around 60%, turning a highly performing model into a very unreliable agent. Now customization can directly help reduce these errors, both at the step level as well as across the steps.

The next one is not specific to agentic AI but still very applicable here. Your domain-specific acronyms, taxonomies, and business process knowledge are not internet average. Customization can help align and steer the model to your specific business semantics. We also talked about tools in the context of agents. In order for these models to make these tool calls perfectly in the first attempts, it's important that they have a good understanding of the underlying schema as well as the parameters needed to invoke these tools. Customization can directly help you do that.

Now related to the problem of compounding mistakes, customization can help compress long-running chains within your agentic workflow. Your model learns micro policies for each step. You get shorter trajectories, fewer backtracks, and fewer dead ends. Customization techniques such as distillation can directly help reduce your P50 and P90 latency and cost per step. But what do we mean by customization?

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/510.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=510)

### The Spectrum of Customization Techniques: From Prompt Engineering to Continuous Pre-Training

 Again, a bare-bones definition that I'd like to think about here is customization is anything that you do to a model to modify its behavior. It has a broad set of options as you can see here, ranging from simple and progressing in complexity. At a high level, we like to break this down into two categories, which is optimizing the output and modifying the underlying model itself. Or in other words, optimizing the output with no weight updates, and modifying the model where you actually make weight updates to the model.

At the very simplest end of the spectrum, you have techniques such as prompt engineering, which is a bucket term that we use to describe everything ranging from simple few-shot examples to more complex chain-of-thought examples to everything in between, such as crafting efficient system and user prompts. Next we have Retrieval Augmented Generation, or RAG. While context sizes of these models have increased exponentially in the past few years, RAG continues to be a very effective way of providing additional context to the models, especially at runtime.

Context engineering has emerged as the most effective way of optimizing the model specifically for agentic AI. Now sticking to our theme of keeping things simple, we can consider context engineering as providing the right information in the right format and at the right time to the model. Now if you think about it, a model needs more than just the prompts. It also needs to have a good understanding of the tools that it has access to, any additional context that was retrieved as part of the RAG pipeline, as well as a memory of all previous interactions.

Now in contrast to prompt engineering, which involves working with discrete prompts, context engineering is an iterative process where the curation happens each time a pass to an LLM is made.

Now moving on to the techniques which involve modifying the underlying model itself, we can break this down further into two subcategories: post-training techniques and pre-training techniques. This is also where we'll spend the rest of this breakout session on. On the post-training side, you have techniques such as fine-tuning or supervised fine-tuning, model distillation, and reinforcement learning techniques such as Direct Preference Optimization, DPO, and PPO. And on the pre-training side, we have techniques such as continued pre-training. We'll go into each of these techniques in much more detail as we progress along.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/680.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=680)

Starting with fine-tuning. Now, just by a show of hands, how many of you  have tried fine-tuning? Good. So when we talk about fine-tuning, this includes both full fine-tuning, which involves modifying the base model weights, as well as parameter efficient fine-tuning, or PEFT, where you freeze the base layers and train only a set of added low-rank parameters, also known as adapters. They're called low-rank because they are of a lower dimension or rank compared to the weights of the base model.

You start by adapting a model to a specific task by using a relatively small labeled dataset. As such, this is also referred to as supervised fine-tuning. We use Direct Preference Optimization, or DPO, when you want to align the model to a very specific user preference without having to train a reward model. You can do DPO either in a parameter efficient way, as we discussed above, or you could do a full DPO.

Proximal Policy Optimization, or PPO, is a way to improve your model based on live feedback. How do you get that live feedback? You can get that via a task completion, a tool execution, or more commonly, via direct human feedback. A few important things to remember here is that you will need a full cycle of reinforcement learning, and as such you will need a reward model and a function. This is also more costly and time-consuming, but can give you really good results for very specific use cases.

Model distillation is a technique where you distill complex behaviors from a larger and a high-performing model known as the teacher model to a smaller and a more efficient model that we refer to as a student model. As we saw earlier, distillation can directly help reduce your latency and cost at a task level.

Continuous pre-training, as the name implies, is a pre-training technique where you continue to train on the pre-training checkpoints of the base model using a large corpus of unlabeled data. Now again, important points here is that you will need a large corpus of unlabeled domain-specific data. You should also watch out for risks such as overfitting and catastrophic forgetting. There are additional techniques that you can do such as early stopping or implementing intelligent data mixing to mitigate some of these risks. And David will cover a lot about this as he goes into the SageMaker customization.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/850.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=850)

### Amazon Nova Model Family and Customization Options on Amazon Bedrock

 Now, moving on to Amazon Nova. So, how many of you have used Nova? Quite a few. So we had a big release just a couple of days back. This slide is not updated to cover all of those releases, but I will happily talk through all of them. So at the very top, we have our understanding and reasoning models, starting with Nova Micro, which is our fastest and most efficient text-to-text model. We see a lot of customers use Micro in their latency-sensitive agentic workflows.

Next we have our multimodal understanding and reasoning models: Lite, Pro, and Premiere, each increasing in intelligence, with Premiere being our most performant model until two days back. Because two days back we released Nova 2 Pro, which is now the most performant model that is available. A lot of customers used to use Premiere in the past as the teacher model within the distillation pipelines.

Now they'll probably use Pro. Next, we have our creative content generation model, starting with Nova Canvas, which is our text-to-image generation model, and Nova Reel, which is our text-to-video generation model. Two days back, we also announced Nova 2 Omni, which is the state-of-the-art and more performant text-to-image generation model.

Next, we have Amazon Nova Sonic, which is the industry's first real-time speech-to-speech model delivering frontier capabilities such as automatic speaker recognition, speaker diarization, polyglot voices, as well as multilingual support. Nova Act became generally available two days back as well, and it is an agentic browser use model specifically designed to build agents that operate within your browser. We talked about agents taking actions in an environment, and one of those actions could be a web browser. So if you are building an agent that's designed to operate within a web browser, then Nova Act is a model specifically designed to do that.

Now we all know that embeddings are the foundation of any generative AI application or an agentic AI application. Very recently, in fact, a couple of months back, we released the Nova multimodal embeddings model, which is the only embeddings model that is available in the industry that will generate embeddings for all your multimodal data, which is text, video, images, and audio within the same vector space. Why this is important is that this enables you to build really powerful applications that utilize semantic search and agentic RAG.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1040.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1040)

Last but not the least, we have our Nova public website, nova.amazon.com, where you can try out any of these models free of charge or without needing an AWS account. Now the chart here looks busy, but if you look at this, Nova provides  a broad range of customization options both on Amazon Bedrock as well as SageMaker AI. Most of these options are available both as full rank options as well as parameter-efficient low-rank options that we discussed above. Depending on the option you choose, you can choose to do your customization either on Bedrock, or you can do the customization on SageMaker AI.

Once you are done, you can deploy this model in a serverless manner on Amazon Bedrock. For example, if you choose to do model distillation, you can choose to do that very quickly on Amazon Bedrock or build your own distillation pipeline on SageMaker AI. Once done, you can import your distilled model onto Amazon Bedrock and deploy it using on-demand inference or provisioned throughput. And we'll talk much more about these deployment options as we progress further.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1100.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1100)

But for now,  let's look at specifically the Nova customization options available on Amazon Bedrock, starting with fine-tuning. Now we talked about fine-tuning. We also covered full fine-tuning where you modify the weights of the base model, as well as parameter-efficient fine-tuning where you train a set of added low-rank parameters. On Bedrock, you can do parameter-efficient fine-tuning and deploy your model in just a few clicks.

Since this is a form of supervised fine-tuning, you start by uploading a set of labeled data and kickstart your fine-tuning job. Once your job completes, the model is available on the Bedrock platform just as any other Bedrock model for evaluation and testing. You could also try out the model on the Bedrock model playground before you decide to scale or deploy them. One common agentic AI use case that we see for this type of fine-tuning is for tool calling and schema stickiness, where you want the model to emit a very specific JSON schema consistently.

Another good example over here is for persona switching, where you want to use the same base model across your agentic AI workflow and have it switch between multiple personas such as sales and support. Now as a quick recap, model distillation is a technique where you distill complex behavior from a larger and a highly performant model to a smaller and more efficient model. Now, how many of you have done model distillation here? Very cool. We have one gentleman over there.

So if you have done model distillation, one of the biggest challenges with model distillation is building a robust data pipeline for your distillation.

With Bedrock, you can do model distillation again in just a few clicks. Now if you think about it, distillation is just a type of fine-tuning with a teacher model in the loop. What I mean by that is when you do this on Bedrock, you start by picking your teacher model and your student model. You upload a set of task-specific prompts, just the prompts, not the responses. Bedrock will automatically use the teacher model to generate the responses for your prompts that you uploaded. It will then use the prompts and the responses generated by the teacher model to fine-tune the student model.

A good example here is productionizing a high-performing prototype. For example, if your prototype only works with a high-performing model, you can choose to distill those specific behaviors into a smaller variant of the model as you deploy and scale it in your production. Now talking about deployment, Bedrock offers two modes of deployment: on-demand inference, which is token-based pay-as-you-go pricing, as well as provisioned throughput, where you pay a fixed price upfront for guaranteed throughput.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1300.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1300)

### Advanced Customization with SageMaker AI: HyperPod Recipes and Direct Preference Optimization

With that, we'll move over to the SageMaker side of customization, which David will happily take us through. Thank you, Sam. I don't know if you guys saw the mind map  before about the different techniques of customizing Nova. I just want to call out that it's a work that Sam did. I think it's one of the best mind maps that I've seen in AWS in the last seven years that I've been in this company, so I really want to do a shout out to the work that he has done and to the work that he will do now that he goes back because we have released new techniques now. So you know that the first thing Monday is going to do when he comes back, it will be to update that slide.

But let's focus now for a second on other customization techniques. You've seen how to perform customization using Amazon Bedrock so far with the fine-tuning and with the model distillation capabilities, but let's go a little bit deeper. If you're a little bit more advanced and you want to have a little bit more control on the fine-tuning process, Bedrock is still going to give you a lot of bells and whistles and a lot of knobs to turn. However, if you want to have a lower-level understanding and a lower-level impact on the training process, SageMaker must be your go-to tool.

It's an end-to-end machine learning platform, machine learning engine, AI platform, of course, which allows you to choose a model that you want to fine-tune. Perform fine-tuning using the option you prefer, and we have two examples here: the training jobs API for fully managed and serverless model fine-tuning. We have announced the serverless fine-tuning just yesterday morning, so I really suggest you guys check that out. Or you can have a complete orchestration managed by yourself truly in the form of SageMaker HyperPod.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1420.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1420)

HyperPod is a managed cluster based either on EKS or on Slurm and allows you to really go a level deeper, managing the infrastructure for your training workloads. It allows you to really customize what kind of orchestration you want to do, as I was saying, through Slurm or Kubernetes. It allows you to also schedule the workloads to make sure that you get the best of the resources that you're actually paying for. But not just that, SageMaker also provides  optimized recipes for you to make sure that your training is successful and works well the first time.

So what you've got to focus on is not figuring out what is the best code that you need to write for your training loop. Most of the time we see customers focusing on the data science process, generating the data if they don't have it, cleaning their data if they already have that, and then providing this data to already well-known scripts. So what we have done in the backend is that we have taken those scripts, we have customized some of those scripts, making sure that they perform the best in our architecture, and we have packaged them in the form of recipes.

Now we call them HyperPod recipes, but they're also working for SageMaker training jobs. So if you prefer a more serverless or managed approach to training your model, absolutely go ahead and use the SageMaker training job, or SMTJ. Or otherwise, if you want to have a lower level of customization and orchestration, those are also compatible for HyperPod. So let's take a deeper look at how those recipes work.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1490.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1490)

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1500.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1500)

First of all, you're going to go ahead and select your Nova recipe. You're going to go ahead and make an API call to SageMaker control plane,  which will then leverage some of the launchers that we have available in SageMaker, spin up the right training job on the HyperPod instance, download the ECR  training image directly on those instances that have been deployed, and then finally load the data.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1510.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1510)

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1520.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1530.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1530)

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1540.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1540)

From the different data sources that you have, whether  they are S3, whether they are FSx, or whether they are EFS. I know the image is a little bit fast, and this is an image that I took from one of the blogs  that is publicly available. So if you want to learn more about how SageMaker Recipes help you understand and simplify training a model using SageMaker, you might  want to scan the QR code that you see here on the right, which will actually take you directly to the blog post introducing this infrastructure.  I see a lot of people are taking pictures, so I'm going to wait a second until the full image is there, and you have to be fast. There you go, now is the moment for the photo. Perfect.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1550.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1550)

All right, those slides will be sent offline anyway, so don't worry if you can't really be as fast as possible in taking that picture.  It's all available online. Once again, the blog is going to be your best friend. So what I'm going to do now instead is walk you through a quick demo of how to perform all of what I showed you in the architecture before and actually do it in the Studio console. So one heads up I want to give you before I start the demo is that as you know, we have released a lot of things during this week, including some minor updates to the console in SageMaker Studio. So what you see right now was true until two days ago, but because we have to prepare the slides ahead of time, we haven't had the time to update it yet to the new video. However, the concepts that I'm going to walk you through and talk you through are actually going to be exactly the same, regardless of what the UI looks like.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1610.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1620.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1620)

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1630.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1630)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1650.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1650)

So let's get started doing Direct Preference Optimization with SageMaker AI on Nova models. How does that work? You jump into SageMaker Studio,  you have a use case that you want to solve, you already have in your brain what is your use case, so you need to figure out where to start. The starting point is always the JumpStart  hub in this case, because Nova is available as part of the JumpStart model hub. Now it's called just Models. You select the model that you want to use. Sam was talking about Nova Micro before,  but now you can use Nova 2.0 Lite and so on. And then select the Train button all the way up top, which effectively allows you to choose the right training option. I think this got stuck somehow, so let me start that again real quick.  The good thing about live demos, even the recording sometimes can break. PowerPoint is not always our friend.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1670.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1670)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1680.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1680)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1690.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1690)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1700.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1700)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1710.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1710)

But you know, let's recap. Let's go to JumpStart, let's choose the right model from the model hub. There are many models out there. We're focusing on  Nova right now, but there's also Llama models, Qwen models, Mistral models. Really, it's just a matter of choice. Choose the right one for your use case.  Some of them will be trainable, so you can select Train all the way up top. And then you get to choose the right techniques that you want, whether it's Supervised Fine-Tuning, whether it's Direct Preference Optimization. It tells you which  compute you need, it tells you what format of data you need, so make sure that you have data in the right format before starting your job. It shows you the code  for the notebook itself, so you don't have to do anything through the UI if you don't want to. You are provided with the code. In this example, I'll walk through the recipe  using the recipe in SageMaker HyperPod, but you can also use the training jobs.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1730.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1730)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1750.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1750)

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1760.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1760)

Make sure you have a training cluster available, so a HyperPod cluster available. In this case, it is a Restricted Instance Group cluster type in SageMaker HyperPod, and then all you have to do is just select that cluster and click the button to open  the notebook in the right JupyterLab space running on top of the HyperPod cluster. Of course, these notebooks are publicly available. They are available in the GitHub repository. So if you don't want to go through having a HyperPod cluster, you can just pull the notebook directly from the web. The notebook contains a bunch of information, contains the usual prerequisites, the usual installation,  running the different code. It shows you what is the code to actually start the training job, and you will see that we pointed to the recipes,  and once you start the actual training job from the HyperPod cluster, all the information will be available in the available tasks in SageMaker HyperPod.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1770.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1770)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1780.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1780)

### Evaluating LLMs: Metrics, LLM-as-a-Judge, and Task-Specific Assessment

 So if we go back to the end-to-end path for model customization and for creating agents, now we've gone through customization,  but the next step is evaluating the model. So how do you go about evaluating an LLM? There are different techniques, there are different metrics, and there is no silver bullet to correctly perform evaluation. You will have different techniques. You will have general metrics. You can use, for example, perplexity or accuracy to statistically evaluate the quality of the model, but you can also go a little bit deeper and, for example, consider task-specific metrics.

This is very relevant, especially in the world of agents, where you want to make sure that the agent actually follows the trajectory of solving the problem in the right way. Although the results might be the same, a model or an agent can take ten steps, whereas another one can take two steps. So it could be a number of steps, it could be the quality of the steps that are being executed in between. Of course, we can't forget about hallucinations, bias, and toxicity metrics. So all of those kinds of metrics need to be taken into account for evaluating the model the right way.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1850.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1850)

So how can you perform evaluation of a model? Classically, you would go with rule-based heuristics.  For example, this is great if all you care about is using publicly available or commonly known metrics like F1, ROUGE, or any other kind of well-known metrics. A really good example is also if you're solving a problem which could be, for example, SQL query generation or execution of the SQL query. Does the SQL query actually execute once you've generated it? Does the result that comes out of the SQL query, is it actually correct or is it wrong? So those kinds of rule-based approaches can be leveraged and used to improve the model training itself. I'm thinking about techniques like RLVR, so reinforcement learning with verifiable rewards. You execute a query in that case, you get the result. Is the result correct? Let's give a reward to the model training. If it's not correct, then just give a negative reward.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/1910.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=1910)

However, some use cases do not necessarily have a way to correctly define heuristics or rules to evaluate the model.  Which is why we go for LLM as a judge or LLM-based critique. It's a really flexible technique and a really customizable technique because it allows you to define a specific prompt which can evaluate through a bigger model, a teacher model if you want to call it that way, or rather a judge model as we should call it. It allows you to evaluate a textual output from another model. So let's think, for example, that as Sam was saying before, we have Nova Pro, that's a great model, it's a very big model, performing super well on a range of use cases, but we want to make sure that Nova Micro can solve the use cases I want.

So what I can do is that I can fine-tune, as I was showing before, Nova Micro, and then leverage, for example, Nova Pro to evaluate the output and the inferences of Nova Micro after fine-tuning, to make sure that I can compare, for example, the base performance to the fine-tuned performance. Is the outcome better? In what measure is it better? For example, if we go back to the SQL query use case, semantic correctness is not something that you can actually compute in terms of a number. Someone has to look at the semantics of your SQL query and say, does writing the SQL query in that way actually equate to writing it in this other way? And LLMs are great at doing those kinds of tasks.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2000.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2000)

If you want to know more about different metrics for evaluation using LLM as a judge, there's a whole list of some examples,  but once again, what I usually suggest is that you evaluate and possibly create the metric that makes the most sense for your use case. In my case, it was the semantic correctness or semantic equivalence. In your case, it might be something completely different.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2020.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2020)

### Deploying Custom Models and Building Production-Ready Agents

So now you've fine-tuned your model  using HyperPod or SageMaker training jobs, you've evaluated the performance of your model. So what do you do after that? The next step is to take that model and make sure it's available for queries and for inferences. Now, you can take a Nova model and deploy it on Bedrock inference using custom model import. You can use either on-demand provisioning of that model, which means it's there, it will be offline until you actually call it. It will take a second to spin up, and then it's available for inferences as much as you want. And this is available for techniques such as SFT, DPO, Direct Preference Optimization, and distillation.

Or it's available through provisioned throughput, which means it's always available, always there, just call it and it's available for you to run a query, but it's available for all techniques. On the other side, you can also take that model and deploy it on SageMaker, whether it's on a real-time inference endpoint, which is effectively a virtual machine, an instance that we run for you managed by the SageMaker control plane. We load it with the image, we load it with a model image, and it's available for you to run queries 24/7 as much as you want. Or you can also use HyperPod. We recently introduced the capability of running inference endpoints in HyperPod so that you can either choose to pay by the minute that you're running the inference endpoint or just have it available in a highly scalable cluster.

Just a note, it's currently not possible for Amazon Nova models to be deployed on SageMaker, neither endpoints nor HyperPod, but just wait for it. I'm afraid I can't say anything more.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2120.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2120)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2140.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2140)

Let's take a look at what evaluation and deployment look like.  So after I've trained my model, what I can do is select evaluate all the way up top. There are also evaluation recipes which are available in GitHub. Just like before, you can select which recipe you want to do: general text benchmark, bring your own dataset for question and answering, or LLM as a judge recipe.  This example will do question answer. You always have the button that says view the format, and once again, if you use HyperPod, it's as simple as HyperPod start job and provide the path to the recipe.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2150.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2160.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2160)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2170.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2170)

 Once again, this will become a task in your HyperPod cluster management. So if you have HyperPod task manager, you will see everything, and you will be able to  output the evaluation metrics and consume them as you prefer in a Streamlit app or directly in a notebook.  Then you can take the model and deploy it to Bedrock for inference. It's literally two calls: create a custom model import or create provision model throughput.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2180.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2180)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2200.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2200)

 That's all you need to do, and then you can ask a question. What is the weather in Rome, Italy? Of course, make sure you provide a tool to your agent, otherwise how does it know what the weather looks like in Rome at this point? And if you can tell, yes, I am Italian as well, so this is why the demo talks about Rome, Italy. I'm not from Rome, but it's a beautiful city. If you haven't been, please  go.

Awesome. So we have customized the model, we evaluated its performance, we have deployed the model to production with an endpoint or in this case with Bedrock custom model inference. How do we go to production? So how do we actually build an agent? Who has already built an agent here in this room? Okay, good. I see a lot of people building agents.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2230.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2230)

And you might have used one of these open source frameworks, right? Our heart goes with Strauss Agents  because it's one of the opinionated ways that we have for building agents in a simple way with AWS technologies and not necessarily only AWS technologies. You're going to see that Strauss Agents is a framework that allows you to connect to Ollama, OpenAI, Gemini, Anthropic, really connect to whatever kind of model provider you want. We think it's the easiest way to build agents, but we know you have your opinionated view. So if you want to use LangGraph, LangChain, OpenAI Agents SDK, CrewAI, Google ADK, LlamaIndex, and probably so many more out there that I don't even know they exist, most of the frameworks out there are compatible with Bedrock custom model inference. They're compatible with SageMaker endpoints to generate inferences.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2280.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2280)

And once you have built your agents, you have written the code to create your agent,  then the very next step is to get that agent into production. We have created an end-to-end managed platform for agent deployment and management and running those agents, which is called Amazon Bedrock Agent Corp. If you haven't been to an Agent Corp specific session, I highly suggest you take the time between today and tomorrow to go to an Agent Corp session. I believe it's a very interesting service that you should definitely check out, also because it comes with many different sub-services which solve different problems in what we call this production chasm.

So what do you need to get an agent from development all the way down to production, ranging from where do you run the agent? Does it have a memory or not? Please use memory with your agents. Identity: how does it have the permission to access the different resources? How does it connect to the available APIs that you have in your enterprise? How can you use, for example, tools like code interpreter or browser to effectively run code or connect to the web? And then finally, arguably the most important, how do you understand how your agent is running in terms of observability?

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2350.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2350)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2360.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2360)

### Key Takeaways: When and How to Customize Models for Your Use Case

I've spoken enough  already. We've seen two demos, we've covered the agent capabilities, we've covered how to customize the model, so let's wrap up things a little bit.  When you go back home after this, of course enjoy the replay tonight first, then go home. What you will need to do in order to solve your generative AI use case: you will likely need to, first of all, take a stab at the already available models. There are many different good models out there. Sam walked us through techniques like context engineering, RAG, prompt engineering that can help you solve already your problem as is.

If your problem is a really custom one, then you might want to evaluate model customization. In a simpler way, you can do it through Bedrock, it's really easy to get started. If you want to have a little bit more control, then you can use the techniques available in SageMaker to customize your model.

Always evaluate the performance of the model after you fine-tune it, because you want to understand if it's actually performing better than the base model, or you might want to go through additional techniques. Then that model needs to be available for queries, so we deploy it either with Bedrock Custom Model Inference or on a SageMaker inference, whether it's an endpoint or HyperPod. Finally, write the code for your agent in the open source framework that you prefer. Once again, shout out to LangChain and Agents for that, and then take that to production using a tool like Amazon SageMaker or Amazon Bedrock Agents.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2450.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2450)

 When do you need to customize? It's a very valid and interesting question to ask yourself at the very beginning. Once again, for me, the easiest way is just test the base model as is and decide from there if you need more performance, if you need to be more aligned to your business, if you need to be more cost effective. We've seen a lot of customers move to smaller language models rather than using large language models because they are much more cost effective, they can be customized, and they can solve one task really well at a fraction of the cost of a larger model. But it could also be a technical question. Maybe you already have the training data, which makes it super easy to customize a model, or you have a team with technical expertise, and that team can then act effectively as a reinforcement algorithm for the model fine-tuning.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2500.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2510.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2510)

 To recap the techniques that are available for fine-tuning, Bedrock supports LoRA and instruction tuning. SageMaker supports a whole range of different techniques, so you can go ahead and choose the one that makes the most sense for you.  And one thing that I want you to take away from this is that do not just settle on one fine-tuning technique. Fine-tuning is a spectrum. It goes from just prompt engineering all the way down to continued pre-training, LoRA, full rank, and all the other different techniques, Direct Preference Optimization, and so on. So really evaluate how much time and effort you want to put in the model customization process, how complicated you want this process to be, and what kind of outcome you expect from the different techniques.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2550.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2550)

And of course, remember that if you want to learn more about Amazon Nova, but in general about AWS technologies,  we do have the Skill Builder portal. It's a great way for you to get more hands-on. First of all, you have to go through the documentation, but if you want to be a little bit more hands-on and learn more about the specificity of some of the techniques, you can go ahead and jump into the Skill Builder process where you have all the trainings available for Nova, as well as for a bunch of other techniques.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a15c9e00534c2951/2600.jpg)](https://www.youtube.com/watch?v=sL9EU_O9Mxw&t=2600)

With that said, I want to thank you. Thank you for listening to us for all this time, and if you have any questions, we're going to be just here next to the room. Feel free to come and ask any question, and please, as you leave, remember if you take a second to go to the app and fill in the survey, we say the feedback is a gift. It's always nice if you can tell us, hey, Sam and David, they were two great presenters, so we're looking forward to their next session. Thank you very much. 


----

; This article is entirely auto-generated using Amazon Bedrock.
