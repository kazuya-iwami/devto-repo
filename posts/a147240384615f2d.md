---
title: 'AWS re:Invent 2025 - Amazon S3 Tables architecture, use cases, and best practices (STG334)'
published: true
description: 'In this video, AWS introduces Amazon S3 Tables updates including Iceberg V3 support with deletion vectors and row lineage, Intelligent-Tiering storage class offering up to 80% cost savings, and S3 Tables replication for cross-region data protection. Indeed shares their migration story of moving 85 petabytes across 15,000 datasets to S3 Tables, achieving 10% annual cost savings and reducing onboarding time from one day to 10 minutes. The session covers best practices for partitioning, compaction modes, and snapshot management, plus a demo using DuckDB WebAssembly to build a natural language query interface for S3 Tables directly in the browser.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/0.jpg'
series: ''
canonical_url: null
id: 3093049
date: '2025-12-08T19:34:27Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Amazon S3 Tables architecture, use cases, and best practices (STG334)**

> In this video, AWS introduces Amazon S3 Tables updates including Iceberg V3 support with deletion vectors and row lineage, Intelligent-Tiering storage class offering up to 80% cost savings, and S3 Tables replication for cross-region data protection. Indeed shares their migration story of moving 85 petabytes across 15,000 datasets to S3 Tables, achieving 10% annual cost savings and reducing onboarding time from one day to 10 minutes. The session covers best practices for partitioning, compaction modes, and snapshot management, plus a demo using DuckDB WebAssembly to build a natural language query interface for S3 Tables directly in the browser.

{% youtube https://www.youtube.com/watch?v=Pi82g0YGklU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/0.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=0)

### Introduction: Amazon S3 Tables at re:Invent

 Good afternoon, everyone. Thank you for joining us here, and I hope you've been having a great re:Invent so far. I'm Aditya Kalyanakrishnan. I go by Adi. I'm a product manager on the S3 team. I'm joined here today by my co-presenters. We have Min Li, who's a senior manager on the Data Platform team at Indeed, and also Yuri Zarubin, who is a principal engineer on the team who I work with closely in S3. We are super excited to talk to you about Amazon S3 Tables and share what we've been up to all year.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/50.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=50)

Here's what we have lined up for you today. We'll start with a quick overview about S3 Tables and then dive into some of the recent launches and updates that we've shipped over the past year.  It's going to be a rapid fire of all the launches that we've had. Then we'll also dig into some of the three new capabilities that we launched just recently. I will cover two of them. It'll be the Iceberg V3 support that we rolled out in AWS as well as the new Intelligent-Tiering storage class support for S3 Tables. Then we'll have Min come up here and share his story about how Indeed is migrating their massive 85 petabyte data lake onto S3 Tables and the optimizations that they'll gain from it.

After that, Yuri will talk about some of the workloads and best practices that you can think about and also cover the third new capability, which is S3 Tables replication. He also has a neat demo where it's an easy way for you to build applications using S3 Tables. So do stick around till the end, and after the session ends, don't worry, we will be here or outside if they kick us out and answer any questions that you might have or get any more details about these new capabilities that we just launched.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/120.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=120)

### Amazon S3 Tables Overview: Purpose-Built Storage for Apache Iceberg

 Last year at re:Invent, we brought you Amazon S3 Tables that lets you create fully managed Apache Iceberg tables directly in S3. With S3 Tables, you get optimized performance and scale, and S3 Tables is purpose-built to store tabular data at scale, which means that S3 is aware that these are Iceberg tables, and it lets S3 lay out the data in the table in a manner that's optimized for query engines to be able to take advantage of S3 scale. With S3 Tables being a first-class database resource, we also simplified security controls, and with automatic table maintenance, we're continuously monitoring your tables to make sure that your tables are optimal both from a performance and cost standpoint.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/170.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=170)

 S3 Tables is really becoming the foundation for our customers that are building data lakes in AWS. It's directly integrated with AWS services, starting with the Glue data catalog, which enables you to discover all of these tables through a number of AWS services and tools, whichever ones that you choose. In addition to that, we also have an open way for you to be able to access these tables through the Iceberg REST catalog interface that comes built in with every table bucket, or you can use an Iceberg REST catalog interface that is also built with the Glue data catalog if you want visibility into your entire data estate within AWS.

It is secure by default. You have coarse-grained controls, and you can set IAM policies directly on these table resources, or you can use Lake Formation to provide fine-grained access controls and make sure that your governance needs are met. There are a number of ways for you to analyze all of this data. You can stream your data through Data Firehose, process it using Glue ETL or EMR to transform your data sets, and you have a suite of tools to analyze all of this data and query them, whether it's using Athena or Redshift to be able to query these data sets or build dashboards and visualize all of these data sets through QuickSight, and also being able to interact with your data using natural language as well.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/260.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=260)

### A Year of Innovation: Feature Launches and Customer Adoption

 Now, when we launched S3 Tables last year, we were honestly surprised by the response and the level of interest that we got from our customers, and that's kept us really busy the entire year, adding new capabilities to make S3 Tables more flexible and optimized for your data lake environments. We shipped a number of features across the board, such as optimizing performance and scale by adding advanced compaction techniques like sort and Z-order, while also performing these advanced compaction techniques in a cost-effective manner. We also expanded to 32 AWS regions and scaled up the number of tables that you can create within a region in your account to 100,000 tables. We also now have a lot more flexibility in terms of security controls like table-level encryption using KMS, and we also recently introduced a way for you to use resource tags for attribute-based access control.

As I mentioned before, there's a whole suite of enhancements that we've made in terms of opening up S3 Tables both within AWS, whether it's being able to directly access Athena and the SageMaker Unified Studio capability from the S3 console, or using a number of partner tools that are now using the Iceberg REST catalog interface to directly connect with S3 Tables. In fact, Yuri will show you later today how easy it is to build a natural language interface to interact with your tables using DuckDB, which is one of our partner tools.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/350.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=350)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/360.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=360)

All of these new capabilities  have enabled customers to really take advantage of S3 Tables. Over the course of just one year, we've had customers build and create over 400,000 tables,  and this has been very encouraging for us. However, it's still fairly early days, and it's just the start of our journey to simplify tabular data storage in the cloud. We are super excited about a number of new capabilities that we just launched.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/380.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=380)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/410.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=410)

### Iceberg V3 Support: Deletion Vectors and Row Lineage

Let me walk you through some of these new capabilities, starting with the rollout of the Iceberg V3  features that are now generally available in AWS. You can now write Iceberg tables with deletion vectors and row lineage as per the Iceberg V3 specification, and this allows you to be much more efficient in the way in which you're making modifications and easily track those modifications over time in your tables. This capability is available through a number of AWS services that now support these V3 capabilities as well. 

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/430.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=430)

Let's unpack this a little bit, and we'll start with deletion vectors. This provides an efficient way for you to delete data in your tables. Your tables are composed of a bunch of Parquet files, and when you delete rows in a merge-on-read pattern in an Iceberg V2 table, your engine  ends up adding positional delete files with each of these updates. These positional delete files are also Parquet files, but they're tiny Parquet files, and they enable your query engine to mask any of these deleted rows or fields and effectively render this data with the latest updates.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/480.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=480)

The more you're deleting rows and the more you're deleting data in your tables, the more you're going to accumulate a bunch of these files over time. This can really slow down your queries as your engines have to sift through a number of these delete files and eventually require compaction to basically do work to consolidate all of these tiny updates to keep your tables optimized. With deletion vectors, all of this information gets consolidated into a single Puffin file with a bitmap  vector. This lets your query engine quickly figure out the deleted data in your tables without having to sift through all of those delete files. An added side benefit of this is that you also end up saving on compaction processing, which no longer needs to deal with any of these delete files, and this makes compaction much more efficient and even cost-effective.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/510.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=510)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/530.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/540.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=540)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/560.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=560)

The other thing I mentioned was row lineage. You now have additional metadata fields that allow you to figure  out a canonical way to represent rows and changes in those rows within your table. Each row now has a row ID and a sequence number field, and this allows you to figure out any changes that have occurred over time in your table. In this case, when you're updating the data, if you happen to make an edit in one of the fields,  that shows up with a new sequence number. This way you can actually see those updates flow through your table snapshots over time. Similarly,  the same thing works when you're appending data to your tables as well, and so these new rows have a new row ID field, and they also have the sequence ID that represents when those changes were made. This makes it a lot easier for you to just track these changes over time using straightforward SQL. 

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/570.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=570)

All of these new capabilities are now available for you to use through the built-in SageMaker Unified Studio integration.  Unified Studio is an IDE that now supports a notebook interface for you to interact with these tables in S3 Tables. It's a single one-click where you go from the S3 console to the SageMaker Unified Studio, and what's great about this notebook environment is that it's not just SQL. You can use Python or even use natural language to query your tables and interact with them.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/600.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=600)

### Intelligent-Tiering for S3 Tables: Up to 80% Storage Cost Savings

The next thing is Intelligent-Tiering support for S3 Tables, and this was announced  just on Tuesday during Matt Garman's keynote. This new capability is a really exciting one and there's going to be huge cost optimization for customers using S3 Tables.

You can save up to 80% on table storage costs over time when you use Intelligent-Tiering to store your tables.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/620.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=620)

Now,  why did we build this? When we talked to customers, what we heard were three main challenges when it comes to optimizing costs in Iceberg tables. First, as your data volumes continue to scale and grow, the value of the data remains important, but you still want to be able to store that in a very cost-effective manner. The second is that you don't want to have to manage lifecycle policies to move the data manually across different storage options, and you really want S3 to take care of all of this. These are properties that Intelligent-Tiering comes with, even in general purpose S3 buckets. The third thing is that you also don't want to manually coordinate any of the maintenance operations that are reading any of these files, and so you don't want to worry about any accidental tier-ups on your inactive datasets that are not actually being used. You don't want that to increase your storage costs.

That's where with Intelligent-Tiering, what we wanted to provide is the same experience that customers have with general purpose S3 buckets, but go beyond that and make table maintenance tier-aware. This way it doesn't impact the storage cost of any existing data when we do maintenance on those tables, and that way you're still able to continue to save on your storage costs.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/710.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=710)

All right, so here's how it works. You have three access tiers,  and data starts its journey in the frequent access tier. After 30 days of no access, it automatically moves and transitions to the infrequent access tier, which is 40% lower cost than the frequent access tier. After another 60 days of no access, that data transitions to the archive instant access tier, which is another 68% lower cost than the infrequent access tier. All of this works at a file-level granularity, and what this means is within a table, you can have actively queried data that is in the frequent access tier, but there's also colder parts of the table that can transition down to the cooler access tiers. That way you're able to be optimal in your storage costs as your data ages.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/760.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=760)

Now, it's also optimized for changing  access patterns. So if the data that is in the infrequent access tier or archive instant access tier suddenly gets accessed, and this could be because you're running a new analytics workload or you want to do some historical analysis on your data, that data can transition back into the frequent access tier. You don't see any performance degradation, and there are no retrieval charges as that happens. In this way, your data fluidly transitions across these tiers based on the access patterns.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/800.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=800)

Now, this is all table stakes for us because this is what you get with general purpose S3 buckets and Intelligent-Tiering there as well.  But where it gets really interesting and exciting for us is table maintenance, right? Whether it's files that are being read for compaction or metadata that's being read for snapshot expiration or unreferenced file removal, none of those operations actually tier up any of those objects. Which means those operations do not affect your storage costs.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/830.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=830)

In addition to that, what we've made is compaction now has a guardrail  where it only operates on your actively queried data in the frequent access tier. This is because data generally is hotter and more active when it is first ingested in its lifecycle, and after that it slowly cools off. We really want the compaction capability to come in and optimize for the query performance for the actively used datasets. This also works when it comes to any inactive data that you might have that gets queried and moves back to the frequent access tier. Now those files get eligible for compaction, and they then get optimized. This way compaction is continually optimizing for the most active datasets while leaving the colder data where it is, and that way you get the optimal return on your compaction spend.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/890.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=890)

It's incredibly simple for you to configure it. You have two options. You can basically set it when you create a table by setting the storage class to Intelligent-Tiering,  and we've also introduced a new bucket-level configuration which allows you to set the default storage class. That way any new tables that land in that bucket can automatically land in Intelligent-Tiering.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/910.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=910)

### Customer Success Stories: Zita Global and Indeed

The features that we just launched add to the value that customers are already seeing  with S3 Tables, and here are a couple of examples of a few of these customers. Starting with Zita Global, their AI marketing platform relies on data that is massive, constantly changing, and immediately actionable. By using Amazon S3 Tables as their foundation for their petabyte-sized lakehouse spanning thousands of Apache Iceberg tables, they've reduced their data freshness latency by nearly 80%. This means their time to insights has now compressed from 15 minutes to just a few minutes.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/950.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=950)

Here's another great example.  Indeed leverages massive amounts of data to understand the job market and connect people with the right opportunities. By migrating their 85 petabyte data lake to S3 Tables, they're streamlining their data infrastructure and reducing their costs. We're really excited to have Min here to come share Indeed's migration story with us. All right, Min, come on up.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/990.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=990)

### Indeed's Data Platform Transformation: Migrating 85 Petabytes to S3 Tables

Cool, thank you, Aditya. Okay, hello everyone. My name is Min Li, and I'm a Senior Manager for the Data Platform team at Indeed.  Today I'm super excited to share the story of our data platform transformation. It is a story about transitioning from a complex traditional setup to a more efficient data lake leveraging Apache Iceberg and Amazon S3 Tables. I'm going to show the incredible business values we unlocked and how we are now able to move faster to better serve our core mission at Indeed.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1020.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1020)

Indeed's core mission  is simple, but it's a massive undertaking. We help people get jobs. Everything we build, every decision we make is in service of that mission, and the engine that powers this mission is data. Data helps us understand the job market, match the right candidate with the right opportunity, and create a better experience for both job seekers and employers. To do this effectively at a global scale, we have to be masters of our data.

A notable public-facing example is our Indeed Hiring Lab. As you can see here on this slide, which is directly leveraging the data from our data lake to offer valuable insights into the labor market. As you can see on the chart, it illustrates the trends of pay transparency, job postings, and wages over the past five years in the US, and it is directly used by policymakers in the government.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1080.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1080)

 The scale we operate at is massive. Our data lake is home to more than 85 petabytes of data across over 15,000 datasets. Every day our platform handles over 170,000 queries and ingests over 550 terabytes of new data to keep our job recommendations up to date. On the right side of this slide, you can see the architecture diagram of our data lake platform.

You don't need to memorize all the details in this diagram, but I want to show it to illustrate the complexity we are dealing with. As you can see, we have hundreds of customer AWS accounts where data-producing teams are building datasets through various technologies and tools. These datasets are then ingested into our data lake AWS account through two different ingestion modes. On the read side, we also need to support various platforms for data consumption, including Athena, Trino, Spark, Snowflake, and in-house built query engines. This scale is a huge asset, but it also creates enormous technical and operational challenges. Managing this is not a trivial task. It became a bottleneck for us.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1160.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1160)

Our first step was to standardize.  Last year, Indeed made a strategic decision to move to Apache Iceberg as our standard table format for the entire data lake. The business reasons for this were clear. First and foremost, interoperability. We needed the flexibility to use the best tool for the job, and Apache Iceberg enables us to utilize the same underlying data with various engines such as Athena, Trino, Spark, and Snowflake without being locked in.

Secondly, compared to Hive, Iceberg offers better metadata handling and partitioning capabilities, resulting in faster queries and lower compute costs.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1220.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1220)

Lastly, Apache Iceberg offers features such as schema evolution, time travel, and rollback, which are crucial for ensuring our business continuity. As you can see on this diagram, our overall goal is to have all data in one place.  But just choosing the Iceberg table format wasn't enough. As we further scaled our operations, we realized that trying to manage our massive Iceberg-based data lake on our own had created a new set of critical challenges.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1240.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1240)

Here are four major challenges to manage Iceberg at scale.  These were the business pains that made us adopt Amazon S3 Tables. First, an unsustainable operational burden. We were spending over 2,000 developer hours per year just on managing the in-house built Spark-based Iceberg maintenance jobs. That is an enormous amount of engineering time not spent building new products or features for our customers.

Second, customer onboarding was extremely slow and manual. When new data science or product teams needed onboarding of Iceberg tables in our data lake, it took us more than a full day of development to get them up and running. When you have over 350 teams to onboard in a fast-moving tech company, that's an eternity. Our team became a bottleneck to innovation.

Third, unreliable access. On multiple occasions we encountered S3 API rate limits during peak workloads, resulting in several SEV-1 and SEV-2 incidents. That means critical systems going down, which directly impacts the business. Finally, data access control was complex and insecure. Our old method of per S3 object tagging was slow, expensive, and extremely hard to audit. This created both a security risk and a data access bottleneck.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1330.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1330)

This is where we saw a great opportunity to partner with AWS and  adopt Amazon S3 Tables. We made a decision to go all in on Amazon S3 Tables earlier this year, and the business impact we have seen has been transformative. First, simplified data governance. A task that had taken us weeks to complete, reclassifying more than 600 million data lake S3 objects, can now be done in less than a minute. This makes our data access story more agile and secure.

Next, increased productivity. Remember that one-day manual onboarding time per team? We reduced that to under 10 minutes. Our internal customers can get their data into the lake in Iceberg format and start delivering value immediately. We also lowered cloud costs by transitioning to the AWS managed service for expert maintenance that Amazon S3 Tables provides. We're seeing over 10% annual AWS cost savings. This is a direct hard dollar saving that goes right back to our bottom line.

On this note, as Aditya just mentioned earlier in this session, we will soon be able to leverage the newly launched Intelligent-Tiering feature of Amazon S3 Tables, which will continuously drive down our total cloud spending. I'm super excited for that. And finally, we freed up our platform teams. Remember that 2,000 developer hours of toil? Now we've got four developer months back to our team every year. That's four months of building and innovation, not just keeping the lights on.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1430.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1430)

### Indeed's Migration Strategy: Three-Phase Approach and Business Impact

So how did we make this migration  happen? We achieved this through a deliberate three-phase approach. The key here was to do the migration without disrupting the business. Step one is to audit and prioritize. We analyzed the data lake query logs to characterize them by technologies and access patterns, and also identified the most business-critical workflows to migrate first.

Step two is to begin the migration incrementally. This was the most critical part. We built a dual-write pipeline to convert data from Hive to Iceberg in S3 Tables buckets so that we could validate everything without any user interruption. I will show you exactly how we're doing this in the next slide. And the final step is to automate and optimize. We have built automation tooling so that all new data sets are migrated directly to Iceberg format and as S3 Tables by default.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1490.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1490)

To dive deeper into how exactly we're  incrementally carrying out this migration, here's a visual diagram on how we are dividing our entire data lake into multiple phases, cohorts, and batches. From the very top, we have approximately 86 petabytes of data in the lake today.

We have divided it into two major phases. The first phase involves migrating Hive tables, which are currently in the ORC file format, into Iceberg Parquet and S3 Tables, where we have approximately 67 petabytes of data. The second phase is where we already have the data in Parquet format, but it is stored in a general-purpose S3 bucket, which contains 19 petabytes.

Out of the first phase, we have further divided it into multiple cohorts, and the very first cohort that we are currently focusing on is to migrate tables that were not being accessed by any custom Spark applications so that we don't need to worry about having each customer update the permissions on each of their jobs. Moving one layer below, finally out of the first cohort, we are incrementally migrating data in batches. We first migrated a handful of POC datasets to prove out that everything works end to end. Then we moved on to the first production batch, which is about 10 petabytes, and then moved on to batch two and three and onward until we finish the entire cohort. In this way, we can achieve business value incrementally without disrupting customers.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1590.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1590)

So what did we learn?  First, streamline access control. A major benefit of S3 Tables is the ability to simplify data lake security. Instead of managing access control by tagging billions of individual S3 objects, we can now use resource policies on S3 Tables themselves. This approach facilitates the enforcement, auditing, and ensuring uniform access controls for the entire data lake, and eliminates potential failures resulting from incorrect object tagging.

Second, prepare for Lake Formation permissions when using Amazon S3 Tables with AWS Glue. Currently, access is not routed through standard IAM policies. Instead, it is managed by AWS Lake Formation. Before migrating, plan to update all identities that require read or read and write access to the data lake. Lastly, proactively audit and update query patterns. Be prepared for potential query performance issues, especially when using query engines like Trino or Spark. If your current system relies on custom plugins or optimizations that rewrite queries to fit your old data format, such as non-standard partitioning strategies in Hive, these optimizations will likely need to be refactored during the migration.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1670.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1670)

 As of today, we are still relatively early in this entire migration journey, but we have already seen some real-world business impact from Indeed partner teams as they get onboarded to this migration. Here you can see some quoted business impact numbers from MDNA, or the Marketplace Data and Analytics team at Indeed, which is a team that directly builds data products that help people find jobs. Look at these numbers. The HA Insights team is seeing reports that are 75% faster. The Smart Sourcing team achieved a 65% cost reduction. The Indeed Interviews team saw an 88% reduction in complexity, and the Partner Analytics team has delivered 98% improvements in their SLOs.

With all this great business impact we have seen so far, I am super excited to see what's coming next. We believe that Amazon S3 Tables can significantly help us in this migration journey as we continue to scale. Now I will hand it back over to the AWS team, where they will share some cool tips and best practices using S3 Tables along with a demo. Thank you.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1750.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1750)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1780.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1780)

### Best Practices for S3 Tables: Partitioning, Compaction, and Maintenance

 Thanks, Min. Since we launched S3 Tables at re:Invent last year, we have spoken to a countless number of customers about various performance problems and cost problems, and we have learned a lot. I want to share with you quickly a couple of these things that we have learned throughout the year.  I am going to cover four areas. The first is Iceberg partitioning, and the other three things are the three maintenance types. We are going to cover compaction, snapshot management, and unreferenced file removal. Let's go through each of these to understand how they impact performance and cost.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1790.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1790)

 Starting off with partitioning, choosing the right partitioning is super important because this is the thing that enables query engines like Athena and Spark to prune data when scanning the table. This improves our query performance and of course reduces our query costs.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1810.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1810)

 Now Iceberg has a bunch of different partition schemes. The two of the most popular ones that we see are time-based partitions and hash bucket partitions. So let's say for example you're streaming application logs into your table. Typically with application logs, you want to be able to query using some sort of a timestamp, so in that case it makes sense to use time-based partitioning. But suppose you're more interested in querying based on some sort of a customer ID. In that case, creating a hash bucket partition on the customer ID column probably makes a lot of sense.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1850.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1850)

Now, while choosing the right partition for your table is a super important  decision, the nice thing about Iceberg tables is that this is not a one-way door. So with Iceberg, if you define a partition, you can always change it later, and so that allows you to change your schema as your data evolves. So if you make a suboptimal decision in the short term, you can always make the right one in the long term. I really encourage anybody who is just starting out with Iceberg to really think about how they are going to query their data so you make the right partitioning decision early on.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1890.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1890)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1910.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1920.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1920)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1930.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1930)

Okay, so moving on with compaction. So the purpose of compaction is to combine smaller files together into larger files for better query performance. The compacted file size is controlled  by a setting called target file size. We recommend keeping this at default because from our testing experience, changing it doesn't seem to materially impact performance for most customer use cases. We also have four compaction modes to choose from.  We've got auto, bin pack, sort, and Z order.  We recommend for most customers to just keep it on auto because this will, the way it works is that if it's set to auto, which is the default,  it's going to use sort if you have a sort order defined on your table on a specific column. If you don't have a sort order defined on your table, then it's just going to use bin pack.

The one thing that you should consider though is using Z order if your queries are filtering on a specific set of columns. So for example, if you have a products table where you want to filter based on size, type, and location, so multiple columns, in those cases you might want to consider Z order because it could improve your query performance. But for the vast majority of customers, just keeping it on auto is the optimal thing to do.

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1960.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1960)

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1970.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1970)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1980.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1980)

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/1990.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=1990)

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2000.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2000)

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2010.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2010)

Okay, moving on to snapshot expiration.  If you're unfamiliar with Iceberg snapshots, this is the thing that allows you to query your table at different points in time. The first thing is there's a maximum number  of snapshots field. This is basically the maximum number of snapshots that our maintenance system keeps for you.  For this one, just keeping a default of 120 seems to work for a vast majority of customers. The second setting is the maximum snapshot age in hours. So this is how old your snapshots  are. Now for this one, for batch ETL workloads, we recommend that you keep it at the three day default. Seems to work fine.  Now for streaming workloads though, we recommend that you reduce it to 24 hours or less, and the reason for that is because a high number of snapshots actually degrades your query performance. 

And the way to really think about this is that for streaming workloads, they commit often to the table and each time they commit to the table they create a snapshot. And if you commit very frequently, let's say once a minute, then the number of snapshots grows into the thousands, and all of these snapshots are stored in your root metadata file. And so we've seen in some cases where this metadata file grows to tens to hundreds of megabytes. And at that point, because your query engine has to read the root metadata for every single read, you're basically reading and writing 100 megabytes for every single commit and read, and that is super inefficient. So reducing this down to 24 hours or less is recommended for streaming workloads, especially high frequency streaming workloads.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2070.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2070)

Okay, so last one on this topic is the unreferenced file removal. So this saves you storage costs by cleaning up failed commits. We recommend that you keep this at the three day default.  There are very few cases where you may want to consider reducing this, like if you have a workload that fails a lot, so in other words, you get a lot of commit failures which can accumulate a lot of garbage in your table. But honestly, in those cases you might want to consider reducing it, but I would really suggest that you keep it at the default of three days.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2100.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2100)

So just to recap our recommendations. For partitioning, definitely spend some time thinking about this one if you have a large data set,  because data pruning is key to achieve good performance at scale. If you're sorting, take a look at the order, but it's not a must.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2110.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2110)

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2120.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2120)

and it is something that you can change later.  For streaming tables, I strongly recommend that you reduce it from the default. Otherwise, keep it at the default if you're doing batch ETL.  And then lastly, for reference file removal, just keep it at the default. It's good enough.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2130.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2130)

### S3 Tables Replication: Simplified Cross-Region Data Protection

Right, so I've just gone through a bunch of tools,  and I am proud to announce that we are introducing a new tool in your toolbox, which is called S3 Tables replication. So let's take a closer look at what this tool can offer us. The first question is, why would you want to replicate your data, right?

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2150.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2150)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2170.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2170)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2180.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2180)

So the first reason is for performance.  Imagine you have a group of data scientists in Singapore, but your data is in Virginia. The performance is going to suffer because of that cross-region latency, so you might want to replicate your data closer to where it's being queried for more optimal performance. The second, of course, is compliance. If you're in a regulated industry like healthcare or financial  services, these industries typically have redundancy requirements. And then third is, of course, data protection. Just for your own sake, you might want to keep a secondary  copy to protect against accidental deletes or malicious overwrites.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2200.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2200)

So if we had to solve this problem ourselves, how would we do it? One way that we could do it is if we have our Iceberg table on general purpose S3, we could use S3 replication to replicate all the data.  But the problem is that the data isn't enough. The first problem is that metadata files need to be rewritten. Every manifest file in Iceberg contains absolute paths to the objects. So when you replicate these manifests to a different bucket, these paths will be wrong, and we need to correct them. We have to basically parse out all the metadata, change all the file paths, and persist them back. It's quite a bit of work.

The second problem is that the catalog that we are using has to be manually updated. So if you are using something like Glue catalog, this Glue catalog needs to be updated for every single new commit at the destination after we have replicated it. And then the third is that we have to maintain all of these complex integration and orchestration logic so that you have to redrive failures, track table states, and then ensure everything is durable end to end.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2250.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2250)

So if we were  motivated enough to actually go and solve this problem, then this is what this replication architecture would look like, and this is an actual example that we looked at from a customer and what they're doing. Suffice to say, this is a lot of work that you probably don't want to do, and if you're using S3 Tables, you don't have to.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2270.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2270)

So what does S3 Tables give you and how does it work?  The first thing it does is that it mirrors the name of your table and your namespace resources. So if you have replication configured, you're going to have the exact same namespaces and tables in both table buckets. The second is that we replicate all your data. When you enable it on a table bucket that has existing data, that's okay. We're going to backfill that data into the replica and then we're going to continuously update it as new commits land in the source.

Third, we are of course Iceberg-aware, so we handle all the data files, all the manifest files, schemas, partition specs. We maintain the correct snapshot history for time travel and audit. And fourth, the replica that you create is read-only and it's immediately queryable, so it works out of the box with all of the compute engines, you know, Athena, Spark, Trino, Flink. You get the same query semantics as the source, time travel works, schema evolution works, everything works as you expect.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2330.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2330)

 And so with this, you first get operational simplicity, so everything is managed. You can configure replication through the console in a couple of minutes. You don't have to manage any Lambda functions, no EventBridge rules to set up. We handle catalog updates for you. When your schema evolves, both catalogs update. When you add partitions, both catalogs stay in sync, so you get the same operational simplicity that you have with S3 replication today. And then the second thing is scale, of course, and it's the fact that it's an Iceberg-tailored experience out of the get-go.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2380.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2380)

So we're going to zoom in into the simplified operations front. When you're using S3 Tables replication,  there are four key capabilities that we give you out of the box. So the first is that we offer flexible replication scopes. What this means is that you can choose to either replicate all tables or just some of the tables, and this is analogous to choosing what prefix you want to replicate if you're using something like a general purpose bucket. The second thing you get is built-in audit trails. So for every replication action, you get a CloudTrail log.

This includes information like when a table is replicated and to which region, so your compliance team gets a full audit history. The third feature is real-time monitoring. We have a special API that tells you exactly what is happening, so you get to answer questions like, is replication complete? Is it pending? Is it failed? If it failed, then why? You can integrate this with your monitoring dashboards, alerting system, and operational runbooks.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2440.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2440)

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2460.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2460)

The last capability is really cool, and it's the way that we actually protect your snapshots. I'm just going to demonstrate that.  Suppose you have table replication already configured. Here we have Table A. You've got the primary on the left, you've got the replica on the right, and at this point they're perfectly in sync. We have four snapshots each. Now suppose your developer or a developer on our team accidentally runs a script that deletes all data.  Then it commits an empty metadata with an empty snapshot history. This is kind of like the worst-case scenario because all of our snapshot history is gone, and now we have this empty metadata that points to no data.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2480.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2480)

What's going to happen is if we have replication enabled, this empty table state is going to get replicated.  But instead of destroying all the snapshots in the replica, it's actually going to merge it. This is a really interesting property because what this means is that you will always have, if you have replication configured, your full snapshot history available in your replica irrespective of what happens in the source. If somebody makes a mistake that destroys all your data, that's okay. You can always recover in the replica. The same applies to anything like if your account gets compromised and they mess with your table. That's okay. You can always recover.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2520.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2520)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2540.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2540)

Just to close out on this topic, just like replication,  we let you configure your replica differently from the source. So you can change your replica's storage class, retention, or encryption, and this helps you meet your performance, compliance, and data protection requirements. Just to close out on this with this last  statistic here, 150 petabytes. This is the amount of data that we currently replicate using S3 replication each week, and this is the same exact infrastructure that's now powering S3 Tables. We are super excited to see customers replicate petabyte-size data lakes with tens of thousands of tables without having to provision capacity or manage any compute. So do please go ahead and try this feature out. We are super excited now that it is out.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2570.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2570)

### Building with DuckDB and S3 Tables: A Natural Language Query Demo

 All right, so the last topic that we are going to cover for you today is for the developers in the room. Now one of the things I love about working on S3 is that it has this really rich history with the developer community because it offers simple APIs for developers to use. It really is kind of like the storage for the internet. There's just a ton of websites out there that all they do is they just write data to S3, and we really want to bring some of that experience to S3 Tables with Iceberg.

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2620.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2620)

What I'm going to do is highlight some of the newest technologies for building Iceberg applications on top of S3 Tables, and I'm actually going to do a demo. The story here is when I'm not working on S3, I'm also running a small business coffee shop called  Lambda Lattes. As part of my job as a business owner, I want to analyze customer trends so that I can better drive sales, do promotions, et cetera. I wanted to build an easy tool for myself that helps me with this task.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2640.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2640)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2660.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2670.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2670)

What are my requirements? I started off using  DuckDB. Anybody use DuckDB in the room? Anybody? Okay, a few hands, nice. I'm a huge fan. For those that don't know, DuckDB is this nice CLI utility that you can use to query tables with a really simple experience. I wanted to use DuckDB, but the issue is that I wanted something where  I didn't want to write SQL, basically. I wanted no SQL. I wanted to use natural language. I wanted something that's accessible from the web. I didn't want to have to install DuckDB on my local machine,  and I wanted to have a fully serverless experience.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2690.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2700.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2700)

How did I do this? First of all, I built a web app using React. This React app features a natural language interface for querying S3 Tables. I use Amazon Bedrock to convert  user prompts to SQL, and I take that SQL and I use the DuckDB WebAssembly client to actually query tables directly from the browser.  So here's what that architecture looks like.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2710.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2710)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2730.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2730)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2740.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2740)

We've got the user. The user issues a prompt to my React app, and inside of the React app, which runs inside the web browser,  it sends that prompt to this agent component that I wrote. Now this agent component is dead simple. It's just a bunch of JavaScript that calls Bedrock in a loop using the AWS JavaScript client. So I send my prompt to Bedrock, and in return I get back SQL.  And so I take that SQL and then I use my DuckDB WebAssembly client to send requests to S3 Tables to access the data. I get the data and then I render it on  screen.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2750.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2750)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2760.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2760)

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2770.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2780.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2780)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2790.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2790)

And so I'm going to show a demo with this. So I start off by just selecting my table bucket. Here I've got my Lambda Lattes  as my table bucket, and now I can begin to issue prompts. So I ask, what are my tables? This sends a request to DuckDB.  We can see that we have two tables. We have customers and orders. So then I say, okay, show customers. This now sends a request to Bedrock,  which returns a select star from customer table. Here's my customer table. I do the same thing for my orders table. And now that we have seen both tables,  we can now do more interesting queries like who are my top spenders. Bedrock is going to figure out, hey, I actually have two different tables and I can do a join between them given their columns.  So it just figures that out. Here I can see that Bob has spent $15. He's my top spender.

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2810.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2810)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2820.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2820)

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2840.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2840)

So what does Bob typically drink? Looks like Bob is clearly addicted to mocha. Okay, that's interesting. So then what's  our revenue per drink? Looks like it's mocha, which is interesting. It's interesting that mocha is at the top,  given that I specialize in lattes. So what I want to do now is actually drive some promotions. So what I do is I actually can upload a CSV and I can ask DuckDB to create a promotions table from this file.  And DuckDB has this really nice functionality to actually create a table from a CSV, and so it figures out all the different column names and just creates this table in S3 Tables. And then I can now of course query it.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2880.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2880)

So then I ask what is my total revenue loss from the drink promotions that I've created. This is probably one of the most complex queries that Bedrock processes in this app. So now it's going to figure out, oh, okay, so you have this promotions table. I can calculate that and it looks like the answer is $19.  But that's actually quite a bit, $19. I don't know if I can afford that. So what I want to do now is update promotions and I want to set the discount of all the items to 10 cents. I want to delete the mocha because I don't need to promote that, it's already popping off, and then I want to insert latte as a promotion because I really want to drive my latte sales, and then I ask it to recalculate promotion impact.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2910.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2910)

So now this is like a pretty complex ask. So it's going to go in and then it's actually going to figure out that we need to have multiple statements now to carry out this order.  So now we have four things. We have an update statement, delete statement, insert, followed by a select, and I've reduced my loss down to $1.30, which is what I wanted to do. Okay, so that's the demo.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2950.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2950)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2970.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2970)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2980.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2980)

What's next? So if you're interested in building something like this, something that's simple, something that's in the browser, some of the things you should try is the DuckDB WebAssembly shell at this URL. This is a really good way for you to kick the tires, to understand how it works. Secondly is just try asking Claude or a coding assistant of  your choice to generate a DuckDB WebAssembly application. I've tried a bunch of them. They all seem to work pretty well. You can basically just get started really easily. Most of the LLMs, they understand DuckDB and they understand how to use WebAssembly, so try it out. And if you get that working, then you can  just connect it to S3 Tables using the attach command, and if you get lost, you can always refer to DuckDB documentation for details. 

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/2990.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=2990)

So I personally think this is like a super exciting new way for developers to build applications and enable new experiences on the web.  And that's basically all we have for you today. Just some takeaways. So with S3 Tables it gives you a fully managed Iceberg storage, and with some of the recent launches we have more customers like Indeed can get the best performance and cost for their data lake investments. You've got multiple levers for you to tune your data lake operations based on your specific needs, and you can try out a number of Iceberg compatible tools such as DuckDB, such as PyIceberg, as well as the AWS services like SageMaker Unified Studio.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a147240384615f2d/3030.jpg)](https://www.youtube.com/watch?v=Pi82g0YGklU&t=3030)

A few additional resources, so we've got a tutorial. We've got a workshop, and then we also have a few additional  SageMaker Unified Studio links for you to check out. And thank you. Please do complete the session survey in the mobile app so that we can do these sessions in the future again. Again, thank you for coming and have fun at re:Invent if you go.


----

; This article is entirely auto-generated using Amazon Bedrock.
