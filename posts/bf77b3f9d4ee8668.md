---
title: 'AWS re:Invent 2025 - Learn how Riskspan is transforming private credit market using GenAI (SMB205)'
published: true
description: 'In this video, Suhrud from RiskSpan discusses how they transformed the $14 trillion Private Credit market using AWS generative AI. RiskSpan processes trillions of dollars across 45 million loans daily. They reduced deal evaluation time from 3-4 weeks to 3-5 days, achieving 10x scalability and 90x cost reduction at under $50 per analysis. Using AWS Bedrock with Claude and RAG architecture, they automated extracting data from unstructured documents. Key learnings include providing domain context to LLMs, breaking problems into semantic chunks, and using structured JSON formats. They employed the AWS PR/FAQ process before coding and are now transitioning to AgentCore for agent orchestration, achieving 87% faster customer onboarding while unlocking the $9 trillion untapped market.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/0.jpg'
series: ''
canonical_url: null
id: 3086615
date: '2025-12-05T13:41:50Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Learn how Riskspan is transforming private credit market using GenAI (SMB205)**

> In this video, Suhrud from RiskSpan discusses how they transformed the $14 trillion Private Credit market using AWS generative AI. RiskSpan processes trillions of dollars across 45 million loans daily. They reduced deal evaluation time from 3-4 weeks to 3-5 days, achieving 10x scalability and 90x cost reduction at under $50 per analysis. Using AWS Bedrock with Claude and RAG architecture, they automated extracting data from unstructured documents. Key learnings include providing domain context to LLMs, breaking problems into semantic chunks, and using structured JSON formats. They employed the AWS PR/FAQ process before coding and are now transitioning to AgentCore for agent orchestration, achieving 87% faster customer onboarding while unlocking the $9 trillion untapped market.

{% youtube https://www.youtube.com/watch?v=tgnyLTV5h1s %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/0.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=0)

### RiskSpan and the Private Credit Market: A $14 Trillion Opportunity Built on AWS

 Today, we're going to understand from Suhrud what RiskSpan does, what the Private Credit market is, what decisions they made in their generative AI journey on AWS, and finally, what considerations to take into account when you want to take this application into production in a customer-facing capacity. Suhrud, thank you for making the time. I'm incredibly privileged to have you here on stage with me presenting this story.

I want to start us off by telling our audience a little bit about RiskSpan and maybe even defining what Private Credit is for us. Thank you, Subhash, and thank you, AWS, for this amazing opportunity. RiskSpan is a technology company that provides analysis for some of the most complex investment asset classes. We are native to AWS and built our entire platform on AWS. As a result, we are 100% cloud-based and run very large workloads on very complex assets. We run trillions of dollars of assets on a daily basis, performing very granular loan-level analysis across 45 million loans and 70-plus asset classes. We've been partnered with AWS for 15 years now, and to reiterate, we wouldn't exist without AWS.

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/10.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=10)

 Regarding Private Credit, when most of us go get a loan, which is credit by definition, we go to a bank. But when a non-bank entity provides a loan to individuals or companies, because those loans are non-standard, that's where private credit comes in. It's a huge market with 14 trillion dollars of assets out there and 9 trillion dollars of untapped assets. Why is that the case? Definitionally, every investment is custom. Every investment looks different, and all the information about the investment is buried in documents, emails, and a lot of unstructured data that needs to be modeled, analyzed, and priced.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/60.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=60)

 Today, analysts build Excel spreadsheets, which, as we all know, doesn't scale. What it does is create an opaque market with very little transparency on a regular basis. Portfolio managers are now managing trillions of dollars of these assets and need to react to changing market and investment opportunities quickly. This looks like a pretty massive opportunity with 14 trillion dollars and a lot of data. What are the challenges? Can you walk us through this because you talked about investors of all kindsâ€”large net worth individuals, institutional investors, or anyone accessing private credit? What's the complexity in providing what they need?

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/110.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=110)

 It's hard to imagine that there's so much capital being invested and at risk where the information is buried in these documents. In most cases, there isn't just one single 200-page document. There are 17 to 20 amendments that occur, which makes it very difficult to extract all the information in a structured format that somebody can act on. As a result, to reach a point where you are comfortable and know what investment you're getting into, it takes about three to four weeks to read through documents, extract the data, put it in Excel, and create an Excel model that you're comfortable with.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/190.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=190)

 The growth today is 1.7 trillion-plus over the last two or three years. Investors want to participate in this investment and need to react very quickly. Sometimes they have less than 10 minutes, at most half an hour, to decide if they want to pursue an investment. Otherwise, they'll commit to spending three or four weeks and still not be competitive. There have been two very major incidents in the private credit space where there were billions of dollars of losses because investors didn't have access to the data and couldn't monitor the investment on a regular basis.

This looks like a complex problem to solve when you have a lot of data, mostly unstructured, that typically takes a long time to process through a fairly manual process. However, your investors are looking to make decisions fast, and that's the challenge. So how do we take this complex problem and solve it in a way that investors can actually make the right decisions based on all that information in those deals?

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/330.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=330)

### From Vision to Architecture: Using AWS PR/FAQ and Bedrock to Achieve 90x Cost Reduction

Tell us a little bit about where you started, because it seems like a fairly complex problem. Talk a little bit about your journey with AWS. We are all technologists at heart. We want to solve cool problems and build cool technology. Working with Subhash's team over the last several years, we've learned a lot of cool techniques to help us build something that was relevant  and relevant to the market, and something that addressed the problem.

Using the AWS PR/FAQ Process, we wrote a press release with our cross-functional teamâ€”sales, marketing, executive leadership, technology, and even some third-party external clientsâ€”to tell clients what they are going to get and what we are building. We combined that with a frequently-asked-questions list, an FAQ, which answered hard questions. This allowed us to focus on that $9-trillion uncapped opportunity instead of reinventing the wheel and building yet another system that didn't solve the problem. Most importantly, we didn't write a single line of code until we had our vision defined and had a customer-centric solution.

Yeah, that's pretty impressive. How many of you here are familiar with the AWS PR/FAQ mechanism? I see a few hands, mostly Amazonians. PR/FAQ stands for a Press Release and Frequently Asked Questions. It's a mechanism where, before we start to invest any development time or dollars or resources, we actually write a press release about how the product is going to show up when it's released. Think of it like a one-pager, an actual press release, where we're talking about the customer. Who is the customer? What is the pain point? And how does the solution address that pain point?

The FAQ is the detailed section where you're putting yourselves in the shoes of multiple stakeholdersâ€”sometimes technical, sometimes business, financial, sometimes the customer themselvesâ€”and answering questions from their perspective on how the solution would impact them and their teams. It's a comprehensive process, and as a customer, this is something that we offer for free. You can work with your Amazon account teams to go through this process. The beauty about this is it's an incredible tool to drive clarity and a consistent vision across multiple functions and multiple teams within your companies. It's something that helps us center our strategy around the end customer because the PR/FAQ focuses a lot on identifying who is the customer, the pain point, and how you're effectively addressing that pain point.

So, the PR/FAQ process, once you go through it, establishes the vision, and now the next part is to move to an architecture. Talk to us a little bit about what were the considerations while establishing an architecture, and how does the architecture look today? In some ways, this is a very standard architecture. You have data ingestion, you have a model, and then we are making the results accessible to end users. It's kind of a standard analytic calculation, but what was complex here was that the data was coming in a variety of formats and the problem was custom. Every deal was going to be custom, so we needed intelligence to not only come up with a final solution for project forecasting, cash flows, and value for these investments, but we also needed to handle unstructured data at large scale.

We use secure storage because confidentiality was key to our clients. We use tools to extract data from scanned PDF images, unstructured emails, handwritten notes, and so forth. We ran a lot of data through this process, and there was no way that Claude, which is the LLM in this case, was going to be able to process that volume. So we needed embeddings and a RAG solution, which we were able to leverage through Bedrock. Having the information flow through in a form that our legacy APIs could take those forecasts and give clients a seamless experience across all their investments was the last layerâ€”the API layer.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/620.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=620)

We built on top of the API layer, which already existed. The impact of that has been quite incredible. We use serverless technology for running the final code that Claude generates.  Claude dynamically generates code to model the investment waterfall logic, and we were able to hand-scale endlessly. The cost through automation was amazing. The AWS services we used were very cost-effective, and the total cost was less than $50, including the human in the loop involvement.

This represents a pretty impressive impact with 10x scalability, but more importantly, a 90x reduction in cost. For those of you who have worked with Generative AI applications, the typical perception is that it will drive up costs. However, in this use case, we are seeing a tremendous amount of efficiency and automation built into the architecture, which is what helps us drive that cost advantage.

### Production Success and the Future: Overcoming LLM Challenges with Agentic AI and AgentCore

Now, Suhrud, one of Gartner's recent observations is that about 90 percent or more of Generative AI projects die a slow death in the pilot phase and do not make it to production. You are one of those customers who have had tremendous success taking this architecture to production and monetizing it. Talk to us about that journey to production, putting this application in production. What are some things that you had to really focus on to get it right at scale in a customer-facing application?

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/720.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=720)

I'll start by saying that we made a lot of mistakes. We walked into it thinking that we had the magic bullet with LLMs. We would feed all the data and out would come the code, because that is what you get when you sit in front of ChatGPT or the Claude Web UI, which is clearly not the case.  One of the learnings was that LLMs are trained on vast amounts of data, but they are not specifically trained on our domain or anybody's domain at the granular level that we understand the domain to be and the expertise we have.

Taking the LLM and providing the appropriate context and domain knowledge for the various deals and structures that we are modeling was one key learning we had. This was also done to avoid hallucination and create deterministic output to the extent possible. Similar to that was breaking down the problems by creating semantic chunks from the document and solving individual aspects of a transaction. For example, this is the context to calculate fees, this is the context to calculate interest income, and this is the context for how you deal with delinquencies and defaults. Breaking that down into chunks was a key learning we had early on.

Finally, something that we as software developers have known but was initially lost, and we got back to it pretty quickly, was providing structure. LLMs perform much better when you give data in a preset-structured format. For the code generation aspect especially, we created a well-defined JSON structure that the LLMs would consume, and the output was also provided in a very structured format. That last bit probably helped us the most in terms of accuracy, reliability, and robustness of the solution.

Thank you for sharing that. I think to summarize, first, everyone has access to the model. It is not the model that will differentiate you; it is your data, the context, and the business context you provide that will be differentiating. Second, to expect the LLM to solve a large problem by itself, even with the best prompts, is unrealistic. You want to break it down into smaller problems, think microservices, think specific solutions, and start to improve on accuracy, latency, cost, and other considerations.

Last but not least, the more structured both your data and your inferences are, the better the LLMs perform. Putting in the work to do data wrangling up front and getting it to an LLM in a structured format does help. Now you have this application in production, you are monetizing it, however, you understand that the technology is changing incredibly fast underneath you. So what does the future look like, and what is the vision for RiskSpan going forward?

I'll start by saying that by no means have we solved the problem completely.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/910.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=910)

There is a lot of work to be done  because the Private Credit space is constantly evolving. The need for custom structures comes about all the time, and the need for real-time risk is pretty key. We laid out a vision of what we wanted to do to make the ability to create a model more robust, more deterministic, and help our clients solve more forecasting-related issues.

We wanted to help our clients create structures more dynamically based on market conditions. Instead of taking a legal document and then modeling the deal, we wanted them to come and say, "Well, in this market, what type of structure would we need?" The few things that we were focused on lead to the punchline, and the punchline is AgentCore. AgentCore solves almost every one of the things that we had planned to build.

Now we are going to figure out where to spend our time next, but having multiple agents that solve very specific problems has been transformative. We were doing that by using prompts, but creating agents and having agent orchestration is helping us tremendously. We are seeing benefits of that already. Pairing up with our SMEs to provide prompts for those agents and giving them the ability to create those agents more dynamically has helped tremendously.

We evolved our RAG approach a lot more to manage these documents in a more semantic way and in a more reliable way. Finally, continuous learning, which the dynamic prompt and agent orchestration have helped a lot through the continuous learning aspect. We have attached a couple of screenshots here to show that this is actually real.

Yeah, I absolutely love that you are starting to transition the architecture to much more of an agent-based approach where we talked about breaking the bigger problem into smaller chunks with a set of agents that are individually focused on a task, driving much better accuracy. Having a platform like AgentCore that can orchestrate all these agents to work together while continuing to maintain that security and compliance piece as they work is crucial.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bf77b3f9d4ee8668/1090.jpg)](https://www.youtube.com/watch?v=tgnyLTV5h1s&t=1090)

Getting to an end state where your investors are able to upload a portfolio and quickly evaluate it in real time and make an investment decision is the grand vision, and certainly you are on the right track. You have driven tremendous impact in your business. It is fair to say that some of the outcomes  that you have achieved are fairly transformational. Talking about 87% faster onboarding for customers and taking the processing time for a deal structure evaluation from three to four weeks to about three to five days is pretty impressive and significant.

While you are increasing your capacity, you are also lowering your costs. I love the fact that you are on path to unlocking this nine-trillion-dollar market. I want to say thank you for giving your valuable time, sharing your perspective on your Generative AI journey, and thank you again for partnering with us and giving us a part to play in your journey. Suhrud, thank you again for the time. Thank you, everyone.


----

; This article is entirely auto-generated using Amazon Bedrock.
