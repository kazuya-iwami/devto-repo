---
title: 'AWS re:Invent 2025 - Build Enterprise AI Apps Faster: Amazon Bedrock''s Multimodal Solutions -AIM3341'
published: true
description: 'In this video, AWS Principal Product Manager Anushri Mainthia and Air CTO Tyler Strand demonstrate Amazon Bedrock Data Automation (BDA), a unified API for processing multimodal content including images, documents, video, and audio. The session addresses the challenge that 80% of enterprise data is unstructured, with only 20% of organizations able to leverage it at scale. BDA eliminates the need for complex orchestration across multiple services by providing standard and custom output options through natural language blueprints. Tyler shares Air''s journey managing 8 petabytes of creative content for 2,500+ businesses, processing 15.5 million jobs since deploying BDA. The demo showcases video analysis detecting chapters, logos, transcripts, and custom smart tags through a single API call. New features include synchronous image processing, multi-language support, speaker diarization, and upcoming custom entity recognition capabilities across 15+ AWS regions.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Build Enterprise AI Apps Faster: Amazon Bedrock's Multimodal Solutions -AIM3341**

> In this video, AWS Principal Product Manager Anushri Mainthia and Air CTO Tyler Strand demonstrate Amazon Bedrock Data Automation (BDA), a unified API for processing multimodal content including images, documents, video, and audio. The session addresses the challenge that 80% of enterprise data is unstructured, with only 20% of organizations able to leverage it at scale. BDA eliminates the need for complex orchestration across multiple services by providing standard and custom output options through natural language blueprints. Tyler shares Air's journey managing 8 petabytes of creative content for 2,500+ businesses, processing 15.5 million jobs since deploying BDA. The demo showcases video analysis detecting chapters, logos, transcripts, and custom smart tags through a single API call. New features include synchronous image processing, multi-language support, speaker diarization, and upcoming custom entity recognition capabilities across 15+ AWS regions.

{% youtube https://www.youtube.com/watch?v=SrB-LIAIx5I %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/0.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=0)

### The Multimodal Content Challenge: Why 80% of Unstructured Data Remains Untapped

 My name is Anushri Mainthia, and I am a Principal Product Manager at AWS. My name is Tyler Strand, and I'm the Co-founder and CTO of a startup called Air. Good morning, I am Nini. I am a Senior Product Manager at Amazon Bedrock, and with my colleague Anushri, we're going to talk to you about how to work with multimodal content and what type of multimodal solutions AWS has. Nini and Tyler will be helping to share that story, but we're really excited to have you all here today.

Before we get started, I want to get a quick show of hands. How many of you deal with some sort of multimodal contentâ€”images, video, documentsâ€”just show of hands as part of your function? Awesome. How many people work with documents? Okay. Video and images, media content? Audio call centers, that kind of thing, conversations? Okay. And how about all of them, where you have to figure out how to manage and deal with all of them? Got it. Awesome. It's a challenging problem.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/50.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=50)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/80.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=80)

  So what we're going to do today, there are going to be about four things. First, we're going to talk about what that multimodal content challenge is, and you are all very familiar with it, given that almost everyone in the audience is dealing with some sort of content. Then we're going to talk about what Amazon Bedrock Data Automation is and how it aims to solve and resolve those challenges. Then Tyler from Air is going to tell us about their journey through how they manage multimodal content. And finally, Nini will share a few of the launches we've had and what's new and what's going on.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/120.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=120)

So let's get into the multimodal content challenge.  These numbers you're probably familiar with. If you're reading the screen, you're probably experiencing deep pain when you think about these things. But one of the biggest problems is that eighty percent of content is unstructured data. It's unstructured, multimodal content, whether it's in images or documents or videos or audio files that you have. And for any process that you're trying to build a solution for, say intelligent document processing, you're going to have to figure out how to get those insights into a usable format to power your downstream applications. Unfortunately, only twenty percent of organizations right now are able to take advantage of that at scale.

Why is it particularly hard? There are a couple of reasons. First, there's an incredibly large diversity of different types of multimodal content both across modalities. So for insurance claims, you're dealing with documents, you're dealing with forms that have been filled in. You're dealing with videos, you're dealing with pictures of a car crash, for example. You're managing all those different types of formats and files across modalities and even within modality. Within documents, a single organization could be managing three hundred different types of documents, all of which require different schemas.

Then you have the additional challenge of how you're going to get the accuracy you need at scale. That's the biggest challengeâ€”you're not dealing with one document here or a video there. You're dealing with hundreds of thousands, millions of these types of content that you need to process at scale efficiently. So how do you get that accuracy? It's a challenge. And how do you get that auditability so that you can go back and figure out exactly what is being used to generate insights, especially if they're generative AI derived?

There's a lack of standardized tooling, which means you have to learn multiple services and multiple interfaces. And finally, which I think is one of the most painful and the unsung heroes, you have to put it all together. You have to do the transformations, you have to mesh all that data together. You know this in terms of ITâ€”it is incredibly challenging to do that, and it's time consuming and tedious.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/240.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=240)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/250.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=250)

### Introducing Amazon Bedrock Data Automation: A Single API for All Multimodal Content

  So what have we done to try and help solve that and make it just a little bit easier? Well, hopefully many of you know what Bedrock Data Automation is. If you don't, I'll introduce it to you. Bedrock Data Automation is our way to help with multimodal content. Imagine a single API that allows you to process images, documents, video, and audio. Basically, what you do is you tell Bedrock Data Automation what you want, and we'll walk through how that works. Bedrock Data Automation does it for you. You don't have to figure out the orchestration, you don't have to figure out which foundation model you're going to use, and you don't have to figure out how to route things from one place to the other. All you have is a single API. You can send any modality through it, specify the output you want, and Bedrock Data Automation generates it for you.

In terms of what this means, it's simpler from an implementation perspective, especially as you move towards more and more multimodal use cases. In a previous era, you'd have documents run through one pipeline and videos run through one pipeline. You can have them run through the same pipeline now if the output is driving the same business process. So that becomes much simpler because you're interacting and you're creating that integration with one service.

BDA also allows you to tailor your output and enables customization. You can define your own schema, and if you have a downstream system or database with specific columns, you can define those and specify your normalizations and transformations through BDA. Additionally, BDA provides confidence scores and grounding in the actual assets, showing exactly where information appears on the page and the page number within the conversation. If you have an inferred insight, you know exactly what was used to derive it. If three inputs were used to calculate one thing, it will show you exactly what that is. This allows you to have easier human-in-the-loop reviews and auditability, enabling you to create your applications more efficiently.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/360.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=360)

### Five Key Use Cases: From Intelligent Document Processing to Agentic Workflows

What use cases can you use BDA for? An API is great, but it needs to power something.  There are five use cases that we see customers typically using with BDA. The first is one you would expect: intelligent document processing and intelligent document search. These include things like insurance claims and loan processing, where there are lots of documents and you need to process them quickly to generate extractions and insights.

Next is intelligent search analytics. Think call center analytics. It is not just the transcript, but also questions like what was the tone of this call? At what point was the customer unhappy? At what point did the agent convert this to a good customer call? If you can get those insights at scale instead of having to post-process and analyze the transcript, it becomes much easier. Media analysis and content discovery is another use case. You might have videos, and if you are an ad agency with hundreds of videos looking for that one ad from 1984, that Christmas ad or holiday ad, because you want to create a series of all the most amazing holiday ads and you remember it had a penguin in it, how would you get that? Well, imagine if you could process all your content, get generalized output and also specific output so you can search right for those assets. We will hear a little bit more about that with Tyler's journey at Air.

Finally, there are agentic workflows. Agents are something everyone is thinking about and how they can use them to make their processes more efficient. If you think about BDA, think of it as the tool in agent calls. Now you have a single tool that handles all multimodal content regardless of the modality. You can set up your agentic workflows to use BDA as that tool. There is an MTP server available that you can use today in this capacity. There are a few builder workshops happening later where you can practice and see how that works.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/470.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=470)

One API is great, but how does that actually work?  With BDA, it is quite simple. As part of the API call, you provide the asset. Along with that API call, you provide two things. You provide a project, which is where you specify exactly what you want. In it you can include a schema, bring your own schema, or choose from options and standard output that we provide. For every modality, it will be slightly different because we have optimized it per modality.

For example, with documents, you have the choice of whether you want your output in plain text, HTML, or markdown because we know systems interpret and perform differently. If you have a RAG system, some perform better with HTML and some perform better with markdown, so you can choose that output instead of just getting plain text. For videos and audio files, you can choose whether you want chapter summaries and what type of summarization you want. You can also choose whether you want to understand if there is content that should be moderated. You can pick and toggle those options.

There will be a set of standard options where you literally just have to click. We already have a default project set up if you do not want to make a choice, or you can provide your custom blueprint. Your blueprint is the schema you bring. You provide exactly what you want and what format you want it in, and we will do that. You send it to BDA, and BDA figures it out. It says, "I wonder what asset this is. Let me figure that out." It is an image file, but it is actually a driver's license, which is a document. So I am going to process it like a document. I see that you included a blueprint for driver's licenses, so I am going to return all the information you want. I am also going to give you the linearized output of that driver's license so you can ingest it and always have auditability. That is how BDA works.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/590.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=590)

### Live Demo: Processing Sponsored Content with Standard and Custom Blueprints

I think it might be helpful to see an example. Do you have one handy that you might be able to share?  How familiar are you with TikToks and Instagrams recently? Like reels or anything like that? You will know that there are actually a lot of sponsored content. There are advertisements embedded directly in the content of the video in short form. I am just going to show you that video.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/640.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=640)

I want you to think about a use case where you're managing a lot of different media assetsâ€”a lot of different short form content, long form content, and so onâ€”and you need to search for it. That's the use case I'm demonstrating. I'm going to pause here and show you the video. The key is to try to understand the context of the video and spot some ads, and then I'll show you how Bedrock Data Automation works with it. 

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/690.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=690)

Had a break and then to having the baby while the baby naps, I'm going to tidy up the house a little bit. It gets back, everything is clean. You feel at peace when she comes home. Yes, I come with my own supply. I never really heard the words "I love you" growing up, and I feel like it's a pretty synonymous experience among Asian culture. As an adult, I've realized that sometimes "I love you" isn't said, it's served, which is why I'm making meals and cleaning as a host. As a guest, it's always been part of my DNA. I think it's a balancing act  because sometimes I feel like we lean too heavy on our words and forget to actually show up. So I'm learning to say those words out loud, accompanying them with my acts of service so that my love could be heard and also felt.

For me, because culturally it's just not practiced often, the saying is what's hard, but I'm slowly getting there. I'm trying to break those generational habits, choosing both the full plates, the affirmation, the clean house, the soft "I'm proud of you," the acts of service, the language of presence. I just want people to know my love in every formâ€”what they see, what they hear, what they feel. And sometimes the loudest "I love you" isn't just served, it's spoken. No, I don't want. Good news, good news is mommy's back. Your toes look great. It's okay. I'm proud you took some time today. You deserved it.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/760.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=760)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/780.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=780)

All right, so it was a video about helping a mother take care of their kids while the mother goes out to get her nails done.  I'm going to bring you to the Bedrock Data Automation console, and you can actually get started very easily. You can go to "Try demo," which we'll do in a second. We have some blueprints, which are things called projects. We also have a sandbox where you can play with your RAG indexing, intelligent document processing, media analysis, and audio.  You can literally just go and try it out. But let's start with the demo.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/790.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=790)

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/800.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=800)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/810.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/820.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/830.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=830)

What I'm going to do is upload that same video  that you just saw from my computer, and you just click "Choose file" and then select the video and then go "Generate the results."  The key here is that it will take some time, but I'm going to show you standard output first. What is standard output? It's just a standard set of things that you can directly get out of the box  when you upload your video. Sometimes your video can be a little bit longâ€”it can even be two hours.  What we have here is we take the first five minutes and process that so you know exactly what you're about to extract. Then you can extract longer because this can all be done through an API. 

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/850.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=850)

I fast-forwarded it a little bit. You'll see in standard output these are some of the options. You have your summaries, you can look at the text, you can look at IAB taxonomies, content moderation. Here, I just chose the summaries to show what are some of the rich summaries that you get.  Remember, we're talking about a mother and her friend, and her friend is taking care of the kid while the mother is out. You'll see that in the video summary. There are a lot of things we talk about like postpartum and how to express love, all in that summary. You can see it. Then you have what we call chapters. There are four chapters in this video, and we just detected that out of the box. These chapters, think of them as scenes. I'll actually show you how we cut them up and what constitutes a scene.

So in the first twenty seconds of the video is an intro. I'll show you again when we've scrubbed to the first twenty seconds. The woman here introduces the context, and then it shifts to the bathroom. From the twenty-second mark to the one minute and ten-second mark, the creator is in the bathroom, talking about the product, and then it shifts over to the laundry room.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/950.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=950)

In each locationâ€”both the bathroom and the laundry roomâ€”she's using the Blueland product, which is the embedded sponsored ad. Then the last scene is when the mother comes back. We detect all these key moments in short form right out of the box. 

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/970.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=970)

Now, one thing I've noticed is where are all the logos? What you can do is detect text in the video. As we see here, we have a bounding box and a confidence score, and it detects that out of the box. Remember, all of this you can still do with just the API as well. We have one unified API. 

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/980.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/990.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=990)

I also want to get the text transcriptâ€”the audio transcriptâ€”right out of the box. And then I also want to get IAB taxonomy. So how do I do that? Just click a few buttons. I'm going to create a new project, which is where you store these output configurations.  In this case, a standard output configuration. And then I'm just going to call it my_first_project_v1. And I'm going to speed this processing up a little bit. Usually it takes about a minute. 

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1000.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1000)

And then we're getting the full audio transcript and IAB taxonomies. I'm going to scroll down a little bit.  We talked about parenting, family, and a few other things that you detect for your advertisers. And then here's the full audio transcript. We also have speaker detection. So in this case there was one speaker, but you can detect multiple speakers and then map that to the person that you want. That's pretty much standard output. You can play with it yourselves at some point, and that gets you out of the box.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1040.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1040)

But I actually want to go more specific. I actually don't like how verbose it was. I want to get more concise results. I want smart tags. I want better summaries. So what I can do is create something called custom output and go to a blueprint. I got started as simply as just clicking add field. 

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1050.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1060.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1070.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1080.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1090.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1090)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1110.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1110)

For the first one, I want my chapter summaries to be a lot more concise. I want the exact details. So I can say, let's do it for at least 30 words in this example, but no more than a hundred words.  And then I just specify that with natural language.  And then I want to get some chapter titles because previously it was chapter one, chapter two, chapter threeâ€”very generic. I want chapter titles.  So I want to specify how that looks. I say the granularity is at the chapter level. And then I want to create some smart tags.  These are tags that are easily searchable if I have a large media content library.  So I say these tags have to be very precise. They contain some marketing terms that allow me to search more easily. Then I want this at the video level. I don't want this at the scene level. I can do that too, but I want this at the video level. 

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1120.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1130.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1140.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1150.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1150)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1160.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1160)

And then finally, why don't we just wrap it up with a summary field? I want my summaries to be a little bit more concise. I want them to be more focused, without a lot of repetition. And you can shape that basically as a prompt.  These natural language instructions for every field is how you would go about it. Great, so I've added everything, and this is called a blueprint and custom output. Let's just call it blueland_demo_v1.  I'm going to click get results, and then I'll fast forward a little bit. It takes some time to process.  But you'll see some of the results. This exactly is the schema that you just custom built. And then some of the smart tags that we see here are starting to come out.  You even have #blueland in the smart tags. And this is allâ€”I did nothing special here other than saying give me some smart tags and some natural language instructions. 

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1170.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1170)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1190.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1190)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1200.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1200)

With each of the chapters, you'll also notice that the timing was exactly the same as in standard output. So it is deterministic. We won't give you different chapters all of a sudden.  And then the other part is the chapter titles. Right now they're more specific to the actual chapters that we were talking about. And then we can look at the different chapter summaries. Great, so I just walked you through standard and custom output on blueprint on Bedrock Data Automation.  So what did you have to do before to make this happen? 

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1220.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1220)

Previously, I had to build my own MLP task-specific models to detect certain objects or stitch multiple models together to make this work. This is something you no longer need to do with Bedrock Data Automation. 

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1260.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1260)

Videos contain audio, text, and visual signals, and many things happen within them. When we were talking about a scene in the bathroom and then in the laundry room, you could do that with the visual model, but on the audio side you might miss the context. What Bedrock Data Automation does is synthesize that together out of the box. There's also summarizing the content, which is becoming easier with foundation models, but you would have to bolt that on and try to optimize the price-performance. 

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1280.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1280)

There are also all your different entities to consider: detecting logos, detecting objects, detecting people, and detecting scene mood. There are many things that you might want to detect as an entity in the actual video, and now this is something you can do straight out of the box. 

Even if you have a beautiful stitched-together machine, you have to orchestrate everything, maintain it, and keep up with new models and features. You have to keep up with the API documentation on how that works, and you have to continuously balance price-performance over time. Customers always want better costs, better accuracy, better performance, and better latency. That is all something you need to continuously balance. With Bedrock Data Automation, we manage that for you out of the box.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1320.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1320)

The development time for putting this together can take a couple of months, and there is also a lot of maintenance required to keep this in shape.  There is a lot of knowledge and context that you have to keep up with. The world is moving so fast these days that for every single new model or feature you bolt on, you have to keep up with that.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1350.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1360.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1370.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1380.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1380)

### Choosing the Right AWS Service: A Game of Use Case Matching

What I just showed you for media also works for intelligent document processing. Here I have an application package in my container with bank statements and W2s. We take that and split it out,  and then we analyze each of those pieces.  You define the type of output that you want, either standard or custom. If you want something custom, you define your own schema  and then process it through Bedrock Data Automation. 

To summarize some of the key features, first, we have an intuitive system that allows you to define your output. What type of output do you want? If you want to keep it simple, go to standard and select from the setup knobs, or you can bring your own schema and define it with natural language for each of those fields. The second part is orchestration. Under the hood, we have our own task-specific models that we pre-optimized, and we have foundation models running that we are constantly updating. These are things that we manage on your behalf and ensure the best price-performance balance.

We also have all the great features of AWS baked in already, including KMS keys, private cloud, and responsible AI. We want to make sure that the content being outputted has confidence and grounding. We have confidence scores in what you saw with the bounding boxes. We actually ground it in the image, and we also have toxic content detection right out of the box. We are also integrated into the set of AWS services in Bedrock. For example, with your knowledge base, if you put a lot of your documents into your knowledge base, you can use Bedrock Data Automation as a parser to parse that content when you want to build it for your RAG applications.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1500.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1500)

Finally, it is a single API. All of this I just showed you was on the console so you can visualize it. Media is a very visual modality, so you can see what is happening. But all of this can be done and should be done through the API.  Now there are other AI services in AWS that serve very specific use cases in intelligent document processing, image analysis, and conversational analytics.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1550.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1550)

In image analysis and conversational analytics, for example, if you want to extract linearized text and get the full blob of text from documents, Textract is the way to go. If you're looking for image analysis, we can detect whether people are live in the image or not, which is something best done with recognition. If you just want a great price-performance audio-to-speech model or ASR model, then use Transcribe. To really help with this, I'd like to play a game. 

The game works like this: I'm going to define use cases and give you some customer needs, and then you can tell me which AWS service would best solve each need. You can shout out the answer, and I'll confirm it for you. For the first one, if you want video understanding with multimodal capabilities, you want latency measured in minutes, and you want a high degree of customization where you can take multiple audio, text, and visual inputs, what service would best work for this use case? I hear Bedrock Data Automation, and yes, that's exactly right. That should be the easy one to kick off with.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1600.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1600)

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1630.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1630)

 Suppose you want stream transcription with really low latency from audio to text. You want high accuracy without too much customization, and at the end of the day it's audio-based. Which service would you use? It's Transcribe, and that would be the best one for that particular use case. Now, if you are detecting faces and analyzing a live face in an image, this has to be pretty accurate at millisecond latency, and it's completely image-based, what service would you use?  Recognition, that's correct.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1650.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1650)

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1670.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1670)

For insurance claims processing, you're okay with latency measured in minutes.  There are a lot of contextual details in claims processing, you want to customize it a lot, and generally you want high accuracy. Which service would you use? This one is Bedrock Data Automation. You can do all that now with BDA.  If you just want to extract the blob of text from a piece of document, you want to extract everything, you want just basic OCR, and you want it very quickly at low latency, which service would you use? Textract. All right, nice.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1710.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1720.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1720)

### Air's Journey: Managing 8 Petabytes of Creative Content for 100,000 Professionals

So I'd like to welcome Tyler onto stage here to talk a little bit more about his journey with AWS and Air. Thank you. Good morning. First of all, thank you all for kicking off your re:Invent journey with us bright and early at 8:30 AM on Monday morning.  I appreciate the full house here. My name is Tyler, and I am the co-founder and CTO of a company called Air, which I'll tell you all about in a second. 

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1740.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1740)

We've been building on top of Bedrock Data Automation for the last six months or so, and it's really changed the way we think about multimodal intelligence and information extraction. To give you a little bit of background on what we do at Air, we are what we call a creative operations platform.  That essentially means we help businesses manage all of their creative files and the work around themâ€”images, videos, documents, whatever it is that they're working with. We manage about eight petabytes of data across all of our customers today, with hundreds of millions of individual assets.

We work with about 100,000 creative professionals day-to-day spread across 2,500 plus businesses. These are businesses like Sweetgreen and NBCU and JP Morgan and Graza, the olive oil company you might be familiar with. So we have a really wide set of use cases and a wide set of content types and file types. We're really interested in providing rich experiences on top of that data set. We think about our product in two core ways. Asset management is the first, which is stuff you're used to getting from a cloud storage providerâ€”it's storage, it's organization, it's search, all of which is made much more efficient with multimodal intelligent extraction.

We also think a lot about the creative work that happens on top of that content library, things like giving feedback, making edits, and sharing content externally when it's ready to go.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1810.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1810)

To make this a little more real, I'm going to give you a quick customer case study.  How many of you are familiar with The Infatuation? Maybe you use them to look up a restaurant here in Las Vegas this week. They started in New York City, but today they operate in cities around the globe, and they manage about 25 terabytes of creative content with Air. We've been working with them since 2019 and a lot of the technologies that we use and the services that we provide have advanced over those years. But we've powered creative operations for The Infatuation for five plus years.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1870.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1870)

They have about 150 teammates spread across the globe: photographers, editors, designers, and creative directors. By their estimates, Air is saving them about four hours a week for folks who are embedded in that creative work because of the functionality that we provide on top of their content library. We are an AWS shop through and through. We've been building with AWS since the beginning of our business in 2017.  We use S3 to store and secure the original files that get uploaded to our system and we use a slew of AWS services to process that media.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/1930.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=1930)

We transcode every file into a lightweight, web-friendly copy for day-to-day use, downsizing images, transcribing videos, and converting documents to PDFs for rendering. We also extract information out of those files in order to provide end user experiences on top, like search, tagging, and organization, reducing manual effort on behalf of our customers. But this has always been one of the big challenges of our business: extracting AI-powered information from a large data set can be really hard, especially when we were first getting started as a company.  Each file type has its specific nuances. There are different services, tools, or ways that you think about extracting information from an image versus a video versus a document versus something else.

The metadata that we get back from those services can often be fragmented or inconsistent, and you have to maintain a fairly complex orchestration system around each of those specific modalities. For each content type that gets uploaded, we were processing the file with AWS compute resources and then sending it to specific enrichment providers, either on AWS or external third-party vendors. At the beginning of this year, we set out to rethink the way that we'd architected this solution and we wanted a solution that was simple, simpler to maintain, easier to scale, and really secure.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2020.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2020)

This is obviously of the utmost importance to our customers. They want to know that the content that they upload to Air is safe and it's not being shared with folks unintentionally or sold to third-party data providers. And we needed it to be cost effective. With eight petabytes of media and hundreds of millions of files, we wanted to be able to do this type of AI enrichment at scale without breaking the bank. So we went out and ran a search to try to find the right solution to solve this problem.  We wound up landing on Bedrock Data Automation, or BDA. It checked all the boxes that we were looking for and then some, and I'll tell you a little bit about how it fit our use case specifically as we go here.

### Why Air Chose BDA: Simplicity, Security, and Processing 15.5 Million Jobs

But first of all, the simplicity was the big winner for us. Rather than maintaining those individual fragmented media pipelines for each type of content we were working with, we had a unified API for all modalities. It was also one shot, so if we had a file and we wanted to get ten different pieces of information back about it, we could ask all of that in one go and wait for an asynchronous notification to come back saying that the media had been processed and all the metadata that we needed was there. And as Nini mentioned, it also abstracted away model selection from us, which was really massive. As you all know, this stuff is changing really quickly. It feels like every week there's a new advancement or change in the right model for the right job.

For us, we wanted to stay one degree removed from that and focus on building end user experiences for our customers and not chasing the best model of the day. By being an AWS product, it was intrinsically more secure for us. We were able to manage all of this data in the transfer between services all within our VPC, hosted on AWS, so we could be confident that the data would never leave our infrastructure and it was cost effective.

As we ramped up the amount of media that we were processing with Bedrock Data Automation, the economies of scale made sense for us, and we were able to provide this to just about all of our customers without increasing cost. Keeping data transfer between AWS products meant that line item remained free as opposed to sending it to an external vendor and paying data transfer fees.

A few of the other benefits that we weren't necessarily looking for when we kicked off our search really stood out. There's tremendous range of file support with Bedrock Data Automation, so we didn't have to do a lot of preprocessing to convert input files into an output format that Bedrock Data Automation could accept. The custom blueprints, which I'll show in a demo in a moment, were massive for us to be able to maintain consistency of user experience. We wanted the metadata to come back in a certain shape or a certain way that was consistent with our product, and a few other benefits really made Bedrock Data Automation the standout solution for us.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2190.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2190)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2210.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2210)

So I'm going to play a little demo video here  to give you a sense of what this looks like in our product. To start, we just drop in a file. This is the Air UI, the file uploads to S3 in a matter of seconds, and that now kicks off a series of asynchronous jobs in the background. The first thing we do is process the media. We produce little previews,  the scrubbable preview you just saw, and then we transcode the video for playback.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2220.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2220)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2240.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2250.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2250)

You'll see on the right side there,  we're now processing some of the smart metadata. We use Bedrock Data Automation to generate smart summaries of every asset as well as smart tags. These are customized and controlled to subscribe to a format that we want in our product. We use chapterization to pull out chapter titles  and summaries and transcription of the audio in the file as well. So three or four different types of extraction happening here, and all of that powers a really  simple search experience.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2260.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2270.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2270)

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2280.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2280)

You're able to type in a natural language query and find the file and even the moment within that file that you're looking for.  So all powered by Bedrock Data Automation under the hood, and the output formats that we see here are customized to meet the experience that we're looking for.  To give you a little peek under the hood of what implementation looked like for us when we were putting this together,  it really is as simple as a single API call to Bedrock Data Automation to create a new job. We passed it a reference to the S3 object that we're looking to process and then a reference to the custom blueprints we want to use for this job.

Those are the things that are defined in the Bedrock Data Automation console where you can specify the format or the schema or the structure of information that you want back. We kick off that job and then we wait. We subscribe to an EventBridge notification. When the job is finished processing, we receive a notification and then write the metadata wherever we need to write it. So it's super simple to kick off a job and then receive metadata back, and I know this is getting even simpler now as the Bedrock Data Automation team moves to synchronous API calls for certain modalities as well.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2340.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2340)

We've now deployed this offering across all of our customers.  About 2,500 paid customers are now using these services as well as many of our free customers too. It's been a massive win for both our engineering team, who no longer has to manage the complexity of the fragmented pipelines across each modality, and it's also been a win for our customers. The graph on the right side here shows the number of jobs that are waiting to be picked up and processed in order to be enriched or to have metadata extracted and then appear in search results and see those summaries and chapters in the UI.

The number of waiting jobs has dropped to next to nothing because Bedrock Data Automation is able to support the scale of uploads that we see at Air. We regularly see hundreds of thousands of new files uploaded in a day, sometimes millions when we're onboarding a large new customer, and we're able to churn through processing that media very quickly.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2440.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2440)

Since launching with BDA a couple of months ago, we've processed about 15 and a half million jobs and at any given moment we typically see about 160 approaching 200 running at a time. Most of them finish very fast, depending on the type of file that you're processing or the size of the file, but for our use cases this has been completely adequate to provide the type of customer experience that we're looking for. 

I do want to call out that this has been a big team effort between my engineering team at Air and the folks on the BDA team at AWS as well. Gaurav is our account manager, and he highlighted the degree of collaboration that we saw with this effort, which was pretty special. Every day we were in Slack together, incorporating their service into our product, finding bugs and reporting them, having them fix them within hours or a day. The near real-time collaboration here was an absolute win.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2490.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2490)

### Looking Ahead: Semantic Search, Custom Blueprints, and Multi-Region Expansion

I think something that surprised me when working with a company as large as AWS was to see their ability to remain nimble and work with an early growing startup like Air. That was really impressive. I think this is hopefully just the beginning of our partnership with BDA and our multimodal journey as well. 

To give you some insight into some of the stuff that we're thinking about next, the first we've actually already launched is we're now taking all the metadata that we get back from BDA and creating vector embeddings of it to power a semantic search experience. You saw this briefly in the demo video we played, but you can talk to Air search in plain text or plain language like you would a coworker. We create an embedding for that query and compare it to the embedding of the metadata that we get back from BDA for each of the files in your workspace. That has been super easy to stand up on top of this multimodal infrastructure that we've just talked about.

We're excited to switch over to synchronous data processing for some of our modalities like images as well, which will speed up processing time and reduce complexity even further. We are still in the process of consolidating some of our use cases over to BDA as well. Video transcription, which we saw in the product demo, is moving over to BDA shortly.

One that I'm particularly excited about is exposing those custom blueprints to our end customers. We've gone ahead and set up requirements and said, "Hey, we'd like to keep summaries less than 50 words or we want 10 to 15 smart tags when you process an image." But you could imagine giving those keys to the customer themselves and saying, how would you like to understand the content that you're uploading to our product? This is something that we're considering as a potential enterprise offering on top of our AI infrastructure.

We're continuing to scale this out to multiple regions to improve latency and reduce complexity. And I'm sure we'll continue to collaborate really tightly with the BDA team as we do all of the above. Hopefully this provides a little bit of insight for all of you as you think about how you might approach some of these challenges at your companies or your use cases. Looking back over the last six months, I think we've been really pleased with how easy it was to get all this functionality stood up and deployed across a really massive set of files across thousands of customers and excited for what's to come next. Thank you so much.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2670.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2670)

### What's New and Coming Soon: Synchronous APIs, Entity Detection, and 15+ Regions

Thank you Tyler. It has really been a great collaboration. We've been working so closely together and it's funny, yesterday night we were trying to get one of these demo videos up, and I said, "Tyler, I need this video." So it's actually been a very close collaboration. I want to talk about what's new and what's coming up. It's funny Tyler that you mention synchronous image processing because that's actually something just launched last week, and so I'll go over each of the different modalities and some of the new stuff that we're coming up with. 

For documents, we've expanded across five different languages now and that's going to be coming up as well for the other modalities. We've also added new file format support on documents. On video and images, on media in general, we've increased the image processing speed by about a half.

The synchronous API processing allows you to take your assets, put them in, and immediately get a result back. This is especially helpful for responsive use cases, particularly on mobile, where the synchronous API provides better performance. We've also added support for many more file formats, including AVI and MKV, so you can process the full spectrum of your different assets, file formats, and codecs natively without having to reconvert or take up additional jobs.

On the speech side, we have channel identification, which allows you to detect multiple speakers. In the earlier demo, we showed one speaker, but you'll be able to detect multiple speakers. Speaker diarization works along the same idea, allowing you to tag different speakers for turns in the conversation. We're going to expand this capability to all the major languages and more.

There are also a few other things I want to preview for you. First, for documents, we understand that accuracy is critical to unlock production. You want to achieve something past 90% accuracy. We're going to launch something that allows you to take your documents, label them with ground truth, and then optimize your blueprint so that you can achieve greater accuracy during runtime.

We're also going to launch something in terms of entities. We understand that different pronunciations of words may show up differently in the transcript. We're going to launch something that allows you to define the correct pronunciations so that we'll detect them in your transcript and audio. Additionally, we will allow you to detect the entities you want to detect, including recognized people that would be pretty much impossible to identify through public domain knowledge. You can bring your own images of people you want to recognize and supply that to Bedrock Data Automation, so we'll detect them out of the box for you.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/064446a10c07b912/2880.jpg)](https://www.youtube.com/watch?v=SrB-LIAIx5I&t=2880)

Finally, we're going to expand the number of AWS regions we cover. We're going to double that to 15 plus regions by the end of this year. We have a lot of exciting updates coming up soon. If you want to take any  of these resources, feel free to do so. We actually have a builder session today at the Win at the Promenade Lafayette 9 at 1:00 PM. Feel free to join us there. We have a few more minutes, so we'll be off stage and happy to share some additional thoughts with you. Thank you so much.


----

; This article is entirely auto-generated using Amazon Bedrock.
