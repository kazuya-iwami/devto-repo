---
title: 'AWS re:Invent 2025 - Build a well-architected foundation for scaling generative AI and agentic apps'
published: true
description: 'In this video, AWS GenAI specialists Chaitra and Aamna, along with Dave Senior from Sage, present a comprehensive foundation for scaling generative AI and agentic applications to production. They explore critical architectural layers including model hubs, gateways (LLM, tool, and agent), Amazon Bedrock AgentCore Runtime, and observability systems. Aamna demonstrates the open-source Agentic AI Foundation Accelerator featuring LiteLLM gateway, Langfuse observability, and offline evaluation with LLM-as-judge. Dave shares Sage''s agentic mesh architecture supporting 200+ products across accounting, HR, and payroll domains, explaining their federated approach with specialized, planning, and main agents. The session emphasizes essential components like guardrails, continuous evaluation using golden datasets, and AgentOps practices for managing the complete lifecycle from prototype to production at enterprise scale.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/60.jpg'
series: ''
canonical_url: null
id: 3086040
date: '2025-12-05T10:09:44Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Build a well-architected foundation for scaling generative AI and agentic apps**

> In this video, AWS GenAI specialists Chaitra and Aamna, along with Dave Senior from Sage, present a comprehensive foundation for scaling generative AI and agentic applications to production. They explore critical architectural layers including model hubs, gateways (LLM, tool, and agent), Amazon Bedrock AgentCore Runtime, and observability systems. Aamna demonstrates the open-source Agentic AI Foundation Accelerator featuring LiteLLM gateway, Langfuse observability, and offline evaluation with LLM-as-judge. Dave shares Sage's agentic mesh architecture supporting 200+ products across accounting, HR, and payroll domains, explaining their federated approach with specialized, planning, and main agents. The session emphasizes essential components like guardrails, continuous evaluation using golden datasets, and AgentOps practices for managing the complete lifecycle from prototype to production at enterprise scale.

{% youtube https://www.youtube.com/watch?v=0sfKGUZ6kcg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Building Production-Ready Generative AI and Agentic Applications

Good morning, everyone. Thank you for being here. I enjoy Monday morning sessions because we are fresh and early in the week, not influenced by other things. We are still excited and eager, so this is a good time. I'm glad you're here. My name is Chaitra, and I am a GenAI specialist at AWS. Today I'm joined by my colleague Aamna.

Hello, everybody. I'm Aamna, and I'm a senior specialist solutions architect for generative AI. We also have Dave from Sage, who is one of our AWS customers.

Hi, I am Dave Senior. I'm based out of Dublin, Ireland, and I'm the head of data and AI for the Central Services team in Sage. We're part of the product organization, and everything we build is customer-facing.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/60.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=60)

Thank you. So why are we here today? What are we going to learn about?  This is what started to happen a few years ago with chatbots. It seems very simple: you put in a prompt, and you get a response. That's how it started.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/70.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=70)

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/90.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=90)

 But we know that in reality, it's much more complex than that. There's orchestration, agents, RAG, and applying safeguards. So it's not as simple as just an LLM that generates everything. Imagine this: you're talking about one application before.  Now we are at a stage where we are going to put a lot of this into production. So what are some of the challenges and complexities here? It's not a trivial job; it's really complex.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/100.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=100)

 That's what we will try to address in this talk today. We will see what a strong foundation looks like for you to take these generative AI and agentic applications to production and even at scale. It is a comprehensive foundation that we have defined, but we'll just dive into a couple of layers and look at what the details are there. Then we'll have a demo of a foundation that Aamna has developed just to demonstrate how it looks in real life and what happens when you start to put all of these foundations together. Then we'll hear from Dave about their journey of building an agentic mesh platform. We'll look at operating agents and then we'll wrap up. We'll show you a few resources to take back.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/150.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=150)

### The Challenges of Scaling Agents to Production

 Can I get a show of hands of people who have developed proof of concepts, either generative AI or agents? Nice. What about taking it to production? That's more than I thought, and I hope that's all on AWS. Okay, so we know that it's not trivial. We already understood that scale is a challenge. But even taking a few applications to production is a challenge. All of the challenges we have talked about before regarding taking things to the cloud and taking things to production are still valid here, but it is a little bit more than that.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/180.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=180)

 In terms of performance, agents are not just reactive. The reason is they plan and they act over extended sessions. So low latency and performance is important, and that is a really hard optimization challenge. Scalability: when you put thousands of agents in production, you want to look at some purpose-built orchestration and some purpose-built infrastructure to handle that scale. These agents are accessing sensitive data, and they may access tools that have access to sensitive data. So without proper isolation controls and identity controls in place, even a small error can cause catastrophic problems. These agents need context, and evolving context. They have to be updated regularly and at scale, and they have to be secure. This memory has to be secure. That is another technical challenge: to give a memory that scales and is secure.

Till we built generative AI applications, applying safeguards was good with guardrails. But now with agents, you have to continuously monitor, continuously evaluate, continuously audit, and maybe even have human oversight in place so that these agents are aligned. Okay, we all understand it's very complex. So what are we saying? We are saying that you should have strong foundations in place. And what does that mean?

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/270.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=270)

### Establishing a Comprehensive Foundation for Agentic AI

 You can look at it two ways. Either you're building one or many applications and you want to set up the foundational architecture in place, or you can look at it another way: you are a platform team or an IT team, you are setting up the central platform, and you are onboarding different lines of business or teams. What does that look like? I'm not going to go into detail here. I have another slide which talks more in detail. But the idea is that you start to consolidate components and offer shared services.

When you offer shared services with centralized governance controls in place, you can implement any policy, compliance, and security measures while starting to see cost efficiencies. One example of a shared service could be a model catalog. Rather than having every team that builds applications maintain their own model catalog, you can put that in a central place as a platform offering that all teams building these applications can access.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/350.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=350)

So what does that look like? We are saying it's a comprehensive foundation. At the core of it is the hub, which is the model hub and the agents and tools hub,  and giving access to these via gateways. When you're building these agents, customizing these models, and building these tools, you need an execution environment and a runtime to host these.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/370.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=370)

Data exists in silos, in data lakes, and in data warehouses, but you don't need all of that data  to build these applications. This layer concerns building ingestion pipelines, building indexing pipelines, taking that data and putting it into vector stores, and having long-term and short-term memory stores for agents.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/390.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=390)

Then on top of that is orchestration.  Orchestration is about having access to all of these components and really building the flow. It could be a workflow, which means a known series of steps where you query, call a model, add another prompt, maybe chain different models together, or build a RAG architecture. It could also involve agents, but that doesn't necessarily mean it's part of the foundation. However, orchestration can become an important layer if you want to give templates. For example, if you want to provide a RAG template so that everybody can use it, that's a good place to put that in the orchestration layer.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/430.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=430)

Then you have clients accessing all of these.  Let's not forget the three most important pillars: operational excellence, observability, and security. Observability is very important to get insights into what is going on, as these are black boxes. How do you really know what is happening under the hood? Operations is not just about operating the foundation itself, but about what you offer to these applications as part of this foundation that helps them operationalize. It could be something like a prompt management store to store and version the prompt templates as part of the foundation that consuming applications and teams can use.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/490.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=490)

### Amazon Bedrock and Amazon Bedrock AgentCore: Core Services for the Foundation

And let's not forget security, data privacy, and data isolation. Now I know it looks like a big thing, but we'll dive into a couple of them. Before I dive into the layers,  there are two services I want to talk about. The first is Amazon Bedrock, which is our fully managed service that lets you build generative AI applications. You get access to state-of-the-art models, both first-party and third-party. You're able to scale cost effectively because you have different consumption modes, and you have a lot of features that let you balance latency, cost, and accuracy. You can customize the models with your data or build RAG architectures, and you can apply guardrails using the Guardrails feature.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/530.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=530)

The other service I want to talk about  which will help you build that foundation is Amazon Bedrock AgentCore. You may or may not be familiar with it. This went GA a few weeks ago, and this is our fully managed agentic platform that gives you a lot of managed services to build and scale agents. It gives you a secure runtime to build and deploy these agents, managed long-term and short-term memory, a gateway to access tools, and all observability and instrumentation of traces built into the platform.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/570.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=570)

When you put the two of them together,  this is what it looks like. You have Amazon Bedrock, which we talked about for the model catalog and the gateway. Bedrock will help satisfy that model catalog requirement. Then we're talking about a runtime to deploy agents and tools, and Amazon Bedrock AgentCore will help you do that. The big advantage of Amazon Bedrock AgentCore is that you can use many different open source frameworks like Strands, Crew AI, LangChain, and LlamaIndex to build these agents.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/620.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=620)

You containerize them and deploy them in the runtime. That's it. The service manages it for you. This is good. 

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/630.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=630)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/650.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=650)

### Gateway Patterns: LLM, Tool, and Agent Registries

We looked at what a comprehensive foundation looks like. Let's dive into one of them: gateway. We are seeing a few patterns emerge. One is called the LLM gateway.  This has been around for a bit. This is all about giving secure access to a model catalog and providing a unified API so that developers don't have to worry about accessing different models with different API specifications and different prompt templates. The gateway can handle all of that. 

The two other patterns that are emerging are the tool gateway and the agent gateway. These are becoming popular. The tool gateway secures and standardizes actions and integrations, with a registry serving as a central repository of tools that agents can access. Similarly, the agent gateway gives access to agents, and the agent registry is a catalog of deployed agents with all of their capabilities and metadata. Registering and deregistering agents all happen through this registry. The question might be whether you need all of them. Not necessarily. It depends on what kind of use case you are building.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/700.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=700)

For example, if you look at the LLM gateway, why do you need an LLM gateway?  If the models are on Amazon Bedrock, which is our service, or if you have customized models and put them on SageMaker, which is also one of our managed services, or if they are on EKS that you have hosted, or if they are not even on AWS but from another provider, you want to reduce that friction for developers in accessing all these different models. You don't want them to worry about it. The gateway takes care of that orchestration and gives a unified API. Mostly it is an OpenAI specification that we have seen with all of these tools.

Most of the cost that we see is token cost that you want to control and attribute. The gateway can do cost attribution. As use cases come in, you can see which use cases are using the most number of tokens and causing the most impact on the budget. You are able to apply rate limits to all of these different teams. The way it's done is you hand over API keys to all of these different teams or users, and then you can start to track costs and all of the actions.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/790.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=790)

Another big advantage is that you can apply guardrails. Let's say enterprise-wide you have some guardrails that you want to apply always. Whoever builds applications accessing these models needs to use those guardrails. This is a great place to put them. Now, all the foundation that we showed doesn't mean that we have a service for every one of those layers.  No, we do have gaps. What we have done is published a solution that shows how you can deploy, in a well-architected manner, a solution offering that is available outside on AWS, securely, cost-effectively, and reliably.

This takes an open source solution called LiteLLM. They have two versions: an SDK and a proxy server. This takes the proxy server and puts it in EKS or ECS and deploys it. LiteLLM comes with all of the features that we talked about for an LLM gateway. This is just about taking it and containerizing it and deploying it on AWS. There are a lot of other solutions as well. For example, Kong AI Gateway has this, Envoy, and OpenRouter. There are a lot of solutions that you could explore.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/870.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=870)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/890.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=890)

The question is, if you are just accessing models on one provider or one service, you may not need this. But still, if you are looking to do cost attribution and segregate and isolate between different teams, it's a great thing to have a gateway in front of the model access.  Now, the next thing we want to look at is the tool and agent registry. The pattern that's coming is that there are purpose-built gateways and registries for each of these, but they can also be combined. There are solutions that are coming that are combining these two things and giving them as a service. 

One example of a tool gateway is AgentCore Gateway that comes as part of AgentCore. It provides unified access to all of the tools. It could be that agents are running in AgentCore or outside, it doesn't matter. You could still use the gateway. You are able to access either API endpoints, Lambda functions, or MCP servers, and you attach them as targets.

This creates an MCP endpoint for all of those, and the agents are able to discover these using the semantic search capability of the gateway. This is an example of a tool gateway that gives a kind of registry and the gateway to access the tools.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/940.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=940)

Now, the other solution I want to show you is an open source solution that some of our colleagues developed. This is a comprehensive gateway and registry.  It works with MCPs and agents, and provides access to both of these. It's open source and available, so feel free to check it out. One of the good things is that enterprise-ready observability is integrated into it.

The way it works is there's an agentic server endpoint in the front, and everything else is deployed as MCP servers in an EKS cluster. It also supports two types of authentication: M2M using JWT, and then 3LO authentication. It works really well as both, and a lot of development is happening. You are free to contribute since it's open source, so take a look.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/990.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1000.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1000)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1020.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1020)

### Observability: Monitoring Intelligence, Quality, and Risk

We looked at gateways, and there are many different layers. We are not going to go into everything.  But I do want to talk about one very important topic: observability. I have built software applications and have been in this industry for a long time.  At that point, observability for those apps was more about keeping the systems alive. We were worried about uptime, latency, and errors. But now what has happened is it's about keeping the systems aligned.  You are worried about quality and bias. Is there any toxicity or hallucinations going on? Whereas earlier it was infrastructure monitoring, now you are almost monitoring intelligence.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1090.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1090)

Why do we need observability? Quality is very important. You want to understand the quality of the response. You want to understand if the right tools are being selected. Agents work by having access to many tools, so is there the right tools at its disposal? Is it picking the right tool? Is it converging on the right path? Convergence means is it taking the shortest path to get to the response. All of that is quality. Cost is impliedâ€”you definitely want to understand cost. Latency is not the system latency, but why is an agent taking time to respond? Which step in the process is taking time? And then you also want to measure risk. Risk is the most important thing. How do you measure it? By seeing if there is any PII leakage or any toxic output. There are ways to do it, and I will talk about how you would do that. 

This is how an observability system works. Your agents are generating operational and semantic signals in the form of logs, traces, and metrics. Another important metric is user feedback, which has to be collected. The observability platform or tool will aggregate and visualize these signals. Another important thing, which a lot of times is not set up, is the evaluation layer. Its job is to interpret the quality and risk using some evaluations. You can collect all the metrics you want, but if you do not use it for evaluation, then there is no benefit other than understanding costs. Once the evaluation layer evaluates all of this, then the optimization loop happens where you know the output did not really generate what you want, so now you will use a different model or different prompts. You make some changes and this loop continues. This observability platform could be any tool. Langfuse, for example, is a great tool. AgentCore has observability, which is also a great tool. You can plug any one of those tools here.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1170.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1170)

Now, what are you measuring?  Functional quality is what is important. Even in a retrieval augmented generation app, is the retrieval relevant to what is happening? Chunks are extracted from the vector databases based on the prompt. Is it identifying the right chunks? Is it finding the right tools? Is it passing the right parameters to the tool? That is the second quadrant. And the fourth quadrant, user feedback, is also important. Yes or no. Does this response work or does that response work?

You could give them two choices, such as yes or no. You could also imply certain attributes. For example, if the user is prompting the same thing again and again, maybe the answer is not satisfactory. Safety, toxicity, and PIL leaks, and bias are very critical to measure. These are not comprehensive lists. There are many more that you can start to measure, but these are very key and important metrics.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1240.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1240)

There are two types of evaluation. Offline is done during development.  Integrate this into your DevOps pipeline to continuously evaluate the system and applications. The dataset that you use is a golden dataset that you have created. It could be user-annotated or synthetic dataset. Once that is done, then comes online evaluation wherein real user interactions are being measured. This is in the live system.

You don't have to measure every trace that's coming out of the observability system. But if you have a very high-risk application, it makes sense to measure every output and see if it's performing the way you're expecting. Otherwise, you can sample certain datasets, 5% or 10%, and start to evaluate those. Continuous iteration is important.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1290.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1290)

Now, let's look at the lifecycle.  You start by defining your goals and metrics, and then you curate this golden dataset. Do not forget that. The golden dataset is so important. You may think you will just develop the application, put it in production, and see how it works. No. You develop the application, you test with a golden dataset, and then you put it in production. Test under different conditions.

There are three different ways to test. One is to use automated evaluation, which means you're using some kind of coding to evaluate, or you do some kind of regex, or you are comparing it to a ground truth dataset. The second way is to use another LLM, another model to look at the output and say, "Here is a prompt, here is the output, and give me some measurement." The prompt for that LLM should be very good. Use a really good LLM. This is called LLM as a judge.

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1360.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1360)

The third way is you can use humans, the most expensive option, but you can create cues and have humans evaluate more closely to accuracy. However, think about the cost. Then comes continuous optimization.  How would you do this? Observability can be integrated and implemented two ways. One is that all of these observability tools come with an SDK. You can use that SDK and build that instrumentation into the application. For example, Langfuse will have an SDK that you can use.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1410.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1410)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1420.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1420)

The other way to do it is you collect all the traces and feedback into an OTel collector. It's a standard for collecting and processing telemetry data. Then you can feed that into different systems based on how you want to measure. It can go into CloudWatch, which is a service of AWS, and there you can create dashboards. Or it can go to Amazon S3, which is a scalable data store.  From there, you can do some offline analysis. Or it can feed into systems like Langfuse, from where you can do a lot of insights.  You can analyze that and feed it into the evaluation layer.

### Demo: Agentic AI Foundation Accelerator in Action

Now let's look at a demo from Aamna. Thank you, Chaitra. Really excited to share the stage with Chaitra and Dave today. Chaitra walked us through the journey of building foundational components to scale generative AI applications into production. But how many of you are wondering how this looks live in action and how you envision this tangibly?

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1480.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1480)

I'm really excited to share that my team has open sourced an accelerator called the Agentic AI Foundation Accelerator, which basically ties together a lot of the foundational components that Chaitra was talking about, including a generative AI gateway, observability, offline evaluation,  guardrails to safeguard your application, and also infrastructure as code deployment. This is a lightweight but well-architected foundation that gives you the opportunity to deploy this solution, leveraging infrastructure as code using Terraform, and helps you host your agent in a secure and scalable fashion, leveraging Amazon Bedrock AgentCore Runtime.

What you see here on the center top is Amazon Bedrock AgentCore Runtime that helps you host the agent. In this demo, we are leveraging an open source framework called LangGraph, but you could bring your own framework. It could be Strands, Crew AI, your own code logic, your own implementation, and host it on Runtime. Then you see on the bottom in the center, the LLM gateway. In our accelerator, we are leveraging LiteLLM guidance to enable the agent to invoke any model of your choice that's not just on the AWS landscape, so not just on Bedrock, but also beyond. Our accelerator also helps you leverage models that are on SageMaker or by third-party providers.

The LangGraph agent that we have in our case is using three different tools. The use case that I will demonstrate today is a very simple customer service chat application. But you can also adapt this foundation to your use case and to your industry. The foundations remain the same. You basically do the code and the implementation based on your processes. In our example, the agent is leveraging three tools behind the AgentCore Gateway. Behind this tools gateway, what we have is three tools. The first one is a RAG-based tool, which is helping you retrieve context from your enterprise indexed data. This is leveraging Amazon Bedrock Knowledge Base. The other tool that we are leveraging is third-party APIs for creating and providing information for support tickets. For example, if your agent is not able to answer a customer query, the agent routes this and creates a support ticket. The third tool is a web search tool, which is also behind the gateway. It's leveraging a third-party API with Tavily.

All of this is then being audited with the help of not just Amazon CloudWatch that you see on the right end, but also a third-party observability tool called Langfuse. We are collecting traces of the agent interaction, and then on top of that, running offline evaluation during the development phase to make sure that this application is working as expected. To showcase how all of this works in action, what we've put at the top is a front-end application powered by Streamlit. All of this is what I'm going to demo and show you how all of this works together, but we've also open-sourced this. At the end of the presentation, we are going to share the GitHub resources with you so that you can also try it hands on or take it to your builders and get feedback on how all of these foundational components work together.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1710.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1710)



This is basically a customer service chat application that is leveraging LiteLLM gateway built on AWS infrastructure. This agent has access to your choice of models. It has two modes. It has a local mode, as well as a mode where the agent is hosted on AgentCore Runtime.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1740.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1740)



[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1760.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1760)

So you test it locally and then you deploy it on AWS, and you provide the ARN. So the identifier, the region, as well as it supports authentication. You provide the user access token that authenticates the user. As you can see, it's integrated with LiteLLM gateway. This agent has a list of models that it can access,  not just Bedrock models, but also models outside of Bedrock. So OpenAI models, for example. And then you also have user identifier to tag your traces.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1770.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1770)



I'm basically going to send it a hello. I'm sending it a quick hello, and the agent responds based on the system prompt that I've set. It's a customer service agent for any company, a dummy company, and I have some data that I've indexed. I ask about how to reset my router hub.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1790.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1790)



There's a device that any company supports. The agent goes and provides an answer. It's basically leveraging the retrieve_context tool to provide me with an answer. You can see that it also lists down the sources that it leveraged.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1810.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1810)

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1830.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1830)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1840.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1850.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1850)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1880.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1880)

 You can see which documents it used and the relevance of the retrieval. It also tells me other things like what model was used, what tools were used, and what was the citation, including the document, page number, and all of that. It also adds a trace ID for observability. I go on to ask another question.  I try to reset the router hub based on the answer, but it does not work. The agent goes ahead and provides another set of steps, but then says  "I can create a support ticket for you if this is not satisfactory." So I go ahead and say, "Yes, let's create one." The agent then uses that information  to further query and ask me about additional details about my email ID, my problem description, and all of that. I go ahead and provide those details. The agent is intelligent enough to pick all of that information to use as parameters for my create_support_ticket tool call.  I'm not explicitly mentioning what those parameters are. I'm just providing all of that information in natural language, and the agent is going to pick that up and understand that it needs to call the create_support_ticket tool that is hosted behind the AgentCore Gateway.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1890.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1890)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1910.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1920.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1920)

In this case, you can see that a ticket was created using a third-party API.  It provides me with all of the information. I can also see that the create_support_ticket tool was invoked in this process. Now I take a different route. I ask the agent if it also happens to provide investment advice. It tells me that your input contains content that violates a policy.  What we've also done is integrate it with Bedrock Guardrails to make sure that this application is safe and trustworthy. You can also have PII detection in place and other capabilities.  I go ahead and then ask what device did I ask help for. This is an example of short-term memory for session management. I don't need to repeat information for the same session. It knows that I asked a question about the router hub and I created a support ticket.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1940.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1950.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1960.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1960)

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1970.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1970)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1980.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1980)

Now I'm switching from local mode to the AgentCore mode.  I am very happy and satisfied with my agent and I hosted it on AgentCore Runtime. I provide it with the identifier for my agent  and a user access token. As I said, it also has OAuth integrated. I provide that authentication token. It now says that it's connected to AgentCore Runtime.  Now it's interacting with the agent on the cloud. I again go and ask similar questions. What kinds of devices  are supported by AnyCompany broadband? It goes on and then again does the retrieve_context tool call and behaves in a similar fashion. What I'm trying to show you here  is that the local mode can then be hosted on the cloud.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/1990.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=1990)

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2010.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2010)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2020.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2020)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2030.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2030)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2040.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2040)

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2050.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2050)

Here you can see LiteLLM. This gateway shows you a model catalog that is basically the shared component.  All of the different models and providers that are supported in your enterprise via the LiteLLM gateway can be checked out here on the UI of LiteLLM.  You can create an access key, an API key for your team or your use case, and then attribute cost to that team or to that use case.  There's also a playground where you can basically use that key and test it out. You also have a comprehensive usage dashboard  which tells you what is your spend per team, what are the top API keys, what are the top models, what are the top providers  that are being consumed in your organization. Moreover, there's also logs available. These are logs for model invocation.  Every invocation that my agent made, I now know what were the total tokens, what was the cost, was it a success, or was it a failure. This is how LiteLLM looks like.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2060.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2060)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2070.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2080.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2080)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2090.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2090)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2100.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2100)

I now switch to the observability tool, which is Langfuse. In this example, I'm going to look at the traces  generated for my agent invocation via the Streamlit application. I go to this tracing tab for my platform and I just filter down for the latest trace,  which was the trace of the interaction that you just observed. In our code, we also tag these traces.  Very easily you can filter out and categorize all these traces when you're creating metrics on top. In my case, it's a LangGraph customer service agent trace.  This is the user ID that you also saw in the front end application. 

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2110.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2110)

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2120.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2120)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2130.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2130)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2140.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2140)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2150.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2160.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2160)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2180.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2180)

You can attribute it to user IDs, project IDs, and so on. These are the different metadata being collected as output of your agent trace.  You have a timeline view, so you can also check what time it took for every step. If you see that your agent is really slow,  you can determine whether it was the retrieve context step or the create support ticket step that took a long time, and you can easily optimize the speed and latency of your agent.  Here, there are different steps with different kinds of metadata. The create support step, for example,  has all the parameters that were used to invoke this tool call. For your retrieve context, you can also get details like what was the relevance score,  what were the items that were retrieved, and so on. What I'm now going to do is show you how we are collecting user feedback.  As Chaitra also mentioned, this is another important metric that we collect. I'm going to ask a couple of questions to my application, and the agent is going to answer these questions. If I am satisfied, which I am, I'm going to put a thumbs up and also provide a natural language comment, which can be worked upon to gain insights about the performance. We are collecting this as well on Langfuse. 

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2190.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2190)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2200.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2200)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2210.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2210)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2220.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2220)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2240.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2240)

As you can see, we have something called user feedback, which we are normalizing and also collecting comments to understand how all of this works.  I'm now going to demonstrate how offline evaluation works.  This is a basic Python script that we've created, where we are using some ground truth data  to send some queries to my agent. Then I'm collecting the traces from Langfuse to use LLM as a judge to create certain metrics like latency, faithfulness, correctness, and tool accuracy.  My expected tool call was, say, retrieve_context, and the agent went and created a support ticket. So the accuracy was zero. These kinds of things you can create during the development phase  and incorporate in your DevOps pipeline. As I said, we've open-sourced this, so I'm looking forward to you checking this out. Chaitra is going to share the resources with you later.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2250.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2260.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2270.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2270)

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2280.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2280)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2290.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2290)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2310.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2310)

Just to recap, we have the LLM gateway that leverages Bedrock and other models.  You have the tools gateway that leverages different tools. And then you also have different components like observability and offline evaluation, where you bring in your golden dataset  and then create different metrics.  I have a query: how do I reset my router hub? And I have expected tool calls like retrieve context. We are using some Python logic to create tool accuracy, parameter accuracy, and also LLM as a judge to create response quality metrics like faithfulness and so on.  Moreover, we also provide infrastructure as code, as I was mentioning.  It's available in Terraform. The main purpose of this is that developers and builders can quickly deploy and test out the solution end-to-end to understand how all of these foundational components are being stitched together. Everything that you see and everything that was deployed as part of the demo can be deployed end-to-end with infrastructure as code. 

This was basically the demo. We saw a lot of things there. We saw the LiteLLM gateway, we saw AgentCore Runtime hosting your agent, we also saw AgentCore Gateway, which had different tools behind it. All of this was tracked and monitored with Langfuse, and then we had offline evaluation on top. This is all I had for the demo, but we have more in store for you. It's a pleasure to move on to the next segment. I would really like to invite David Senior on stage, who will tell us more about how Sage is implementing agentic AI in their organization and leveraging foundational components. Welcome on stage.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2370.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2370)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2400.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2400)

### Sage's Journey: Building AI Solutions Across a Distributed Architecture

Thank you, and thanks for the invitation to present here today. We'll start off with a little bit about who Sage is and what we do.  We mainly work in the areas of accountancy, HR, and payroll. We produce software in those domains. We have some growth numbers there on the screen as well. There are 11,000 employees worldwide, spread across 20 countries. We have 6 million businesses worldwide that use Sage products. A fun fact is that one quarter of the UK employees are paid via Sage software.  So, what do we have today in the agent space? We have our Help Search.

I think everybody's building these types of solutions at the moment. So RAG-based Health Search. Our product teams can integrate this into a product within five days into production, which is great performance. We have our Instant Analysis. Instant Analysis uses LLMs to summarize financial reports and answer customer questions on them.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2470.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2470)

We have our Accounting Agent. This wraps up a lot of functionality, where we allow customers to interact with the product through natural language, and we also deliver insights into changes in their data and potential errors in their data entry. Then we have our Agent for Close, which is another example where we have an agent that helps customers do a period close in the accounting product. But we have some challenges as well. 

We have a very distributed architecture. Sage has over 200 products worldwide, and we've grown massively over the years through acquisitions. Because we have such a large organization, decision-making is distributed as well. While my team is a Central Services team and we're building AI solutions, we're not the only team in Sage that is working with AI. Everybody's working with AI.

In order to address our suite strategy, where we want to sell software in packages to customers, we also need to have an AI experience that straddles those products. That means a customer needs to be able to use our agents to interact with our HR products from our accounting products, and vice versa. We also want to be able to work with payroll. This is where our agent mesh approach comes in.

### Sage's Agentic Mesh Architecture: A Federated Approach

Just a moment on agentic mesh architectures. We have a few examples here on screen. We have the basic mesh, which would be for a smaller setup, maybe with an agent registry, maybe not, which allows agents to interact with each other using the agent-to-agent protocol. We have the gateway mesh. A gateway mesh would require all agentic requests to go to a central point to be routed to the appropriate agent.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2580.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2580)

The challenge with that is when you have a large organization trying to centralize everything like that, you build a single point of failure and you build a requirement on every team in your organization to interact and integrate with that. We also have an example here of a hierarchical mesh,  where we have, as it describes, a hierarchy of orchestrators and agents. Then we have a more complex mesh, which would be a federated mesh, where you have bridges between multiple meshes and maybe multiple authentication sources as well.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2610.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2610)

This is what we've come up with for our architecture.  This is our agentic strategy for Sage, and it's using a combination of the different meshes that we showed previously. If we talk about this from right to left, we can discuss what we have today and what we're working on for the future. We have our tools on the extreme right-hand side. We have our MCP servers. We also have MCP-like solutions that we built pre-MCP before the industry had settled on that standard.

We have workflow orchestrators and solutions like that. Now we're starting to use the AgentCore Gateway for MCP as well. We have these spread across multiple business units and multiple products in Sage today. In front of our gateways, we have our agents. We have three types of agents that we've defined in our architecture. We have specialized agents that do particular role-based tasks.

We have planning agents that can plan complicated tasks to be orchestrated across those role-based services. Then we have our main agent. Our main agent is our orchestrator. Our main agent would be the point of entry for the agentic experience within a product. When a customer opens up the agentic experience within the product, they'd be connected to the main agent. Based on the user's request,

the agent may need to use the planning agent to request a complex workflow to be orchestrated across the specialist agents, or they may just trigger a specialist agent for a simple task. The other alternative is that the user requests an action that is not supported in the local product. For example, maybe you're doing some data entry in your accounting software, and then you remember you've got a new start next week. So you ask your agent to create a new user in the HR platform.

In this situation, the main agent will have to go to the agent mesh, look up the customer's entitlements based on the tenant that they're signed in to, and this happens via MCP. We have an MCP gateway for that, where we're looking up our entitlement service and getting the HR product that this customer's tenant has purchased from Sage. Then we would do an agent-to-agent handoff to a main agent for that product using the agent-to-agent protocol. The user can then interact with the main agent for HR in a similar manner, where maybe it's a complex task that requires a plan to be generated by the planning agent, or it's a simple task that's just triggering a role-based agent.

Outside of our products domain, within our mesh, we also have observability. As was mentioned earlier, in order to have a good understanding of what is happening within your environment, you must have observability. We need to be able to trace not just the agent interactions, but the triggers that go through our products as well. If a user is triggering an API within our product, we need to be able to see the successful completion of that action or the failure.

We also have a centralized identity, which is part of our mesh here using our Sage ID service. All users are authenticated via Sage ID. However, access control is not centralized. Access control exists within a product domain. If you did request an action for the HR agent, you may or may not have permission to complete that action. The agent would have to look up the entitlements within the local product and understand whether or not you can perform that action on your behalf.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2900.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2900)

We intend to also build a just-in-time elevation of privilege, where the agent could request a more privileged user to perform the action on your behalf for that one time. So what's next? We're looking at modernizing the agents that we've built  to use Bedrock AgentCore. Most of what we've built today within my team has been built on AWS services. The phrase modernize is kind of funny to use, considering how new AI and LLMs and this world is, but that's how fast the industry is moving.

We're looking at bringing our architecture up to date with Bedrock AgentCore and consolidating into our mesh. Today, there's a lot of isolated AI experiences within Sage. We want to bring this together as a single mesh. Then our next step after that is to build more agents. We will be publishing internally our own reference architecture for how we need to build agents. Thank you for your time. I'm going to hand you back over now for another piece on AgentOps.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/2970.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=2970)

### AgentOps: Managing the Lifecycle from Prototype to Production

Okay, AgentOps.  It's not a single solution that we can provide. It's not a single tool. Ops is a collection of processes and tools so that you are able to take these and manage it in production. If you take the lifecycle from prototype to production, every step is ops is about giving that capability and components to move to the next step. So in the prototype phase, developers need access to different models. We need a way to experiment quickly with all of these different models and a way to experiment with different agent frameworks.

A/B testing is one approach. Then when it goes to development, you give an environment where they can deploy this in a sandbox environment and start to integrate observability into it, start to run evaluation. Development can also do evaluation and then integrate into these other services that we talked about, like gateways. Then when it comes to testing, it's more about end-to-end QA testing, but you can also run evaluations there because maybe you have a different set of data in the QA system. Then once it goes to production, there's live monitoring, which we already talked about, but it's also about security and governance, keeping auditability, doing audits of the system regularly, and then having all of this bias and toxicity and risk assessment done regularly.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3070.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3070)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3090.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3090)

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3110.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3110)

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3140.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3140)

It's also about the DevOps process, managing the lifecycle of these applications and agents. Agents have to be decommissioned, and models undergo version changes.  So how do you put all of this together? That forms the big term, what we call operations. Now, giving an example of what a pipeline might look like: let's say an agent developer has developed an agent.  You can have git flows to check out that code and build a container. I'm talking about a container here because I'm giving an example of AgentCore. You deploy to AgentCore and then run some evaluation.  This evaluation could be run in the dev account as a container, but not necessarily. As shown, you can run a script locally and test that dev endpoint against a golden dataset to make sure it's performing as expected. All good. You put it in a registry. Agent registry is something where there are some tools out there, or you can start to store this metadata in a database. From there, you're deploying it to QA and running more evaluation.  Maybe this dataset is different than the dev dataset. All of the time you are sending traces to, as an example, Langfuse. Now Langfuse has a feature called Environments where you can have dev, QA, and prod, so that you're isolating these traces, and that's a very good way to architect these different accounts.

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3170.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3170)

[![Thumbnail 3180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3180.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3180)

If it is not as expected, you start to optimize, and the cycle begins again. You make more changes, maybe use different models, maybe use different prompts.  Then once everything is okay, there's a manual approval always before going to production, and you deploy to prod.  Here you're doing online evaluation. This online evaluation could be something you build, or these tools also have capabilities to do online evaluation. For example, if you use Langfuse, you can use their online evaluation capability to do that. Just make sure you're sampling certain required data to be able to do that. Then you're collecting user feedback from the app. Everything goes into the observability system. This is an example of how an agent DevOps can be set up. Not necessarily saying everything has to be this way, but just an example for you to look at.

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3220.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3220)

[![Thumbnail 3250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3250.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3250)

### Operating at Enterprise Scale: Platform Mindset and Key Takeaways

 What operating at enterprise scale means: let's say you're a platform team or an IT team, and you are setting up this foundation. How do you set it up? If you go back to the cloud operating models that we all have seen, centralized gives a lot of good standardization. Decentralized gives a lot of agility to those end teams, but federated, where it's a balanced approach. So maybe where I say the platform core, that is our foundation,  maybe that gives some shared services. For example, model catalog and gateway is a great example. That could be a centrally hosted thing. Now that doesn't mean it's one deployment. It could be several deployments, but you are controlling it and governing it in a central fashion.

Agent foundations, what I mean by that is runtimes and memory. Tools can also be part of the platform. If you are customizing models, meaning fine-tuning or pre-training, that infrastructure can be part of the central platform. And then observability. On the left-hand side is what these consuming applications are. Either they own their data, and a lot of the time, the platform team does not want to bother about the data, so maybe it's the consuming team's responsibility. Again, this can be sliced and diced in a different way.

[![Thumbnail 3310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3310.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3310)

[![Thumbnail 3320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3320.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3320)

The key thing to understand is to adjust to your operating model based on how you have been organized.  Just to recap, this was the foundation we talked about, and it is extensive. What you need to keep in mind is to set up the core capabilities.  You don't need everything. What you need to build those applications is more important. However, there are certain things that you absolutely have to build, which are observability and safety.

Have a platform mindset. I think it is increasingly important because governance is a big piece, especially for agentic AI. How do you centralize this governance? How do you manage the risk centrally? Having that kind of platform mindset is valuable, but not so much that it stifles innovation. So design for flexibility. When you centralize, it often stops the agility factor, right? You become a bottleneck for all these different teams to build and deploy these applications. So there is a fine balance, like we talked about with the federated model. Here are some resources for you.

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3380.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3380)

[![Thumbnail 3410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5bc203215be0b191/3410.jpg)](https://www.youtube.com/watch?v=0sfKGUZ6kcg&t=3410)

Not all of it is in the first blog, which is the first one.  The second one is a blog that we published talking about the LLM gateway solution that we showed. The first one at the bottom is the guidance that Aamna was talking about, the solution they have published as open source. Take a look at it. It will give you that leg up to start building your foundation. The second one at the bottom is the MCP registry and agent registry, our solution that we talked about. 

If you want to continue building your agentic AI skills, here is a Skill Builder link. Please feel free to explore that. Do not forget to take the survey. It is important for us to get feedback, both good and bad. Thank you for your time. I appreciate all of you being here.


----

; This article is entirely auto-generated using Amazon Bedrock.
