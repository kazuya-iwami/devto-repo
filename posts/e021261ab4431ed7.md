---
title: 'AWS re:Invent 2025 - SageMaker HyperPod: Checkpointless & elastic training for AI models (AIM3338)'
published: true
description: 'In this video, Amazon SageMaker HyperPod team introduces two major features: elastic training and checkpointless training. Elastic training enables dynamic scaling of training jobs from 4 to 12+ nodes based on available cluster capacity while maintaining loss convergence, eliminating manual resource management. Checkpointless training reduces recovery time from 15-30 minutes to under 2 minutes on 2,000+ GPU clusters, achieving 95% goodput through optimized collective communication, memory-mapped data loading, in-process recovery, and direct state hydration from healthy processes. Salesforce''s Antonio shares their two-year HyperPod experience, demonstrating batch inference workloads and introducing the LZ penalty algorithm that reduces degenerate repetitions to below 0.1% while maintaining inference performance impact under 0.5%.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/0.jpg'
series: ''
canonical_url: null
id: 3093258
date: '2025-12-08T21:55:57Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - SageMaker HyperPod: Checkpointless & elastic training for AI models (AIM3338)**

> In this video, Amazon SageMaker HyperPod team introduces two major features: elastic training and checkpointless training. Elastic training enables dynamic scaling of training jobs from 4 to 12+ nodes based on available cluster capacity while maintaining loss convergence, eliminating manual resource management. Checkpointless training reduces recovery time from 15-30 minutes to under 2 minutes on 2,000+ GPU clusters, achieving 95% goodput through optimized collective communication, memory-mapped data loading, in-process recovery, and direct state hydration from healthy processes. Salesforce's Antonio shares their two-year HyperPod experience, demonstrating batch inference workloads and introducing the LZ penalty algorithm that reduces degenerate repetitions to below 0.1% while maintaining inference performance impact under 0.5%.

{% youtube https://www.youtube.com/watch?v=r9J10L2K0F4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/0.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=0)

### Introduction: Speakers and Session Overview

 Good morning. Hi, everyone. Welcome to our session today. We'll be talking about checkpointless and elastic training on Amazon SageMaker HyperPod. I am Anirudh Viswanathan. I'm a Senior Product Manager with the Amazon SageMaker HyperPod team. I work primarily on distributed training at scale and framework level optimization. I'm here with my colleague, Arun.

Hi, everyone. My name is Arun Nagarajan. I'm a Principal Engineer with the Amazon SageMaker AI team. I provide technical leadership for many of our products in SageMaker, including HyperPod. Thanks Arun, and we have a special guest. We also have Antonio from Salesforce.

Hey everyone, I'm Antonio. I'm a researcher on our Salesforce AI Research team. I've been using HyperPod for almost two years, and I'm happy to tell you about some of our experiences and some of the cool things we've been working on with HyperPod. Awesome. Thank you.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/70.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=70)

So let's dive straight in.  Today, here's the agenda for our talk. We'll cover an overview of what Amazon SageMaker HyperPod does. We'll set the background on what it takes to train models at scale, and then walk through two new features, elastic training and checkpointless training, followed by a session from Antonio and close with a few takeaways.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/90.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=90)

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/100.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=100)

### Amazon SageMaker HyperPod: Purpose-Built Infrastructure for Foundation Model Training

So let's start off with what HyperPod does.  Amazon SageMaker HyperPod offers you  purpose-built infrastructure for foundational model training and deployment. With HyperPod, we've taken away all of the heavy lifting associated with manual cluster creation and managing your own self-managed environment, and have built training and deployment stacks up from the ground. We're providing you a way to improve your efficiency of model training, which in turn reduces the amount of time it takes for you to train and also lowers costs.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/140.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=140)

The benefits of HyperPod is that it offers you an extremely resilient training environment. The idea is that when you're training on HyperPod,  you're typically training on multiple AI accelerators or GPU devices. The idea with HyperPod is that it proactively screens every node or every device that's part of the training cluster for health checks and makes sure that the environment is set up correctly to give you the best training performance. All of this is done automatically for you and the environment is managed by Amazon.

It also offers an extremely scalable environment for model training where you can run training workloads on single-spine topology, where all of these devices are on the same rack and have the same physical wiring, so it gives you the best training throughput and performance. It supports a variety of instance types spanning the newest accelerators for both GPU-based and Trainium-based instances. HyperPod offers this managed service which is fully observable, lets you highly customize your environment, and has a variety of DLAMIs that can run on HyperPod as well.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/220.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=220)

So here's an overview of what HyperPod offers. It offers a choice of both Amazon EKS or Slurm for orchestration. It's compatible with a variety of frameworks from PyTorch, TensorFlow, NeMo, and so on. Under the hood, it runs  either with the NVIDIA NCCL libraries, which is the collective communication library, or with the Neuron runtime for Trainium-based instances. It also supports both GPU and Trainium-based accelerators, and of course you can connect a variety of storage via your data sources for model training.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/240.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=240)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/250.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=250)

### The Growing Challenge of Large-Scale Training: Cluster Failures and Underutilization

So, with this background,  let's now transition into the crux of the talk. Let's step back and think about some of the fundamental challenges with large-scale model training. I'd like to leave  this up on the screen for a moment. The graph here is data from an organization called Epoch AI, and they've tracked usage patterns in terms of cluster sizes used for model training over the past decade. We've seen the size of these clusters grow from hundreds of accelerators to thousands to now hundreds of thousands of accelerators, which is nearly a 20 times increase over the past 10 years. Cluster sizes are becoming larger as models become more sophisticated.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/280.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=280)

 The challenge is that as you have larger clusters, the likelihood of failure also goes up. So what we've got here, and let me walk you through this chart, is that every column represents a different cluster size. A cluster with four nodes has 32 GPUs, and we go all the way up to 1,000 plus nodes, which is upwards of 8,000 GPUs. Every row out here represents the probability of a node in that cluster failing during a specific hour.

The way we read this chart is that as cluster sizes grow larger, the likelihood of a failure actually increases. This means that if you're on a cluster with 1,000 nodes and you have a probability of failure at 0.02%, that cluster is going to likely experience a fault every four or five hours.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/360.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=360)

Let's think about this practically. Over the duration of a single day's worth of training, this cluster is going to be experiencing four or five faults, each of which can probably take upwards of one hour for recovery operations. Think about this: that's essentially 1,000 nodes, 8,000 GPUs sitting completely idle during recovery operations, just waiting for recovery and resuming from your checkpoint.  That's essentially a significant challenge.

Here's what happens when model training actually kicks off. Typically with model training, you have a sequence of steps where at every step, there's some amount of data that the model sees, does a weights update, and then goes on to the next step. There are periodically model snapshots or checkpoints that get saved, and checkpoints are typically done at a predefined frequency or cadence. Anytime there's a failure, so in this case there's a failure that happens after step number four, you need to essentially roll back to your last safe checkpoint, redo steps three and four because they happened after the last safe checkpoint, and then resume training.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/430.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=430)

During this entire process of checkpoint-based recovery, you need to roll back training, redo a bunch of steps, and then resume training, which means that you're essentially paying a cost where a cluster is sitting idle and you're redoing work that was already previously done. That's one challenge where as cluster sizes grow, the probability of failure tends to increase, and recovery from failure also starts to become more complex. Additionally,  there are other challenges with clusters that become larger.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/470.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=470)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/480.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=480)

The larger the cluster, the more likely this cluster is going to be used by multiple teams within the same organization. You can think of this cluster running a bunch of fine-tuning jobs, short-lived experiments, inference workloads, long-lived pre-training runs, and so on. Even if you look at specific inference workload patterns, you have essentially a peak during the day and then a drop-off at night, which means that your cluster utilization is going to vary over time. There are going to be these pockets of opportunity at night  and during weekends where the cluster is completely underutilized. Again, this means millions of dollars potentially sitting idle, where other workloads could have opportunistically used this capacity. 

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/490.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=490)

### Elastic Training: Dynamic Scaling for Optimal Cluster Utilization

With that background, let's talk about the first feature that we've launched. It's called elastic training on Amazon SageMaker HyperPod.  The fundamental challenge that we're solving with elastic training is this problem of infrastructure underutilization. Going back to the previous graph where we saw diurnal cluster utilization patterns, we wanted to think about ways to optimize cluster utilization.

The fundamental challenge that we have today is that training workloads are fixed in the sense that once you kick off a training job, you have the same number of nodes and the same number of accelerators that participate in this job from start to finish. What this means is that anytime there are more resources available, these might be nodes that free up from other jobs that had finished, you need to essentially stop this training job, reconfigure a bunch of hyperparameters, and then resume training to use the additional set of resources.

On the flip side, let's say there's a higher priority workload that needs nodes occupied by this current training job. The only way to give back nodes is to halt this training job completely and return resources. It's this all-or-nothing choice that customers face, where you have to either overprovision the cluster to ensure training continuity or essentially stop the job entirely to give back resources. This entire overhead of workload management can mean customers spend hours observing resources in their cluster, manually defining hooks to dynamically scale up or scale down their training workloads.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/580.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=580)

With elastic scaling on  Amazon SageMaker HyperPod, we've enabled your training workloads to dynamically scale up and down in response to available cluster capacity. What this means is that training jobs on HyperPod can now automatically scale up when free resources become available and scale back down when those resources are requested by higher priority workloads. All of this happens under the hood.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/610.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=610)

You simply configure your training job, kick it off,  and then anytime new resources become available, the job scales up and it scales back down when those resources get consumed. In turn, this simplifies your operations because you no longer need to manually babysit the cluster, observe free nodes, and reconfigure these training parameters manually. This is taken care of completely under the hood, and it also preserves training convergence by ensuring that the global batch size, or the amount of data that is seen at every step, remains constant across the duration of the training job.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/660.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=660)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/670.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=670)

With this, customers can easily get started either through the HyperPod recipes, or you can make custom modifications for your own scripts to accelerate your training workflow. So here's what it looks like in an animation. Let's say you start off a job with four nodes. That job now can scale up to eight  nodes because four additional nodes became available. It's running at eight nodes, four more nodes become available, so we've gone from four to eight to twelve.  At twelve nodes, what happens is there might be a higher priority workload that needs a couple of nodes. And so instead of halting completely, this job is able to scale back down from twelve nodes to eight nodes and continue to run in the cluster.

Once resources open up, it scales back up to twelve nodes and concludes. What's notable to observe here is that despite the number of nodes participating in this workload fluctuating over time, the loss function continues to decrease, which is most important for training continuity. With that background, let's dive under the hood, and I invite Arun to walk us through some of these details in terms of how elastic training works. Arun, over to you.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/730.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=730)

### Technical Architecture Behind Elastic Training

Thanks, Anirudh. Now, let's dive into the technical architecture behind elastic training. We have modified the  HyperPod training operator to support elastic training mode. The operator works through four key features, and we will talk through them. The first is continuous cluster monitoring. Whenever you have new resources available in your cluster, for example, you scaled up your cluster or a currently running training job completed, new resources are available, and the cluster sends a scale-up notification for your jobs.

The second is graceful preemption. Whenever you have a lower priority job, currently the only option for you to give up resources is to fully terminate the job. You cannot give partial resources. Even if the higher priority job did not need all of the resources occupied by the lower priority job, we simply have to terminate the lower priority job. With our system, we are offering an ability so that you can continue to run the lower priority job with reduced resources. Sure, the lower priority job will run with lower throughput, but your cluster utilization is going to be high.

This is a game changer because of two things. One is that previously the lower priority job, which would have been kicked out, is currently running and making progress. And the second thing is that your cluster utilization previously would have been lower because nobody was going to use that compute. Now it's going to be used for the lower priority job to make progress. The third part is automatic training stack reconfiguration. You may know that whenever you have to scale your training job from smaller count to a larger count or vice versa, you have to adjust the training parameters which are critical to maintaining the training quality and convergence.

And you typically have to manually adjust these parameters and restart your job whenever you change your world size. Our system offers you an ability to specify scaling policies so that you can tell SageMaker, here is the set of parameters which we need to apply for various world sizes, and you don't have to manually be involved in this process. The training operator automatically applies these training parameters at the right point whenever the scaling activities happen. And the fourth one is automated workload management, where the training operator works with the HyperPod task governance system so that your cluster utilization is maintained high.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/940.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=940)

Your administrator can continue to set the policies on limits and quotas on the workloads, teams, and projects involved in your cluster, and the elastic training still respects those policies so that your cluster utilization can be maintained high. The key thing to note here is that all of this is happening automatically. Your data scientists can focus on model building and not have to worry about infrastructure  stuff.

Now, building on that understanding, let's walk through how the system works end to end during one of these scaling activities. As we talked about, the system continuously monitors changes in resources, including addition or removal of resources. Once the system observes a change, it kicks off a scaling process. The system will send a synchronized coordination signal to all the training processes, telling them that a scaling event is imminent. Now the training processes know that they need to take a checkpoint so that at a later point in time, you can resume. The checkpoint typically consists of the optimizer state and model weights, and presumably your training code already has code for taking the checkpoints in the traditional sense.

Now that the operator has told all the training processes that they need to go take a checkpoint, the training operator will terminate all the training processes and restart them with a new configuration. That would be either with a greater number of training processes if it's a scale-up event or a lesser number of training processes if it's a scale-down event. When these training processes bootstrap, they realize that they have been restarted. They will load the checkpoint, understand that they need to reconfigure and redistribute the checkpoint, and they will continue training. All of this happens automatically for you, and it takes a few seconds for this to complete, so that your training job can continue and your GPUs don't have to sit idle.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1060.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1060)

### Getting Started with Elastic Training: Zero-Code and Custom Adoption Paths

All right, now to the fun part. How do you get started?  We have made this incredibly simple for you to get started. If you are using standard architectures like Llama, Qwen, or DeepSeek, you can get started with zero code change. That's right. There is no code change needed. All you have to do is bring in your data and set up the minimum node count and maximum node count, and that's it. You can kick off the training on HyperPod with the kubectl command or with the HyperPod CLI and you're good to go.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1120.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1120)

The HyperPod recipes do all the heavy lifting needed, including setting the right parameters to invoke the HyperPod training operator to be running in an elastic mode. They also supply the elastic scaling policies needed for various node counts so that the learning rate and batch size parameters are optimized for various node counts. Now, if you have an advanced setup  and you have custom training scripts, then we have a path for adoption as well. This is slightly more involved, where you have to make some changes to the job specification. You need to supply the elastic scaling policy where you have to specify the minimum replicas needed, maximum replicas needed, and the scaling unit, which is what we call the incremental step count.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1150.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1150)

Then you need to make some changes to the training script itself.  You have to import the elastic event handler from our elastic agent, and you have to handle the events sent by our operator. Remember how we talked about the operator being responsible for notifying all the training processes that a scaling event is imminent? This is the code behind that. You can see that you can register for those notifications and you can invoke your existing checkpoint load and save primitives, and that's all you need to do.

I want to highlight that even with the custom adoption path, the heavy lifting is still done by the operator on detecting the resource changes, notifying all the training processes, terminating them, and restarting them with a new configuration, and applying various scaling policies.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1220.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1220)

All of that is handled for you automatically. You just need to implement certain hooks so that the operator and your training code can work together. Here's a visualization of the elastic scaling job in action. You can see that  we had varied the world size from 1 to 2 to 8. You can see the tokens per second closely following the throughput, which is closely following the world size available for the training. You can also observe that the loss continues to decrease and the training job was not interrupted in this process.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1250.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1250)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1260.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1260)

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1270.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1270)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1280.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1290.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1300.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1300)

To wrap up,  elastic training marks a  fundamental shift in the way you think about AI infrastructure. Long gone are the days where you have to manually allocate certain resources for certain jobs and then you have to play Tetris to make sure that every job  fits in your system. We now have a system where you can configure it to dynamically adjust to the cluster  reality, and we also offer you a mechanism so that you can continue your training without sacrificing your training quality and convergence.  Whether you use the recipes or you go with the custom path, the path to adoption is pretty straightforward.  We encourage you to start with a pilot project and see the gains for yourself.

### Checkpointless Training: Reducing Recovery Time from Hours to Minutes

With that, I will hand it over to Anirudh, who's going to talk about our next part, which is the checkpointless training on HyperPod. Thank you, Arun. All right, so this is the next shiny feature that we have for you today. Checkpointless training on HyperPod was announced yesterday during Swami's keynote. In this section, we'll dive deeper into how it works under the hood.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1330.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1340.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1340)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1360.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1360)

 So, going back to sort of set the context here, right? The  fundamental challenge is that with large clusters, and even smaller clusters for that matter, checkpoint-based recovery leads to idle time during training. So anytime there is a fault, essentially you need to halt the cluster, reload your last safe checkpoint, and then resume training. But the challenge is that even recovering from the last safe checkpoint isn't as trivial as it sounds,  and this is primarily because with checkpoint-based recovery, there's a sequence of events where every event needs to happen first before the next event can happen. So it's this cascade of events that are blocking and sequential.

The idea is that the moment there's a failure, the failure needs to be detected. Then you need to reinitialize communication and set up the topology of all of the nodes that participate in training, go back, identify your last safe checkpoint, reload that, reload data loaders, and a whole sequence of events happen before you have that first training step that happens post recovery. So the challenge essentially here is that the sequence and the sequence of sets of steps which are blocking means the cluster is idle, and with hundreds of nodes or thousands of nodes, that's millions of dollars in wasted compute during recovery operations.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1420.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1420)

So with checkpointless training, we've enabled cluster recovery to drop from hours on self-managed  clusters to under a few minutes on HyperPod. And in turn, accelerating recovery times essentially means that you come to market faster, because if you're training models over a long duration, there can be days often spent in recovery operations over a long running training workload, which now can be done in basically a few minutes. That's the fundamental shift in checkpointless training and what it enables.

The idea with checkpointless training is that once you've configured and set it up, failure recovery is fully automated. And there's no longer any manual overhead that you need to take care of when recovering from training. The idea is that you are able to maintain continuous training progress without having to roll back to your last safe checkpoint. In turn, that means essentially no more lost progress or no more wasted compute because you essentially pick up from the current epoch that you are running, even in the event of a fault, without having to roll back a few steps to the last safe checkpoint.

And so this capability essentially enables you to scale with ease across tens to hundreds of thousands of accelerators, and we have pre-created recipes that you can use for your own training workloads, as well as offer a suite of components that you can pick and choose and integrate with your own custom PyTorch-based scripts.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1510.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1510)

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1520.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1520)

Here's an overview of what checkpoint-based recovery looks like versus what checkpointless training really does.  On the left, we have a cluster showcasing traditional checkpoint-based recovery, and the one on the right is essentially checkpointless training.  We start off where the cluster might experience a fault. Let's just let this loop in one sequence.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1530.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1530)

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1540.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1550.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1550)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1560.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1560)

 If you look at node 7, node 7 is going to experience a fault, so it turns color, and that's a fault that occurs.  With checkpoint-based recovery, you need to sequentially reinitialize each of those nodes, which means that it can take several minutes for that initialization process to happen.  You need to then reload your last safe checkpoint. Once that's done, you rehydrate your data loaders, so you essentially preprocess all of the data batches that need to get fed to the model,  and then resume training.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1570.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1580.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1580)

This entire sequence of recovery operations can mean tens of minutes to potentially an hour on large clusters, versus with checkpointless  training, you're able to recover in a couple of minutes or so. Here's what performance actually looks like. The latest  generation of Amazon Nova models were trained using this capability on thousands of AI accelerators. Internally, we've tested with cluster sizes with upwards of 2,000 GPUs, and traditional recovery, which is checkpoint-based, can be anywhere between 15 to 30 minutes. But with checkpointless training, we're able to recover in under 2 minutes, and this is essentially an 80% improvement in fault recovery capabilities.

We have similar results that hold true even at smaller cluster sizes. Even with clusters with 16 GPUs or 256 GPUs, we see that traditional recovery might take about 5 minutes or so, but with checkpointless training, it's an order of magnitude faster, and recovery happens in under a minute. What this enables is that even with clusters with thousands of accelerators, goodput is upwards of 95%. You're basically maximizing utilization of your training infrastructure with this capability.

To put this in perspective, if we look at this cluster of 2,300 GPUs, if you have a training run that's spanning a couple of months, traditional checkpoint-based recovery overhead over the duration of that training run of 2 months can mean multiple days are spent cumulatively just in recovery operations. Whereas with checkpointless training, that now drops to a few minutes, saving you millions in training costs over the duration of this training run.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1690.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1690)

### Four Key Innovations Powering Checkpointless Training

With that overview of performance, I invite Arun back to walk us through how checkpointless training works under the hood. Arun, over to you. Thanks, Annu. Checkpointless  training is built on four key innovations that work together to eliminate bottlenecks in the traditional distributed training systems. Let's walk through one by one.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1710.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1710)

The first one is the optimized collective communication initialization.  Typically in a distributed system for training, you have many processes running across many nodes, and they need to talk to each other to exchange data during training. For example, in a distributed data parallel training, the ranks need to all-reduce the gradients after finishing the back propagation operation. However, before the ranks can talk to each other, they need to establish communication with each other, and typically this is done through a centralized coordinating system, which is going to be a bottleneck when we are talking about thousands and thousands of processes, which we demonstrated in our previous slide.

This can add up to minutes because of this bottleneck in the centralized initialization system. With our system, we eliminated this bottleneck by moving away from the centralized coordinating system towards a peer-to-peer connection establishment through HyperPod signals. This helped us save minutes, and we were able to do this in seconds.

The second innovation is about memory-mapped data loading. As you know, the training processes need access to the data whenever they need to start training. However, the data needs to go through some sort of preprocessing before the actual training can begin.

Depending on how complicated your preprocessing pipeline is, this step can take up to minutes as well. You may be wondering what's the big deal about spending a few minutes before the training starts to finish the preprocessing. That intuition is right. However, when you have failures, you still have to pay the cost repeatedly. So every time a failure happens, you need to pay the cost of preprocessing your data and feeding it into the training pipeline. During this time, this is a synchronous operation, and all of the expensive compute is sitting idle.

We solve this by introducing a cache so that the preprocessed data is saved in the cache using shared memory and memory-mapped data files. When the training processes resume, they do not have to wait for reprocessing of the data which they are expected to wait on. In this new world, they can instantly get started because the preprocessed data is already readily available for them.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/1880.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=1880)

The third innovation is the in-process recovery.  In traditional distributed systems, as we talked about, there are many processes and many nodes which are collectively running together. The current failure recovery mechanisms require you to terminate the whole job and restart it, meaning all the processes are terminated and all their state is lost. Then the new processes are kicked off, and they need to rehydrate their state.

With our system, we are replacing the failed processes with hard spare processes, and we do not touch the healthy processes. We leave them alone with their states. These spare processes join the training, and they continue to train with minimal interruption.

The fourth and most innovative component is the checkpointless recovery itself. Traditionally, you will have to take checkpoints. The training processes need to take checkpoints at certain intervals, and when a failure happens, the processes need to roll back, repeat some of the steps, and continue. With our system, building on top of the in-process recovery, what we have achieved is that the newly joined training processes do not need to load from the checkpoint. Rather, they hydrate their memory from the other healthy processes directly using the high-speed network.

This way, the training interruption time is reduced dramatically, and that's how we achieve the sub-minute recoveries that Anil shared, and also the 95% goodput, even at large scale. All right, that sounds exciting. So how do you get started?

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2030.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2030)

### Implementing Checkpointless Training: From Recipes to Custom Integration

Just like the elastic training, we have made this quite simple for you to get started, and we have two approaches. First is, of course, the HyperPod recipes, where you can get started if you're using standard architectures  by using the recipes, downloading it, and then using the kubectl command to run it on HyperPod. The recipes do the heavy lifting for you, including setting up the operator to run in this checkpointless mode, setting up the collective communication optimization, and then the memory-mapped data loader in-process recovery. All of that setup is handled for you, and you can just kick it off. We think this is the fastest path for adoption for most teams.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2070.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2070)

But if you have special requirements and custom training code, we have an incremental adoption path for you.  We have componentized each of these so that you can incrementally adopt each component, observe the benefits, and continue to adopt all of the features together.

The first one is the optimized collective communication initialization, and it's the easiest to adopt. You just need to set a couple of environment variables to your training jobs. The first one is instructing it to not use the centralized initialization system.

The second one is telling that it needs to use the HyperPod-based optimized initialization, and that's it. You already saved a few minutes of recovery time every time a fault happens.

The second one is the memory mapped data loader. We talked about how we have a cache where you can store the pre-processed training data so that during fault recovery, you do not need to spend all the time reprocessing those data. This one is also pretty easy to adopt. We have provided a library with a wrapper, which is quite non-intrusive, and all you have to do is to point to a cache location for storing the memory mapped files.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2120.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2120)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2160.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2160)

 Moving on to the in-process recovery and checkpointless recovery components, this one is a little more involved. You have to do a couple of changes, including changing the strategy used by your PyTorch training code to point to the HyperPod specified checkpointless strategy. This instructs the framework to not use the classic checkpointing, but rather use the checkpointless recovery that we have implemented.  Secondly, you do have to wrap your training loop with the HyperPod wrapper annotation that we have provided, which takes care of the heavy lifting for health checks and having the coordination with the HyperPod PyTorch operator. You can imagine that all the complicated handshakes which are needed to rehydrate the state from healthy peers, all of that is taken care of for you by these two changes.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2230.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2240.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2250.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2260.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2260)

 I want to conclude by saying that we took a deep look at the existing framework level limitations that are present in the traditional  distributed training systems, and we have eliminated a lot of bottlenecks from these systems  so that you as our customers get the most value out of the compute instances that HyperPod offers.  I hope that we have convinced you to give it a try on all these new features, and I'm excited about what you're going to build with them.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2270.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2270)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2300.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2300)

### Salesforce's Experience: Managing Heterogeneous Workloads and the LZ Penalty Innovation

 With that, I am thrilled to invite Tony, who's going to talk to us about how Salesforce is using HyperPod. Tony, thank you. Hey everyone, so I'm Tony with Salesforce AI Research, and we've been HyperPod customers for over two years now, and  I wanted to take a little moment to just share with you what our use case looks like. So we have very heterogeneous workloads, of course, standard LLM training, but we also have things involving multimodality, which is a lot of speech and image processing that also happens on the cluster. We do a lot of fine-tuning in RL, so these are high volume jobs that are often much shorter than long running LLM training and can be kind of bespoke and custom, and we've got batch inference jobs which can run at varying sizes or varying time periods. And checkpointless and elastic training helps us manage this heterogeneity in an easy way.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2350.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2350)

 So I'm going to dive a little bit deeper into what exactly we do for batch inference. So I like to use SGLang. It's an open serving framework, happens to be my favorite, and the way we set this up is we have a job script which can run on kind of a head or a login node style architecture. It's really lightweight. All it does is load balance to one of your end worker nodes, and each worker node is a very simple, straightforward SGLang server. Of course, all of the workers are connected by disk substrate distributed file system or S3, anything of that nature. And I'll explain, I'm going to dive a little bit deeper into one particular use case within the umbrella of batch inference because we have several, and I will motivate this use case as follows,

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2410.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2410)

I want to pause and draw a little bit of contrast between when we think of AI or even just learned software and how that is a little different than how we've come to traditionally consider human-programmed software. One key contrast is this idea of nondeterminism.  You can think about this at the app layer. At the app layer, traditionally if software was working correctly, it would give you the same answer every time. Now we have a lot of AI systems where even in the app layer there are key icons like regenerate and regenerating responses, and trying different prompts and tweaking is actually a core part of the application experience.

Even at the algorithmic layer, outside of the application, you have things where the neural networks themselves are sampled in a stochastic manner, and that's actually at the algorithmic level the entire way that it's supposed to run, again in contrast with how most applications have been built on deterministic algorithms. Even going one layer lower into the stack, standard CPUs are highly deterministic pieces of hardware, whereas when you're running on AI accelerators, it's actually in the spec that you can get nondeterminism when running due to non-commutative properties of the floating point arithmetic on these accelerators. All of this is to say that there's a very deep way in which nondeterminism and stochastic behavior is embedded in the AI stack.

From our perspective at Salesforce, we think about what is the business impact of this because we have customers across a variety of industries. Healthcare and financial services are two industries in particular that are very much concerned with this concept of nondeterminism. They care a lot about having deterministic and auditable outputs, and for them having a way to grapple with this nondeterminism is important. They're still figuring out how to handle this and they're really trying to navigate this. One thing that could be of interest to them is how could you build AI systems that help bridge this at various levels of the stack.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2560.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2560)

As I mentioned, it was a multi-layer problem. That was setting  the broader picture. Let's now zoom in and take a look at what actually happens with an AI model and how this manifests. In sampling of a language model, there's this parameter where you may have seen, depending on if you've used things like an open source or an OpenAI API, where there's a temperature flag. This temperature flag can be set to various values. Zero is deterministic in theory, although of course there's hardware nondeterminism, but you can set this higher and that makes the answers more or less random.

What can happen is that when you have reasoning models and you set too low of a temperature, the reasoning model will degenerate and it will basically produce completions that are repetitive nonsense. The reasoning models, for example, many providers no longer even expose the temperature flag on those APIs because they don't want you to basically run into this edge case. They basically set a higher temperature and they don't let you lower it. You may also recall the frequency and the repetition flags, which were two tools in the API that helped you more easily control this and mitigate against this. As we've seen, it doesn't fully always work.

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2700.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2700)

To motivate why we're studying this problem, you can actually see this screenshot. This is an actual screenshot from a leading software agentic coding tool. You've probably used it if you've written code or tested it. As you can see, there is very clearly degenerate repetition. While this is infrequent, it does happen in the wild, and the main penalties to address this fail in all cases, or they don't succeed in all cases, I should say. One work we did that heavily relied on batch inference on HyperPod was this LZ  penalty development that we worked out this year. So why are we working on a new penalty?

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2710.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2710)

As I mentioned, there are already industry standard penalties that you can find in standard  inference APIs: the repetition penalty and the frequency penalty. But neither of them fully solves the problem.

So what is the repetition penalty? You can think of it as a binary approach, which basically says if the token already appears in the generation, I'm going to downsample and make it less likely for that token to appear again. This basically entirely ignores token frequency. It's a very crude metric and it's also lacking in memory. It penalizes all tokens equally regardless of recency, so it doesn't actually work and experiences up to 4% degenerate repetition rates.

For the frequency penalty, which is the other industry standard, we can also see that empirically it doesn't fully work. It tries to be more sophisticated than the repetition penalty by actually scaling the number of appearances and scaling the penalty based on this. But the problem is that it doesn't actually account or normalize for the length of the sequence, which can result in a situation where for very long completions, which is what you get when you're doing reasoning because the models can basically think for thousands of tokens, you get very long sequences where the frequency penalty starts to fail catastrophically when common tokens eventually get banned. So after a certain point, it's like saying you have a certain number of commas you're allowed to use in your response. Once you exceed that number of commas, you can't use anymore. What results is the model wants to use a comma but it can't, so then it makes up another token in its place, and it starts to degrade the response until it just becomes degenerate nonsense.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2820.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2820)

So I'm going to tell you about the LZ penalty, which  you can think of as a variant of the two previous penalties, but it's different on a fundamental level. It's based on information theoretic principles that enable you to more rigorously address this problem. So you may have heard of LZ77, but I'm certain you've used it because you've zipped a file on your computer. If you've done that, that's LZ77. It's a data compression algorithm that has this property of being universal, which means it can basically work for any data type.

We won't go too much into the details of it, but essentially what it does is it lets you look at the historical parts of the file and then looking forward to the next chunk of the file, it allows you to pattern match the next chunk compared to a lookup table or a sliding window of previous chunks. And that pattern match is what enables the compression. So as I mentioned, it addresses n-gram repetitions, and it's based on pattern matching, not just naive frequency counts.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2900.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2900)

So how do we actually apply this LZ algorithm to the language model? During the sampling of a language model, it looks like this, where we have a context. That context gets fed into the language model, which produces what's called logits.  These logits are what tell the sampler what probability to assign to every possible next token. With the LZ penalty, we in parallel from the language model are running the data compression algorithm. So you can basically think of it as running gzip or a very similar algorithm right there in parallel with the language model. And then we're taking the output of that data compressor, and we're fusing that output with the logits that come out of the language model. Then we're using this fused representation for the sampler.

What this enables is that the outcomes that the data compression algorithm thinks are very repetitive become very much downweighted in the generations from the model. Because this data compression algorithm is actually the perfect thing to, in some sense, detect this kind of degeneration and repetition and redundancy.

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/2960.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=2960)

So empirically,  we can see here, the charts on the left are showing average score across temperatures. And so the temperature, again, is this parameter that controls how random the sampler is. Temperature zero, being the value most there to your left, is going to be greedy decoding that's perfectly deterministic. And so as we can swipe or scan the temperatures, we can see that the LZ penalty there in red is consistently within error bars of the best with no penalty and the best with the various two other penalties, frequency and repetition. Again, those are our baseline penalties.

And without degrading accuracy, which is very important, you don't want to degrade your accuracy. This is, by the way, a reasoning model, a Qwen 32 billion parameter reasoning model open source. We can see that the degenerate repetitions are uniformly, essentially virtually eliminated with the LZ penalty uniformly across the board at any temperature. Below 0.1% empirical discovery of any kind of degenerate repetition, whereas all the other penalties are maybe generously let's say somewhere between 1 to 4%, which is again pretty infrequent, but it also depends on your perspective because getting a situation where you actually do see degenerate repetition is very bad. And so this also again goes back to why many of the model APIs don't actually let you use reasoning models at temperature zero to let you kind of not have to deal with this problem of running into these degenerate repetitions.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/3070.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=3070)

So here is a little bit more of an in-depth  result. These heat maps show every value of the repetition penalty in terms of its strength. So again, if you can tune both the temperature and then you can tune the strength of the penalty, and the stronger the penalty, the more it impacts the ultimate sampling from the model. The very top row of every heat map is the row that is the LZ penalty, so that's kind of like our row at just the one strength that you need. You don't need to tune it, and again you don't need to tune it because of the universal property of the data compression algorithm. It works for any data distribution. But the other ones you do need to tune, and you can see that on the repetition penalty heat maps, you very often at low temperatures and occasionally sometimes even at higher temperatures do run into these kind of catastrophic failures with the degenerate repetitions.

[![Thumbnail 3130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/3130.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=3130)

Bringing things back to inference,  you might be asking, okay, while you're doing all this extra work with the LZ penalty, how does it impact your inference performance? And the answer is that the amount of work you do with the LZ penalty compared to the language model is very small. One, it parallelizes very neatly on the GPU, and two, as I mentioned, very, very small. At even just a 1.5 billion parameter model, the throughput slowdown is less than half a percent, right about 0.5%. Latency is nearly on par, and then as you go up to even 32 billion parameters, it goes down from 0.5% to 0.03%, essentially vanishing as the model even reaches medium size, the kind of inference impact of having to do this extra work for the LZ penalty.

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/3190.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=3190)

So one thing that we did was, you know, this is one line of research we worked on  in parallel with dozens of other jobs all running on the same HyperPod. The final set of experiments that we presented, a subset of them here, you can reference our technical report for the full set of experiments. That was validated over about a week with other jobs scaling up and down. We went through over 25 billion tokens and over a zettaflop of compute. And yeah, so again, being able to run this and having elastic and checkpointless scaling up and down is very synergistic with how we want to scale up and down our batch inference and all of our other job workloads.

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/3240.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=3240)

### Conclusion: A Paradigm Shift in AI Infrastructure and Training Resilience

So with that, I'll have Anirudh come back and wrap us up. Thanks, Antonio. So folks, to summarize, we covered a couple of  key launches in the session. So with elastic training, it fundamentally changes the way training jobs run on HyperPod, and so we can dynamically scale them up or down to take advantage of idle capacity in the cluster. This ensures training continuity and better cluster utilization. You can think of elastic training as sort of playing Tetris with your cluster capacity, growing and shrinking jobs as more capacity becomes available.

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e021261ab4431ed7/3300.jpg)](https://www.youtube.com/watch?v=r9J10L2K0F4&t=3300)

And the second key launch is checkpointless training. This is a paradigm shift in terms of training resilience, and instead of going through a checkpoint-based recovery, which can take up to one hour, checkpointless training can offer you recovery times that are in minutes. This has goodput numbers upwards of 95% on clusters with thousands of nodes. And with that, we wrap up our talk. Here are some resources. We're excited to see what you build with this. We'll be available offstage for any questions that you might have.  Please do leave feedback on the session within the app. Thank you so much.


----

; This article is entirely auto-generated using Amazon Bedrock.
