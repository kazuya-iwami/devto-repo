---
title: 'AWS re:Invent 2025 - Implementing Human-in-the-Loop Controls for Multi-Agent AI Systems (CNS428)'
published: true
description: 'In this video, Dhiraj Mahapatro, Principal Specialist Solutions Architect at AWS, explores human-in-the-loop (HITL) implementation in Agentic AI systems. He traces the evolution from simple Generative AI assistants to autonomous agents, emphasizing that while systems move toward autonomy, human oversight remains crucial for ownership and accountability. Key HITL implementation points include high-stakes decisions, irreversible actions, regulatory compliance, and trust-building phases. He demonstrates practical HITL mechanisms using MCP elicitations, Step Functions wait for callback, Lambda durable functions, and LangGraph capabilities. The talk introduces progressive autonomyâ€”starting with maximum human control and gradually reducing intervention as agent core evaluations mature. He references the Harvard paper on human-algorithm centaur systems and provides GitHub resources for A2A protocol implementations with travel and weather agents.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/0.jpg'
series: ''
canonical_url: null
id: 3093037
date: '2025-12-08T19:23:53Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Implementing Human-in-the-Loop Controls for Multi-Agent AI Systems (CNS428)**

> In this video, Dhiraj Mahapatro, Principal Specialist Solutions Architect at AWS, explores human-in-the-loop (HITL) implementation in Agentic AI systems. He traces the evolution from simple Generative AI assistants to autonomous agents, emphasizing that while systems move toward autonomy, human oversight remains crucial for ownership and accountability. Key HITL implementation points include high-stakes decisions, irreversible actions, regulatory compliance, and trust-building phases. He demonstrates practical HITL mechanisms using MCP elicitations, Step Functions wait for callback, Lambda durable functions, and LangGraph capabilities. The talk introduces progressive autonomyâ€”starting with maximum human control and gradually reducing intervention as agent core evaluations mature. He references the Harvard paper on human-algorithm centaur systems and provides GitHub resources for A2A protocol implementations with travel and weather agents.

{% youtube https://www.youtube.com/watch?v=SC3pHo-CycI %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/0.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=0)

### The Evolution from Simple AI Assistants to Complex Multi-Agent Systems

 Hello everyone. Thank you for coming to the session, Lightning Talks CNS428. My name is Dhiraj Mahapatro. I'm a Principal Specialist Solutions Architect. I focus on Amazon Bedrock and Agentic AI, and I focus on startup customers. I've been with AWS for a little bit more than six years, and this aspect of what we're going to talk about, human-in-the-loop, is applicable to all workflows, microservices, and now we are talking about agentic systems and why this is useful.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/50.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=50)

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/60.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=60)

Just a show of hands. I asked this question before, but how many of you are using agents in production today? None. How many of you are using inferencing, LLM inferencing in production today? Most of you. What I wanted to show is  how did we evolve from normal assistants to actually autonomous agents and why autonomous agents are important in this era. And where human-in-the-loop is important to implement  and how can you implement HITL with some of the mechanics, some of the key takeaways, and a couple of resources that I find very useful for you.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/70.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=70)

 As this progression, this is a timeline that shows you how we started with simple Generative AI assistants, mostly towards chatbot functionality, but we slowly evolved over time. Right now we're talking about all the way on the right, where we are talking about Agentic AI systems. Agentic AI system doesn't mean that it has to be front-ended by a chatbot. You can invoke agents asynchronously. Let's say something happened in your system, it emitted an event, and you have an event subscriber that can go and work with multiple agents. So agents can work behind the scenes. It doesn't have to be a customer sitting at the front on a chatbot.

That's where we are. As and when you go right, you will see we are focusing from higher degree of human oversight to low degree of human oversight when you're moving towards autonomous systems. However, there is a concept of ownership in every aspect when you build applications and you have agents in place. When you run them in production and something goes wrong, you can't blame an agent. It's the developer or it's the person who owns that application who has to take the ownership. That's where Agentic AI with some degree of human-in-the-loop is very important to build.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/190.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=190)

Before we jump into how and when HITL should be implemented, I wanted to give you a background of how we started with model inferencing, specifically on AWS, and where we are right now. That will give a clear picture of why we should think about human-in-the-loop right from the very beginning when we build Agentic AI systems. A couple of years ago, if you had to build or if you had to use a model on AWS, these are the options you had. As a user, you'll come, you need a compute layer where you have to run a model for inferencing,  and some of the options for you were SageMaker. Today we have Bedrock. You can run your model getting from Hugging Face, and you can run it on ECS, EC2, or EKS. And then this is a simple example of a ChatGPT if you think about it. You have a model, you can ask questions, you get answers back, so on and so forth.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/210.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=210)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/240.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=240)

 Then we evolved into systems where you need some context or some history to be maintained in that session or in that discussion so that you can get better results. That's why we talked about having a persistent store where you can persist some of the chat history, and then when the LLM is coming back with a response, it can understand the chat history and provide a response. Then we evolved our systems to build contextual  responses with chat history as well. Now you have a user calling a level of compute that talks to a model, but before talking to a model it has to use a vector store to get additional context for your organization or anything. This is simple RAG. Get the vector embeddings and augment those embeddings to the LLM to get a better response. So this is where we were.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/290.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/300.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=300)

In this scenario, all of those contextual data is mostly static in nature. I have my organizational standards, I want to bring my architectural standards, and if I want to use that as part of a vector store, these are all static data that I have to deal with. But what if you actually want to leverage the power of LLMs with real-time  data? That's where the discussion around tools and agents come into the picture. In addition to doing  RAG or LLM inferencing, if you have to deal with real-time data, you have to build something like this with agents and a bunch of tools that can go out on your behalf, call an API, execute a database query, and get real-time information.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/330.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=330)

That information will be augmented to the context of the large language model to do some work. Now this evolves because when you start building agents and tools, the very next step comes into the picture is  how can I secure the setup? How can I get observability? Because when you build APIs or applications, you need that observability in place. So with agents and tools, you will still need the same observability in practice.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/360.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=360)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/380.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=380)

You need some inbound authentication on who can call the agents, outbound authentication on who the agent can call, and so forth. Then you need guardrails around Bedrock, some observability using CloudWatch or third-party providers, and so forth.  This also evolves when you think about these concepts. We are evolving from simple LLM inferencing to how we are building agents today with MCP servers. If you look at it now, the big picture of how you do agents on AWS will look like this.  You will have agents running on a set of compute, some memory that maintains history and long-term memory, MCP server integrations for external tools that you want to use, and also A2A integration. If you know, A2A is a protocol that Google came up with for agent-to-agent communication.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/420.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/430.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=430)

While they're doing all of these things, you need observability with CloudWatch or third-party tools. This is one agent. We're talking about the capability of one agent that talks to multiple MCP servers or tools or other agents with some capability of memory and identity.  However, when you want to run in production, the actual production scenario will look like this.  You'll have an agent in your purview, but then multiple agents in your department, in your organization, and then you have to integrate with external tools, and so forth. You can imagine the complexity that you have to deal with, the chaos of this whole setup that you have to manage.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/450.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=450)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/470.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=470)

### Implementing Human-in-the-Loop: Critical Decision Points and Technical Approaches

That's where you have to figure out, if you build systems,  how and when and in which areas you have to build human-in-the-loop capabilities. You have to find those checkpoints where human judgment is very necessary, and if possible, whatever human insight that was given goes back to the AI system so that it can do a better judgment next time.  So what are those points? Think about high-stakes decision points. I'll take an example. If a doctor has to prescribe a medicine to a patient, you need some intervention and not rely on the entire agentic system to build a prescription for the patient. The doctor has to provide some capabilities to say, okay, this is a good medicine I can prescribe.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/500.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=500)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/520.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=520)

Then there will be cases where you  have to deal with irreversible actions. What if money gets debited from one account to another account, which you cannot undo? What if you completed a transaction which you cannot undo? These are the high-touch points where human-in-the-loop will be very influential.  What about regulatory compliance? There are countries and compliance standards where human oversight is always needed. Finding out those places in the setup, you will have to add some checkpoints where a human can come and provide a decision to move forward.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/540.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=540)

The most  important part is the trust-building phase. Let's say if you're starting new with agentic AI, it will be really good to have the human aspect in the entire setup so that you are building trust with the system. Once you see that the human is actually repeating the same task and has figured out that the AI or the agent is actually building what it is supposed to do, then you can slowly remove the human intervention part. This trust-building part is where, if you're doing a proof of concept, if you're starting with agents, then it will become very easy to start with human-in-the-loop and then go from there.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/580.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=580)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/600.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=600)

Plus, there will be some edge cases  and some ambiguity where you will definitely need human-in-the-loop wherever it is necessary. Going back to the big picture, there will be areas where you need human-in-the-loop when you're dealing with, let's say, MCP server integration or A2A integration with different agents,  or finally when you're sending responses to the customer. You need some oversight to see whether everything is okay and you should be good to send responses back to the user or complete a task.

Now that we've talked about where human-in-the-loop is required, where are the cases where human-in-the-loop can be avoided? Let's say you have a case where you have confidence in the threshold that you have set. So the agent came up with some response and the confidence score is higher. That's where you can avoid having a human in the loop.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/660.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=660)

If you maintain high audit trails and you go through those metrics and you know that the agent or the multi-agent system is doing whatever it is supposed to do, then you can slowly remove the human aspect from the whole setup. If you build a feedback loop, as I was talking about earlier, where the human can feed it back to the system, then eventually  you will tend toward removing the human aspect from the entire setup. This will give you a good process to gracefully degrade when human aspects are not required anymore. So what we're talking about here is progressive autonomy. You start with the highest level of control with humans in place, but you slowly progress toward getting full autonomy when you have built trust in the system for your clients and for your customers.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/690.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=690)

 If you want to build human-in-the-loop aspects in the case of MCP, MCP provides this capability called elicitations. If you look at this example, if the request is what is the flight status, then the MCP server will ask a question, give me a flight number, then I'll be providing you a flight status. So this is the core capability of MCP as the protocol, elicitations, and you can use it. If you're using Step Functions as a service to build workflows, then Step Functions has this capability of wait for callback.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/720.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=720)

 The Step Functions workflow can wait for you until an approval or a task token that is sent back with a success message, and then it will proceed. So this is how it works. When you ask for a token to be sent externally, the token will be sent through different applications or to an approver, and if the approver says okay, everything looks good, then the token is sent back and then Step Functions will resume the workflow execution and it will go to the next steps.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/750.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=750)

 Now, if you heard the keynote earlier, we talked about durable functions in Lambda also. So durable functions will do similar work to what Step Functions has to provide. If you're using tools like LangGraph for your agents, some of the capabilities include this: you can have a human aspect where if a response is coming back from the LLM and the human can approve, if it approves, then it goes to Node B or the graph Node B or A, and then it goes to graph Node C if there is a disapproval.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/790.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=790)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/810.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=810)

 You can change the state behind the scenes if a human is in place. So if a human says I'm getting an input which says foo bar, now I want to change that to baaz, and then the next node will get the latest updated value for the underlying state. So these are the ways where a human can intervene and  change the course of action that the agent was supposed to take. The last one is a human can intervene and say whether I want to execute a tool or I don't want to, or should I execute a different tool. This is in LangGraph.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/820.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=820)

 If you're building inter-agent communications with MCP and A2A, then there are other capabilities. So if you take an example of a travel agent, here is an example which I built using API Gateway, and A2A is the protocol, which is an A2A client in a Lambda function that calls a weather agent, which is again an API Gateway with an A2A server behind the scenes, and the A2A server sends a push notification back to the travel agent saying what is the weather right now.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/850.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/860.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=860)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/870.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=870)

 At this point in time I can  put some HITL aspect of it, either using Step Functions or other capabilities,  and then I feed that information back to a state that the Lambda can use, the A2A client can use, or I can send responses back directly to the user. So this is another example.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/880.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=880)

### Building Progressive Autonomy Through Continuous Evaluation and Graceful Degradation

 Now the other aspect I talked about is when human-in-the-loop will slowly go away when you're building autonomous systems. So as continuous agent evaluation grows, you will see the human-in-the-loop aspect will slowly reduce. Now if you've heard the keynote, we talked about agent core evaluations in preview mode. So if you're running agents on agent core, then agent core evaluation is something that you can use to make sure that you get more autonomy and less human intervention.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/910.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=910)

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/920.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=920)

As your evaluation  maturity grows and the percentage increases on the x-axis, your human-in-the-loop aspect will decrease proportionally. Overall, what we're discussing is  when you want human oversight, and you will need it in different places, that's where you need those critical decision gatekeeping capabilities. That's where you need humans.

If you have workflows that build and have errors that you need to recover from, you can have human insight or human presence in place to provide capabilities to recover from those errors. You want to build trust and transparency. I talked about this progressive autonomy, right? You start with human controls, and as you grow and mature in your agentic setup or multi-agent setup, you can build the trust and transparency, plus having those agent evaluations in place will help you.

Most importantly, graceful degradation is essential. Things break in production, and our CTO also mentioned this in multiple keynotes. If you have a capability to fall back and gracefully degrade, why not have a human in the loop setup so that the trustworthiness will still be maintained and the human can decide how to degrade this whole setup gracefully.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/990.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=990)

 There are some resources here. The QR code will have all the information. This is the blog that I wrote on Agentic AI using AWS serverless. There are some technical papers, most importantly the one from Harvard that talks about the human-algorithm centaur. The concept of a centaurian system, which is half human and half horse, is well mentioned in this Harvard paper, so I would encourage you to read that and understand how eventually we should move towards building that centaurian system so that humans and machines or AI agents will symbiotically work together without hampering each other.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/fa56af459e025f64/1070.jpg)](https://www.youtube.com/watch?v=SC3pHo-CycI&t=1070)

There's a sample application in the GitHub that I built using the Lambda functions example that I showed, a travel agent and weather agent which uses A2A client and server. You can definitely check that out and add a human-in-the-loop aspect. This is the lightning talk for you, and if you have questions, we can head down and discuss more. The resources are here to check out. The QR code also talks about different sessions that I did earlier around Agentic AI and AWS serverless, so please check them out. And with that,  thank you everyone for being here.


----

; This article is entirely auto-generated using Amazon Bedrock.
