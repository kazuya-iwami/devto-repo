---
title: 'AWS re:Invent 2025 - Angry Birds Meets GenAI: Transforming Rovio''s Asset Creation with AWS (IND398)'
published: true
description: 'In this video, Rovio''s Principal ML Engineer Ignacio and AWS Senior Account Manager Ruphar explain how Rovio uses generative AI to accelerate game asset creation while maintaining brand identity. They detail Rovio''s journey from failed experiments with diffusion models in 2022 to successfully building Beacon Picasso, a suite of AI tools powered by Amazon SageMaker and Amazon Bedrock. The presentation covers their three-tiered approach: a Slackbot for company-wide experimentation, Beacon Picasso Studio Cloud for advanced users using EC2 G6E instances, and Beacon Picasso Studio with Claude integration for simplified workflows. Key achievements include 80% reduction in season pass background production time (from 20 to 4 days), enabling artists to generate hundreds of illustrations daily. They emphasize critical learnings: focusing on non-brand essential assets, assembling high-quality datasets with artist involvement, building trust through artist-led captioning, and adapting workflows to embrace AI''s creative surprises rather than exact control.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Angry Birds Meets GenAI: Transforming Rovio's Asset Creation with AWS (IND398)**

> In this video, Rovio's Principal ML Engineer Ignacio and AWS Senior Account Manager Ruphar explain how Rovio uses generative AI to accelerate game asset creation while maintaining brand identity. They detail Rovio's journey from failed experiments with diffusion models in 2022 to successfully building Beacon Picasso, a suite of AI tools powered by Amazon SageMaker and Amazon Bedrock. The presentation covers their three-tiered approach: a Slackbot for company-wide experimentation, Beacon Picasso Studio Cloud for advanced users using EC2 G6E instances, and Beacon Picasso Studio with Claude integration for simplified workflows. Key achievements include 80% reduction in season pass background production time (from 20 to 4 days), enabling artists to generate hundreds of illustrations daily. They emphasize critical learnings: focusing on non-brand essential assets, assembling high-quality datasets with artist involvement, building trust through artist-led captioning, and adapting workflows to embrace AI's creative surprises rather than exact control.

{% youtube https://www.youtube.com/watch?v=ZDrqhVSY9Mc %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/0.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=0)

### Introduction: Rovio's Journey with Generative AI at AWS re:Invent

 Good morning, everyone. Thank you for being here. It's the fourth day of re:Invent. I hope you guys are having a nice time. Today we're going to be talking about Angry Birds and how they're using generative AI. So how many of you are trying to integrate AI into your generative AI process, or you're trying to integrate AI into your game development process? Are you finding it difficult to balance creative control with production efficiency? Nice.

My name is Ruphar, and I'm working as a Senior Account Manager at AWS. I've been supporting gaming customers for the last three years, and I have the pleasure to be joined on the stage with Ignacio, who's working as a Principal ML Engineer at Rovio, and he's been working on building the exact same capability for them. We're going to be diving deep into real examples of some of the most popular games of Rovio and how they're using generative AI to accelerate their asset creation and at the same time protecting their brand's unique identity. We are also going to be talking about how you could use tools like Amazon Bedrock and Amazon SageMaker to make these things work for you and your game development process, basically to help you revolutionize your game development process.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/100.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=100)

All right, but before we dive into the topic, let me quickly paint a picture of Rovio for all of you.  So I think most of us know Rovio for Angry Birds, right, but there's just so much more to the story. Rovio is a Finnish gaming studio that accidentally created a global phenomenon when they launched their fifty-second game. That's right, it was their fifty-second attempt to build Angry Birds, and today it's loved and cherished by all of us. Today, Rovio is not just about flinging birds at pigs, though they are still pretty good at that. It's much more than that. Rovio is a modern entertainment company, a game-first entertainment company who's pushing boundaries in movies, animations, gaming, and now AI.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/150.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=150)

### The Challenge: Artists Drowning in Repetitive Asset Creation

Talking about that, let me quickly tell you a story about how they stumbled upon AI.  All right, so picture this. On one side of Rovio, we had the tech wizards, the ML team, Ignacio standing at the right side. And on the other side we had the creative souls of Rovio, the artists. They nearly made one-third of the workforce, but they were drowning in deadlines. Why? Because they were not able to create assets and art at the rate at which the people playing the games were devouring it. So that was a big challenge.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/190.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=190)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/200.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=200)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/210.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=210)

Imagine that for each new season,  Halloween, they need spooky birds. Valentine's, they want all the birds and pigs to look lovely.  Christmas, bring the Santa hat for everyone,  and that's just different seasons. Imagine new game features, more art needed, special events, more art needed, and that's just making the life of an artist and their pipeline very daunting, and that's what they were facing. So here's the real challenge. While the artists were trying to focus on building new art and new assets, they were doing repetitive tasks like creating multiple variations of the same background or doing some incremental changes in the existing assets, and that is something that they were not looking to do. That is not something that they were passionate about.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/250.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=250)

Then came the aha moment, and that is where the ML team came up with an idea.  They were diving deep into generative AI capabilities, and they realized something. What if they could give creative superpowers? What if they could turn some of the tasks of the artists that would take days in some cases into an hour's job? The numbers were compelling for a specific use case that we're going to talk about today, which is a season pass background. They could save production time by 80%. So 80% savings in production time, and that's not just savings, that's a game changer. It was like discovering a new power-up in their own game.

The ML team had the technology and the artists had the experience and the creative expertise and massive amounts of unique Angry Birds data. Together, they could create something neither of them could have achieved alone, and that's where their AWS journey begins.

### Early Experiments: The Struggle to Generate Red the Angry Bird

I would now like to invite Ignacio to throw some more light on the journey.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/320.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=320)

So let's go back to 2022. As Rupert mentioned,  the ML team and the art team were not working together back then. But what happened then was that diffusion models came out. In 2022, we started seeing a lot of image generation models that could generate really good images, and we wanted to try those with our brands, with our Angry Birds. But before trying anything, what we did was to set some ground rules. We wanted to always use our own proprietary material. We also wanted to keep our data safe, that's why we did only things locally or on our cloud, and we wanted to respect everyone's intellectual property, even in prompts.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/380.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=380)

So with those ground rules, we went and tried the tools, but they didn't work so well with our brand characters, with our Angry Birds. So what we did was we wanted to explore these tools, but we wanted to build something ourselves. We started a proof of concept in 2022, where the goal was to learn and try to explore this area to find our own journey  and our own way to work with generative AI tools.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/410.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=410)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/420.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=420)

We did a proof of concept, and the goal of this proof of concept was to start training a model of Red, the Angry Bird, our main character, in the style of our movies. Here you can see one of the first data sets that we put together. They contain a lot of images of Red in the style we want to generate. And guess what? We failed.  It didn't work out so great. We failed in all possible ways. Here you can see some images from the  first generation we got trying to fine-tune a model to generate Red riding a bike.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/430.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=430)

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/450.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=450)

We kept trying and we kept failing again and  again. We got a bit better. Now, some of the pictures of Red, the Angry Bird, are a bit more recognizable in the style of the movie, but still, they are a bit scary, some of them. But eventually, after trying a lot, we learned how to do it. Here you can see  some images generated from the model we got back in December 2022 in the style of Red from the movie.

Now I want to ask you a question. What do you think about these images? Please raise your hand if you think these results are good. Okay, I see some hands up, maybe not many. I have to say that back then in 2022, I was super proud of this model. It was way better than what you could achieve back then with any out of the box tool. But when we showed it to our artists, they didn't agree with me. This was not good enough. You can see in some of the gestures of Red, it looks a bit creepy and a bit awkward, so this is not up to the standards of quality that our artists expect.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/500.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=500)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/520.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=520)

But why is it so hard to generate Red?  So one of the main reasons is that all of these are Red. You can see on one side, the classic version of Red, and in the middle, the Red from the movie. They look very different, but they are all recognizable by our fans. Everyone knows this is Red. So that's why the models were struggling a bit  to generate Red, because we have many different versions of them.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/540.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=540)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/550.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=550)

For example, here you can see some of our Angry Birds in one of our best games, Angry Birds Dream Blast. They are cute baby versions of the Angry Birds, they also have hands and legs. While in Angry Birds Friends, in this spooky pirate ship,  they are the most classic version of Red, they don't have any legs, for example. And here you can see them in the style of Angry Birds 2, a bit more modern look  of the classic Red.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/560.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=560)

### Quality Standards: Why Pretty Good Isn't Good Enough for Brand Characters

So, even though we failed, we kept trying. In 2024,  we trained a model using the style of our game, Angry Birds Dream Blast. You can see on this side that we have the target style, that's an image that has been generated by our artists. And we can see on the other side, some AI generations from the model we fine-tuned. And I want to ask you again the same question I asked before. What do you think now about these results? Please raise your hand if you think these are good.

Okay, I can see more hands raised up. I raised it myself, I think these results are really good. But when we asked our artists about these results, when we showed them these pictures, they found a lot of defects that I could not even see.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/610.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/620.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/630.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/640.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=640)

Because pretty good is not good enough.  We have hundreds and hundreds of pages in slides explaining how you should draw our Angry Birds. For example, the eyes,  how you should do the eyebrows and the cheeks to create expressions in our Angry Birds.  Also, the wings and even the feet. We also have decks explaining  how you should draw the different facial expressions to express happiness, anger, or surprise in our characters like Red, and we have this for all our different brand characters, our main characters, because it's really important to us.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/660.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=660)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/670.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=670)

Our artists have really  high quality standards because it's our brand, and we don't want to mess it up. And AI should help us raise the bar, not lower it.  But even though we failed, our artists were not satisfied with these results. Red showed us the way. We learned a lot along the way doing these failed experiments, and I'm going to show you now some of those insights we got from there.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/690.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=690)

### Key Learnings: Finding the Right Use Cases and Building Trust with Artists

One of the first breakthroughs we had was  that even though we had trained a lot of models with Red that didn't work out, we could use those same models to generate secondary characters. And why that worked is that we didn't have hundreds of pages of slides explaining how you should draw them, so there were not so many guidelines explaining how you should draw the eyes, the wings. So this was one of the first things we discovered. You can see here the picture of a cute baby crocodile and shark in the style of our game that we could use for different purposes.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/730.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=730)

We also learned that it made sense to focus on non-brand essential assets.  As Rupert mentioned before, some of the tasks of our artists were a bit repetitive and boring, like, for example, creating multiple variations of a background. So if we start doing AI for those tasks, then the artists have more time to spend on things where they can create the most impact with these characters, emotions, and story.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/760.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=760)

But even though we found some use cases like secondary characters and backgrounds,  there were a lot of challenges. The first one was about control. It was really hard when the artists used these tools the first time to get the results they wanted, especially if they were focusing on generating Angry Bird characters, which was not the right use case. They got frustrated, so they tried once and they quit. And even if you are focusing on the right character in the right use case, it takes a lot of practice to master these tools.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/800.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=800)

But we also learned that AI is better for things where you don't require exact control, like, for example, creating new locations and dioramas,  because in these ones we can be more flexible about the style. If we generate an Angry Bird with the wrong proportion or with a third eye, that's not good, but in the background we can have a second tree, third tree, another rock here, and that's okay. We can be way more flexible, the same as with secondary characters.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/820.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=820)

 The second challenge was about volume. We needed to generate hundreds of images just to get a few good ones. So we needed to learn a certain level of randomness when working with these tools. Sometimes you have an idea in your head and you want that exact same thing being generated, so the artists needed to learn, and also we needed to learn to be more flexible there. You needed to learn how to be surprised by the AI generations. And each generation takes a different seed, but we also learned that you can lock the seed and modify the prompt to only modify some parts of the images. The images still change, but you can have a bit more control, which addresses the first challenge we had, which was about control.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/870.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=870)

And the third challenge we had was that we didn't want  any generic image or style. The models that are out there, the base models, they generate really good realistic images, but they are not in a Rovio style, so we really want to generate things in our style. So what we learned is that we needed to assemble a really high, good quality dataset, and it doesn't take so many images, just thirty, twenty images is good enough, as long as they are well crafted by an artist. In the first proof of concept I showed you from Red, the ML engineer team was the one gathering those images and captioning them, but we learned that if you have an artist

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/940.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=940)

that captions these images, it's way better. They will also capture the style they want to generate in a few images better than machine learning engineers can do. They also have way more rich language to put in the prompts explaining what's in the image. At the end of the day, the artists are the ones using our tools, so the captions will be closer to what they will be prompting. Even more important, it also creates a lot of trust because they are part of the fine-tuned models we are training. When they change some images in this dataset, they  can see how it impacts the results, and that builds trust.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/960.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=960)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1000.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1000)

By crafting these good datasets, you can generate images in our style like these ones. These are unedited outputs coming out of the AI when you use the dataset for fine-tuning the model. We learned that we don't need to be flexible with the style  as long as we lock it really well with the dataset. But the biggest learning of it all was that it takes an artist to use this tech and an artist to review the outputs. It takes an artist that has a really deep level of understanding of what makes visuals work. It takes an artist that also has a rich vocabulary to explain what's in the image to create those captions, and it takes an artist that can then review the outputs to make sure that they have the high quality  we want and also make the necessary edits to reach that quality when needed.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1010.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1010)

### Technical Deep Dive: Base Models, Fine-Tuning, and Workflows on AWS

Okay, but you may be wondering how does it work? Don't worry, I will try to keep it simple and brief. There are three main steps. First, pick your base model.  There are many, and there's no silver bullet here. One works better for you and your use case than for others. Second, fine-tune your model. As I mentioned before, create a good dataset with your artists. The third step is to build a workflow. When artists generate images, they don't usually interact directly with the model. They use workflows, which are a set of steps where they can turn an idea into a final generated image.

These workflows can have inputs like the prompts, but not only. They can also modify, for example, the number of steps and many more things. They could also paint on the image to only modify one part of the image, so these workflows give them more control. We need to build those workflows from those fine-tuned models we have trained. But this is maybe a bit high level, so let me dig a bit deeper.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1070.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1080.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1090.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1090)

Let's start with the base models. How are these trained? They are trained with billions of images  from the internet with some text descriptions associated. Here you can see one example of a cat in a living room. Those captions are transformed into embeddings,  which is a representation in a high-dimensional vector space that gets into a neural network. But we also put the images into a neural  network, but not the original image. We destroy the image by adding noise and transform it into latents. Latents is a more abstract representation of the image, capturing the main information.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1110.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1120.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1120)

The neural network gets as input both the latent and the embeddings, and it learns how to revert that noise guided by the text  that it gets from the embeddings. In a nutshell, these diffusion models are all about adding noise and removing noise.  But these are just the base models, the generic art, not Rovio art, so we need to fine-tune them. As I mentioned before, it all starts with a good dataset that captures the style.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1140.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1140)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1160.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1160)

The base model can have this new concept from our dataset  on top added. Then we also need a trigger word. This is a word that will capture our style. It's that word there on the top in red. You can notice it's a very weird word, and the reason for that is that we don't want it to conflict with the dictionary that already exists in the base model.  We have, same as before, the latents and the embeddings going into the neural network. Then we decode those latents and generate an image.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1190.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1190)

In the first step of this fine-tuning process, the image looks really realistic, as you can see in the picture, but in each of the steps, it will start looking more and more similar to the style from our dataset. Behind all this, we have the images stored in Amazon S3, and all these fine-tuning trainings  happen in SageMaker training jobs. We can run dozens of trainings in parallel and get the results in a couple of hours. Why do we need dozens of trainings? As you can imagine, if you have worked with machine learning, the first training is not always the best one.

You have a bunch of hyperparameters like learning rate and the usual suspects, but also we have a lot of versions of the datasets and different ways to caption that we need to experiment with. So that's why we need a lot of trainings, and that's where SageMaker Training just comes really handy.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1230.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1230)

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1240.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1240)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1260.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1260)

Let's see one training in action. You can see  how the images start looking more and more similar to our style, and at some point, it starts to overfit. So the later step is not always the best one.  So one challenge is how do we decide which model to pick, because in each of these steps we have in S3 a safe tensor file, which is the model artifact that then our artists will use to generate the images. So what we do is we show these videos to our artists, and they decide. 

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1290.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1290)

So we have fine-tuned our model, and the artists have decided what's the best one from the previous video. Then we take this safe tensor artifact from S3, and we put it into tools or workflows so they can generate images. This is one example. Imagine one of our artists wants to generate a northern lights in their style. So during inference, these models start from random noise, but after several steps, it will start looking more and more similar to the style you trained it with.  And at some point, it will look like this, northern lights in the style of our game.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1300.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1300)

But as we mentioned before,  we don't use the model by itself. We use workflows. So in these workflows, we can fine-tune and tweak a lot of different things, like for example, the steps. If you pick a lower number of steps, the image generation will be quite fast, in just a very few seconds, but it will also be not so high quality. So for experimentation, it's always good to start tweaking the prompt and having a low number of steps to be faster.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1350.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1360.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1360)

We also have things like model strength, how much you want your generated images to look like your style. You can even combine two different fine-tuned models to combine two styles. We also have a guidance scale that tells you how much the AI should follow the prompt. So if you pick the right value there, you can let the AI be more creative. And all this is powered by  EC2 G6E instances,  which makes this generation happen in seconds so artists don't get frustrated because they don't have to wait.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1370.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1380.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1380)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1400.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1400)

### From AI Images to Game Assets: The Season Pass Production Pipeline

But we have images in our style, but we need to turn them into game assets. That's where the value is.  So this is how our pipeline for our artists looks like. We put the artist at the center of it all, because as I mentioned before,  we want them to be the ones in control. And the first thing they have to do is to come up with an idea and find a way to prompt it. For example, imagine they want a machine to generate rainbows. So they generate a bunch of images using our AI tools. 

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1410.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1410)

And then they pick the best one, the one they like the most, as a starting point. And then they still need to do a bunch of stuff. They need to clean the image. There may be some weird artifacts  there. They need to expand it. They also need to upscale it because these AI models generate images in lower resolution. It's also faster if you use lower resolution to generate them. And at the end, they also need to create different layers and put our brand characters, our Angry Birds, on top to create the story they want to tell and embed that into our game UI.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1440.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1440)

So you can see at the end that we turn images generated by AI into our games.  And that's how we could generate some season pass content way faster than we could before. For those of you who are not familiar with mobile games, season pass content is a limited special event that we run. Usually it takes a month, and the players can experience new content, seasonal themes, and rewards. For example, right now, it's Christmas time soon, so most of the games are going to be releasing some Christmas season pass.

And it takes a long time to generate all the assets for the artists to generate all these images. So that's why using AI they could speed up the process to generate some backgrounds, and then they will still be in control of generating the characters and putting them on top, but the process became much faster. It went from just taking weeks to days, but they were still in control. And another thing to mention is this is not only about the AI, it's also about changing the way they worked, because the first time they tried generating images with AI, they had a brainstorming session.

They decided what they were going to do and went and generated the images. But then they had a clear idea of what they wanted to generate, and as I mentioned before, you need to learn how to be surprised by AI. So it didn't take just a few days. It actually took almost the same time in that first iteration as it takes generating this without AI.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1550.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1550)

But in the second iteration, what they did was they did the image generation first. They generated a bunch of images, and then they ran the brainstorming session afterwards, and that helped speed up the process a lot. So it's not only about AI, it's also about changing the process of how this content is created. And you can see here, in fast motion,  how the artist edits these images and puts them together into the final result. This is basically a couple of days then editing the images and creating the story.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1570.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1580.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1580)

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1590.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1590)

### Beacon Picasso Suite: Building the Right Tools for Different Users

But how are artists using our tools? Because at Rovio, every artist thinks differently about AI.  Some people loved it at the beginning, others found it confusing, and some didn't even want to try it.  Some got disappointed because it was not one click. They just wanted to generate one image and have it be a perfect one. But others got disappointed.  They didn't want to try it because they thought it was one click. They were really used to really complicated tooling that gave them a lot of control, so they wanted that complexity.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1600.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1600)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1610.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1610)

 Some people at Rovio, like myself, I'm an engineer, we like to generate the images using Python, but the artists are not that technical.  They may prefer creating prompts like this to explain what they want to generate. They have the vocabulary to create prompts like this, explaining the lighting, the camera angle, the characters, but it's not for everyone, and this is not even the longest one, it's just the one that could fit on the screen. And so not everyone enjoys writing this.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1630.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1630)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1640.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1640)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1650.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1650)

Also,  these flowchart tools that we call workflows are very complicated, so not everyone enjoyed that, but some did.  So our goal was clear. We had to give the right tools to the right people. So that's why we built multiple tools  powered by AWS, and we call them Beacon Picasso. So I'm going to explain now about this, but before that, I want to say that I don't work in a game team.

I work in Beacon, which is a central unit that provides games as a service, as a platform for our games. So basically, making games can be a science and an art, and Beacon takes care of the science so the games can focus on crafting joy. That's the spirit, and AI is part of those tools. Beacon Picasso, our suite of generative AI art tools, is part of this offering that we gave to our game teams.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1710.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1710)

So they are also all powered by AWS so they could interact with our fine-tuned models that were trained by Amazon SageMaker with our data never leaving our VPC. And I'm going to explain now what each of these tools we built are and also why we built them.  We started with the Slackbot because in our company, everyone had Slack, and our first goal, this was the first thing we did, was to democratize and learn together how to use these tools. So we wanted every people at Rovio to at least generate an image with AI and learn together.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1730.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1730)

 But we had a lot of adoption barriers. There were a lot of concerns. We still have a lot of concerns, and they are valid ones. For example, data privacy was one of them. Fortunately, it was solved by AWS. But, for example, harmful content is still a big concern. You can generate really convincing deepfakes with this technology that can be really harmful.

There are also really concerning environmental issues or also acidification, which means that you can flood the internet with crappy content. Also, IP rights issues were really sensitive ones. Back then, three years ago, there was this website called ArtStation, which is one that artists used to showcase their work, and a lot of base models used those images without their consent to train these base models. So they were really upset and they put a lot of no AI pictures on the site because they refused to take those AI-generated images down.

So as you can imagine, this is a very sensitive topic for the artists, for good reasons. And there were other concerns, like, for example, biases, and actually building the bot helped a lot with that.

Because we were prompting together in public Slack channels, one time a product manager prompted a very important meeting at Rovio, and we only got white male people in the meeting room. We are a very diverse company, and that didn't represent Rovio. But the best way to combat biases is to be aware of them. So we knew that bias existed, and next time you want to generate a meeting at Rovio, you put more effort on the prompting to make sure it's diverse enough. So that was one of the big wins with the Slackbot, to open the conversation. It's not about trying to convince someone that AI is good, it's about talking openly about all of these issues, and that helped a lot.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1850.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1850)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1870.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1880.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1880)

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1890.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1890)

Where did we build the Slackbot? We picked SageMaker.  Why? Because we were already familiar with it. We had already a lot of machine learning models in SageMaker, and SageMaker also provided support for GPUs that we needed for the inference. So let's see the Slackbot in action.  First, you ping Beacon Picasso and you give it a prompt. In this case, a mystic forest clearing in our target style.  Then Beacon Picasso will reply, "Okey dokey, I'm working on it," so you don't get nervous, but it only takes a couple of seconds until you get the final result, which is the image in that  style.

What we have in the back is a Lambda replying that first message. That Lambda sends a trigger, a request to SageMaker to start generating the image, which is then stored in S3. Another Lambda then sends the image generated back to Slack. All the messages are passed using SNS. We also have a third Lambda at the end that stores some metadata into RDS that can be used for analytics, like how much time the generation took, how many generations per day, those kinds of things.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1930.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1930)

### Scaling to the Cloud: From Local Laptops to EC2 and Amazon Bedrock

So we had the Slackbot,  people were generating hundreds of images at Rovio, and we were learning together. But the Slackbot was really simple. It didn't work for our artists. It wasn't helpful to generate content in our games. We needed something more advanced. The good news is that there were a lot of open source tools available that provided us a lot of power and a lot of flexibility, but they are really hard to use and they are also really computationally expensive. So we needed to make it easy for artists to experiment with the cutting edge models and tools from the open source community.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1980.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1980)

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/1990.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=1990)

We didn't want to build some new UI ourselves or new tools ourselves, because this field changed so fast that it could become really quickly obsolete. So that's why our focus was to rely on open source projects. We started with our local laptops. That was the fastest  way, trying different projects, open source projects, trying different models, and that worked for a while. But our artists started getting very excited. We had to sit with them  and configure these models on the laptops. These are non-technical people, and we had to deal with Python virtual environments, Bash scripts, all those kinds of things. Every time we had a new model or we had a new update to get the latest updates from the open source projects, we had to sit again with them and update everything.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2030.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2030)

Some artists got even so excited that they ordered from our IT super expensive GPU desktops that got delivered to their home, and they were saying they were as loud as washing machines, but they were really fast to generate the images. But this was clearly not scalable. We had to do something. So that's why we went to the cloud.  We moved all this code, all these projects into EC2 instances and allowed our artists to connect with an SSH tunnel, so we started simple.

The nice thing was that we started with the G6 family, and it was a bit slower than these expensive GPU desktops that the artists had ordered at home. But then Amazon released G6e instances, and it was much faster than that expensive desktop that the artists had purchased. They stopped using them. They just went to the cloud and used our models that we provided because it was better and easier. But this is still a bit rudimentary because we needed to still set up the SSH tunnel. We didn't have a friendly URL that the artists could bookmark in their browser, so we needed to do a bit better still.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2080.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2080)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2090.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2090)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2100.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2100)

 Here you can see one example of an open source tool. This is a UI that the artists interact with.  So they access it through our VPN or from our offices, and there's a friendly DNS  thanks to an inbound Route 53 resolver. So we have a friendly URL they can bookmark.

But also, behind this, we have auto-scaling groups that allow us to provision these instances across different availability zones. This is important because there aren't so many GPUs, and it's really good that if there's no GPU available in one availability zone, you can provision it in another one. It is, of course, good also for fault tolerance purposes and also for saving money, because as you can imagine, the GPUs are expensive. So what we do is we turn these machines off during weekends and outside working hours.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2170.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2170)

But I'm talking about these two instances. In there, we just made easy to access EC2 instances, but we don't have just one, we have many. And our engineers, our AI engineers, had to create new versions of these EC2 instances to create new models or also to fine-tune different things. So actually, it was not very flexible because all the previous architecture I showed you was in Terraform, which our AI engineers are not so comfortable  with. So our goal was to make it simple for AI engineers to push the latest version of the open source tool or to put the latest models out there.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2190.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2190)

So they would just, with what we built, push the code to Git. It goes into a Git repository and triggers an AWS CodePipeline.  But what it does is it just spins up an EC2 instance, a creator instance, that then installs in another EC2 instance all the stuff that we need. And it could be downloading the base models from S3, downloading all safe tensors, fine-tuned models from S3 also, and setting everything up with different scripts. And all of this is automated. And at the end, what we get is an AMI that we can use to spin new EC2 instances powered by powerful GPUs that artists can use.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2230.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2230)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2250.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2250)

So we have now the Slackbot, super simple for all the company.  And we had Pigasso Plus Studio Cloud for advanced users, but it was really complicated to use. So we needed something in the middle, something that allowed us to generate game content but was more accessible to all Rovians. And at this point, because we developed this one this year,  we already had a lot of fine-tuned models. We have many, and our artists were starting to find it really complicated to find which fine-tuned models should I use for my use case.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2270.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2270)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2290.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2290)

So imagine we have an artist that wants to generate an art background in their style. They go to this page in our internal documentation and they find, okay, this is the one I want.  They still have a bunch of workflows behind it that they need to decide which one to use, so it's still not super trivial. So we wanted to hide all this complexity away for the end users. We wanted to organize all these workflows and help them also to prompt. And that's where  Amazon Bedrock and Claude came to our rescue.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2340.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2340)

Why? Because usually, what we would have done, we would have requested resources from our other teams, like maybe front-end developers and back-end developers, and we would have needed to wait for some time because, of course, they are busy with other projects. But we have recently started playing a bit with Bedrock. So what we did was we created a new inference profile with Bedrock and connected it with Claude Code to start prototyping something. So two of our AI engineers, interns, and one UX designer intern, they started working together to try to create a nice UI on top of all this complexity that we had. And after a few weeks, they got something like this. 

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2350.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2350)

So now our artists can go to this page. Imagine they are from Dream Blast. They just on the top click on Dream Blast, and they will just get the workflows and the use cases that are relevant  for them. So imagine they go to a background use case. They will also have the prompt and the image that was used to generate the prompt. So basically, in this use case, you can start tweaking the prompt, change it if you want something different, but we also had there at the bottom, Pigasso Assistant. This was really helpful because they could just text and chat with the Pigasso Assistant to modify that prompt. So they didn't need to spend hours crafting the prompt.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2390.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2390)

So let's see it in action. Imagine that we want this forest now in winter, with some fog around. So we  ask Pigasso Assistant to generate that. And behind this Pigasso Assistant, we have also Amazon Bedrock. We created a different inference profile, and the nice thing about this is that we can track costs associated with this versus the other use cases we had before with Claude.

So it's really easy to track cost across use cases. Also, another nice thing about Bedrock is that we could switch very easily from one model to another using the Bedrock Converse API.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2430.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2430)

So we have done, we have in a matter of seconds got this suggestion  for the prompt. So we click on apply changes and we get the result just in a few seconds. Even if we are not yet fully satisfied, we have some parameters there that we can still play with. We could fix the seed and keep changing the prompt to, for example, remove that nest that is in the tree, and the image will not change drastically. So that's how it all comes together.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2470.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2480.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2480)

On one side we have the AI engineer, and on the other side we have the artist. So the AI engineer fine-tunes images in Rovio style.  Then those fine-tuned models are used to generate these workflows that can be used for different use cases.  And then we hook these workflows with these Beacon Pigasso to the front end that our artists can use to really simplify the use cases they want and generate the images.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2520.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2520)

So with this, a lot of our artists that were using before Beacon Pigasso Studio Cloud, they started using Beacon Pigasso Studio because it was much simpler and they could generate things way faster. They only had to go to Beacon Pigasso Studio Cloud when they really had really complicated use cases that really required that extra control. And Beacon Pigasso Studio is just a front end built on React with Claude code, but the back end is still our fine-tuned models and our workflows in Beacon Pigasso Studio Cloud. 

### Results and Future: Empowering Artists to Innovate Beyond Efficiency

So suddenly, in some cases, our artists could generate hundreds of illustrations per day, and I would say that's pretty extraordinary. And now Rupert will talk about what were the benefits for artists. Thanks Ignacio. That's definitely some journey.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2550.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2560.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2560)

So coming back to a story where we started, what do you guys think would have happened to the artist  after getting the right tool for the right people and going through this amazing journey? You guessed it right, we had a bunch of happy artists.  Even though we could not get the artist here to re:Invent, we managed to get some testimonials for all of you.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2580.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2580)

So Tat here says that Beacon Pigasso is really vital for her team since they've started using it. They're able to produce twice as much content which they could before using Beacon Pigasso. This is giving them  additional time to experiment and enable fast test-to-learn cycles. The second testimonial is from Javier, who said that Beacon Pigasso helps navigate through the slow part so that they could move straight into what artists love, which is expression, character, and soul.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2600.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2600)

And we have tried to summarize some of the key benefits  that Rovio achieved by Beacon Pigasso. The first benefit is data privacy. As Ignacio mentioned at the beginning, data privacy was actually a big concern for Rovio. But when they started building this, they initially built all the data on-prem, and they started moving stuff to AWS. So Rovio had higher control of their data because they're fine-tuning their own data. The images were created in their own style, and all the data was fed with their own IP.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2650.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2650)

All of this was possible because of AWS as it was not able to leave their VPC at any point in time, so everything stayed within their VPC. And that's how they achieved a higher level of data privacy.  The second benefit is of course flexibility. So I think for all the three tools Ignacio mentioned, they had the flexibility in terms of choosing the instance type or the families.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2690.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2690)

So in the specific case that he mentioned, they were initially using G6 and they moved to G6E family and they get a lot of benefits by moving there, not just having faster GPUs, but also better price performance. Also, they had the flexibility of using Bedrock, which provided them access to more than 50 LLM models, and they could use this with managed API to access all of these diverse models. 

The third benefit was higher velocity. I think at the beginning I mentioned that they were able to achieve more than 80% of production time savings.

And that is for the season pass background use case where they initially used to take 20 days and now they could achieve the same outputs in 4 days. It is important to mention here that it is not just because of GenAI that they were able to achieve the higher velocity, but by implementing AI in the process, the process improved as well, so process is also the key here.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2730.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2730)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2760.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2760)

 Last but definitely not the least, and I was told by Rovio to focus more on this benefit more than higher velocity, I think the bigger benefit was that Rovio could now innovate. The artists had more free time, they had more time to experiment, and all of this free time could help them with more experimentation and building stuff for the future. And that is something that is resonated by Tatu, who says the same thing about AI if  used well, isn't just about saving money, and that is what you achieve by higher velocity. It is about creating something extraordinary, something that wouldn't exist without it.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2790.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2800.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2800)

On that note, I want to ask you a question on what is held in the future when it comes to GenAI for you. That's a great question, and I can share that we are exploring. We are researching because we need to be prepared for  potential futures. We don't know what the future may look like. And we are researching in a lot of different areas, like, for example, 3D. We can create,  for example, this crocodile in our style and turn it into 3D already, but this is not yet ready for production use in our case, but it's advancing so fast that who knows what could happen in a couple of months.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2820.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2820)

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2830.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2830)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2850.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2850)

Another area is animation. We can turn these images  and turn them into life, animate them. So, this, even though it's not maybe to the quality we want it to be for in-game use, it could maybe use it for marketing purposes already.  Then, we are also exploring video. Here you can see one of my colleagues walking a dog. This model decomposes that image into depth, 3D pose, mask, and then we give a reference image. And again, one fine-tuned  character in the style of our games, and we have the character walking the dog. Looks cool, right?

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2860.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2860)

 And last, we keep exploring brand characters. Even though we failed, we keep doing it, and we keep failing because we want to learn. These are our last results from some months ago. They look really good to me. When I show them to our artists, they say they still find some defects in here, but they are almost there. So, things are looking really promising in this area too, because we need to really be open-minded, even though we have failed in the past. We need to be open-minded and keep trying, keep learning, because things are changing so rapidly that you need to adapt.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2900.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2900)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab05fb630838f937/2910.jpg)](https://www.youtube.com/watch?v=ZDrqhVSY9Mc&t=2910)

Because some research will make today's work obsolete,  but it will also enable new use cases. So remember to adjust your path. Thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
