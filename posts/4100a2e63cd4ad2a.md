---
title: 'AWS re:Invent 2025 - How Netflix uses Amazon S3 Storage Lens to track exabytes of data (STG214)'
published: true
description: 'In this video, AWS and Netflix discuss managing exabyte-scale data using Amazon S3 Storage Lens. Netflix engineers explain how they track over 2 exabytes across big data, media, and ML use cases, ingesting Storage Lens data into Apache Iceberg tables to catch over 500 petabytes of unintended growth. AWS announces three major Storage Lens enhancements: expanded prefix analytics supporting unlimited prefixes up to depth 50, 72 new performance metrics including request size distributions and Network Origin Metrics, and direct export to managed S3 Tables for simplified analysis. The session includes live demos showing cross-region access identification, error detection with Concurrent PUT 503 metrics, and natural language querying using MCP servers with AI assistants like Kira to analyze Storage Lens data without SQL expertise.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - How Netflix uses Amazon S3 Storage Lens to track exabytes of data (STG214)**

> In this video, AWS and Netflix discuss managing exabyte-scale data using Amazon S3 Storage Lens. Netflix engineers explain how they track over 2 exabytes across big data, media, and ML use cases, ingesting Storage Lens data into Apache Iceberg tables to catch over 500 petabytes of unintended growth. AWS announces three major Storage Lens enhancements: expanded prefix analytics supporting unlimited prefixes up to depth 50, 72 new performance metrics including request size distributions and Network Origin Metrics, and direct export to managed S3 Tables for simplified analysis. The session includes live demos showing cross-region access identification, error detection with Concurrent PUT 503 metrics, and natural language querying using MCP servers with AI assistants like Kira to analyze Storage Lens data without SQL expertise.

{% youtube https://www.youtube.com/watch?v=Q2YoHfhFuI8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/0.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=0)

### Introduction to STG 214: Managing Exabyte Scale Data with Amazon S3 Storage Lens

 All right. Well, hello and welcome to our session STG 214 on how Netflix manages exabyte scale data using Amazon S3 Storage Lens. I'm Roshan Thekkekunnel. I'm a Product Manager with the Amazon S3 Services. Joining me now on our session today is my colleague, Christie Lee, Principal Solutions Architect with Amazon S3, and our friends from Netflix, Austen Keene, who's a Software Engineer at Netflix, and Bi Ling Wu, who's a Data Engineer with the Netflix Data Science team.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/60.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=60)

Just some logistics. We have a lot of content to cover today, so we will not be taking any questions during the session. But my colleagues and I will be available in the hallway after the session to answer any questions that you may have. So, we'll start with a quick look at the agenda.  We have broken the session into five parts. I'll start with a quick overview on the Amazon S3 Insights portfolio and double click specifically on S3 Storage Lens, which is the focus of our session today. Then I'll invite Austen and Bi to walk you through some of the Netflix use cases, how they leverage the S3 Insight services to optimize their storage deployments.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/120.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=120)

Then I'm really excited to talk through some of the announcements we made yesterday on S3 Storage Lens that further enhances your optimization workflows. And then finally, Christie will be running us through a demo where we'll see all of these new features in action, live in action, solving real performance and cost optimization use cases.  Before I transition to our first section, just a quick show of hands. How many of you are familiar with S3 Storage Lens or any of the S3 Insight services? Cool. I see around 25 to 30% of the room. That's awesome. I think by the end of this session, I'm hoping everybody will learn about S3 Storage Insights, S3 Storage Lens, and how you can utilize these services to optimize your storage.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/150.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=150)

### The Importance of Storage Insights and AWS S3 Insights Portfolio Overview

 I'd like to first establish why you want insights, right? As you're storing data, where you store your data and how your applications access that data is critical for various reasons, your cost profile of your workload, the performance profile of your workload, as well as the security posture of your data sets. Therefore, it's critical for you to understand how your data is being stored, where it is being stored, and how your data is being accessed. As the saying says, if you can't see it, you can't change it. In this case, you cannot optimize it, right?

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/210.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=210)

Therefore, with S3, we have built a number of services that gives you this visibility on both your storage and activity metrics, both at an organization level, aggregate metrics, and down to very granular insights at an object level or at a request level.  So, let's start looking at the insights portfolio. First up, we have S3 Storage Lens. This is our observability tool that provides daily insights on both storage and activity metrics in one interface, right? Now, this is a recommended first step for you to start understanding how your storage is deployed, how it's being accessed.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/250.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=250)

We make it very easy for you to use. You get a default Storage Lens account as soon as you create your first bucket, and you have free metrics, around 60 plus metrics that helps you understand how your storage is deployed.  Now, if you need more granular data beyond the organization level and bucket level, we have services like S3 Metadata and S3 Inventory. Now this goes down to an object level report, right? So you get a list of objects and the associated metadata. And with S3 Inventory, you get this on a daily or a weekly basis in a CSV or Parquet format, right?

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/300.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=300)

Now, if you need more real-time data, last re:Invent, we launched S3 Metadata, which gives you near real-time information of all of your objects and your metadata, right? And S3 Metadata is delivered in managed S3 Tables, which means there's no additional processing. You can directly use your preferred analytics tools to start querying and discovering your data.  Now, similarly, if you're looking for granular request level information, you can turn on S3 server access logs. This will give you detailed records of each and every request made to your bucket.

Finally, we have Amazon CloudWatch, which is our monitoring service. This gives you visibility of your storage metrics in context with all of your other AWS service metrics as well as your own application metrics. So here you can do near real-time analysis, create dashboards, set up alarms, essentially anything you need to do with your operational workflow.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/360.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=360)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/380.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=380)

### S3 Storage Lens Use Cases and Capabilities: From Cost Management to Prefix-Level Analytics

Now, let's take a deeper look at S3 Storage Lens and how customers use it. The primary use case with S3 Storage Lens is cost management. Questions like, which are my largest buckets, which are my largest prefixes?  How has the storage been growing over time? Do we see any anomalies, sudden increase in storage in any of my buckets or prefixes? That's the kind of questions you can get answered through Storage Lens. The second most common pattern is around access. Which of my buckets and prefixes are hot,  which of them are cold? You can find out which buckets and prefixes are getting a lot of requests, which of them are not getting as many requests and therefore need to be moved to a colder tier. Those kinds of questions can be answered through Storage Lens.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/400.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=400)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/430.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=430)

Third, around performance monitoring.  Now, you can answer questions about which buckets are experiencing 503 errors, that is our throttling errors, or 403 authorization errors. You can identify these buckets or prefixes. These are potentially slowing down your applications, and those are the ones that you want to focus on. And finally, we hear from customers around security audits with Storage Lens. So you can ask questions like  which of my buckets have unencrypted data, or which of my buckets do not have versioning turned on, or do not have adequate replication rules. Those are the questions you can get answered through data protection in Storage Lens.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/450.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=450)

Just to give you a flavor of some of the capabilities in Storage Lens,  if you log into Storage Lens through S3 console, you'll first come up with this overview page where you can see a snapshot of all of your data across your entire organization. So this is across multiple accounts, multiple buckets in multiple regions. You'll get a summary snapshot here. You can then filter by categories of metrics. So you have performance metrics, cost optimization metrics, and you'll get all of those metrics categorized in this view.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/490.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=490)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/510.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=510)

You can then start drilling down from here in Storage Lens. You can go down to account level metrics, region-level metrics, storage class-level metrics,  and what you're seeing here is a bucket-level metric. So, this here is the top 10 buckets in my account. The very same metrics distributed across these 10 buckets. And then finally, in Storage Lens, we have an advanced tier. If you turn this on,  you'll get additional metrics and also advanced features like prefix level metric aggregation. So what you're seeing in this example is the top 10 prefixes by size across all of my buckets, all of the same storage metrics and activity metrics distributed across these 10 prefixes.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/560.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=560)

### Netflix's Challenge: Tracking 2+ Exabytes Across Multi-Tenant Buckets and Diverse Use Cases

So, rather than me talk more about how customers use Storage Lens, we have Netflix here, for one of the largest deployments of storage across the globe. And we learned from Austin and Bi how they use Storage Lens and other insights services to optimize their storage. Austin. Hi everyone, my name is Austin. I'm a software engineer at Netflix,  and for those of you without an internet connection, Netflix is a streaming company. We do everything from F1 reality TV to Stranger Things.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/570.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=570)

So when we talk about tracking our data, let's talk about why that's difficult in the  first place. We have over 2 exabytes of data stored in S3, and that's spread out across 3 major use cases, big data, media, and ML. These use cases have different access patterns. They may be accessing abstractions built on top of S3. They might be these big multi-tenant buckets where different teams own different prefix paths. So it really quickly gets quite complicated to provide org-wide visibility into usage and cost.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/600.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=600)

What do we do about it? We take the data, the Storage Lens  information, and we ingest it into Apache Iceberg tables.

We aggregate all of that into high-level growth trends and then materialize that for our application owners. This is an example dashboard we have. This is the high-level usage dashboard that we use to get a sense of the general health of S3 at Netflix. We'll also create more tailored dashboards depending on the use case. So there will be a media dashboard, a big data dashboard that lets application owners understand what's going on at a high level.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/640.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=640)

But not everybody likes getting DMs from me on Slack about their S3 usage, so we have some better tools as well. This is where the growth tracking comes in. What we do is we provide automated alerts  for application owners, so when their storage footprint crosses certain thresholds, they'll get an automated message, and that takes them to fine-tuned dashboards. As you see here, this user got a Slack alert. Their growth crossed over a certain percent over a period of time, and then they can go to this dashboard to take a look at what's going on.

As I mentioned, we might have these really large multi-tenant buckets. Different teams own different prefix paths, so it's really important that they get some low-level data and can really dig into what's happening. This mechanism alone has caught over 500 petabytes in unintended storage growth. That might look like media buckets where studio partners just dump assets and forget about it. It'll be misconfigured tables where data is racking up costs, but nobody's using them.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/700.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=700)

Just recently, we had a 6 petabyte growth in a log bucket for containers, and the team that owned the bucket didn't even really know about its size until we reached out to them. So just having some simple mechanisms that inform people about what's going on is enough to start really saving on costs.  Another thing that we do with S3 Storage Lens data is optimize the data placement. You might hear that data has inertia. So once you put a bunch of data somewhere, it's really expensive and time-consuming to move it somewhere else.

### Optimizing Data Placement and Platform Insights: Netflix's Approach to Storage Performance

If you're an application owner and your data is in S3, you're going to point your application at S3 and you're going to call it a day. And for the majority of use cases, that works totally fine. But if you have low latency or high IO requirements, you might run into some trouble. We saw that with our ML training jobs when we were first launching our ML training platform.

We saw these jobs have really long cold starts, and that correlated to idle GPUs. What was happening is S3 was throttling them as they were trying to hydrate data. So in this effort to save a little bit of money on storage, we ended up spending a lot more money on idle GPUs. What we did is we looked at the request and throttle metrics from S3 Storage Lens to figure out which applications were having problems, and then we placed them on more performant storage like EBS or FSx Lustre.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/770.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=770)

That eliminated our cold start times, and so we spent a little bit more money on storage and ended up saving money by optimizing for GPU utilization. Another thing that we do is we take all of this  information and we turn it into platform insights. We take all the S3 Storage Lens data and we vend it through internal APIs to our platforms. As I mentioned, we have a lot of abstractions built on top of S3, and so we run into this problem where, how do you tell a user that their S3 bucket is growing in size when they didn't even know they had a bucket in the first place?

That's where all of this comes in. As you can see here, these are two different dashboards that people will use to interact with their applications. We take the data from S3 Storage Lens and we put it right into the UI so they don't have to go looking for this information. We find that application owners understand their data best, and we don't want to be prescriptive about how they use S3 or whether or not they use S3 at all.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/820.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=820)

We find it works really well. What we really want to do is take this data, make it available to them, and let them make the best decision.  So if you're not paying attention to me up until this point, now is your time to check in. I'll tell you how I think you should adopt S3 Storage Lens if you're not using it today. The first thing you should do is track trends. Take the data, put it into a dashboard, and review it periodically like you would with any other on-call review.

Just having some group that looks at this information and follows up on it is enough to already start affecting change. But if you're a little bit bigger than that, then you'll want to invest in automated growth alerts. You want to take this information and you'll want to send it to the application owners who it's relevant to, so they don't have to go looking and you don't need some central authority to manage all that information.

Finally, if you're even larger, that's when you should invest in platform-level insights. If you have a lot of abstractions and platforms built on top of S3, you want to take the information you have and you want to provide it somewhere so it's easy for everyone to access. So that's a brief overview of all the things we're doing with S3 Storage Lens. I'm going to hand it over to my colleague Bi to talk in more detail about what we're doing with S3 telemetry.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/880.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=880)

### Deep Dive into Cost Metrics: Building Unlimited Prefix Depth Analytics with S3 Inventory

Hi, thank you, Austen.  Hi, my name is Bi, and I'm a data engineer at Netflix. So while Austen's use cases are around the broader growth and metrics tracking for our S3 ecosystem, my goals are specifically for cost and efficiency. There are three key areas where deep insights make tangible differences and where Netflix has chosen to focus on. First, we aim to provide visibility into the cost of each prefix for the top 250 buckets across our ecosystem.

This allows us to pinpoint exactly where the spend is going and helps us identify opportunities for optimization. Second, we aim to offer advanced analytics for iceberg table storage and prefix usage. This means that we can analyze how our data is structured and use that information to make smarter decisions about our storage. Finally, we have millions of iceberg tables in our S3, and all that data is stored in S3. So we aim to deliver clear visibility into the costs associated with those tables themselves so that at the end of the day there are not too many surprises and we can manage our resources more effectively.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/950.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=950)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/960.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=960)

So this portion, I'll actually just do a deep dive into how we construct these cost metrics for all  the prefixes to an unlimited prefix depth as a foundational data set. I'll take a step back here. To enable  efficient analysis of our storage utilization, we need to track storage metrics at all prefix steps. So on the right here I have a sample file directory representing what we're dealing with and what we're hoping to achieve. We have bucket A with prefixes A to D and any number of objects all in between. Just to reiterate, the goal is that for unlimited prefix steps we can understand what the size and cost is at each step.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1000.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1000)

You might be thinking to yourself right now, this seems to be exactly what S3 Storage Lens is for. Why are we rebuilding the wheel? Well, you wouldn't be wrong. When we started this work, my team and I evaluated several S3 services that may fit our needs.  At the time in 2023, we faced a few limitations that caused us not to choose S3 Storage Lens. First, S3 Storage Lens did not include prefixes that represented less than 1% of the total size of the bucket. This was an issue because some of our buckets are really big and fairly multi-tenant. For example, a prefix could represent a large portion of an iceberg table but did not yet meet the 1% requirement for the entire bucket.

Second, S3 Storage Lens had a max prefix depth of 10. Again, this was good but not yet enough for us. Finally, as many folks in this room know, cost is not only determined by size but also by intelligent tiering and storage class. At the time, S3 Storage Lens lacked this dimension and granularity. Because of these three gaps, we turned to using S3 Inventory and S3 server access logs.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1060.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1060)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1080.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1080)

I'll now go into how we join these two data sets to get cost. I'll bring back my file directory here.  As I've said, this is the file directory that we could be dealing with, and I've turned it into a pared down version of what you might be getting in your S3 Inventory report, including size, prefix path, intelligent tiering, and storage class that I've combined into a very convenient field for us. On this slide I provide an example of how we created that prefix roll up from the inventory  report. Like S3 Storage Lens, each row represents the storage class and total size. We have the total recursive cost, which is the cost of that prefix and everything underneath it. So at prefix step 0, this is the size of the total bucket. We also have exclusive cost which represents only the objects in that level, non-recursively. With this prefix roll up, we can then apply pricing at intelligent tiering, storage class, and size to get the total cost of the bucket at each prefix up to an unlimited depth.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1120.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1120)

### Access-Based Retention Recommendations: Leveraging Server Access Logs for Cost Savings

Okay, so now we know the cost of the prefix. We want more. We want to save real  dollars. We want that AWS bill to go down. I can relate. Unfortunately, a very easy way to save money is to delete files. But how do we do that without deleting important files? This is where the server access logs come in. In this diagram, I've highlighted how we join the server access logs to that prefix roll up that we just built to understand which files are used and which files are not used.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1170.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1170)

Based on this example, we can see that prefix C has been accessed pretty recently and we can make the recommendation to keep prefix C. However, we see that prefix B hasn't been accessed since 2010, which is about 15 years ago, and we can make the recommendation to delete this file. I know this is only 30 megabytes, but at a larger scale like we have at Netflix, this can present  a big opportunity for cost savings.

Now if you were to take this back to your own organizations, I would make a couple of recommendations. You should understand the type of data in your bucket and take all of these recommendations with a grain of salt. Some files may not be accessed all the time, maybe every 6 months or annually. There are a couple of levers that you can pull on to make this work for you. One example is maybe expand the time range at which you call acceptable access or recent access. Another way is to possibly change the recommendation that you offer. So instead of offering to delete a file, maybe you can ask your data owner to move it from frequent tier to cold storage.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1230.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1230)

That's an option. One of the ways that we've implemented this access related recommendation is through Iceberg table storage. So I'll take a step back here and bring back just our prefix roll up. As I've said before, this is a foundational set. Many  use cases are unlocked now that we have this kind of granular information.

We've mostly been exploring prefix cost, but this can be easily extended to Iceberg table cost. For example, we have the Iceberg partition inventory that tells us table A has prefix B and prefix C has partitions. Now we can easily link this back to the roll up. Well there we go. So not only can we tell what storage tier each partition has, we also know the size and cost of each partition. And now when the sum of the partitions are put together, that's the cost of the table.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1280.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1280)

Layered with access logs again, we can then extend our access-based retention recommendations to a specific Iceberg partition. I'll reiterate what's been said before and leave you with some key takeaways about these analytic tools. S3 Inventory, as Roshan  has pointed out already, is a periodic snapshot. And it's great for granular analysis of the object level. Server access logs capture every request made to these S3 objects and serve a large variety of use cases. S3 Storage Lens for the big picture. They're for growth tracking for your whole organization.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1310.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1310)

Ultimately these tools serve to complement each other and it gives you a comprehensive view of your S3 environment from the smallest object to the largest trends. The suite of storage insights are incredibly powerful tools to help ensure the health of your organization from the megabyte  to exabyte scale. A special thanks to our teams back at Netflix, and I'll pass it back to Roshan for some exciting new features. Thank you, Bi. Thank you, Austen.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1340.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1340)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1360.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1360)

### New S3 Storage Lens Features: Expanded Prefix Analytics for Billions of Prefixes

So, hopefully, you get an idea of how Netflix is continually optimizing the S3 storage and you get some ideas on how to optimize your storage deployments as well. Next, we'll transition  to some of the new announcements we made yesterday with the new feature launches with S3 Storage Lens. Personally, it's quite fascinating for me to hear from folks like Bi and Austen on how our customers are using our services to optimize their storage.  And my team and myself, we have been working very closely with Netflix and other customers to understand what their end to end workflow is and to further streamline their optimization workflows.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1380.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1380)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1400.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1400)

So, we have been focusing on three primary areas. Number one, on visibility,  precisely what Bi was trying to explain here, that folks like Netflix need metrics for all prefixes in their buckets, not just the top prefixes. So that's been an area of focus. The second one is around performance insights. Customers want to understand how can they improve their application performance  when it's accessing data in S3, right? Very much like the use case Austen went through around machine learning or data analytics. You do not want your compute waiting on getting data from S3. That's expensive.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1420.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1420)

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1440.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1440)

And finally, ease of analysis.  We have a lot of metrics, we have a lot of dimensions that you can slice and dice it in Storage Lens, all the way from organization, accounts, regions, buckets, and prefixes. How do we make this analysis simpler for you? So, we're really excited by the three launches we made yesterday. Number one,  in Storage Lens now, you have a new capability, it's called the expanded prefix analytics. Essentially, you can now get metrics for all prefixes in your buckets in Storage Lens.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1460.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1460)

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1480.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1480)

Number two, we have added new performance metrics, and very specifically, these metrics are around  how your applications are accessing your data in S3. It gives visibility to that interaction, right? So the idea is it will help you identify any inefficient requests and eliminate those bottlenecks. And number three, around ease of analysis, we  launched the capability to now export your metrics directly into S3 Tables. These are managed S3 Tables that are automatically created, and you can start querying your data right away using your preferred analytics tools.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1500.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1500)

I'll walk you through each of these and their use cases.  First up, we have expanded prefix analytics.

We went through this use case where Netflix wants to understand at a prefix level what is the storage consumed by each prefix, what is the activity, and the request cost per prefix. Similarly, we have customers asking questions like which are the prefixes that have had no activity in the last 100 days. Those kinds of questions require you to have precise knowledge of each and every prefix and the metrics for each prefix.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1540.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1540)

Yesterday, we launched a new capability called the Expanded Prefixes Metrics Report.  This is available in your Advanced tier of S3 Storage Lens at no additional charge. Just to give you a comparison of the two reports, you have the default metrics report with your storage limit, the one petabyte size limit, and your maximum prefix level of 10. That gives you roughly around 100 prefixes per bucket today. With the Expanded Prefixes Metrics Report, there is no limit on storage size. You can get up to a maximum prefix level depth of 50, and you can get billions of prefixes per bucket. There's no limit on the number of prefixes. This is now available at no additional charge for you.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1590.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1590)

### 72 New Performance Metrics: Optimizing Application Performance and Reducing Latency

Second, on performance insights, we've added 72 new  performance metrics to S3 Storage Lens Advanced tier, again at no additional charge. This brings the sum total of metrics in S3 Storage Lens to 198. As I mentioned, these metrics are specifically designed to show you how your applications are interacting with your data in S3. We have broken those metrics down into three broad categories, and I'll go through these.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1620.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1620)

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1630.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1630)

First up, we have a category of metrics that will help you optimize  your application performance. An example of that is the new request and object size distribution. Now,  you can view in S3 Storage Lens in each and every prefix how your objects are distributed by size, ranging from 0 kilobytes all the way up to 4 gigabytes and above. Similarly, you can see the distribution of requests that are coming into each and every prefix, whether they are small requests ranging from 0 kilobytes, again the same bucket categories, up to 4 gigabytes.

Now, why is this important? If you're seeing a lot of small requests or large requests, that is not good. That means your application is not getting the best performance. The reason could be you have a lot of small objects in your prefix or in your bucket. The easy remediation there is to potentially look at compacting the small objects into larger objects. You get better throughput with the larger object size. Now, you might have right-sized objects in your buckets and prefixes, and yet you're seeing small requests or large requests. There's the potential for you to investigate more on the client side if there's room to improve your applications.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1710.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1710)

Another example is a new category of metrics we have introduced called Concurrent PUT 503 Errors.  As the name suggests, it's a category of 503 errors which are throttling errors, but these are generated when multiple applications are writing to the same object. This can be self-remediated by proper backoff mechanisms, or if it's a multiple writer scenario, you want to build consensus mechanisms to avoid these kinds of errors. Again, these are a category of errors that are slowing down your applications, and you can easily avoid these.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1740.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1740)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1750.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1750)

The  second category of metrics are directly addressing latency and cost, and these are what we call Network Origin Metrics.  Now you can see in S3 Storage Lens what is the origin of the request coming into a prefix. Is it from an application that is running in the same region, in the home region as the bucket, or if it is coming from another region, a cross-region request? That's what you're looking for. You want to avoid cross-region requests where you can because it's adding latency and it's adding cost. This capability is now available where you want to co-locate your application with your data in the same region.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1790.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1790)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1800.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1800)

Finally, the third category of performance metrics is what can potentially boost  your application performance. This is by identifying if your applications are frequently accessing a small subset of your objects. 

Within a bucket or a prefix, if only 10% or 5% of your objects are being accessed every day, that's a potential candidate to be moved into a caching layer or a high-performance storage tier to improve your application performance. So that's in a nutshell the new metrics we have in Storage Lens. Again, as I mentioned, this is available in the advanced tier at no additional charge.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1830.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1830)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1850.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1860.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1870.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1870)

### Live Demo: Exploring Cross-Region Access, Hot Prefixes, and Object Size Distributions

 And finally, on the simplification of analysis, we've added a bunch of new metrics and support for billions of prefixes. How do we simplify the analysis?  Today, the three primary ways you consume your Storage Lens metrics is, first, through the S3 console. The charts I showed you earlier are pre-defined charts that are available.  Second, you can publish these metrics into Amazon CloudWatch where you can view these metrics in context with other service metrics.  And the third option was to export to an S3 general purpose bucket in CSV or Parquet format, where you can then process it and start querying your data.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1880.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1880)

 We now added the fourth option, which is to export these metrics directly to managed S3 Tables. These are automatically created for you. You can set retention periods to eliminate the data from these tables, and you can start querying right away with your preferred analytics tools, including things like MCP servers which will enable natural language-based queries. So I've done a lot of talking about the new features. I'll invite Christie Lee to show you how these features work in real life. Thank you.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1920.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1920)

Thanks, Roshan. Alright, so we're gonna run through a couple of demos today. I'm pretty excited for this.  Storage Lens has been around since 2020, and ever since it launched, we've continued to improve it. Customers have just asked for more and more and more, so I'm really excited for this. But when we polled the room earlier, it seemed that there was about 20% to 30% of folks who were familiar with Storage Lens. So we've got to start at the very beginning. How do I even get started?

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1940.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1950.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1960.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1960)

 So hopefully this view will be familiar to folks. How do I even create a dashboard?  We browse to Amazon S3 in the AWS console, and we'll go ahead and click create a Storage Lens dashboard. You can certainly do this through the CLI, API, SDK, whatever your preference is, but we'll just show the console view for today.  Many customers just want to get their feet wet. Maybe they just want to start with the metrics for a handful of buckets, but you can certainly expand this out to your organizations.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1970.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1970)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1980.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1980)

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/1990.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=1990)

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2000.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2000)

 You can customize the metrics that you configure, being either the free tier and just get your, dip your toes in the water, see what's available,  or you can go right to our advanced metrics to see all the metrics available to you, the 498 metrics that encompass Storage Lens.  You can pick and choose which ones you want. For the demo we'll choose all of them.  But just keep in mind these are configurable, so if later on you change your mind, you can come back and modify how you're configured.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2020.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2020)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2030.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2030)

For the advanced metrics, it's very popular for our customers to want to also see their prefix aggregation. This can also be customized to either the lowest being 1% and then a prefix depth of 10 prefixes, 10 depth for prefixes. We'll export to CloudWatch as part of this config, and then there's two different exports you'll notice here.  One of them is the metrics export, which we've had as part of Storage Lens for the most part for a while now.  What's new is the ability to export directly to S3 Tables, which is our managed Iceberg offering for S3.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2040.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2040)

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2050.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2050)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2060.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2060)

 You can certainly do both. You can still export the traditional way, going to a general purpose bucket, storing it as a CSV or Parquet, or you can just send it straight to S3 Tables, or either one, whichever one you like, or both.  Once we're happy with our config, we'll go ahead and submit that.  Cool, and now our dashboard's configured.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2070.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2080.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2080)

 Now Storage Lens metrics, they get updated on a daily basis, but when you first create your dashboard, it will take a little bit of time to generate those metrics.  So we're gonna flip over to a dashboard that I pre-created so it had some time to generate some metrics, and we're gonna look through a couple of different views. So these views, we'll sample some of the metrics. We won't have time to go through all of them, I wish we did. But we'll start off with latency improvements. This has implications for both performance as well as for cost management. As many of our customers know, whenever you go from one region to another, there are costs involved in data transfer.

If I can localize to the region that my applications are trying to access those necessary objects, I can save on those costs. The performance aspect of it being latency is if I can be closer to where I'm accessing my objects, it also means I will access those objects much faster. So we'll look at views of how to identify for cross-region access.

Next, one of our most popular views, especially when it comes to cost management, is tell me which prefixes have the most access, whether it be the very busy, very hot datasets, or maybe they are really cold. Maybe these are the ideal datasets for me to consider configuring lifecycle rules for and transitioning to a more cost optimized storage class. We'll then take a look at one of the new metrics for identifying errors.

We have plenty of applications, and over time our customers have only demanded more and more from S3 in terms of performance, and this scales broadly across machine learning, AI use cases, and analytics use cases. But being able to target down to which prefixes are seeing those errors is really insightful because it then helps me make decisions around do I want to consider an alternative storage class, do I want to consider caching that data. I want my applications to get the best possible performance for cost optimization because especially GPUs, I don't want them idling.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2190.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2190)

Last but not least, one of  I have a story. So I was working with a customer a few months ago that was seeing loads and loads of errors. I wish we had this view at that time where we could show them the object distributions. But what it turned out to be was they had just misconfigured their application and it was doing these tiny little 1 kilobyte reads and writes over and over and over again. Once we found it, we were able to solve it, but with these views, we would have found it straight away.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2220.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2220)

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2230.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2240.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2240)

 All right, let's jump into the dashboard. So the first one we're going to do is show you how do I quickly identify where my cross-region access is coming from.  We'll do two different views of this, and we'll also show the comparison of not just cross-region, but also how those are compared to my in-region access too.  So this first view shows you I've got on two different days some cross-region traffic that's happening.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2250.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2260.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2270.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2270)

 I'm now going to scroll down to the data transfer metrics, where this can help me normalize relative to all of my data access,  what's cross-region and what's in-region. Now, some of you may be thinking maybe I'm not expecting to localize this data, maybe it's replication traffic and I  expect to have some cross-region data transfer. Not a problem. There is a filter that lets you filter out the replication bytes too.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2290.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2290)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2300.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2300)

For our next view, we're going to take a look at which prefixes have the most access. Tell me where are my hottest possible datasets. We'll flip over to the prefix tab to do this, and once it loads,  here we can quickly get a graph as to what requests are happening on my bucket. Now, typically for customers who are operating at scale with hundreds  of thousands of TPS, they're also going to be a little bit worried about potential errors. Am I seeing errors? Am I seeing retries because of these high request counts?

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2310.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2310)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2330.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2330)

 So we'll filter for our Concurrent PUT 503 errors, and we can see there's some traffic going on there, and that's a little concerning. But maybe I want to go and normalize that as well. Tell me how much percent of all of my total requests is because of those error codes. So we can see for this particular prefix we've  highlighted, about 0.03%. It could be better, not too bad, but it could be better.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2340.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2350.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2350)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2360.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2360)

We'll now use the filter view to zoom into one particular bucket where we know that  we're seeing some of these errors as well as some of these high requests.  And the view we're going to highlight here is the object size distribution. So down in  this graph, we can see the object count, the object reads, and the object writes across the different size distributions.

This is where I can easily see how many of my requests are those really small objects, maybe I've got those little 1 kilobyte, 2 kilobyte, 4 kilobyte objects, up to all the way at the other end of the scale where I'm doing accesses to gigabyte objects. So this is an easy way for us to get that. We could certainly derive this from server access logs or other insights solutions, but from here it's just really quick and easy for me to just gauge what's happening at my prefix.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2400.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2400)

 All right, so hopefully that gives folks a quick taste of what they can do with the dashboard views that are now available.

### S3 Tables Export and Natural Language Queries: Simplifying Analytics with MCP Servers

Now let's take a quick look at what we can do with the S3 Tables exports for Storage Lens metrics. One quick note is that we've had the export to S3 general purpose buckets for some time now, and this includes storing data as CSV or Parquet format, which you can then ingest and consume however you'd like. So one question I expect our customers will have is why should I choose exporting to S3 Tables? One of the benefits is that you can connect it to existing analytics services, especially if you're already comfortable using or interacting with Iceberg. But the other benefit of S3 Tables is that you'll have managed compaction and managed snapshot management that you typically would need to worry about, but with S3 Tables, you don't have to worry about it.

Because of the way we've segmented out the metrics, we've split these out into five different tables. The first one is the bucket property metrics, which contains your default settings on the buckets you've configured. Next are the default storage and default activity metrics. Remember in the dashboard we configured that aggregation of 1% prefix threshold and depth of 10? That's what's going to be included in those default storage and activity metrics. Now, the view that Netflix is probably most excited about is our expanded prefixes storage as well as activity metrics. This is going to provide that prefix depth of up to 50 and unlimited prefixes, so you can get full visibility of what's happening across your buckets, accounts, and organizations.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2510.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2510)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2530.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2530)

 For this demo, we're going to choose to use Athena as our preferred analytics tool, but you're certainly welcome to use any tool that works with S3 Tables. We're going to run a couple of different views just to show what you could do with the metrics.  In the tables, which will be automatically configured for you as part of the S3 Tables integration, you can preview what metrics are available for both the default as well as the expanded metrics. I've pre-set up a couple of SQL statements, so we'll just pop those in and run those.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2550.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2560.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2560)

 The first example we're going to look at is for our default activity metrics. We're going to see if we can find where there are 503 errors happening on my bucket.  This is for where I think I might have some hot datasets or hot prefixes, and I might want to consider moving that to a cache or more performant storage. From here I can quickly see I've got about 800 results. Remember this is going to be my aggregate for some prefixes, but not all of my prefixes, and certainly not any that are beyond depth 10.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2580.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2580)

 If I flip this now to my expanded prefix activity metrics table, I can see all of my prefixes. So independent of size or independent of depth, I'll be able to get a full view across what I've configured for my Storage Lens, all the prefixes that I want to get visibility into. So pretty neat, a quick and easy way for me to get that granular visibility.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2610.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2620.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2620)

 For our next example, we're going to take a look at the cost management aspect.  This is typically how do I find the coldest prefixes, how do I find the largest prefixes, where am I going to get the best possible savings across my organization? So we'll do a quick search here for the activity metrics where I can sort by number of requests. However, I've only looked at the activity metrics. I haven't really been able to contextualize this because I want to be able to also sort based on prefix size. Some of these prefixes are going to be pretty small, and some of them are going to be much larger.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2640.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2640)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2660.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2660)

 So what we can do here is a neat little table join between our storage metrics and activity metrics. Give me the largest prefixes by size and then give me the number of requests that I'm seeing across them.  We'll just highlight that and make sure the join is happening upon the prefix. We'll give that a few seconds to run, and then once it finishes, we'll be able to see where our largest prefixes are that are really, really quiet.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2680.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2680)

There we go.  So we've got a slightly different view. We still get the same number of results because we're still showing all the prefixes, but this time we get a top 10, top 20, top 30 list as to which are the prefixes where it's really cold and I'm going to get the best value trying to optimize it. Often, this is where customers will want to go work with their application owners to say, hey, you know, this is your prefix, this is your bucket, do you want to take another look at that?

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2710.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2710)

So this really empowers you and gives you the data to do that.  So for our final demo, we're going to take a look at how do I use natural language to interact with my Storage Lens metrics. For the demo, I'm going to set up an AI code assistant. You can certainly use your preferred AI code assistant, whether it be Amazon Q or others. I've used Kira in my demo. There are popular ones out there, but we're also going to connect it to a Model Context Protocol server, so an MCP server, so that we can actually have the AI code assistant through the LLM understand external data sources.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2760.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2760)

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2770.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2780.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2780)

It's going to be interacting with S3 Tables, but since our Storage Lens metrics are stored in S3 Tables, it means we can interact with it that way. All right, let's get into it. So first I just want to show you how do you set up Kira. Kira was  launched a couple of weeks ago. If you're familiar with VS Code, its IDE is  inspired by VS Code. So it's pretty quick and easy to set up  and to get started with.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2790.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2800.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2810.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2810)

In this vanilla Kira install, I'm going to start off by prompting it by asking a question of what S3 Tables live in US East 2.  It's going to try to figure out what I'm asking there. It's asking me about tables, it's asking me about buckets. So really, I've not really configured the MCP  server, so it doesn't really know what to do. So let's go fix that. The first thing we'll do is put in the config for the S3 Tables MCP server.  The only thing I need to make sure of is I give it the appropriate credentials so it can access my AWS account.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2840.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2850.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2850)

So once we've got that set up, we can then save it. And I'll just expand out on the left that there are a number of queries that it now understands that it can run in terms of APIs to interact with S3 Tables, and it's going to take advantage of that as part of our prompts. So just to show that it's now working, we're going to ask it the exact same prompt again, what S3 Tables are in US East 2.  Excellent. So we know it's able to use the MCP tool we've just configured.  It knows that we want to talk to US East 2, and it's going to wait for me to give it permissions. There we go. You can set it to not prompt you, but for this demo we've got it prompting us.

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2870.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2870)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2880.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2880)

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2890.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2890)

Excellent, it's been able to find a couple of namespaces. Some of these are Storage Lens related, not all of them are, but some of them are. We're going to  next prompt it to go look at one particular namespace where we know I've got my Storage Lens configured for all of my buckets. I'm then going to ask it essentially the same  question I was running in the Athena queries of give me my top prefixes based on size with the lowest number of requests. We're not going to tell it where exactly it'll  look aside from the namespace, but we'll try to let it figure it out.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2910.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2910)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2930.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2930)

So what's nice about this is it's going to have very minimal human interaction. I've just given it the prompt of tell me where my lowest prefixes are, and it's going to do a little bit of discovery. It doesn't really know anything about the schemas or what's inside of those tables, so it's just going to run some select statements, try to find out  what's available in there. What kind of metrics can I derive that will essentially tell or answer the questions that the user's looking for? It's going to peek at both the storage metrics where I can get my usage trends, but it's also going to peek at the activity metrics where I'm going to get the API requests that I care about as well. 

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2940.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2940)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2960.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2960)

We already know that a join is going to need to happen in order for us to be able to connect both my prefixes based on size but also the requests.  But we haven't told the coding agent that it needs to do that. We're going to let it figure it out on its own. So sometimes it'll go back and forth a little bit, and all I'm doing at this point is just clicking yes go run this command, yes go run this command, and it's trying to create the appropriate SQL statements so that it can essentially find  out the answers to the questions about largest prefixes with lowest number of requests.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/2970.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=2970)

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4100a2e63cd4ad2a/3000.jpg)](https://www.youtube.com/watch?v=Q2YoHfhFuI8&t=3000)

Perfect. So it took a couple of select statements, but it  eventually got there, and it was able to print out the buckets and prefixes with the larger size that have very minimal requests. Pretty cool. So all of this without needing to necessarily know about the schema, not necessarily needing to be a SQL expert, I can get started very easily with natural language to query my Storage Lens metrics. And with that, we thank you all for joining us today. A couple of  key takeaways that I would hope to impart on all of you is that if you haven't tried Storage Lens yet, please take a look at it.

It is part of our storage insights portfolio across our logging solutions. We've also got our holistic usage views as well for Storage Lens. We've expanded out the number of metrics that are available to you, so please do check them out. The free metrics are a great way to get started, but you can also experiment with the advanced metrics. For customers who have a larger footprint or want that granular visibility, those expanded prefix metrics can be opted into so that you can get 50 depth for your prefixes and unlimited prefixes.

There are additional cost and performance optimizations that you can get from the 72 new performance metrics that are included in the advanced tier. Last but not least, have a look at the S3 Tables export if you want to take a look at that natural language processing or taking advantage of the analytic tools that are available to us. So hopefully that gives you a good place to start. I thank you all, and I hope this session was informative.


----

; This article is entirely auto-generated using Amazon Bedrock.
