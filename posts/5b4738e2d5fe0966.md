---
title: 'AWS re:Invent 2025 - Enterprise-scale ETL optimization for Apache Spark (ANT336)'
published: true
description: 'In this video, Vivek and Giovanni from AWS discuss enterprise-scale ETL optimization for Apache Spark, focusing on three pillars: security, unification, and performance. They address common challenges like misconfiguration issues (S3A vs EMRFS), fragmented governance, and inconsistent Spark behavior across AWS Glue, Amazon EMR, and Amazon Athena. Key announcements include unified Spark runtime (version 3.5.6) across all three services, S3A as the default connector with Glacier support, native Lake Formation integration with fine-grained access control (FGAC) and full table access control (FTAC), and multi-dialect Data Catalog views. Performance improvements are significant: S3A is 15.8x faster for dynamic writes, materialized views provide up to 8x speedup, Iceberg operations are 4.5x faster, JSON reading is 20% faster, encryption overhead reduced by 85%, and string manipulation functions like uppercase/lowercase are 10-12x faster. The session demonstrates how AWS has eliminated silos between services while delivering substantial performance gains.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Enterprise-scale ETL optimization for Apache Spark (ANT336)**

> In this video, Vivek and Giovanni from AWS discuss enterprise-scale ETL optimization for Apache Spark, focusing on three pillars: security, unification, and performance. They address common challenges like misconfiguration issues (S3A vs EMRFS), fragmented governance, and inconsistent Spark behavior across AWS Glue, Amazon EMR, and Amazon Athena. Key announcements include unified Spark runtime (version 3.5.6) across all three services, S3A as the default connector with Glacier support, native Lake Formation integration with fine-grained access control (FGAC) and full table access control (FTAC), and multi-dialect Data Catalog views. Performance improvements are significant: S3A is 15.8x faster for dynamic writes, materialized views provide up to 8x speedup, Iceberg operations are 4.5x faster, JSON reading is 20% faster, encryption overhead reduced by 85%, and string manipulation functions like uppercase/lowercase are 10-12x faster. The session demonstrates how AWS has eliminated silos between services while delivering substantial performance gains.

{% youtube https://www.youtube.com/watch?v=bNiFEpzv3_A %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/0.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=0)

### The Configuration Confusion: A Common Spark Job Challenge

 A few months ago, I was working with a team on their Spark job. It was processing a large amount of data and ran for hours. So they added a number of optimization parameters to it. Everything in the code looked right, but the job was still not performing well. After some back and forth, we realized the issue. The job was configured with S3A parameters, but it was using EMRFS. It's a very common situation. With so many connectors out there, it's easy to mix things up. That's one of the problems that we will aim to simplify in today's session.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/60.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=60)

Welcome everyone. My name is Vivek, and I'm here with Giovanni today to talk about  enterprise-scale ETL optimization for Apache Spark. I'm a Principal Delivery Architect, and Giovanni leads the Spark team within AWS that is used by AWS Glue, Amazon EMR, and Amazon Athena. Here is what we will cover today. We will start with the pain points and what makes ETL complex today. Then we will look at how AWS and Spark solve these problems through security, unification, and performance. We will go through a few real-world use cases and wrap up with some action items that you can take back to your teams. This is a 300-level session, and having prior experience will help you with the technical details.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/130.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=130)

Businesses move fast.  This happens in seconds, not in days. But the data systems can still feel stuck in the past. A team recently told me that their ETL jobs are so slow that by the time their job finishes, the question they are trying to answer has already changed. It's a race against time that many feel today. Enterprise customers spend a significant amount of time tweaking their Spark configuration. The data platform team of a large fintech company spent nearly 25 percent of their engineering time tweaking Spark configuration across their 500 production jobs. Still, many are either overprovisioned or fail altogether. A big travel company shared something similar. About 40 percent of their job failures are due to Spark misconfiguration, meaning engineers have to jump in, fix things, and rerun the jobs. That's the reality for so many organizations right now.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/240.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=240)

### Understanding the ETL Landscape and Its Pain Points on AWS

Before we dive deep  into our session, let's take a quick look at the ETL landscape within AWS. It usually starts with your raw data in S3, which is your data lake. Then comes the AWS Glue Data Catalog, which manages metadata like schema, table, and partitions. To secure and govern the data, we have Lake Formation, which steps in with fine-grained access control and centralized permissions. Once that foundation is ready, you plug in your analytics engine. Use Athena for interactive or ad hoc queries. AWS Glue and Amazon EMR are used for large data transformation and machine learning pipelines.

This Spark integration has been part of the AWS ecosystem for many years and is quite reliable. In short, S3 stores your data, Glue catalogs it, Lake Formation protects it, and Spark helps you analyze it.

ETL today is powerful, but it is also challenging. Let's start with security. Teams still manually enforce row and column level rules. Policies are scattered across rules, and data leaks happen because governance is not consistent. Then there is performance. Spark jobs slow down from metadata bottlenecks, redundant scans, or schema and encryption complexities. Finally, there is consistency. Spark behaves differently across EMR, Glue, and Athena. Each environment has its own tuning, its own libraries, and its own maintenance overhead. The result is a fragmented ecosystem that is hard to govern, hard to trust, and hard to tune.

So let's take a step back for a second. What if we completely reimagine this experience from a different perspective? What if your Spark job did not just run, but fluke? Reading smarter, writing faster, and making the most out of S3. What if the governance was not voted on at the end, but baked right in from the start? And what if you could use one Spark engine, the same one across Glue, EMR, and Athena without a single tweak? That is exactly what this new chapter of AWS Spark on AWS is all about. A platform that is secure, unified, and fast by design.

This brings us to the core theme for today: Apache Spark on AWS, built for security, performance, and unification. Think of it this way: three services, one Spark foundation. Security, making sure your every query is protected and every data set is under your control. Unification, giving you the same Spark experience everywhere, whether it is EMR, Glue, or Athena. And performance, delivering faster results that scale effortlessly as your data grows. Each of these pillars reinforces the others, and together they completely redefine what modern data processing looks like on AWS.

### Security Pillar: Addressing Fragmented Governance Across Spark Engines

Let's break this down a bit. What is secure?

We now have fine-grained access control built right in, with native integration with Lake Formation and even multi-dialect views that can use invoker-aware permissions. In simple terms, your data stays protected no matter who is accessing it. Next, unified. There's now one consistent Spark runtime across Glue, EMR, and Athena. EMR in fact now uses S3A connectors by default and supports every storage class, even Glacier. So wherever you are running Spark, it behaves the same way. And finally, performant. You will see faster Iceberg reads and writes, optimized JSON processing, and materialized views that dramatically reduce query time. The best part is that all of this works right out of the box with no code changes and no extra configuration. It just works.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/700.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=700)



Now we have covered the foundation. Let's move into security and what that means across Spark engines.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/710.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=710)



If you look here, you will notice these challenges feel pretty familiar across data teams. Power users are asking why they can't access everything they need.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/720.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=720)



Analysts are stuck filing access requests again and again, and data admins are juggling policies across EMR, Glue, and Athena. It sounds chaotic because it is, and it all comes down to one big issue: governance is fragmented.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/730.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=730)



Policies are scattered, enforcement is manual, and visibility is pretty blurry. That's when the real problem starts. Security gaps, compliance issues, and a whole lot of frustration.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/760.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=760)

### Evolution of Lake Formation Integration: From Record Server to Native Enforcement



Before we dive deep, let's take a look at the Spark and Lake Formation integration history and how it has evolved over time. You can see a clear shift towards simplification and native control.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/790.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=790)



Back in 2022, when we first introduced Lake Formation with Spark, everything flowed through an internal component known as Record Server. It acted like a middleman, enforcing table-level permissions through a centralized full table access control, also known as FTAC policies.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/810.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=810)



By 2023, we introduced fine-grained access control, FGAC, with row, column, and even cell-level filtering. But it still relied on that intermediary layer.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/840.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=840)



Then came 2024, when the enforcement finally moved inside Spark itself. No more proxies. The Spark runtime could directly apply Lake Formation permissions.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/860.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=860)



And this year, we have gone fully native, allowing Spark to directly read, write, and manage tables under Lake Formation governance.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/890.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=890)



[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/920.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=920)

Under Lake Formation governance, you get seamless, secure, and far more efficient data management. But what does that impact on ETL processing?  When Spark accesses data through Lake Formation, it goes through many steps before data from S3 can be accessed. Every read and write request is checked against permissions under fine-grained access control. These checks happen at the row or column level, which means Spark doesn't always have full visibility into partitions and column statistics. That can affect query performance and optimization significantly.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/980.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=980)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1010.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1020.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1020)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1040.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1040)

With full table access control, on the other hand,  Spark operates with complete visibility and direct access. It can perform DML operations like create, update, and match, making it ideal for trusted ETL pipelines, ML preparation, and batch loads where speed and flexibility are critical. Your ETL job will see a performance improvement.  However, if your workload involves interactive, multi-user analytics or environments  where data privacy and compliance matter, fine-grained access control is what you could choose. That will ensure row and column level segregation. 

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1130.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1130)

### Glue Data Catalog Views: Solving Multi-Dialect SQL Challenges

So far we have focused on how Lake Formation secures data and Spark workloads through FTAC and FGAC policies. Now I want to shift gears to a challenge that shows up even before authorization: SQL logic across multiple analytics engines. Today, teams often rewrite the same business logic separately for Athena, Spark whether it's Glue or EMR. That means a single logical view turns into two physical views, one in SQL and one in Spark SQL. Even though the intent is identical, the syntax and functions differ. This leads to duplicated logic, logic drift, and governance issues because it's unclear what is the source of truth. 

Let's zoom out and look at the broader challenge organizations face when managing SQL views across multiple engines. A couple of things come out very clearly. First, fragmented SQL dialects: every Spark engine speaks a slightly different version of SQL, so teams end up rewriting the same logic and dealing with schema drift. Second, inconsistent access control: that means separate view definitions, duplicate policies, and gaps in audit visibility. Third, high operational overhead: you need to rewrite views, fix them, constantly onboard new users, and manage constant changes and maintenance.

These challenges reinforce why a unified cross-engine view model is so important. Now that we have outlined the challenges, let's see how AWS Data Catalog views all these things. Data Catalog views give us one centralized definition that supports multiple SQL dialects behind the scenes.

First, it provides unified governance. Instead of managing separate views in every engine, we keep one logical definition, and Lake Formation enforces consistency. Second, it crosses inconsistency. Whether teams are using Athena, Glue, or Spark, they will get the same result because the logic behind the scenes is the same and in one place. Third, it enables faster collaboration. Teams can reuse the same logic without rewrites and without coordination.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1220.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1220)



Let us see how to create a view. When you want to share data, you first pick the tables you will expose through a view and make sure they are governed by Lake Formation. Then the designated role creates the view using SQL syntax as you would see here. Let's dissect the syntax. When you label it protected, it's locked down for governance. Multi-dialect means a single view works across Athena, Glue, EMR, and even Redshift. Security definer means the permissions of the definer rule apply, not the end users.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1300.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1300)



After it is created, you grant select on that view, not the underlying table. You get simplicity in access management, strong governance, and you keep the raw underlying data and the table locked down. So let us see how Glue Data Catalog views fit into existing governance. The good news is nothing changes once a view is created. Lake Formation treats it exactly like any other Lake Formation object or table.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1380.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1380)



You can grant similar type of permissions like select, describe, alter, and drop the same way you do it today. Your existing models still apply, and then you can use it to share this view across accounts to other consumers also using Lake Formation governed processes using resource links. Lake Formation enforces access consistently across engines. Even if the view uses a different SQL dialect behind the scenes, your governance stays the same: centralized, predictable, and easy to manage.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1460.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1460)



Now let's take a look at how Glue Data Catalog views fit into your architecture. Let me walk you through some of the key business benefits. First, we can use it for targeted data sharing. That means you can expose only the slice of data that a team needs without duplicating anything. Paired with Lake Formation permissions, it makes secure sharing much simpler.

Second, Multi-service analytics. Create a view once and use it across Athena, EMR, Glue, and Redshift. No extra pipelines. No data copies. Third, Centralized compliance. Because all your data lives in the catalog, you get unified CloudTrail auditing for easier reporting and regulatory visibility.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1560.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1560)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1580.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1580)

### Unification Pillar: One Spark Foundation Across EMR, Glue, and Athena

With this, let me hand this over to Giovanni, who will take us to the next part of the session.  Thanks to Ira for the first part of the talk. I'm going to make the presentation a little bit more dynamic and provide some visibility on several aspects that we just released. However, I'm continuing to pick up the torch from Vivek and talking about the second pillar, which is unification. 

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1590.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1600.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1600)

Now there are several challenges to unification. Most of them fall back to a simple root cause: several versions spread across EMR, Glue, and Athena. Several different versions of libraries like Iceberg, Delta Lake, and Hoodie spread across these services.  This brings several challenges. Why do I need to maintain different configurations across the board? Why does functionality that is working in EMR not work in another service? Why do I need to rewrite the job? These questions bring us to the concept of silos between services. 

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1630.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1630)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1650.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1650)

As I mentioned, in AWS we have three services running on top of Spark: AWS Glue, EMR with all its flavors including EKS, EC2, and Serverless, and Amazon Athena. All of them run on top of Spark.  I'm glad to announce that we just made a unified data experience. Last week, we released EMR 7.12 with Spark 3.5.6, Iceberg 1.10 with V3 support, Hoodie 1.0.2, and a bunch of other libraries. The same version and the same flavor of Spark is now running in Glue 5.1, which got released last week. Similarly, for Athena, we released last week and have refreshed the code to run the same flavor. 

No more silos. No more query rewrites. Consistent performance, consistent experience, consistent features across the board. This leaves you with only one decision: where do you want to run your pipeline? Do you want to run in Glue for an interactive service? Do you want to run on EMR because you want to manage your own cluster with EC2? Or do you want a fast speedup with Amazon Athena? That's the only decision you have to make to run your ETL pipeline. No more breaking changes. No more creative rewrites.

Unification is not only about unified versions across the board. It's also about stepping away from customizations. Here I want to provide some historical context. EMR released early on a storage connector. It was customized because there was nothing open source that matched it. After EMR released EMRFS, the open source Hadoop ecosystem created a connector called S3A. However, history was never at par with the original connector. At that point we saw in recent years some challenges. Customers coming from on-premises where they have S3A get challenges when they onboard to AWS. It mainly goes back to the configuration mismatch.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1820.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1820)

We saw that the entire community, plus AWS, started contributing to open source to address this. I'm glad to announce  that with MR 7.10 and Glue 5.1 and Athena, we made S3A the default storage layer. At this point we have open source alignment with no more customization. We standardized the configurations in the S3 namespace. We have unified authentication and optimized committers, and we will see at the end what the benefits of those are. This is great because we stepped out from customization.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1870.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1870)

### S3A as Default Storage Layer: Enhanced Capabilities and Performance Gains

But if that's all? S3 comes with new capabilities.  S3A is based on AWS SDK, which gives access to all S3 storage classes. Of course, you can read S3 Standard. It adds the capability to read S3 Express One Zone for low latency object retrieval. It gives access to S3 on Outposts if you have your own storage on-premises. And my favorite feature of all is Glacier access with all three S3 storage classes: S3 Glacier Instant Retrieval, Flexible Retrieval, and Deep Archive.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/1930.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=1930)

Usually customers use Glacier for cost savings to store archive data. Now, why is Glacier important for Spark? We have several use cases including compliance and regulation.  For example, you want to run an audit for some data that is more than ten years old and is in Glacier. What used to happen was that you had to restore an object, convert that object to S3 Standard, and move it to the right directory because with MRFS and previous versions of S3A, you could not cross-pollinate folders with Glacier and Standard objects.

But now, thanks to this capability, you can cross-pollinate. You just need to restore the object and configure your job with a simple configuration to read restore objects and skip Glacier, reading only the restored data. No more data movement, just restore the object that you need. This is my favorite feature in S3A. We wrote a blog about it, and feel free to check it out. The link will be at the end.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2020.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2020)

We mentioned open source and new capabilities like Glacier, but that's not all. The best is yet to come. We made  AWS S3A faster than EMRFS and faster than open source S3A. This is a simple test where we ran a standard benchmark, TPCDS, across the three storage layers. As you can see, S3A, which is the purple column, is the fastest one and also the most cost effective one. We also made S3A not only faster but also more cost efficient by reducing the number of GET and LIST calls to S3.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2070.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2070)

These numbers are great, but not the best yet.  The best results come from our committers. In ETL pipelines, the last stage is load, which has a write operation because at the end of the day, the data needs to be written to an end table. AWS S3A is up to 15.8 times faster for dynamic writes, which is great for your workload. AWS S3A is great for your health and is 3 times faster than the alternative.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2130.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2130)

In the case of a static insert, we made it around 10% faster than EMRFS. With this, I want to talk about the third pillar: performance. 

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2170.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2170)

### Performance Pillar: Materialized Views for Faster ETL Pipelines

Unification is important and security is extremely valuable. However, you want your job and your ETL pipeline to be faster because that is the first thing you notice. You want that dashboard to be refreshed every hour. You want that report at 8:45 a.m. for your 9 a.m. meeting. You want that job to finish within SLA of 4:45 p.m. for your end of day email out. 

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2180.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2180)

There are several challenges here, but at the end, the real challenge is that you want your job faster and your pipeline to complete within a specific timeframe. 

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2200.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2200)

I want to talk about a database construct called materialized views. A materialized view can be seen as a cache.  You create this materialized view and there is an external component that refreshes this cache. The query engine can use it to optimize their query execution. Materialized views are important because they reduce overhead across all the stages of your ETL pipeline.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2230.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2240.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2250.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2250)

During extraction, you can create a pre-filtered data thanks to the view to minimize data movement.  During transformation, you can create a view to cache complex intermediate results to avoid recomputing them.  During load, you can maintain reporting views with scheduled refreshes. 

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2270.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2270)

Now, you might ask what this has to do with AWS Spark. I am glad to announce that literally last night, AWS Spark released materialized views. I am going from bottom up. Thanks to the consolidation and unification, these materialized views are integrated with all the AWS services described before: EMR, Glue, and Athena Spark. 

After you create a materialized view, this materialized view is fully managed by AWS. You set a refresh rate and the AWS infrastructure will refresh that cache. Everything is managed. We gave AWS the capability to understand whether the query or the job that you submit can benefit from existing materialized views. If the answer is yes and the cache, the materialized view is updated and refreshed, Spark will rewrite your query and your job to speed up performance up to 8 times faster.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2370.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2370)

The materialized view itself is built on Apache Iceberg format, which means it is an Iceberg table. Other engines that do not understand materialized views will see this as an Iceberg table, and you will be able to use it as well. Finally, it is SQL based. Creating a materialized view is as simple as shown here. 

You create a materialized view as select. In this case, we want a customer's name with the amount of orders they did and how much they spent. This is a materialized view, however, it can be seen as a table. I mentioned that the materialized view comes with automatic refresh done by AWS infrastructure.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2410.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2410)

What if you want to run a refresh because your job requires it?  We have added a new syntax called REFRESH MATERIALIZED VIEW that will trigger a Spark job to refresh your materialized view without waiting for the automatic refresh. Materialized views are a great database construct, and during this event there will be several mentions about this again. We just released it last night. I do want to shift gears and provide some announcements soon. However, these announcements are related to optimized pipelines, and something we have observed by talking to customers are four patterns: Iceberg, JSON, encryption, and string manipulations. All of these are common or are starting to become common in ETL pipelines.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2500.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2500)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2530.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2530)

### Optimizing Common ETL Patterns: Iceberg, JSON, Encryption, and String Manipulation

Let's start with Iceberg. We started seeing a lot of customers using Iceberg as a starting point for your ETL pipelines or as an endpoint during extraction or during load.  With EMR 7.12, we made Iceberg with Spark run 4.5x faster than open source. In this slide I put EMR 7.5 because I was on stage literally last year explaining the same slide with just EMR 7.5, and as you can see, we also improved compared to last year. This is for extraction. What about load?  With EMR 7.12 and, of course, as I mentioned before, unification with Glue 5.1 and Amazon Athena, we made simple right operations like merge, update, delete, and insert more than 2x faster than the respective open source. This benefit will also apply to views as they are based on top of Spark and on top of Iceberg.

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2590.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2590)

The second pattern we see in ETLs is reading raw data coming from formats like text files and so on, because these are formats that come from sensors, machinery, and so on. We took a look at JSON. With EMR 7.12, Glue 5.1, and so on,  we made the performance of JSON reader 20% faster than the previous version. No application changes required. You just need to upgrade to the latest version. This improvement will be seen across the board because we have improved how Spark reads JSON itself, so every job that reads from JSON will see these benefits.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2670.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2670)

The third pillar is encryption. Something we started seeing, and we see specifically in jobs, is that customers enabled encryption because they handle data that needs to be secure. They enable it at rest and in transit. However, encryption comes at a cost. It has an overhead to encrypt and decrypt data. We have validated that overhead is around 32% of the entire benchmarking.  With EMR 7.9, we reduced that impact to only 6%, reducing the overhead by 85%, and the entire benchmarking ends 20% faster. We just released a blog a couple of days ago that talks about this. We have done this by improving the encryption itself and the shuffle performance. Therefore, any job that uses encryption will benefit from these improvements.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2720.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2720)

Last but not least, string manipulation. String manipulations are extremely  common in pipelines. However, standard benchmarking like TCPDS and TCPH barely use them. These operations include substring, concat, trim, uppercase, lowercase, and so on. They are really common in the industry because during a pipeline, you want to do general text cleanup like trimming whitespace, correcting cases, and removing noise. In telecommunications, you want to parse your phone number by putting the country code, such as +1 or +32, the area code in parenthesis, and the hyphen between the 6th and 7th characters. You want to standardize IDs, which is important for government and public sector applications, or you want to mask data like credit cards or birthdays. You also want to standardize names like having the first letter in capital and all the rest lowercase.

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2810.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2810)

We took a look at these operations and made them faster. In this graph, we show  the results of the performance improvements from one character to 100 characters. These improvements are linear, so as the size grows, the improvement will be even higher. However, what we see in pipelines are usually between 1 to 100 characters as input. Here we see uppercase and lowercase, which are the most common in pipelines, getting a boost of 10 to 12 times. Trim also gets a nice boost around 4.5 times to 3 times. Then we have a chunk of other functions like concat and substring that are not in the graph, and they get an improvement between 10 to 80 percent, which is still great but not as great as uppercase, lowercase, and trim.

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2890.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2890)

We have two more functions that get so much improvement by applying the same methodology that we had to put them in a different graph. They are length and reverse.  Length at 100 characters gets a boost of 96 times, at most 1 times per character, and as you can see, it is linear, so it will continue to grow. Length is really common to validate if, for example, a phone number has 10 digits or a specific name cannot be passed 25 characters and so on. It is extremely common in pipelines. The other function is reverse with a boost of 56 times improvements.

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2940.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2940)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/2960.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=2960)

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5b4738e2d5fe0966/3020.jpg)](https://www.youtube.com/watch?v=bNiFEpzv3_A&t=3020)

With this, I want to go to the summary.  We have seen how in the past year or so from fragile pipelines, we make them futureproof. Security is not an add-on anymore; it is built in.  With native integration of Lake Formation and its performance, Spark is unified across all AWS services that run on Spark. One foundation to run them all. We have scalable performance for your ambition. With this, I leave you with some resources. Some of these are the blogs that we spoke about during the talk. There will be other blogs as well, so feel free to ask at the end of the talk. Thank you so much for your time. I hope you learned how to optimize your ETL pipeline. Thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
