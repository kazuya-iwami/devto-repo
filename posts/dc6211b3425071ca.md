---
title: 'AWS re:Invent 2025 - Breaking the legacy barrier: How AI is revolutionizing modernization (MAM402)'
published: true
description: 'In this video, Anand Bilgaiyan and Sreekumar Nair demonstrate a seven-phase approach to modernizing legacy monolithic applications using GenAI. They use a .NET Core 3.1 Unicorn e-commerce store as an example, upgrading it to .NET 8 with AWS Transform and performing decomposition analysis through Domain Driven Design and event storming methodologies. The presenters showcase how Kiro, an AI-powered IDE, analyzes codebases, identifies bounded contexts, and generates event storming boards in Miro. They also demonstrate using Model Context Protocol (MCP) servers to create architecture diagrams in Draw.io format and automatically refactor the monolith into microservices with separate APIs, databases, and Kubernetes deployment files. The session emphasizes combining architectural best practices with AI capabilities to accelerate modernization while achieving proper service granularity and avoiding common pitfalls like distributed monoliths or nanoservices.'
tags: ''
series: ''
canonical_url: null
id: 3085124
date: '2025-12-05T03:17:21Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Breaking the legacy barrier: How AI is revolutionizing modernization (MAM402)**

> In this video, Anand Bilgaiyan and Sreekumar Nair demonstrate a seven-phase approach to modernizing legacy monolithic applications using GenAI. They use a .NET Core 3.1 Unicorn e-commerce store as an example, upgrading it to .NET 8 with AWS Transform and performing decomposition analysis through Domain Driven Design and event storming methodologies. The presenters showcase how Kiro, an AI-powered IDE, analyzes codebases, identifies bounded contexts, and generates event storming boards in Miro. They also demonstrate using Model Context Protocol (MCP) servers to create architecture diagrams in Draw.io format and automatically refactor the monolith into microservices with separate APIs, databases, and Kubernetes deployment files. The session emphasizes combining architectural best practices with AI capabilities to accelerate modernization while achieving proper service granularity and avoiding common pitfalls like distributed monoliths or nanoservices.

{% youtube https://www.youtube.com/watch?v=aq2OEssc-qM %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Breaking the Legacy Barrier with GenAI and the Unicorn Store Monolithic Application

Good evening everyone. I know it's been a long day for all of you, but I hope that re:Invent has started great for all of you. We do have an exciting topic to cover as part of this session where we'll be talking about how we can break the legacy barrier and accelerate modernization using GenAI. I'm Anand Bilgaiyan. I'm a Senior Specialist Partner Solutions Architect for Enterprise Transformation, and I'm joined by Shri.

Hi, good evening everyone. My name is Sreekumar Nair. I'm a Senior Specialist Solutions Architect for Migration and Modernization at AWS, and I'm based out of Singapore. Thank you, Shri. So let's get started. Many of our customers view digital transformation as a key priority to support their business growth. However, legacy IT applications and landscapes become a barrier for their business growth. Modernizing these legacy applications can be complex and may warrant a significant amount of cost as well as effort. That's where we're going to look at how we can accelerate the modernization of those legacy applications using architectural methodologies, best practices, and the power of GenAI coming together to transform those legacy applications.

[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/0.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=0)

 So here's a quick agenda for how our next hour is going to look. We are going to talk about a use case from monolithic to microservices, where we are considering monolithic as a representation of the legacy application and microservices-based cloud-native architecture as the target state. Then we are going to look at how we can converge from these legacy architectures in terms of monolithic to microservices using some architectural methodologies. After that, we'll explore how we can leverage GenAI to accelerate that transformation. Since this is a level 400 session, we will be covering different demos toward the different modernization phases as part of this session.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/100.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=100)

 Let me first introduce a sample application, which is a Unicorn e-commerce store representing a monolithic legacy application for our session. We will be converging this into a more modernized-based architecture throughout our session, and in the end we will have a modernized state of this Unicorn store application. Now, what are some of the business capabilities this standard e-commerce monolithic application has? It's a standard web-based e-commerce application which allows users to log in, then search for different unicorn items and check their specifications and pricing. Based on their needs, they can select these unicorn items, add them to the cart, review the cart, and then provide their payment as well as shipment details.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/140.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=140)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/180.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=180)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/210.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=210)

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/220.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=220)

    Then they can place the order, a standard flow like what we all use on any typical e-commerce application. So let's look at the functional flow from this Unicorn store monolithic application. Users log in, they start searching for products, and then based on their requirements they add those products or items to the cart. Then they make the payments, and after the payment they place the order.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/230.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=230)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/240.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=240)

  Now these functionalities have been built using a standard .NET Core 3.1 framework. What are the properties this application has which make it a legacy monolithic architecture? If we look at the different modules which support these different business capabilities like cart, payments, checkout, and product catalog, all these modules are built and bundled as part of a single application.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/250.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=250)

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/270.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=270)

  Now we've looked at this flow from the business perspective. Let's revisit the same application and its properties from the architecture perspective as well. As I mentioned, this application comprises different modules which support the different business capabilities around the cart, products, and payments. All these modules are bundled as part of the same application.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/290.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=290)



[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/310.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/320.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=320)

All these products and modules are bundled as part of the same application, mostly  leveraging the same common database. What's happening here is that the single application is doing everything. All the business capabilities are being supported by this application.  This is where it has a lot of hidden complexities. The different modules are all dependent on each other and may be referring to each other implicitly. These are some of the properties that make this application monolithic.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/340.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=340)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/350.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=350)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/380.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=380)

Now let's look into the different challenges this application will  face. One of the challenges is around scalability. Let's say we are running a Black Friday sale or a Cyber Monday sale  where we are anticipating two to three times the amount of orders. We want to scale the order module to handle these Cyber Monday or Black Friday events. What we end up doing instead is scaling the entire application, because all other modules are bundled as part of the single application. This causes operational challenges and increases the overall cost of deployment, because you are deploying the entire application as a whole, even if you only want to scale the order module. 

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/410.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=410)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/420.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/430.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=430)

The next challenge is the significant time to change, which impacts business agility as well. Suppose different new features need to be released. Any feature requires testing the entire application and then deploying the entire application to production. This increases the overall time to market. Another challenge imposed by monolithic applications is that any issue in any module or any part of the application may bog down the entire application.  This impacts your business continuity and also imposes  a lot of operational challenges and introduces technical debt in your environment. 

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/470.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=470)

### Seven-Phase Modernization Approach and AI-Powered Tools: Kiro and AWS Transform

Now that we understand the challenges of monolithic applications, if we want to modernize, the modernization is not a single step process. Modernization is a phased approach that requires careful consideration to transform this legacy application into more modern, agile, cloud-native architectures. Given the case of this particular application, if we have to transform or modernize it, what could be the possible different phases we may have to think through and take to transform it into a more modern state? Let's have a view on that  first.

Definitely, the first phase or the first step is to do an application analysis. This application analysis is very important because it may give you some properties around the application that may help you take the first step toward your modernization. For example, as I mentioned in the previous slide, this particular UnicornStore application is built on .NET Core 3.1, which is an older framework. The first thing is to improve the overall performance and security posture of the application, so it's better to transform this application into the latest version of the framework. Those kinds of insights I can get by doing the right application analysis.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/520.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/530.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/540.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=540)

Once the application analysis is done, I want to definitely transform the runtime  and the framework so that I can improve the performance and security posture and make the application better in terms of introducing new capabilities around modern cloud-native architectures.  Then I definitely have to build and validate the application with whatever changes I have done architecturally from the framework upgrade standpoint.  Once that is done, I need to perform a decomposition analysis, and we will talk in much detail about the decomposition analysis and what are some of the architectural methodologies to perform this decomposition analysis.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/560.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/570.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=570)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/590.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=590)

Once the decomposition analysis is done, what it provides us is  an identification of the different services in which this monolithic application could be transformed or decomposed.  Last but not least, once you have identified those right services, you start refactoring this application and then modernize it accordingly with the new architectures.  We are going to look at all these different phases and define the right architecture, then perform the right decomposition and refactoring of the application.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/610.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=610)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/630.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=630)

To perform these different steps in the process of modernizing a legacy application, we are going to leverage some of the key GenAI-based capabilities provided by AWS using the different architectural methodologies and the capabilities of GenAI.  Let's look at those very quickly. The first one is Kiro, which is  an AI-based IDE to accelerate your prototype into production. Developers have mostly leveraged code companions or AI-based IDEs to perform VIP coding. Now we want to move away from that phase. Instead of doing VIP coding, we want to write quality code and then move into the spec-driven software development approach where your prompts become the specs. This allows you to perform the right business analysis and requirement analysis, and then from there you design your application according to your requirements, implement those requirements, and finally execute that code. The entire lifecycle of building software can be powered by this AI-driven IDE called Kiro.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/700.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=700)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/720.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/730.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=730)

Kiro has different capabilities.  The first capability is that it can help you build a new application using the spec-driven software development approach that I just described. You can also modify existing applications where you need to adopt new change requests based on your business requirements.  And last but not least, it also supports refactoring and modernizing your existing application codebase. We are going to focus on this particular capability throughout our session as we go into the different phases of modernization. 

The next very important and key service that we will be covering in our session is AWS Transform, which is the first agentic AI-based service to perform large-scale migration and modernization. This particular service has four different key capabilities. The first one is on VMware migration, where it can help us assess the existing VMware environment, perform the right networking design in the context of migrating to AWS, do the right capacity planning with intelligent grouping of applications, and effectively migrate these applications to the cloud. Last but not least, it can execute the actual migration.

The second capability is on full-stack Windows modernization, where we have released some additional and advanced capabilities. AWS Transform has become more powerful to perform your end-to-end Windows modernization. Along with .NET modernization, meaning .NET application transformation, it also supports modernizing the associated databases. So if you have a .NET application and want to transform it, let's say from the legacy .NET framework to .NET 8 or .NET 10, along with that, the associated databases of this application can be modernized from SQL Server to Amazon RDS for PostgreSQL, Aurora PostgreSQL, or other open-source databases. This can significantly improve your overall cost and give you Windows licensing freedom.

The third capability is on mainframe modernization, where it has the capability to perform mainframe application modernization assessment and do the right decomposition analysis based on the domain-driven approach. We also have a new capability where it can support all three phases of mainframe application modernization: reimagining, replatforming, and refactoring the application. It also helps customers significantly reduce the testing effort for mainframe workloads. Last but not least, there is custom modernization, where not only .NET, but you can transform applications from Java, from .NET to Python, and even your custom applications based on your enterprise code patterns, design patterns, and architectural patterns. It can adopt these and accordingly perform the transformation.

We are going to focus on the Windows modernization capability as part of this session because the Unicorn Store application is a .NET-based monolithic application. Now, let's look into the first phase of that modernization workflow where we want to perform the application analysis of the given application and how AI can help us to do that.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/930.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=930)

### Application Analysis with Kiro: Identifying Technical Challenges and Framework Limitations

Thank you, Anand. Anand has taken you through the seven-phase approach that we are going to talk about today, as well as the Unicorn e-commerce application. The next step for us is to use Quiro  to look at the Unicorn application itself and understand its current architecture. In the next video that you are going to see, we will open the Unicorn application codebase in Quiro. Then, using natural language queries, we will ask Quiro what framework is used in the application, what are the current technical and business challenges the application has, and explain the architecture of the application.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/990.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=990)

Once that is done, we will use the insights that Quiro has given us to develop a comprehensive modernization strategy for this application, including short-term quick wins, medium-term service decomposition, as well as long-term architectural transformation. The idea is that after this demo, you should be able to use this methodical approach within your own applications and break down those into manageable, scalable services. Let's go into the demo now. 

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1000.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1000)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1020.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1020)

This is the Quiro interface. You can see the explorer window as well as the chat window on the right side. I am going to ask Quiro to analyze my codebase, determine the .NET version used,  and explain what are the business and technical challenges. Quiro is thinking and going through the codebase. It is trying to understand what is the current version used. It says it is .NET 3.0. What is the recommended target? .NET 8, because .NET 3.0 has already gone end of life a few years ago. 

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1030.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1040.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1040)

The current architecture is a classic monolith inter-application. It also recommends the key business and technical challenges, including cloud-native limitations.  It highlights the shared data management issue because it is using a shared database across multiple services. The monolith API makes it difficult to expand the different business capabilities.  What is the immediate recommendation? Upgrade to .NET 8, short-term extract the bounded context, which we will explain about later, and medium-term go into a complete microservices architecture.

### Framework Transformation: Upgrading from .NET Core 3.1 to .NET 8 Using AWS Transform

Quiro has analyzed the codebase and provided us with the recommendations. The next step is that we have seen it is using .NET 3.5 framework, and it is recommending to upgrade to .NET 8. The service that we are going to use is AWS Transform to upgrade the codebase to .NET 8. What is AWS Transform? It is an agentic AI-powered service developed to accelerate enterprise modernization of .NET, VMware, Windows, mainframe, and more. It is based on 19 years of AWS experience in migration and modernization, using AI agents to automate complex tasks like code analysis, assessment, refactoring, decomposition, dependency mapping, validation, as well as transformation planning for your applications.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1140.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1140)

In this particular demo, we will be using the AWS Transform for .NET capability. There are two ways that you can use the AWS Transform for .NET service: either using the web experience or web interface, or using the Visual Studio toolkit extension. In this demo, we will be using the Visual Studio toolkit extension. Let's see the demo of how AWS Transform does it. 

Before going into that, I also want to expand on how the AWS Transform for .NET follows through the three critical stages. The first step is the analysis phase. Basically, what it does is look at the existing Windows-dependent application and identify the components which need to be modified for Linux compatibility. Next, it goes through the transformation phase, where the AI agents will convert the applications into .NET 8 applications. This is where AWS Transform does the heavy lifting. It basically handles the framework upgrade as well as platform-specific code changes that are required for the application to run on Linux, which is .NET 8. Finally, it does a validation to ensure that the transformed application functions correctly before deployment, which is key to making sure that the migration and modernization is successful.

How does it accomplish this key task? First, it upgrades the language version.

AWS Transform upgrades the language version by finding outdated C code in your application and replacing it with Linux-compatible C code. Next, it upgrades packages from Windows-dependent .NET Framework to more Linux-compatible .NET versions. Third, it rewrites the code for Linux compatibility. Finally, for open-ended tasks where human intervention is required and the transform is not able to make the changes, it provides you a detailed report of what changes you need to make so that you can get the application built and successfully run on Linux.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1270.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1270)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1280.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1290.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1300.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1300)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1310.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1310)

By modernizing your .NET applications, you can break free from Windows dependencies, reduce costs, and gain the flexibility of Linux environments. Let's see the demo now. We have opened the Unicorn Store in GitHub, and now we're going to open Visual Studio Code and go to the AWS Toolkit.  You need to make sure that the AWS Transform is connected and authenticated using IAM.  We have the project opened on the right side window.  Now we right-click on the project itself and go to port solution with AWS Transform option. Here it shows the target versions, so you can see .NET 8 and .NET 10 are available.  You click on start, and it opens the AWS Transform Hub, which is used to track the entire transformation plan. 

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1320.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1330.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1340.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1340)

It shows the plan, and you can either customize the plan which it provides or use the default plan that Transform provides.  With customization, you can add more steps. It's going through the transformation plan, so essentially it tries to build the application and make sure all the dependencies are available.  Then it completes the transformation. You can see on the top that the transformation has been completed. Now you can download the detailed transformation report. 

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1350.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1360.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1370.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1370)

In the transformation report, you can see the packages it has updated and the APIs it has changed, whether it is a partially successful transformation or a fully successful transformation.  For partially successful transformations, you can go down and see the project summary to see if there are any build errors and what packages have been updated.  You can see the files that have been changed to make sure it is Linux compatible. You can use a diff of the files so you can see what was there and what has been changed, and you can view that option as well. 

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1380.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1380)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1390.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1390)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1400.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1400)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1410.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1410)

You can go to the Linux readiness section, which provides a detailed report on Linux readiness.  It shows what the current framework stack is being used. You can download the next steps, which will tell you what critical errors you need to resolve to make sure this application is ready to run on Linux.  You can go through and validate the steps to make sure you execute it.  It provides all the commands and changes that are required before you can do it. Then you can view the diffs on all the files, and you can select all and do an apply changes, which will do an in-place change of all the files. 

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1420.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1420)

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1430.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1430)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1450.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1450)

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1470.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1470)

Now the application is already on .NET 8.  It has been upgraded to .NET 8 version, which is Linux compatible.  Now it has been upgraded to .NET 8, and we need to make sure that this application runs successfully. I'm going to use Kiro to build that application. I'll go into Kiro and prompt Kiro to ask what .NET version is being currently used.  It tells me that now it is .NET 8, which was our target version that Kiro suggested earlier. Now I'm going to ask Kiro to build the application and run it locally. 

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1480.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1490.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1490)

Kiro is actually doing the build. It checks the dependency and sees what .NET version I'm running on the local computer. It finds that it's already running .NET 9, then it tries to restore all the dependencies.  It runs a .NET restore to restore all the dependencies and everything. It started building the application, and you can see the build has succeeded. It has provided me a URL saying that it's listening on port 8080. I click on the URL, and it opens the .NET application, the Unicorn application.  Now I can go in, choose unicorns that I want to purchase from the store, and then click on submit. As you can see, this is the application demo that Anand showed earlier.

Now it is running on .NET 8. The key thing to remember here is we haven't decomposed this application. It is still running as a monolith app, but its version has been upgraded to .NET 8. Now let me call Anand on stage to explain the decomposition process.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1540.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1550.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1550)

### Decomposition Analysis: Applying Domain-Driven Design and Event Storming Methodologies

What we have seen so far is that we have upgraded our application to the latest version, and now we need to perform the analysis on how we can  decompose the Unicorn Store application. From our seven-step  process, steps one, two, and three have been completed. We have done the application analysis, we have done the transformation of the code, and we have also validated that the application is successfully working after the transformation. Now we need to look at the decomposition analysis to identify the probable right candidates for microservices.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1590.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1590)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1610.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1620.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1620)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1640.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1640)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1650.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1660.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1660)

This is the toughest part, trust me. How do we break a monolithic application into a more modular, decoupled application? There are different ways  in which we have observed application teams and application owners try to decompose the existing monolithic application into a more decoupled microservices-based architecture, and they fall into different pitfalls. What are those pitfalls? The very first pitfall is the distributed monolithic.  Often, we have seen application owners converge to the distributed monolithic.  How? Because they try to apply horizontal tiering to the existing monolithic application. What does it mean? It means grouping functionally related modules together, which are similar or common components, and trying to carve out submodules or subgroups out of your monolithic applications. You have a monolithic  application, you are grouping the related components together, and what you have is smaller submodules coming out from that application. But still, they are doing  multiple things or more than one thing, and they are inheriting the properties and design challenges of monolithic applications. 

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1670.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1670)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1700.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1700)

The other pitfall which we have seen application teams go into is nanoservices. What does it mean? It happens due to vertical tiering of the application. Now, what does vertical tiering  of the application mean? Let's say you have a UI and the business capability driven by those UI pages. What application owners try to do is break or split the application functionalities using the different UI screens they have right now. Imagine the situation if your monolithic application comprises hundreds of UI pages. Now, what could eventually end up happening is you are having that many nanoservices in your environment.  It imposes a lot of operational challenges because you need to take care of hundreds of services, and it also increases the chattiness in your environment.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1720.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1720)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1750.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1750)

These are the common pitfalls, and that is where the right granularity is the key to decompose these existing monolithic applications. Now the question comes: how do I arrive at the right granularity?  The key here is to not only assess or analyze the existing monolithic applications from the implementation and technology perspective, but also to have the right analysis done from the business domain perspective. Then combine the right business analysis and the technical analysis together to arrive at the right granularity. 

How do we achieve that? There are some architectural methodologies which we are going to talk about that can help us arrive at the right granularity. One of them is Domain Driven Design, the first book by Eric Evans. What he mentioned is that technology is not important and should not be the primary focus for software development. Rather, the business features and the capabilities which we want to perform through the technology should be the primary focus. What does it mean? Domain Driven Design helps us arrive at a shared understanding from the business owners and the application owners together so that we can build the right software, applying the right granularity and the right architectural patterns. This is what Domain Driven Design provides and how Domain Driven Design works.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1810.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1820.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1820)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1840.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1850.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1860.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1870.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1880.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1880)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1900.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1900)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1930.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1930)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1950.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1960.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1960)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/1980.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=1980)

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2010.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2010)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2030.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2030)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2060.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2060)

This is what domain driven design provides and how domain driven design works. Let's look into some of its key concepts.  So, what is a domain? Domain driven design helps us to do the right analysis of the given application from the domain perspective and helps  us to understand the domain and its different boundaries. A domain could be a business for which we want to work and provide services to our end customers. In our use case for Unicorn Store, retail is a domain because it's an e-commerce application running over the web and providing different capabilities to the end user. Now this retail  domain could have different subdomains. For example, the order management module is a subdomain which helps us to manage the overall orders for our application.  The second could be fulfillment. Once the order is placed, how does the fulfillment of those orders happen, and the shipment, of course, so that the products which are purchased by the end users can be delivered to them.  So these could be subdomains for a given domain. Now, this subdomain further needs to be categorized into three different categories.  One of them is the core subdomain. What does the core subdomain mean? The core subdomain is the subdomain which is a differentiator for you, right, which is what you should care about to modernize, which consists of your business logic.  In our case, it is the order management, fulfillment management, and the shipment management. The second subdomain type is the supporting subdomain. Supporting subdomains are required for the core domain to achieve its functionality. Take an example of order management. You might be running some loyalty program, and for that, the user information might have to come from the CRM system.  So supporting subdomains are the domains which sometimes can be built or sometimes can be bought as external software products. The last is the generic subdomain. For example, if any user has to purchase any product, they need to first authenticate and have the right access to the application.  The generic subdomain could be our identity system, which is often bought, not built, because that's not the core domain for the retail application. So if we just focus on the core subdomain in the context of our Unicorn application, the core subdomain will again have different contexts within it, which help us to  provide the right business capabilities around that subdomain. In the case of the order subdomain, the order subdomain has different contexts because an order requires items, requires the end users who purchased those items, and then the payment which has been done.  So there are different contexts associated with it within that subdomain. The same could be in the case of the fulfillment  subdomain as well. So domain driven design is helping you to assess the right analysis of the domain, its subdomains, what is the core subdomain, and what the core subdomain is comprising of in terms of different contexts. Eventually, these contexts will help you to define the right boundaries around the subdomain, and those boundaries will then finally converge into the right services.  But the question comes: domain driven design is good because it is helping you to come to a common understanding between the business owners and the application owners. But how should I start understanding these subdomains? How should I start getting this information about these different contexts? That is where Alberto Brandolini, the inventor of event storming, made  a very good quote. What becomes the software is not the expert knowledge but the developer's understanding, or sometimes the misunderstanding, because the business owners may have one understanding and the application owners may not have the same understanding. That's where we have to bring both of these parties together so that they understand the domain in a shared context.  That is where event storming as the next architectural methodology can help to achieve a shared understanding between the business owners and the application owners of the domain. How does this happen? Event storming is not an architectural tool or a design pattern. It's a methodology, a workshop which happens between the business owners and the application owners. The intent of the workshop is to understand the domain by identifying the different domain events and identifying these domain events' producers and consumers. Once we identify that, then we can identify the different bounded contexts associated with those subdomains, and then those bounded contexts eventually become the blueprint for the different services.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2110.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2110)

Event storming has different  elements, and the intent of the workshop is to identify these six or seven elements, which are represented by different multicolor sticky notes. The first sticky note or the first element is the domain event. We identify what the different domain events are that are part of the given domain space. For example, order created, order canceled, or order placed are these different types of events. Events are always represented in the past tense, meaning something has happened in your business context.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2150.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2160.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2160)

The second key element to identify under the event storming exercise is the command.  What commands generated these events? For example, somebody would have placed a command like place order or cancel my order. These commands are eventually generating those events.  The third thing to identify is who are the actors who are actually issuing those commands. In our unicorn store use case, I am the user. I am logging into the application and I am placing those commands. It could be the machine as well. It could be business processes as well who can become an actor issuing the different commands.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2180.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2180)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2190.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2190)

The fourth key element is aggregate.  Aggregate is something that actually understands these commands and executes certain business logic to fulfill those commands. The next one is the business policies.  As part of the event storming, we identify the different business policies, and the business policies could be like when somebody is placing the order, whether the inventory has been validated, whether the payment is successfully done or not before even placing the order successfully. These are the business validation rules which you will apply for the different business flows.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2210.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2210)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2220.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2220)

The next is the external system.  For example, if I am placing the order, I need to perform a payment first, and for payment, I might be leveraging the external payment gateway. So what are the external components which are involved in my business processes?  And last but not the least, the view, which is represented in the form of a UI screen where the user can then understand the current state of the application or the business and then accordingly take the action. For instance, understand whether my order is placed, whether my order is shipped, or whether it has been canceled. It could be anything for that matter.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2260.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2270.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2270)

If we identify all these elements, then we arrive at the domain event-driven decomposition approach. It is the domain events who drive the decomposition. Applying the domain-driven design principle along with the event storming can help us to identify the microservices  based architecture. How does this happen? We perform the domain analysis. We apply the event storming to further advance the domain analysis with more bounded  context-specific information, identify the right bounded context, and then from there convert to the right microservice.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2280.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2280)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2290.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2290)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2300.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2300)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2310.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2310)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2320.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2320)

If I take the example of order management,  how the outcome of the event storming for the order processing use case might look like, I am an actor. As a user who is performing a  command saying place order, that command produces a domain event like order is created. When this event is  created, there are some business policies which are getting validated like validate the inventory. Based on the inventory, you successfully place the order, and once that is done,  then it could again issue separate commands and there could be external systems like a payment gateway. They can also produce events like the payment is successful. 

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2340.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2340)

Once we do this event storming analysis and identification of these different sticky notes and different domain events, what we do is we try to group the related domain events together and then converge to the right bounded context.  That right bounded context then represents the different domain services for us. In this example for the order processing use case, what we saw is the entire event storming analysis helped us to identify what are some of the application capabilities in terms of what commands it could support the order management flow, what are some of its external systems to which it depends upon like payment, and what are its own repositories, meaning the data or the current state which it needs to maintain in terms of the order lifecycle. This is how we can actually perform the right decomposition for a given monolithic application with the right granularity.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2380.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2380)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2390.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2390)

### AI-Accelerated Event Storming: Generating Bounded Contexts and Virtual Collaboration Boards

Now let us see how GenAI can  help us to accelerate this entire event storming and the domain-driven design-based analysis for our given application. We again leverage Kiro here.  We have already done the transformation of our Unicorn Store application to .NET 8. Now what we use is we ask Kiro to perform the event storming analysis for us.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2410.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2420.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2430.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2430)

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2440.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2450.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2450)

We provide a prompt that tells Kiro to perform event storming analysis.  In the prompt, we identify all seven elements of event storming: from domain events to commands to actors, external systems, business policies,  aggregates, and key roles.  Kiro then introspects the entire application codebase and attempts to identify all these event storming elements. It then tries to map the probable bounded contexts that can be defined using this event storming analysis.  Currently, Kiro is performing that analysis, and it has completed its analysis. Now we can see the different domain events that have been identified by Kiro for a given application.  If we highlight on that, we can see, for example, the order management events: order created, order completed, order failed. Or even the payment process events. In the case of shopping cart, there are different types of domain events that have been identified as well.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2480.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2480)

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2490.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2490)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2500.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2510.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2510)

The intent here is not to perform a complete event storming and domain-driven design analysis based on the AI, but this information certainly becomes the base on which the business owner and application owners can perform their own assessment and refine it further.   We are seeing that not only the domain events, but also the different commands and different aggregates have all been identified by Kiro for a given monolithic application, the Unicorn Store in our case.   Not only is it identifying the domain events, but it is going one step beyond and also giving us the probable bounded contexts and highlighting the probable microservices that could be carved out from this given legacy monolithic application.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2520.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2520)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2550.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2550)

This is important because this recommendation is not just coming from horizontal or vertical tearing, which we saw earlier. It is coming from architectural methodologies like domain-driven design and event storming that have been applied and working backward from there to determine what could be the probable bounded contexts and the respective microservices.   Now that we have done this analysis, the next thing is to actually perform event storming where the outcome of Kiro becomes the base for the business owner and application owner. They can go into a shared workshop mode to further optimize this analysis and then modify it according to their business understanding and implementation understanding. This event storming workshop can happen in person or it can happen virtually.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2630.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2630)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2640.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2650.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2660.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2670.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2670)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2680.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2680)

We are taking a use case where it has to happen virtually. There are some platforms that provide virtual event storming boards. One such platform is Miro. Miro helps us perform a virtual event storming exercise by providing a virtual event storming board and supporting different sticky notes. As a developer, you can create your developer account, generate the API keys, and using those API keys, you can then programmatically generate the different event storming boards. We want to do exactly that and are leveraging the AI capability to perform the same.  We are asking Kiro to generate a script based on this event storming analysis that has been done.  The script will create a Miro board for us and apply all this understanding that we have obtained via event storming analysis on the Unicorn Store application.  Kiro goes ahead and generates the script.  With that script, it will generate a virtual event storming board for all of us. The intent is then to use that virtual board so the product owner and application owner can perform further brainstorming and optimize or refine the different service boundaries and bounded contexts accordingly.   It has created the board successfully.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2710.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2710)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2740.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2740)

We open the mirror board and can see how all the different domain events are associated with their commands.  This shows how the business policies, external systems, and other elements have been grouped together in the different contexts. We are taking an example of zooming into the order management domain. If we see how the bounded context for order management has been built via our GenAI capabilities after performing event storming and Domain Driven Design based analysis, we have all the different events from the order processing flow in place.  We have all their respective commands, all their respective external systems, and the other elements that are part of it. This becomes the blueprint for us to then build the right microservices with the right boundaries and granularity.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2760.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2760)

### Architecture Design with MCP: Creating Microservices Diagrams Using AWS Architecture Diagram Server

For us, the next phase is to see, based on these bounded contexts that we have generated, how we can then refine the architecture for our monolithic application and then build these new microservices which we have seen as part of the event storming outcome in the form of those different bounded contexts.  For that, I call Shri over to you. Thank you. I also want to explain what the MCP is. MCP is an open source protocol that standardizes how applications talk to large language models, providing them with the right context.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2800.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2800)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2820.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2820)

If you look at the bottom of the slide, you have the MCP.  On the left side, you have the agents. These agents could be chatbots, autonomous workflows, or your application. On the right, you have data sources, which could be your APIs, internal systems, databases, and so on.  MCP sits right in the middle, so it acts as a glue. Instead of an agent integrating with every system in its own custom way, MCP gives you a common, consistent way to describe tools and share data. This means that the agents become more portable, integrations are reusable, and you can avoid the one-off plumbing that you have to do every time a new agent talks to a new source or you want a new capability.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2850.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2850)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2880.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2880)

AWS has a GitHub repository where we have provided a lot of reusable MCP servers.  One of the MCP servers that we will be using here is the AWS architecture diagram MCP server. It's a Python package that helps you design architecture diagrams, flow diagrams, sequence diagrams, and so on. We will be calling that MCP server. The next step is basically for you to go into Kiro and configure the MCP server. It's a very simple step. You go to GitHub, you get the MCP server, and the instructions that you need to follow to add the MCP server into your IDE are mentioned there. 

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2890.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2890)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2920.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2920)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2930.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2930)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2940.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2940)

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2950.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2950)

Once you configure that, I have already done it.  Now I'm asking Kiro to generate an architecture diagram for me. By default, it creates a PNG diagram or a JPEG diagram. But I want something that I can iterate and modify. So I use a tool called Draw.io very frequently. What I'm telling Kiro is to generate that architecture diagram in a Draw.io XML format so that it would be easier for me to start iterating from there. Let's go ahead and play the demo.  I'm going to ask Kiro to use the AWS diagram MCP server to create a Draw.io XML diagram showing AWS microservices architecture based on the bounded contexts that have been identified from the previous session.  It's going to call the MCP server. You can see that it is calling the MCP tool get_diagram, and then it's going to generate the architecture diagram for me.  It has actually generated a PNG diagram. Since I have asked for a Draw.io format, it's actually going to recreate that file in Draw.io.  Also, if you have noticed on the left-hand side, it has created that particular file. It has created the architecture diagram with data flow patterns all defined there. I have opened my Draw.io, so I'm going to open the diagram that it has created.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/2970.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=2970)

Yes, see, this is created by the MCP server directly.  Earlier this used to take days for an admin, a platform architect, or an application owner to create it. Now it has already been done through the MCP server. This is actually organized based on different domains. So you have user management domain, product catalog domain, and how the different data flows work with each other. Again, there is Lambda mentioned there, but it is easily replaceable since it's a microservice. You can either replace it with an ECS or any EKS or a container solution. It has also identified the external services where notifications are involved.

Specifically, this includes external services like SES and third-party APIs that need to be called, as well as the monitoring and logging capability in the architecture diagram. This is a raw diagram that has been created by Kiro. You can iterate on it based on your requirements.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3030.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3030)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3050.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3060.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3060)

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3070.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3070)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3080.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3080)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3090.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3090)

### Refactoring to Microservices: Complete Decomposition and Key Takeaways from the Modernization Journey

The next step, the seventh step that we have seen,  is to actually decompose and refactor this entire application into microservices with Kiro. I'm going to ask Kiro to break down this monolith code into different microservices. I'm going to prompt Kiro now to analyze and decompose it into microservices.  I'm also asking it to generate web APIs, Entity Framework models, database, and data components everything separately.  It has done the analysis and completed the extraction of the microservices. You can see that it has created a comprehensive project structure.  There are independent service folders, shared artifacts, Kubernetes manifests, and deployment files. We'll explore the folders and shared artifacts on the left-hand side.  You can see the project structure with services organized as catalog, cart, order, and user.  If we explore the folder structure on the left-hand side, you can see the services, including the cart service. It can also identify the service contracts between APIs, identify data ownership issues between services, and identify shared code which it needs to extract as common libraries.

You can see that earlier you saw the project files organized under one folder, if you remember the first slide that we showed. Now you can see that the project files have been organized into a different folder structure. I'm just running through each of those folders to make sure that all the files are there. It has also created the deployment files. Depending on the platform that you are asking for, in this prompt that I gave, I asked it to create a deployment file for Kubernetes or EKS deployment. It has created a CloudFormation template which I can then use to deploy the infrastructure and the application components for that application.

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3200.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3200)

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3220.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3220)

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3230.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3230)

Let me invite Anand to summarize the modernization flow. Thanks, Rie. So finally, what we have achieved is that we started with the legacy monolithic application and we have run through the seven-step process to modernize this Unicorn Store application.  First, we did the code analysis leveraging Kiro. Then we transformed from the older .NET framework to .NET 8 for our existing application using AWS Transform. After that, we did the event storming and domain-driven design analysis for the given application.  We generated a different event storming board to represent the different bounded contexts we could arrive at with the right granularity, which then finally converted into the right microservices. From there, we did it all through Kiro and we leveraged the MCP integration of Kiro to generate the architectural diagram for these new microservices.  Finally, we decomposed our legacy monolithic application and refactored it into a more modern microservices-based architecture again using Kiro. The end-to-end journey of modernization has been accelerated using our AI-based capabilities along with the different architectural methodologies and practices.

[![Thumbnail 3250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3250.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3250)

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/dc6211b3425071ca/3290.jpg)](https://www.youtube.com/watch?v=aq2OEssc-qM&t=3290)

What are some of the key takeaways?  One of the key takeaways is definitely leveraging domain-driven design and the event storming-based approach to do a right analysis of your domain and then get the right shared understanding between the application owner and the business owners to define the right boundaries and granularities to decompose the legacy applications. The second is leveraging AI-based capabilities like AWS Transform and Kiro to accelerate your modernization, which can significantly reduce your overall effort, time, and cost to modernize your legacy application stack. You can drive innovation faster.  That's all, folks. Thank you very much for joining our session. If you have any queries, Anand and I are available, and we can take your questions if you have any. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
