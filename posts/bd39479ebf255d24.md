---
title: 'AWS re:Invent 2025 - Multicloud security best practices (HMC318)'
published: true
description: 'In this video, AWS Senior Security Specialists Geoff Sweet and Luis Pastor discuss multi-cloud security best practices, defining multi-cloud as operating in two or more cloud service providers. They address key challenges including centralized federation for identity management, data protection through automation and DSPM tools, centralized logging via SIEM systems for visibility, and scaling teams with specialized expertise or multi-cloud tools. They emphasize using cloud-agnostic frameworks like NIST Cybersecurity Framework, focusing on security outcomes rather than implementation details, and extending existing tools across clouds. The presenters stress that data governance, understanding data flows, and maintaining consistent security controls across all cloud environments are critical, regardless of business drivers like mergers, regulatory requirements, or ISV partnerships.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Multicloud security best practices (HMC318)**

> In this video, AWS Senior Security Specialists Geoff Sweet and Luis Pastor discuss multi-cloud security best practices, defining multi-cloud as operating in two or more cloud service providers. They address key challenges including centralized federation for identity management, data protection through automation and DSPM tools, centralized logging via SIEM systems for visibility, and scaling teams with specialized expertise or multi-cloud tools. They emphasize using cloud-agnostic frameworks like NIST Cybersecurity Framework, focusing on security outcomes rather than implementation details, and extending existing tools across clouds. The presenters stress that data governance, understanding data flows, and maintaining consistent security controls across all cloud environments are critical, regardless of business drivers like mergers, regulatory requirements, or ISV partnerships.

{% youtube https://www.youtube.com/watch?v=v4JwkLsjyx0 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/0.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=0)

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/10.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=10)

### Introduction: Multi-Cloud Security Challenges and Business Drivers

 Welcome to Multi-Cloud Security Best Practices.  This is Luis. I'm Geoff Sweet. I'm a Senior Security Specialist here at AWS. I've been here a little over 6 years, and one of my biggest loves in life is tacos. My name is Luis Pastor. I'm in Geoff's team as you can see. I've been here for 3.5 years, and love is never a competition, but if it was, I love tacos more than Geoff. We're taking recommendations, so at the end of this talk, if you have a good recommendation for a taco in town, let us know. We'll be over there.

Because this is a silent session, we won't get the chance to do active question and answer during the talk. However, Luis and I will be out back here afterwards, so if something comes up that you want to ask us, please come back. We'd love to chat with you.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/70.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=70)

 Our session today will cover some of the challenges that arise with multi-cloud security strategy. Specifically, we're going to talk about common challenges like how and where to federate, how to manage your data protection and privacy, how to gain visibility around what's going on in your multi-cloud architecture, what it means to scale your teams or tools to help you in that endeavor, and what are some next steps and resources available to you. I'm happy to tell you that Luis and I are not here to talk you out of multi-cloud, but we're definitely here to help.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/120.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=120)

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/130.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=130)

 Let's start with a definition. At AWS, we're defining multi-cloud as an organization that operates in two or more cloud service providers.  That means you have IT solutions or workloads in two or more cloud service providers. Is there anybody here that's already operating in this way? I know a lot of us already are there.

We really want to make sure that you have the freedom to operate wherever you can do that more efficiently. As Geoff said, we're here to help if you're interested in security. We have this talk, and there are other conversations in the session going around FinOps, observability, and developer experience. We will also be in the multi-cloud booth, so come and say hi. We'll also show you how we created the picture in the beginning if you want. No AI, only Photoshop.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/180.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=180)

 Many of you are already multi-cloud, so you already know that most organizations don't just wake up one day and decide to deploy in a different cloud. A lot of us have different business drivers that push us to go that way. We have some examples here. They're not all of the different drivers that may push you, but these are some of the ones we see constantly through our conversations with our customers.

The first one is mergers and acquisitions. This is where your organization acquires another one and then maybe operates in another cloud, so now you're a multi-cloud organization. If you operate a taco place and you buy a quesadilla place, now you're operating both, right? So you have to understand both considerations. Another driver is differentiated capabilities. Maybe you have a workload that has specific business requirements, and you're going to deploy that workload in another cloud that may better fit your needs, so you're going to become multi-cloud.

Regulatory requirements are another driver. This is more to do with security and compliance. Think about things like data sovereignty. Maybe you have to deploy in one place, a specific geolocation, and that drives your decision to go with one cloud or another one. And then of course, there are independent software vendors. Maybe you start working with an ISV, and that ISV is operating in another cloud. Maybe your share of the responsibility model and your controls that you're responsible for are lower on that use case, but you still have to understand how they're addressing your security concerns.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/290.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=290)

 I think the takeaway from this slide is that it doesn't matter what the driver is. The constant here is going to be governance, security, and compliance. That's going to have to be extended throughout your multi-cloud environment so that you have the mechanisms in place to protect the data that you're using and the systems that you're using. So we'll go through that journey now.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/310.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=310)

### Key Questions for Multi-Cloud Security Operations

Let's talk about some of the questions that you may want to ask yourself. You may already be thinking about these, but it's a good thing  to start working through some of these considerations to understand the challenges you've encountered and what might come next. We have more than this, so please talk to us after this presentation. We can have longer conversations, but one of them is going to be: how do your security operations look like now? Do you have a good understanding of your current state? Do you understand where your controls are and how they're protecting your data and systems? Do you have the capability to extend those security controls to a multi-cloud environment? Here we're talking more about people and process. Do you have the mechanisms in place to extend those processes to a multi-cloud environment? Are you ready to move to another CSP and expand your estate to multi-cloud?

Another question is: who will own your security response and monitoring capabilities? A lot of organizations have that distributed, and a lot of organizations centralize those capabilities. So what are you going to do? What's your decision going to be, and what considerations are going to drive that decision? This is true for other security capabilities, but in our opinion, it's most important for security response because timing is going to be of the essence when you're responding to an event. And then monitoring, because visibility is going to be so much more important in a multi-cloud environment. It's already important when you have one cloud, and it's going to be more important when you have different clouds. When you talk about two or three clouds, it becomes even more important.

Another question you may ask yourself is: what tools do you already have that are multi-cloud? A lot of our customers focus on tooling, and this is an important consideration. The good news here is that a lot of the tools you may already be using are already multi-cloud, so you can start extending those. If you have expertise and your teams already know how to use and deploy them, it's a lot easier to deploy them to a multi-cloud environment and get those benefits right away.

Another question is: what are the divisions for things like security operations, application security, governance, auditing, and security? All those teams are going to have to communicate and coordinate. If they're doing it in one cloud, it's going to add a lot of complexity to do it in two or three. So how do you think through how to best manage your teams and the tools they need to drive those security requirements throughout the multi-cloud estate?

Finally, and maybe most important, how do you enforce data governance? Do you know where your data lives now? Do you have good data flows and good diagramming that tells you where your data is, how it's being processed, how it's being transmitted, and where it's stored? Because it's going to get more complex in a multi-cloud environment, and you really want to understand where your data lives so that you can apply those secure mechanisms where that sensitive data is and protect it.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/510.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=510)

### Adopting a Cloud-Agnostic Security Framework: Outcomes Over Implementation

These are some of the considerations and challenges. Let's start addressing some of those. One of the things we recommend here is to start by using a multi-cloud security framework.  Right here we have the NIST Cybersecurity Framework as an example. It doesn't have to be this one. Pick whatever fits your needs, whatever you may already be using, or even an integrated framework. But the point we're driving here is to pick a high-level, cloud-agnostic, cloud-neutral framework that can drive your posture forward. Focus on those outcomes, not just recommendations. We'll talk more about what that means in a little bit.

The multi-cloud security framework is going to help you in a couple of different ways. We talked about measurability. It's going to help you understand where you're at, and it's going to use that cloud-neutral language to evaluate your entire state across the clouds to understand where you're going. When you have a good security baseline that is comprised of those cloud-agnostic controls, it gives you something you can plan for, budget for, and measure against.

The second thing it's going to provide you is the ability to communicate more effectively within the teams. You have that cloud-agnostic framework that you can use to communicate. We need this requirement or this control to be implemented throughout our state. It really doesn't matter how it's implemented. What matters is that it's implemented and we know that it's in place and it's doing it effectively. Technologies may change, but the outcome should be the same. So let's put it all together. Security doesn't care where your data is and it doesn't care where your workloads are.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/610.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=610)

I had an idea about our supersonic drone taco delivery program. My uncle Elroy runs a cloud provider. Really? Yeah, it's in his basement. Does it comply with all the security frameworks? No, then no, we cannot use that. So here we really want a cloud service provider that is adhering to a lot of the compliance frameworks.  Once you have established which requirements you would have to align to from those compliance frameworks, you can drive that operational excellence. Again, you really don't have to focus on what are the specific mechanisms in place to drive those controls, but that the controls are in place and are acting efficiently. And so it's outcomes, not implementation.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/670.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=670)

Outcomes versus implementation is something that we start to see customers  become challenged with when their growth starts to expand into other clouds. We want to have policies that define our control objectives. We want our development teams to turn around and be able to implement those effectively. One of the challenges is that if you are a company just operating on a single cloud provider or maybe you are a company migrating into cloud from on-premises, you might end up with security teams that have defined security policies that include implementation. Here's a good example of something like that. We've told our development teams for data that is classified as internal that we want you to store it on S3 with some given KMS encryption policy. For our development teams that are operating in AWS, this makes perfect sense. We can deploy it, it's easy. But what happens to our teams that need to take that security policy and go to another cloud provider like Azure or GCP? We now have created confusion, and in that confusion, you're going to begin overwhelming security teams because your development teams are going to consistently keep coming back and asking, well, wait a minute, what do you mean? Is this going to be okay?

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/700.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=700)

Instead, we want to look at security policies that still achieve our security  outcomes. Here, with just a few small tweaks to the policy, we've now said internal data when stored in cloud must be stored in an object storage with the following types of encryption. If you look back a few years ago, Steven Schmidt spoke on the top ten things to be doing in your AWS, and he talked about implicit policies, policies that are easy to follow and easy to understand. This is where we've come to. Now we have a policy that achieves our outcomes, can be applied anywhere, including on-premises, and then out of that we can begin creating implementation guidelines. In fact, those implementation guidelines might come out of those very same development teams who then say here's how we did it and we were approved by security.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/780.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=780)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/850.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/860.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=860)

### Centralized Federation: Establishing a Single Source of Truth for Identity Management

 Let's talk about common challenges. A lot of what goes into this list comes out of Luis and I and other folks   in our conversations with customers. If you have a challenge that is not on this list, I would love to talk to you after this. I'd love to hear your voice and understand what you're up against. We grouped these into the categories mapped to the CSF. So in the Identify function, we start with identity management. You've got a bunch of places where your developers have been approved to run their workloads. How do you identify who they are? How do they get there? How do they authenticate? What do your authentication and authorization mechanisms look like? Again, in the Protect category is data and data security. There's going to be a lot of data.

We need to manage it effectively. In the detect category is continuous monitoring. You now have a lot of architecture, and you need to have visibility into what's going on around it. In the respond category is incident management. We need to be able to do something when something occurs, whether it's a security event, a misconfiguration event, or simply something that needs to be cleaned up because our policies and constraints have changed.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/950.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=950)

When we look at how and where to federate, how we get our users authenticated really comes down to centralized federation.  I need to know who is who. When it comes to encryption and privacy, we're really looking at a lot of automation, and we'll talk more in depth about each of these. I need to control my data. When we talk about visibility, it's really going to become centralizing that event and log data because I need to see what's going on. With team skills and tools, this might be a little bit of a more complex conversation, but I need to look into centralized specialized tools or specialized teams because I need expert people and expert resources that can help me manage all of the above.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/1010.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=1010)

Centralized federation is a kind of single source of truth.  We'll talk about what those sources can look like, but essentially we need to make sure that all of our cloud providers and our on-premises resources are going back to a single source of truth. Automation can mean monitoring configuration and actively responding with remediation around our data, and using cloud guardrails and configuring them automatically. Centralized data is centralized logging, which is pretty straightforward. There are many different approaches to how we do that. With specialized teams and tools, we'll get into this further, but essentially you might think about specialized sub-teams as part of your cloud center of excellence, or you might be looking at tools to extend into the different cloud providers that you are operating with.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/1110.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=1110)

Let's start with identity and access management. We technically call this "who's who."  The good news here is that we are tackling a challenge that we already know and understand. Authentication is about who am Iâ€”whether that's a user, system, machine, or agent when it comes to agentic AI. Authorization is about what am I allowed to do and what permissions do I have. Accounting is about tracking what do I have access to in terms of data or resources. I know many of you said that you're already operating in multi-cloud environments. Who has already established a centralized federation here? A few of you. This is one of the first things we see some organizations do when they go multi-cloud because you want to have that control in one single place.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/1200.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=1200)

You're probably going to use things like Microsoft Active Directory, open source LDAP, Ping, or Okta to give you that one source of truth where you can control your identity. You'll have a better idea of who's accessing your systems, what those permissions are to access the systems and the data, and you can keep track in case you have to investigate or do some forensic analysis later. One of the first challenges that comes up, and one of the first ones that we see our customers tackle, is the recommended pattern for this.  In our opinion, it's going to be that single source of truth for federated access for multi-cloud resources. Have that one place where you manage all the identities and all the access in your multi-cloud state.

This is enabled by a third-party federation provider that provides the identity source you need for federation to work. Once you have this established, it gives you two very powerful benefits. First is a single point of identity and access management. Your team only has to focus on one place to understand your estate, who is who, and what access they have. Second, it enables you to put higher controls and standards in place. For example, it might be more palatable to users to deploy things like multi-factor authentication and complex passwords because it's just one place where they have to log on. Your users are going to be much happier with one place where they can log in rather than having to plug into disparate dashboards and disparate systems to go through their day-to-day operations.

It's going to be much easier for them, and they might be more accepting of tougher security controls when it comes to privileged users with higher privilege levels. The trade-offs are going to be, in my opinion, the first one is more of an investment than a trade-off. You're going to have to spend time and effort in the beginning setting everything up. It's going to be more complex the more clouds you use, and it's going to be exponential if you have operating systems, applications, and portals. The level of complexity is going to add up, but it's going to pay dividends at the end of the day. It's going to save you time from operational overhead managing all those different places where your users may live, and it's also going to be a lot easier for those users.

The last three trade-offs are going to be related. The reliance on a third party is going to create that dependency risk, so you have to have really tight break glass procedures so you can respond when there's an outage. You're going to have to be very clear on how you're going to operate if something breaks because there's that one place where everybody is logging on and where all the administration is going to happen. Again, it's going to be a lot easier to sign on and we're going to be able to avoid things like identity sprawl and operational overhead. It's going to be a lot easier to establish your identity management once you centralize.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/1400.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=1400)

### Data Protection and Privacy: DSPM, Automation, and Cloud Native Guardrails

Now that you have an understanding of who's logging on to your system, let's see how to protect your data. I think anybody that is operating on one cloud probably starts to understand that when data is all in one place, it's a lot easier and more straightforward to control that data  because you can use consistent guard rails and controls within that cloud provider. The challenge that becomes apparent in a multi-cloud environment is that data begins to sprawl. We've got source code that has been deployed, maybe it's part of a workload, maybe we're doing development in virtual instances. We have a ton of logs, and those logs might contain sensitive information with their own data classification. We've got the data that our workloads work on, and that can be anything from PII to health information like HPI.

What do we do about that data? How do we understand it? We have policies that are driving that. You really should have a data classification, and if you don't, you should really think about having one because that begins to define things like data access. Who can access that data? Through what mechanisms can they access it? How is that data encrypted? Do we need to apply a multi-level approach to our encryption, or is it something simpler?

Your kids' love letters to their girlfriendâ€”we could just ROT 13 it. Who knows what ROT 13 is? I'm dating myself here. What does the data lifecycle look like? Data gets left out all the time. We've seen a number of security events over the years that have occurred because data gets put in a place without the proper controls around it and forgotten about, only to be later discovered by someone we didn't want to discover it. This is why we need to have these controls, classifications, and access policies.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/1550.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=1550)



How do we go about doing that? It seems very daunting to think about having developer development teams ingesting data and producing data. How do we get a hold of it? The first place we want to start is data security posture management as an option. Is anybody running DSPM tooling now? A couple of people. This is a very new space. A number of really great partner companies with AWS have great product offerings around this. They're here in the village, so make sure you go check them out.

Data security posture management can really give you the tooling you need to understand how your data exists. That way you can start questioning where you need to make changes and whether your policies are actually as effective as they need to be. Cloud native guardrailsâ€”you should always question what cloud providers offer you in terms of protecting your data. Obviously, I'd love to tell you all about AWS's offerings, from encryption to tracking and finding your data. But really, you should be looking at those guardrails and leveraging them where possible.

Then question automation. How can you pull automation in to do things like monitoring data ingestion, monitoring data migration, and monitoring how data is being protected and managed? That way you can turn around and assure compliance for that data. If you're under a compliance framework that requires you to protect that data because of its sensitive nature, you're probably already operating in this space to assure that your data is configured correctly.

And remediationâ€”if you plug in something like one of our DSPM partner tools, you're probably going to find a lot of stuff that maybe needs remediation. The cool thing is, when we talk about automation nowadays, even just a year or two ago, I knew security teams that would break out in cold sweats when you told them they needed to write their own code to do some of this automation. But nowadays with agentic AI and vibe coding, this is so much more approachable by teams because this code can now get created by literally anybody that can write an effective prompt. Find your prompt engineering talk this week at re:Invent.

What are the benefits of this? DSPM is really streamlining efforts in this space, and that's great. That means it's one less thing that security teams need to worry about because tooling is emerging in this space to really get a hold of how your data is protected and what controls are in place around it. Automation adds efficiencies. The story around automation has always been the sameâ€”automation adds virtual people to your team. In one of my past roles, we were a very small security team of four or five people operating on thousands of AWS accounts.

Embracing automation is a powerful thing for any security team. There are some challenges around this, though, and you will hear some of these repeatedly throughout this talk. Cost is one of them. Tools come with a cost to purchase and might come with a cost to bring in professional services from that tooling company to help get the most out of the tool as quickly as possible. So there are budgetary questions that you need to consider. While creating automation has gotten easier, there is still complexity to it. Deploying it and making sure that your automation is reaching all the places that it needs to, all the nooks and crannies to discover data and work on any type of remediation, becomes more challenging, especially as you become more distributed.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/1910.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=1910)

### Centralized Logging and Visibility: The Foundation of Multi-Cloud Security Monitoring

Guardrails are another consideration. While the cloud native guardrails may be great, you should evaluate them against your security policies to question if there are gaps. When there are gaps, you should look at other options to fill that in, making sure that you meet the objectives of your security policies. Let's talk about centralized logging and visibility because you cannot secure what you cannot see.  This is one of those topics that sounds really easy. Just centralize your visibility and you will be fine. However, putting this in place is more complex than it sounds. The complexity adds up exponentially because we are not just talking about logs from one cloud. Adding another cloud means each cloud has its own tech stack. You will be adding logs from your operating system, from your database, from your applications. So it is an exponential level of logs that you have to ingest, consume, and analyze. Add that to the data protection mechanisms that need to be in place to protect those logs, and it starts to get complicated really quickly.

Still, our recommendation is to centralize those logs. We will talk through some of the considerations. At the high level, centralizing those logs will give you the visibility you need. One place to see everything in your multicloud environment is the way to go. This is going to be easier said than done, but if you have that visibility, you will understand your multicloud environment. You can create a security baseline throughout your multicloud environment. You will be able to derive themes and trends. You will have alerts and notifications from everywhere, and it will give you the ability to respond faster to events, understand what happened with a lot more context and nuance. Having that visibility in one place is great, but we have some tradeoffs.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/2020.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=2020)

For the recommended pattern of centralized logging,  for the enablement we have a Security Information and Event Management system. This is another place where you may be able to expand the tooling that you already have. If you already have expertise, it may accelerate the implementation in your multicloud environment. Who here is already using a SIEM for their incident response and monitoring? If you have that in place, you are already halfway there. Of course, you will have to take those considerations into account when it comes to how to implement it. Let's talk about the tradeoffs first, and we will talk about the considerations after.

The benefits that are going to be provided are real. You will have that central visibility, one place to see everything that is going on in your environment. This will give you the ability to understand your environment a lot better. You will be able to differentiate normal from suspicious or anomalous behavior. It will give you all the information you need in one place and will help correlate across clouds for faster response. If an event happens, you can respond really quickly and know if it is an incident, then try to do any remediation you need. The first tradeoff in my mind is going to be an investment. You are going multicloud anyway, so a lot of this complexity is going to be audited by default.

If you spend time and effort on the front end understanding what logs you need, where you need them, and what visibility you really need, it is not going to be about collecting everything in the same place like in one database, one lake, or one cloud. Whatever your decision is about where you put that data, whether central or diversified throughout your cloud estate, you need one place to get all that visibility. That complexity and analysis you are going to do on the front end is going to pay dividends on the back end because it is going to be a lot easier to respond once an incident is happening, and that is when time really counts.

Cost is a true consideration here because of data and tooling. As mentioned, tooling is going to be a cost consideration, so you may have to start thinking through the use cases you need so that you know what data is ingested by your tool and you are not incurring too much cost. Data is also going to be a consideration when it comes to cost. You are going to make sure that you are not duplicating data. If you are moving data around, cost may also be an issue because transferring that data around may be costly. You may want to push data through its lifecycle so that you are putting data in the least costly storage systems and you do not incur unnecessary costs.

Timing is going to be an important consideration here because you want to respond as quickly as you can. Getting those logs ingested, aggregated, and analyzed fairly quickly, almost in real time, so that you can react to them is going to be very important. Time synchronization is also critical because you may have different clouds in different time zones, so you may have different times that you have to coordinate and correlate before an incident happens. Any time queue during an investigation can throw off the entire incident response timeline, making incident response a lot harder. Those are a lot of the considerations you have to have in place for all that visibility.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/2280.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=2280)

### Scaling Teams and Tools: Specialized Expertise and Partner Solutions

Let's talk about the expert teams and tools that are going to help you once you have that visibility.  I have some unique experience in this. I came from a large company prior to coming to AWS, and we were large enough that when we started to embrace and expand into other cloud providers for a number of reasons, we were building sub teams. The benefit of the sub teams is that you are creating a team of dedicated experts around that cloud, and then that team becomes part of your cloud center of excellence that is able to provide security and architecture expertise and voice to your development teams, to your leadership, and to whoever needs that type of input.

It also means that the management of that cloud provider is pushed down into the subteam who knows it best. However, I am also keenly aware that not a lot of companies can scale in this way. This is typically more of a large enterprise type solution. I think a lot of customers are more commonly on the other side of this, which is the dedicated tooling approach where you have security teams and maybe a cloud engineering team as part of your Cloud Center of Excellence, and what you need is tooling to augment that team.

Tooling can provide a lot of the inspection, monitoring, and general well-being of your architectures and workloads in each cloud. There are a lot of tools available, and I encourage you to look for ones that are cloud neutral or tools that operate in the cloud that you are using.

You don't want to go down the path of one tool for one cloud provider and another tool for a different provider, because that doesn't really fix the problem in this particular challenge. You should also look at the tools that you already have. Customers buy a lot of tools that have features in them that don't get used simply because they don't have workloads that need them.

There are many tools for configuration and security posture management, or CSPM, and the newer CAPM tools. A lot of customers buy those for their AWS workload, but many of those tools are also ready to go for Azure and GCP. You should look at tools that can simply be extended from what you already have. Question whether you should be buying a tool, developing a tool, or looking into the open source community for tooling that can achieve what you need.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/2500.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=2500)

There are lots of partners out there that have deep experience in this space and are ready to partner with you and become an effective extension to your team.  The enablement around this is that your folks are going to need some training, or you're going to need to hire additional people. If you're operating in one cloud provider and suddenly start operating in another, you won't necessarily have even foundational expertise in that other cloud. We're going to have to train and potentially hire to scale for that.

Partner tools exist, and there are a lot of them. You should definitely be talking to them if this is a challenge you're facing. Look again at the tools that you have and whether they can be extended. Open source is a place where a lot of tools come out of the box ready to operate on multiple cloud providers because of the nature of open source. You don't want to be locked into a vendor or forced to do things one way.

The open source community, especially when you look at many of the services out there like Terraform, is really focused on not being locked into one cloud provider. Terraform wants to allow you to deploy and manage architecture on any cloud provider that you're running on. There are also cloud native services that might perfectly meet or exceed your requirements. You should always evaluate these against your security outcomes and security requirements to find out if there are gaps that need to be filled through one of these other mechanisms.

The benefits of the person scaling approach and the subteam scaling approach have a tremendous amount of value in terms of specialized cloud expertise. Your cloud center of excellence is going to benefit from this because you're going to have folks and tools attached to it that can bring expertise and additional controls or monitoring to benefit your development team. The challenges are probably pretty obvious. If you have a team and go the team route, you can get that siloed effect. If Luis is your AWS expert and his team just does AWS, that creates a silo.

If we need to migrate workloads from one cloud provider to another and require one team to work on the other platform, that transition will require significant training. Skills can become very siloed, which presents a real challenge to consider. Finding that expertise natively can be difficult. If you're looking to hire security personnel, most of us in the security space are aware that hiring security professionals is challenging right now, but asking for very specific expertise becomes even more difficult.

Complex tool deployment is another consideration. Complexity comes with the territory when we're talking about multicloud environments because we have to deploy tools across multiple cloud providers and endpoints. Management becomes critical as well. More tools and more people means more resources are required to keep everything functioning smoothly.

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/2810.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=2810)

### Resources, Frameworks, and Key Takeaways for Multi-Cloud Security Success

There's a lot of help available to you. AWS has many internal resources ready to discuss any of these challenges with you. Your account team knows how to connect you with dedicated security specialists like Luis and me who have extensive compliance, architecture, and security experience. If you're facing one of these challenges, talk to your account team and pull in these resources. We can definitely provide significant input.  We'll be at the multicloud booth on the conference floor. We'll be there tomorrow, but there are people there throughout the event. Come talk with them. You'll find security people, operations people, FinOps specialists, and professionals with various other areas of expertise.

We also want to emphasize frameworks, which is really important. You might be surprised by the number of customers Luis and I talk to who struggle to define their own security policies when excellent frameworks exist to guide you in this space. Be sure to check these out. These are very common and well-known frameworks: the Center for Internet Security, the NIST Cybersecurity Framework as we discussed earlier, and the Cloud Security Alliance, which has extensive resources available. The MITRE ATT&CK framework is very cloud agnostic and provides excellent security information. ISO has numerous frameworks, including ISO 27001, and even in the emerging AI security space, they have published new frameworks. FedRAMP and FISMA are essential if you operate in that space. You essentially cannot operate without them. PCI is also worth examining even if you're not required to be PCI compliant. They publish extensive materials, and many auditors and compliance professionals are familiar with operating around PCI standards, which provides a solid framework for much of this security work.

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/2980.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=2980)

 Security requirements don't change regardless of how excellent the cloud provider is. You need to apply your security policies, understand your security outcomes, and hold true to them. Evaluate any location where you're going to run workloads against those security policies. Start cloud agnostic and cloud neutral at a high level so that you can drive those requirements.

Remember to look at outcomes, not implementation, when it comes to the high level, and then look at the implementation after. Look at extending your tools. Most companies already own tools that can be extended into multiple cloud providers. So if you're already starting to think about this, if you've started to move that way, if you're trying to rope in some shadow ops that are going on in other cloud providers, a lot of people already have tools that can be easily deployed to begin handling that.

Finally, data matters, right? Data governance really matters. Understanding where your data lives, where it's transmitted, where it's processed is critical. Jeff talked about data protection and mechanisms, automatic mechanisms to put in place to understand where data is, but also non-technical approaches. Talk to your people, understand data flows, understand what business centers are using what data so that you can understand how to protect it more efficiently.

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/bd39479ebf255d24/3090.jpg)](https://www.youtube.com/watch?v=v4JwkLsjyx0&t=3090)

You might want to capture these with your phone. These are a couple of really good resources that you can explore from AWS's end.  The first one is the main AWS multi-cloud landing page. This has a lot of really great information and links to other resources within AWS that can help guide a lot of this. This page is rapidly growing with content as we ourselves are beginning to publish and develop helpful content for customers operating in the multi-cloud space.

The other one is the blog. Just like we have a number of other blogs, for instance the security blog that Luis and I have published on, MultiCloud has a blog that is going and content is being developed fairly quickly. So definitely check it out. From Luis and I, we super appreciate you taking the time to be here and listen to us. We would love feedback. When you evaluate this session, please let us know what you think and how we can develop this better for you. After this, we will be right back there and we are happy to talk to anybody that wants to follow up on this. Thanks very much and enjoy the rest of your week.


----

; This article is entirely auto-generated using Amazon Bedrock.
