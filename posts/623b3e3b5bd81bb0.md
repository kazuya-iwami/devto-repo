---
title: 'AWS re:Invent 2025 - AWS migration journey: 2025 itinerary for Microsoft workloads (MAM309)'
published: true
description: 'In this video, AWS solutions architects demonstrate how to migrate Microsoft-based workloads to AWS using AI-powered tools. The session covers AWS Transform''s capabilities for discovery, assessment, and migration planning, including demos of VMware migration with agentic AI, the new AWS Managed Microsoft AD Hybrid Edition for extending existing domains, SQL Server modernization to Aurora PostgreSQL with automated schema conversion and code transformation, and file server migration options using FSx and DataSync. The presenters showcase Cloud Migration Factory integration with Qiro CLI agents, demonstrate full-stack modernization combining database and .NET application transformation, and highlight migration incentives and programs available to customers.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - AWS migration journey: 2025 itinerary for Microsoft workloads (MAM309)**

> In this video, AWS solutions architects demonstrate how to migrate Microsoft-based workloads to AWS using AI-powered tools. The session covers AWS Transform's capabilities for discovery, assessment, and migration planning, including demos of VMware migration with agentic AI, the new AWS Managed Microsoft AD Hybrid Edition for extending existing domains, SQL Server modernization to Aurora PostgreSQL with automated schema conversion and code transformation, and file server migration options using FSx and DataSync. The presenters showcase Cloud Migration Factory integration with Qiro CLI agents, demonstrate full-stack modernization combining database and .NET application transformation, and highlight migration incentives and programs available to customers.

{% youtube https://www.youtube.com/watch?v=6cuUx_KfZK4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/0.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=0)

### Welcome to the Migration Session: Planning Your Digital Journey

 OK. Hello everyone, thanks for joining our session. I actually wanted to double thank you guys because let's face it, it's the third day of the summit already. It's Las Vegas. It's late evening. The happy hours have already started. We're already aware of that. I mean, as Lady Gaga says, you had millions of reasons to ditch us, but I guess you had your one good reason to be here, and that's migration, I guess.

By the show of hands, how many of you are from overseas? Do we have anybody from overseas? Welcome. I assume the rest of you are from the United States. Do we have any locals from Las Vegas? I mean this light doesn't let me see, but anyways, welcome. How many of you have planned your trips in person? You did it yourself. Really, only you? So I guess the rest of you just had the luxury of an executive assistant doing the traveling for you. Good for you. I'm really impressed.

Regardless of who did this for you, there are a few things that you needed to clear out before being here. First off, you needed to know where you're going, why you're going, when are you going, how are you going. How much money are you allowed to spend? What's your budget? Do you need any accommodations? All of those answers, regardless of whether you're planning for a fairly short trip like this or a rather longer journey, need some decent answers before we can actually have a meaningful and successful meeting.

Today I, Sassan Hajrasooliha, a senior AWS solutions architect specialized in infrastructure modernization and migration, along with my peers Mangesh Budkule and Adi Simon, are going to show you how you can get data-driven answers to the same questions when it comes to your digital journey of migrating your workloads to AWS with the focus of Microsoft-based workloads.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/140.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=140)

 OK, first we're going to talk a little bit about a generic portfolio of what the typical migration would look like. We're going to talk about discovery, then we're going to go deeper into each category of portfolio of workloads that you might be running in your environment. The demos are all blended. We're going to have an engaging session, so let's hit it.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/160.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=160)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/180.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=180)

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/190.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=190)

### Why Migrate to the Cloud: From Discovery to AWS Transform

 You guys are here because most probably your organization is one of these organizations that still have workloads that include the 70% of workloads that are still running on premises. Why do you want to move to the cloud? Again, you could have a million reasons to move out there.  The latest addition to this mix is embracing AI and ML in general and generative AI in particular.  But we know that you need answers to the questions that I brought up again, and we know that those answers are not easily gotten. The main thing is resource scarcity. That resource could be human resources. It could be budget. It could be timing. It could be the skill set. We know that you need to be able to figure those out before you can have a successful migration.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/210.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=210)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/250.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=250)

 When you look back at the journey that people have already been gone through, you see some milestones out there. First, in the late 90s and early 2000s, everybody was just running the workloads in their server room in their office. Then they decided to embrace the commercial data centers and the concept of co-location. Moving forward to 2005, it was the early days of cloud. People started to embrace more of what cloud had to offer. And we had different milestones moving forward and right now our milestone is  generative AI, moving to the cloud for AI and by AI.

Maybe in 2023 when you were thinking about generative AI with your projects, you were just thinking of having your prompt somewhere, getting some feedback, getting some recommendations from your model, and then implementing it manually. Then came 2024. Now you could get better, more contextual prompts to the AI model using systems like RAG and augmented retrievals and more streamlined experience. But still everything was manual. And now in 2025, we just want the AI to not just tell us what to do, we want it to do things for us, of course under our supervision and under our security guidelines, and that's what agentic AI means.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/300.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=300)

 And we're going to show you how we can leverage that for our migration project.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/330.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=330)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/350.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=350)

For a typical journey, one of the trivial questions that you already know the answer to is where am I coming from. You already know where you are, but in a digital journey like migration, that is actually a question that needs to be answered. What are we migrating and why do we have it? That would be the concept of discovery. Our primary go-to mechanism to  answer that question has always been, and still is, this mechanism that we call Optimization and Licensing Assessment, or OLA as the acronym goes. Through this process, AWS along with our partners would come to your environment, scan your environment with our tools, see the relationships that you have with different  vendors, and come up with recommendations including licensing recommendations from different vendors, whether it's a good choice for you to use the Bring Your Own License scenario versus going and purchasing license included. These are the answers that we will have for you if you go through the OLA. If you're interested in this topic, we have some resources that we're going to share with you towards the end of the session.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/390.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=390)

The question is, is there an alternative way of doing it? And the answer is yes. You probably have already heard about this service that we  put a lot of emphasis on, and it's called AWS Transform. It's an umbrella of services and modules that will help you at each step of your way towards your migration. It can actually help you with assessments as well. You would feed in your inventory data. Many of the common and popular formats are supported, but it's a generative AI. Most probably, if there's a format that you understand, most probably our generative AI will understand it as well. So it gets your inventory data, and based on the other supporting data that you can possibly feed into it, you will get some insight into it. Let's actually see how it works in action.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/440.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=440)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/460.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=460)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/470.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=470)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/480.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=480)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/490.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/500.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=500)

### Hands-On Discovery: Using AWS Discovery Tool and Transform for Assessment

Starting with the familiar face, your vCenter.  I have a cluster of two ESXi hosts with a bunch of virtual machines running on them. They're hosting a plethora of applications with different environments. For example, this one is a travel expense application with three tiers of  architectural elements into it. I'm going to use the AWS Discovery Tool. This is our own tool for discovery. The first time I run it, it requires a new password.  I'm going to put my password here. This is the interface for it. The first thing I'm going to do is to  give it access to my vCenter environment so that it can inventory my virtual environment. The moment I type it in, as you see at the bottom, the agent starts scanning the environment,  but I want more. I want dependency mapping data as well as SQL versions that I can later optimize, and this is why I'm going to give  credentials to the guest virtual machines so that it can go deeper and talk to the virtual machines themselves.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/510.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=510)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/530.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=530)

Again, the moment that I do that, the  database discovery and network discovery agents all kick in too, and as you see, each one of them has a different refresh cycle: every hour for VMs, every 24 hours for databases, and every 15 seconds to be able to grab that network dependency mapping data. Already we got some information here.  Let's see what we have for our inventory. This is the same environment that you saw through the vCenter. Some of them, as you see, have errors because the credentials were different, but for whatever reason, we can actually download the inventory in a zip file. If you look at the name, it says 28 days. That's how long we recommend that you run and keep running the tool.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/560.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/570.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=570)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/580.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=580)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/590.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=590)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/600.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=600)

Logging into AWS Transform. This is Identity Center. A plethora of identity  providers are supported. This is the front page for AWS Transform. As you see at the bottom, we have some customer testimonies and some what's new on those topics.  First thing we need to do is to create a workspace. A workspace is just a space that holds on to all the activities you need to do for a specific migration  project. We created our workspace. Now we go in. As you see, we have some suggested prompts on the right with the categories  of migration tasks that you can select. I can directly type in the category, the action that I want to do  to the model, and it will open up a proper job for me.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/610.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/620.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=620)

There it is suggesting I accept this suggestion for the job names and all that, and here's where the actual  action happens. Humans in the loop. Now it's asking me for the inventory data that I got from the previous step.  I'm going to upload my zip file, and I'm going to choose my destination region as the possible target for my migration and hit analysis.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/640.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/650.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=650)

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/660.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=660)

While it's processing my data,  I can see what the model is doing through the workload. It's showing me what the model is doing. We have the artifacts.  Everything that we fit into the model, as well as whatever the model generated for us, will be in the artifacts. There are some collaboration features within AWS Transform,  and of course the main panel will be the job panel. Each step that is done, there's a checkbox. The yellow means human in the loop, and this one is still working.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/670.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/680.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=680)

 While it's working, I can ask for anything, for a status object, for example, in this example.  It's giving me some examples. These are the actions that have been done. These are the left ones, and of course in the middle this is done. Let's see what this assessment has provided for us.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/690.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=690)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/710.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=710)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/720.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=720)

 The first thing is a PDF report of an executive summary, pretty much of what is our prediction of the savings we'll get on the cloud, what are an overview of our recommendations, all that in a nice human-readable format. You'll get the same information  as part of a PowerPoint presentation as well that you can use as a basis for your presentations if you're a partner to your customers or to somebody in your hierarchy.  All the data is already populated there. You can use that.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/740.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=740)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/750.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=750)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/760.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=760)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/770.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=770)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/780.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=780)

And the meat of our assessment is the Excel spreadsheet here. You would see that all those  inventory data that we fed in, now we have a line-by-line analysis of them, the server name, the storage attached to them, the specs, the recommendation  that we have for you on which instance type to move it, and of course estimate of each one line by line with the separation of licensing versus compute.  All those will remain in your workspace even when your job is done, and you can come back to it and  use these artifacts again when you're creating other jobs for the next steps of your migration, as you see here.  The dependency mapping and all that, we can use them later on for other steps of our migration. Great.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/790.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=790)

### Understanding Microsoft Workload Categories and Migration Strategies

 Now we know that we have discovered our environment, and we know that our workloads most probably fall under one of these categories with regards to specifically Microsoft-based workloads, servers and VMs, anything generic you think about your infrastructure servers as well as generic application servers. If you're a Microsoft shop, 95% of chance that you are using Active Directory as your identity and access management. SQL databases has always been the database of choice for Microsoft-based development. Same thing with .NET, but when it comes to Windows and web applications, you might already be on your pathway for modernization, maybe through some containerization features that you see out there, and the file storage.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/840.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=840)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/870.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=870)

 For those of you who are familiar with our terminology, we have this concept of 7 Rs of migration. The real ones are those that are highlighted, of course. Rehost, just lifting and shifting it to another host. Replatform, using the same technology but maybe offloading some management elements to a third-party provider, in our case AWS. And refactoring means just decomposing the application and creating a new one to be able to grab as much  of the cloud-native features as possible. Now let's dive deep into each category.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/890.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=890)

First one, servers and VMs. By the show of hands, how many of you are not using VMware? That's what I, oh, actually we have two. That's great. Okay, so for the rest of you who do, we have plenty of sessions  covering this new service that we have called Amazon Elastic VMware Service or EVS. For this session, it's just enough to say that we will give you the ESXi host in your account, and then you can use the technologies you're already familiar with to migrate your VMware virtual machines to AWS.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/920.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=920)

We're not going to go deeper than this into the generic service that we have for migration  and for the lift and shift. It has been around for a while and has been called Application Migration Service, or MGN. This is a service that installs a little agent on your source servers, then it transfers blocks of zeros and ones to your staging environment in your own account.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/960.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=960)

When you're ready for the cutover, either test cutover or a final production cutover, we will hydrate EC2 instances based on the launch templates you defined for each server. And then you'll have a replica of your on-premises servers running on AWS. There is a minor variation to that as well. In that case,  for the on-premises environment, if you're using VMware, you don't need to install agents individually on each server, but rather you rely on a central appliance to do the replication for you, and we rely on VMware's snapshot technologies to get those deltas of zeros and ones to us.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1000.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1010.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1010)

Regardless of which one you would use, MGN is just the engine. When you're doing the migration at scale, you need to be able to orchestrate the wave planning and make sure that the dependencies are met. So far these were the services that you would be using to track your  scaled migration project. But now we've taken all those experiences and put them in  AWS Transform for VMware Migration category, and our mighty Mangesh here will go deeper into that.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1060.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1060)

### AWS Transform for VMware: Agentic AI-Powered Server Migration

Thank you, Sassan. Hello, everyone, Mangesh here. So let's continue the server migration journey. We are living in an artificial intelligence era, so speed matters in how we work. Large migrations are complex, take time, are error prone, and require a lot of efforts and coordination. So raise your hand if you believe that agentic AI has the potential to transform your traditional migration processes. You're right. With that, I'm excited to talk about  our first agentic AI service, AWS Transform for VMware, which simplifies moving your servers from your environment to Amazon EC2 with minimum human oversight, using our goal-driven specialized agents, which we have built with the seventy-plus years of our experience.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1100.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1100)

This new service provides you more flexibility and AI-powered agent-driven experience throughout your migration process. So let's see how it works. Flexibility starts with your onboarding to Transform using your  own choice of identity, whether it's OKTA, Entra, or maybe AWS Identity Center. It has four key phases, starting with the discovery, where you can provide your VMware information using your choice of tools, whether it's RVTools or maybe if you're using a partner like Cloudamize or Modernize IT, or the tool Sassan already explained, AWS Transform for Discovery, which is what we recommend.

Once the Transform agent has this analysis of your environment, then that agent will group your servers into different waves and applications. Now, you can work with the agent to guide them to group them based on your business or technical constraints, like for example, application owner-based, functions, department, or maybe OS or IP subnets. Transform agent also understands your source network and it translates that into AWS-specific networking components like VPC, subnet, and more, and provides you with the CloudFormation and CDK templates which you can download, make the appropriate changes as you want, and you can do your deployment.

If you're using any third-party tools for the firewall like Cisco ACI or maybe FortiGate or Palo Alto, which is what customers normally use, or maybe NSX-driven networks, you can import that information in Transform for more context. Transform agent also gives you different design options when you deploy, whether it's a hub and spoke model or it's an isolated VPC, and now it supports multiple flexible IP address mapping along with different multiple account deployments, and that is very much required for large migrations.

Once the Transform agent has all this networking information, waves, planning, and everything, it goes to the next phase, which is the migration phase, where it will orchestrate end-to-end your migration using behind the scenes AWS MGN service, which Sassan explained. Now, Transform can migrate your servers not only from VMware, but if you have Hyper-V, KVM, or maybe your physical servers, it can do that for you.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1240.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1240)

The important thing is about human in the loop. It's very important where it creates that balance between end-to-end automation and human oversight so that you can provide the user input on every critical task throughout this migration process.  And it creates that unified web collaborative experience so that your teams are able to do streamlined communication, resolve the blockers quickly, and avoid these coordination delays which normally come across all large migrations.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1280.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1280)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1300.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1300)

### Cloud Migration Factory with AI Agents: Automating Large-Scale Migrations

Now, with that, do we have any alternative method to do the large migrations?  So as I told you, large migrations are complex and time-consuming, but also large migrations communicate with different data sources, different services, and tools.  They have a number of pre-migration tasks and post-migration tasks. These are the normal scenarios actually. So how do you overcome that? We have already one proven service called Cloud Migration Factory, which customers are using to migrate thousands of servers at scale at a time, right?

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1340.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1340)

But the question here is, can we take this to the next level, considering we are now living in an AI era? So how it works, how we work matters the most. For example, I'm saying can I use an AI agent to, or ask an AI agent to analyze my migration flowchart, which I have in my mind, right? And ask the agent to analyze this, work with the Cloud Migration Factory, and create all these pipelines and automation jobs to do the end-to-end migration. And I will be, as a human expert, making sure that the agent is doing right.  Yes, it is possible with Qiro CLI as an agent.

Now in this case, my migration agent assistant can work with Cloud Migration Factory to create all these pipelines and everything which I draw in my picture as a migration, right, which I want to do with the help of MCP server, which is a Model Context Protocol server, which has all the required tools to communicate with the Cloud Migration Factory. And Cloud Migration Factory, between, if you see, has the automation engine. That is nothing but it's already pre-built automation using MGN. So you can understand MGN service is now so critical, which Sassan explained. It's used in all our services behind the scenes.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1390.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1390)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1410.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1410)

Can you take this and enhance it more?  Yes, you can take the advantage of AWS Transform now, which I think Sassan explained, as an assessment to do the assessment for the large data centers, or you can use the wave planning and everything out of that and then import that into  Cloud Migration Factory to consider. Now, you may have a question, what about the pre-task and post-task, right? So you can take the advantage of our Amazon Bedrock Agent Core, where you can build your own migration agent to handle the tasks for you, for example, ticket creation, Jira stories, or maybe after migration DNS updates, or many more things with your email communication. All these things you can handle through our secure platform Agent Core.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1440.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1440)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1450.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1450)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1460.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1460)

So now let's see this in action. Yeah, so this is the WordPress application which we want to migrate.  Yeah, this is just servers, multiple servers. One of them is obviously the WordPress servers.  This is just on-premises to AWS. This is the Cloud Migration Factory which I already deployed to save the time, but this is a blank factory. If you see, it's zero servers, zero applications, everything.  I launched my Qiro as my AI agent, and now this is the MCP's tool, which is having all the tools for the Cloud Migration Factory, right, which it will use.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1470.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1470)

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1480.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1490.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1490)

 This is the Agent Core where I have built migration agents, like one for ticket creation, one for email communication.  And this is the flowchart I was talking about, like there are some manual tasks, automated tasks, and a simple flowchart starting with the ticket and ending with the notification, in between migration steps.  Now, this is the prompt I gave to the AI agent. Go and analyze this JPG file which I have shown you before and do the job for me. Create the pipelines, work with the Cloud Migration Factory and work with that.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1500.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1500)



[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1510.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1510)

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1520.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1530.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1530)

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1540.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1550.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1550)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1560.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1560)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1570.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1580.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1580)

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1590.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1600.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1600)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1610.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1620.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1620)

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1630.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1630)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1640.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1640)

The system is creating the entire pipeline behind the scenes with the Cloud Migration Factory using the MCP protocol. As you can see now, it is doing that.  Then it will also ask you for the inventory of your servers, which you can take from AWS Transform or your own inventory in the CSV format. You can provide it to the system. In our case, we have  three applications, five servers, and two waves created. So if you go to our Cloud Migration Factory UI,  you see that two waves, five servers, and three applications are displayed. This is the UI, but you can also ask the status from the AI agent.  What is my status of the migration? And now you can see that it will also try to create Wave 1, because that is the one I'm showing in the demo to migrate the WordPress servers,  right? This is the UI of the Cloud Migration Factory where you can control things as well, but we want to control through AI. So if you go back now, yes, you can ask what is the status of my migration.  It will go, but if you see that migration has not started, why? Because there was one manual process previously, that ticket creation. It's asking,  provide the ticket. So it understands manual versus automated processes. Now we are providing that, yes, I have a ticket. Now it's going to follow my  flowchart and do the migration for me behind the scenes using AWS Application Migration Service. So if you see now in the UI, this is installing the replication agents  behind the scenes, starting the block-level migration, and step by step it will follow all the processes, test instance creation, testing, and then cutover, and then  deploy the production instances. All of this it will do for me. For example, if you see there were two newly migrated servers there, and now I can just ask what is the status there. Everything looks good.  It's a demo, but there may be failures also. You can ask about possible issues. Now you can see all the pipeline here. Again, the main motive is that we don't want to work with  multiple UIs, Cloud Migration Factory, and then AWS Application Migration Service. I just want to check everything. You can see notifications, and I got the notification.  This is the application from the newly migrated server, and this is the overall status. So this is how fast you can work now with  AI, right? You can ask these questions, and in this case everything works fine. But in reality, maybe it fails, it will not be able to install the agents. You can work back and forth with the AI agent and ask why it failed, and you can fix it using natural language communication. That is the main intention here.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1690.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1690)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1700.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1700)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1710.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1710)

### Active Directory Migration Options and AWS Managed Microsoft AD Hybrid Edition

So again, we have covered how you can do the server migration. We have multiple ways to do that. I will suggest working backwards from your own requirements, whether they are technical or business requirements, and see what solution best works for you. For example, AWS Transform, maybe Cloud Migration Factory, or maybe our partners and services tools. Okay, so let's now change the topic to a core service, Active Directory. How many of you are using Active Directory? Almost everyone, maybe, all right.  This is the most used application across the industry and a very critical application when we are thinking about migration. So let's see what design options we have.  We have three native options on AWS. The first is self-managed Active Directory domain controllers.  When I say self-managed, it means in this model you are responsible for managing these domain controllers, whether it's managing, monitoring, or maintaining them throughout the lifecycle of the domain controllers. The key point on this is that you can extend your own domain to Amazon EC2. If you have fifty thousand or maybe one hundred thousand users, you can extend that easily with Active Directory replication and start providing authentication to your migrated applications. This is the fastest way you can start, and most customers do this.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1750.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1750)

The second option we have is AWS Managed Microsoft AD. This is managed by us. We deploy Active Directory for you,  and behind the scenes we manage the domain controllers, so you don't need to worry about the undifferentiated heavy lifting of the domain controllers. We manage that for you. And this is an enterprise-ready service, so it comes with seamless integration with AWS applications, maybe enhanced features like directory sharing or replication and everything. But the key point on this service is when you deploy the Active Directory, you deploy a single forest for a single domain, a new single forest for a single domain. That means what? You cannot extend your existing domain to the AWS Managed Microsoft AD. So if you want to provide authentication, you need to create trust relationships, like one-way or two-way trust relationships, depending upon your use case.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1800.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1800)

The third is Active Directory Connector. AD Connector is not an Active Directory. This is like a gateway, or maybe in simpler terms, it's like a bridge between your on-premises  Active Directory and AWS-specific applications which support AD Connector.

It provides authentication and lookups. This is a one-to-one relationship, so you may end up deploying multiple AD Connectors just to make sure that you use the unique service account to avoid the blast radius issues.

Now these options are used by our customers all the time across the segment, whether it's government, education, finance, life science, or healthcare. I'm supporting these customers to deploy the Active Directory from a long time, and we get the feedback. They are looking for a solution, a managed solution in AWS which is flexible and easy to maintain, and also get the managed experience like the Managed AD, but they can be able to extend their own domain to Managed AD, which is not possible right now. So that they don't require to do the trust relationships or maybe actual migration from their own domain to Managed AD, which is a big project, as you guys know already. It's so difficult to do the AD migration.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1870.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1870)

With that, as you guys know, most of our services and features come from your feedback. So I'm excited to talk about our new service,  AWS Managed Microsoft AD Hybrid Edition. Now, in this service, you can extend your existing domain to Managed Microsoft AD Hybrid Edition, so that you can take the advantage of managed experience because we manage the domain controllers for you. And same thing like the EC2, you can quickly start giving the authentication to your migrated applications.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1900.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1900)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1910.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1910)

How you design this, if you have domain controllers on-premises, you can just  extend that as additional domain controllers on Managed Microsoft AD just to make sure it is onboarded with the Systems Manager, which is one of the prerequisites. If you have EC2 domain controllers, you  just extend that now to Hybrid Edition. So, but in this case, Hybrid Edition domain controllers, which is your domain only, is managed by us, but EC2 and on-premises is managed by you. So it's a co-management now between us.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1930.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1930)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1940.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1940)

So let's see this in action, how this works actually, this new service. So if you see now,  these are the two domain controllers I already migrated from my own premises. This is onboarded with the Systems Manager. This is the important thing to notice, the OU structure.  You can see this is your OU structure, right? And these are self-managed domain controllers by you. This is the secret we need to create because we need some access to your domain to add the additional domain controllers, so we created the secret. And these are very specific name secrets. This is documented in our website. These need to be the same names.

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1960.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1960)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1980.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1980)

And once  your secrets are there, your domain controllers are onboarded with the Systems Manager, now you see the new option here in the Managed Hybrid Edition along with the normal Managed AD. Provide all the domain information, DNS, networking is very important behind the scene. Provide all that.  Now you can see the two domain controllers which you have provided, which is minimum requirement. Minimum two, you need to provide to use that for the additional domain controller deployment.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/1990.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=1990)

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2000.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2000)

And everything looks good, but you cannot install,  we are not installing Hybrid Edition directly. We are creating the assessment. Why? Because we need to make sure that whatever domain controllers you have provided to us is healthy,  working fine. So behind the scene, we are going to check, 58 tests we are going to run, which is very simple, AD specific tests like resolution, replication to make sure they are healthy. And this assessment, we are going to use successful as another prerequisite to create the hybrid domain controllers.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2020.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2020)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2030.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2030)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2040.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2040)

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2050.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2050)

And now you have to provide the service  account. You can see the service account which we have created, and then once everything looks good, all the information is good, now you can say that  create the hybrid directory. So, behind the scenes, what we are going to do is now, we are going to deploy the domain controllers for  you in our account, obviously, and just going to be managed by us. And you can see now IP hyphen. So the IP hyphen is domain controller deployed by us for your  domain, but managed, and the OU structure. You can see now new OUs are added, AWS Reserved. So that is what exactly managed by us. All other is your domain anyway.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2060.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2060)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2070.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2080.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2080)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2090.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2090)

And  two domain controllers by yourself and total, if you see now, there are four domain controllers. So we, you can see the two are  managed by you and two are managed by us under the co-management with this. And if you go to the actual directory and see that,  you know, it all looks like successful, everything, and the assessment is also successful. If you click now the domain, you will see this is a managed kind of  experience where you can see the application management and all that. So you get that managed experience now. With that, I will hand over to Adi for the next journey.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2110.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2110)

### SQL Server Migration: From Rehost to Modernization Approaches

Thank you. Now, as part of customers'  migration journey, the next workload that is most commonly talked about is database and application. Can I get a raise of hand if you run Microsoft SQL Server today? Okay, great. Most of you. All right, so my name is Adi. I'm going to talk to you about your database layer and application layer.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2130.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2130)

Let's get started with SQL Server.  Now, let's try to understand what are your options to run SQL Server on AWS. The first option is rehost onto SQL Server on EC2. Now this gives you the most flexibility in terms of SQL Server editions, SQL Server versions, and you have a very familiar admin experience and full control of your SQL Server. Now you can choose license included or BYOL if you meet the eligibility requirements, but full control comes with full responsibility. That means you are responsible for managing everything on EC2, except for the physical infrastructure.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2180.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2180)

So, what if you're thinking, is there a way to offload this operational overhead? The answer is yes, you can replatform to our managed database solution, which is Amazon RDS for SQL Server.  With this, you get the managed experience, we optimize the architecture for you. AWS automates the patching backup. You also get the high availability if you opt for Multi-AZ and license is also included in this option.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2200.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2200)

We also have a third option called RDS Custom for SQL Server.  This is where if you have a use case where you need to access the operating system to install things like third-party agents such as your security agents, your monitoring, log forwarder. This can be done with RDS Custom. Now that you know what is your destination when it comes to SQL Server on AWS, let's talk about the approaches for you to get there.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2230.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2230)

What are the migration approaches? I'm going to present to you four very common approaches, and the first one is to use AWS Application  Migration Service, which Sassan and Mangesh have already talked about, to do a full lift and shift from your SQL Server, be it on-premises or on other clouds, onto SQL Server on EC2. This does block level replication, so that means everything, including your database, operating system, data, configurations, they all come along onto EC2. Very simple, good for lift and shift, good for individual database.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2260.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2260)

The next approach is going to be very familiar for all of you database administrators out there,  native backup and restore. So with this approach, what you do is you back up your SQL Server as usual, then you upload the backup file onto our storage services such as Amazon S3 or FSx, then you restore them onto either EC2, RDS or RDS Custom. But one caveat of this is that you've got to take into account downtime because backup and restore undoubtedly involves some downtime.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2300.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2300)

So what if you want an approach that minimizes the downtime? Well, if you run a clustered environment today and you're taking advantage  of built-in high availability features, such as Always On Availability Group, Failover Cluster Instance, log shipping, what you can do is you can spin up an EC2 instance with SQL Server on it, and then add that as an additional node in your existing cluster. Let the nodes synchronize, all of your data comes along, and once you are ready, you perform a controlled failover on your source database. Finally, when you're ready, you can decommission your source database. So this is great if you run a clustered environment, but this only works if you're coming onto SQL Server on EC2. But the upside is this can also double up as your disaster recovery approach or solution.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2350.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2350)

Now, finally, the one that gives you most flexibility is DMS. Database Migration Service allows you to achieve continuous data replication,  CDC change data capture from your source database to any of the options that I've just talked about. So you can come onto EC2, RDS, RDS Custom with minimal downtime. So this is great for highly available applications.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2370.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2370)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2390.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2390)

But when it comes to database migration, it goes hand in hand with modernization.  If you run SQL Server today, I would guess that you would run it on Microsoft's Windows Server. Is there a way to save on licensing costs, at least on the Microsoft Windows Server side? Yes, there is. With Replatforming Assistant, this is a scripting tool. It allows you to  export your SQL Server on Windows Server and restore it onto EC2 Linux, so you save on Windows licensing costs. But SQL Server itself is still quite expensive in terms of license.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2420.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2420)

So what if you're thinking about moving the database and modernizing it to open source alternatives such as MySQL or more commonly PostgreSQL, such as on Aurora, you can use Database Migration Service to migrate the data. But before you do that, you need to convert the schema, and we have a tool for that as well. As part of DMS, we have Schema Conversion  Tool that allows you to convert your SQL Server schema onto the PostgreSQL equivalent or any database of your choice. But up until now, you have to convert this manually using SCT, then orchestrate DMS manually, and what about your application code?

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2450.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2450)

### AWS Transform for SQL Server Modernization: Full-Stack Database and Application Transformation

Now modernizing database without touching the application is considered a breaking change for your application, right? So you're going to make sure that you tweak your application to make it work with PostgreSQL. Up until now, you have to orchestrate all this  manually. But with that, I'd like to introduce to you AWS Transform for SQL Server Modernization that allows you to use the generative AI to orchestrate and streamline all of these proven migration and modernization tools to modernize your database and application layer from Microsoft SQL Server to Aurora PostgreSQL.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2470.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2480.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2480)

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2490.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2490)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2500.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2500)

Okay,  let's see this in action. So as you saw from Sassan's demo, the entry point to AWS Transform is to create a workspace, right? And you can create jobs within the workspace.  The job that we are interested in is under this category called Windows Modernization, and we're going to need to select the SQL Server Modernization option to create  this job. Once the job is created, you can see that on the left-hand side, you really got a guided experience all the way from prerequisites to the actual wavefront.  This is an interactive experience. You can converse with the agent, and if you look at the prerequisites, it gives you clear guidelines.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2510.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2520.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2520)

Now for AWS Transform to work with your  database, it's going to need to access your database. It's going to need a user for it to use to log in. So this is our source database. We have a few tables and we have a user here.  This user is going to be stored in Secrets Manager, and we're going to tag it in a very specific way so AWS Transform can work with it. Within our database, we have a few tables, as I mentioned, and if I do a select statement here, you can see that we have some data in our table.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2540.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2550.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2560.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2560)

On the application side, this is an Entity Framework SQL Server provider, and this is a web application that resembles  a bookstore. Okay, now back in AWS Transform. In the prerequisites, it tells you how you should tag your secrets  that contains your database credentials. So make sure that you tag it exactly. This is documented on our website. And as for the code, make sure that you have got Code Connections ready,  set up onto your repository. This can be GitHub, GitLab, Bitbucket, or Azure DevOps.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2570.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2570)

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2580.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2590.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2590)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2600.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2600)

Now establish the connection to your database secret and the code connection from AWS  Transform. Once you approve that, it will make sure to validate that all of the access is okay, IAM roles are okay, and that it will move on to discover your resource.  As you can see, it has successfully identified that there is one database through the secret that it can access, and it has also accessed and  found successfully the repositories that is associated with this database. Okay, now after it's done, then it's going to move on to the assessment phase where it's going to look at the complexity  of your database and application.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2610.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2620.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2620)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2630.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2630)

So it's going to look at all of your database objects, determine its complexity, look at how your application accesses the database, making sure that  it's able to do the wave planning which comes next. After the assessment is finished, you get a downloadable report. One of the aspects that you can download is the CSV file  that contains a summary of what you have in your database. A well-formatted PDF report is presented to you as well. On the application side, you see your repositories,  complexity rating, and high-level lines of code count.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2640.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2650.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2660.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2660)

Once that is okay, you will move on to the wave planning phase.  I've only got one application and repository here, but if you've got multiple, the wave planning means that you will actually do application and database grouping. So make sure,  human in the loop, make sure that you review this before you hit okay, because once you do so, it will move on to the actual wavefront. The first task in the wavefront is schema conversion, and you  can get Transform to create a new database for you, or you can select an existing database as target as we do so here.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2670.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2670)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2680.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2680)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2690.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2700.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2700)

 Once you confirm that this is the database that you want to write to, hit okay, and under the hood, this will orchestrate Schema Conversion Tool to convert your schema from SQL Server  to Aurora PostgreSQL. Now if you head over to pgSQL on the PostgreSQL side, we now have our bookstore schema, and if we expand that, all of the tables that you saw on the SQL Server side  are now present here. But if I do a select statement, there is no data in these tables yet, which is natural because that comes next.  Data migration, second step, and you can also ask AWS Transform to orchestrate data migration for you.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2710.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2710)

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2720.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2720)

What they will do is under the hood, instead of you manually having to create tasks in DMS,  it will create and configure the DMS task for you to do the data migration. Once the data migration is ready, and if I select  this table again, I will now see all of the data that looks exactly the same as the data that we have in the SQL Server site.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2730.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2730)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2740.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2740)

What's next?  Code, right? So it's going to need to look into your code, it's going to scan your code and do actual changes to your code. It's going to  write it into this new branch that you can specify instead of overwriting your existing code. So it's going to change the Entity Framework classes to make sure that it works with the new database. If you have embedded SQL code in your application, it will change that as well. It will swap out the connection string to make it a PostgreSQL connection string. It will also validate the application. It will also resolve any build errors. If you have unit tests, it will run the unit test for you.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2770.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2780.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2780)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2790.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2790)

Once that is done, as I mentioned earlier, it will write the results into this new target  branch and you get a nicely downloadable report. This is quite frankly a detailed report. You can go project by project.  You can even drill down to individual file changes within each project as part of this report.  Now, let us switch over to our code repository. You can see that we now have this new branch created by AWS Transform.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2800.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2810.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2810)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2820.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2820)

If you take a peek at the code changes that it has done, we can see that  it has made all of the necessary class mapping from the Entity Framework side. If you scroll down, you can see that it has actually swapped  out the SQL Server provider with a PostgreSQL provider. And it has also swapped out the connection string and links the connection string. So if you externalize your connection  string, which is best practice, it will find those and it will swap out the secret for you as well.

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2830.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2830)

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2840.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2850.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2850)

On the application side, before I show you the application, I'm going to make a tiny change to the data on the PostgreSQL site  to say that this book title now says 1002, instead of 1001 that you will see on the SQL Server site to prove that the application is indeed talking to PostgreSQL,  right? So if I start this application now and browse through the book, it should now say 1002  instead of 1001. So this application is definitely talking to PostgreSQL now.

### .NET Application Modernization: Containerization and Cloud-Native Transformation

Databases don't exist in isolation. They go along with your application. So let's talk a little bit about your application. If you're a Microsoft shop, you might be running some legacy .NET Framework application with a mix of more modern cross-platform .NET applications. Most likely you run this in Windows Server, maybe some Linux server, and you might have already started your journey towards containerization. You might have already started using container technologies.

So what are your options to run this on AWS? If you need full OS level access, you can run them on EC2 or Elastic Beanstalk. If you need automated patching, automated scaling, if you're on VMware, you can relocate your entire estate to EVS like what Sassan has already touched on. Now, if you want to run containers, we have a lot of options. ECS, which is quite simple but powerful orchestration platform. EKS if you want to run Kubernetes. If you run your containers on Red Hat OpenShift, you can continue to run it on AWS on ROSA, Red Hat OpenShift on AWS. If you're on VMware Tanzu, you can run VMware Tanzu on Amazon EVS. Or you can also refactor to serverless event-driven functions such as Lambda.

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2940.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2940)

So how do you get from the left to the right? Of course, MGN, but it's less common when it comes to application. If you have access to your source code, you are most probably going to use your CI/CD pipeline to deploy to these environments, and CodePipeline and CodeDeploy can facilitate this for you.  Now what if you want to use this opportunity to modernize as you migrate? Let's say that you want to containerize, replatform running on containers, or you want to refactor, rearchitect to embrace the more cloud native architecture.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2970.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2970)

This is what traditionally gets quite complex, because you have to manually orchestrate each one of these tools that solve one piece of the puzzle. Porting Assistant for .NET for porting, App2Container for containerization. If you want to break down microservices, you want to use another tool. So now we have AWS  Transform for .NET that is really designed for transforming .NET Framework application and making .NET Linux ready. So this in combination with AWS Transform for SQL Server Modernization, gives you a full stack modernization capability that is assisted by generative AI.

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/2990.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=2990)

 We also have Q Developer and Quiro, which is our coding assistant with agentic AI capability. Q Developer is available as an extension in most of the popular IDE, and Quiro is a full-featured IDE with advanced agentic capabilities such as pattern development and agent hooks.

So with that, I will hand it back to Sassan to talk to you about the next workload.

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3020.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3020)

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3030.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3030)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3050.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3060.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3060)

### File Server Migration Options and Key Takeaways: Resources for Your Journey

Thank you, Adi. Well, the last but not the least important one, the file  servers. What are we going to do with them? I mean, this section will be more or less a review of the options you already have. First off, self-manage. Bring up your EC2.  Make sure that you have proper tiers of EBS and the size of the EC2 that will support your throughput and transfer rates and the IOPS, and you'll be good. If you want to offload some of its management tasks to us, you would go to the FSx system.  If all you need is SMB with Windows under the hood, then FSx for Windows File Server is the service to go. And if you need to go beyond that,  meaning NFS protocol, iSCSI, tier storage and controls as such, then it would be Amazon FSx for NetApp ONTAP, the service that you'll be looking for.

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3090.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3090)

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3120.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3120)

How you can migrate to them, you already have quite a few options. We talked deeply about MGN for the lift and shift. Depending on the target that you want to use, you might actually be able to leverage some of the native services in Windows environment, for example,  DFSR. It has some integration capabilities with our FSx for Windows system. Or if you want to go to EC2, your other option could be the Storage Replica component of Windows servers. I mean, if it's actually a very small one, you can simply use Robocopy or use a third-party system and just do a manual copy. If it's too big to do it over the network, ship it to us from Snow Cone all the way to Snowmobile.  We ship you the device, a secure device. You load the data, ship it back securely to us, and then we will put it on Amazon S3 for you. And from there you can decide what to do next with your data.

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3140.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3140)

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3170.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3170)

And of course, the most straightforward service that we have for grabbing all this data is AWS DataSync.  Depending on what your source environment is, what access protocol it supports, SMB, NFS, or S3-compatible APIs, we would get those objects and we will sync it to our backend. And then depending on the target storage type that you want your data to be, we will just put it there for you. Really quickly before we go to the summary, we just wanted to make sure that  you know there are some programs and incentives available to you. Discuss these with your account managers to see if there are ones that you're eligible for, but they are there and we want you to be able to leverage them for your migration projects.

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3190.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3190)

[![Thumbnail 3210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3210.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3210)

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3220.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3220)

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3230.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3230)

Overall, our whole purpose for this session was to make sure that your takeaway is that your  questions might remain the same, but now, given the new tools and generative AI-based tools that we have, we have answers for you. And we're by your side as a team of solutions architects, account managers, CSMs, and the wide network of partners that we have. We're here to help you  make your migration journey easier. Specifically, you saw how you can now offload some of your management to us with the hybrid Active Directory service that we have out  there. You already saw a wide demo of the wide capabilities that AWS Transform provides for you all the way along.  And of course, we just talked about the programs and incentives.

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3240.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3240)

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/623b3e3b5bd81bb0/3290.jpg)](https://www.youtube.com/watch?v=6cuUx_KfZK4&t=3290)

If there's only one slide that we want you to take away  from our session, it would be this one. Here is an exhaustive and extensive list of resources that will go deeper into every individual subsection that we talked about today. We didn't dive deep into VMware or MGN. For those topics, we have some resources. Some of them are from this re:Invent, and right now they point you to the catalog for you to attend. And later on, when we have the sessions recorded on YouTube, we will update this page to have the YouTube recording of all these footages along with this one, of course. And you'll see some blogs, some technical notes, some training materials through our Skill Builder. So all of those we've combined here, and we want you to bookmark and go deeper into them if you're interested.  And with that, I wanted to thank you again.


----

; This article is entirely auto-generated using Amazon Bedrock.
