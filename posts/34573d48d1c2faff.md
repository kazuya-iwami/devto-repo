---
title: 'AWS re:Invent 2025 - Hassle-free multicloud connectivity with AWS Interconnect - Multicloud (NET205)'
published: true
description: 'In this video, AWS and Google Cloud announce AWS Interconnect, a new service enabling private, encrypted connectivity between the two clouds. The presenters explain how customers previously struggled with multi-cloud networking through VPNs, colocation facilities, or third-party fabrics, facing scalability, management overhead, and support challenges. AWS Interconnect solves this by providing built-in resiliency across multiple routers and facilities, MACsec encryption, dynamic scaling, and simplified management without BGP configuration. The service uses an open-source specification under Apache 2 license, allowing other cloud providers to adopt it. Available in preview across five regions with 1 Gbps connections at no charge, it integrates with AWS Direct Connect Gateway, Transit Gateway, Cloud WAN, and Google Cloud Router. The session includes detailed routing architectures for single-region, multi-region, and non-overlapping region scenarios, demonstrating automatic route propagation and traffic engineering capabilities. Azure support is planned for 2026.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Hassle-free multicloud connectivity with AWS Interconnect - Multicloud (NET205)**

> In this video, AWS and Google Cloud announce AWS Interconnect, a new service enabling private, encrypted connectivity between the two clouds. The presenters explain how customers previously struggled with multi-cloud networking through VPNs, colocation facilities, or third-party fabrics, facing scalability, management overhead, and support challenges. AWS Interconnect solves this by providing built-in resiliency across multiple routers and facilities, MACsec encryption, dynamic scaling, and simplified management without BGP configuration. The service uses an open-source specification under Apache 2 license, allowing other cloud providers to adopt it. Available in preview across five regions with 1 Gbps connections at no charge, it integrates with AWS Direct Connect Gateway, Transit Gateway, Cloud WAN, and Google Cloud Router. The session includes detailed routing architectures for single-region, multi-region, and non-overlapping region scenarios, demonstrating automatic route propagation and traffic engineering capabilities. Azure support is planned for 2026.

{% youtube https://www.youtube.com/watch?v=yfxS9Lizu5M %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/0.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=0)

### Welcome to AWS re:Invent 2025: Introducing the AWS Interconnect Session

 Line up for the session. Hello everyone and welcome to re:Invent 2025 day one plus two, right, because there's no day three. I'm Alex. I'm a Networking Specialist Solutions Architect with AWS, and I'm joined today on stage by Santiago. You want to introduce yourself?

Sure, my name is Santiago Riesco. I'm a Senior Product Manager for AWS Direct Connect and now AWS Interconnect. Awesome. And by Judy. I'm Judy. I'm a Senior Product Manager at Google Cloud, and I work on the Cloud Interconnect product that has been named that before Santiago chose this name.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/60.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=60)

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/70.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=70)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/80.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=80)

Cool. So this is a first, and we're going to have many of those as we do the presentation, yeah, for sure. So yeah, we're going to be talking about AWS Interconnect, a service that we launched together with Google Cloud in preview on Sunday. We're going to dive deeper into  the architectures and how the service works, but before we begin,  this is a 200 level session. However, at some point, you'll see that we're going to dive into level  400 routing. So for those of you here who are familiar with BGP and routing protocols, that's going to be fun.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/100.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=100)

We'll let you know when buildouts are  complete for photos, so we'll pause for a second on a slide if you folks want to take photos, but just keep in mind that the session is recorded. It is going to be on YouTube, so slides are going to be published, so no worries about taking photos of every single slide. And last but not least, this is a breakout session, so we won't have Q&A during the session, but we can meet you folks outside. We can have Q&A there and we can dive deeper with you all about all topics.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/110.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=110)

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/120.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=120)

### AWS Networking Foundations: VPCs, Transit Gateway, and Cloud WAN

 Now, first and foremost, before we dive deeper into AWS Interconnect and how it's built and what it's useful for, let's set up the baseline for AWS networking foundations.  First and foremost, you have the Amazon VPC, which is a regional construct, unlike Google Cloud VPC, which is a global construct, so keep that in mind. This is the network level boundary where you deploy your workloads. VPCs are account level constructs.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/140.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=140)

Within your VPCs you can create subnets in Availability Zones.  These subnets can be IPv4, can be IPv6, can be dual stack. Subnets are bound to Availability Zones, right, and subnet CIDRs are from the Amazon VPC prefixes in IPv4 and IPv6, right? So that's what you use to create these subnets.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/160.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=160)

 Now to connect many VPCs in a region you can use VPC peering at a smaller scale, right, which is direct VPC to VPC communication at layer 3. You can also use AWS Transit Gateway, which is the AWS routing hub in a region. Now I'm sure that some of you are familiar with these constructs, some of you are not. For those of you who are hearing about all of these for the first time, I would highly recommend diving a bit deeper after this session into all of these. So Transit Gateway, regional routing hub, right.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/200.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=200)

Now if you want to connect globally on AWS and in your hybrid network, we have what we call AWS Cloud WAN which  is a fully managed global networking service. Cloud WAN comes with the concept of segments. You see there as an example, production, development. These are global spanning network layer 3 segmentation constructs. They're very similar to your layer 3 VPNs and MPLS, right? They're fully managed by AWS Cloud WAN.

Cloud WAN under the hood in every single region maintains a Core Network Edge which is actually very similar to a Transit Gateway. It just has more capabilities. It's smarter and knows BGP. BGP, by the way, Border Gateway Protocol, for those of you who are not familiar with the acronym. Now Cloud WAN supports many attachment types, right? These things that you can connect to Cloud WAN, VPCs on one hand. You can have Connect attachment types to integrate your SD-WAN appliances, and you can actually attach Transit Gateways in region to facilitate integrating existing environments and Transit Gateway with Cloud WAN, right.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/270.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/280.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=280)

### Hybrid Connectivity Options: Site-to-Site VPN and AWS Direct Connect

But not all workloads live in  AWS and not all workloads live in the cloud, so we have hybrid connectivity options for  integrating your hybrid workloads. The first one being AWS Site-to-Site VPN. You can terminate the Site-to-Site VPN connections directly on your VPCs through a construct that's called Virtual Private Gateway, right? If you attach a VPN to it, many of you call it VPN Gateway. It's all good, same thing. We will meet again with the Virtual Private Gateway throughout the session, so keep it in mind. And if you are using regional or global routing constructs like Transit Gateway or Cloud WAN, you would be attaching these VPN connections to these constructs.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/320.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=320)

Now, a bit more on the VPN side. Site-to-site VPN supports  IPsec, so essentially it's a well-known standard protocol. We've recently launched large bandwidth tunnels in site-to-site VPN, so you can have now up to 5 gigabits per second of throughput. If you attach your VPNs to AWS Transit Gateway or Cloud WAN, you can actually use equal cost multi-path, right? So you can have many VPNs and you can use all of them to split your traffic.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/350.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=350)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/390.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=390)

 Now, another very well-known hybrid connectivity option is Direct Connect, AWS Direct Connect, which allows you to connect to your on-premises workload. These are dedicated connections that you build with the help of either your routers or partner routers in Direct Connect facilities. What you see here on the slide is this line between the Direct Connect router and the customer router, which could be a customer router or could be a partner router. That is the Direct Connect connection. It's a physical connection dedicated to you. Now, on top of this Direct Connect connection, you have to have that layer 3  connectivity, right, so routing BGP sessions and so on to help facilitate that.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/430.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=430)

I mentioned we're going to meet again with the virtual private gateway, so you see it up here. But because connectivity is in nature not single region usually, so customers come to AWS and span across multiple regions, we have the AWS Direct Connect Gateway, which is a global construct. It's a highly available construct, as available as the virtual interfaces that you apply or attach to it. And talking about virtual interfaces, these are what you get to attach to either your  virtual private gateway, and that is the private virtual interface to your VPCs directly, or to your transit gateways or Cloud WAN through a Direct Connect Gateway.

What is the role of this Direct Connect Gateway here? Well, it allows you to attach either virtual private gateways or transit gateways or core network edges in Cloud WAN across many regions, right, because it's a global construct. Keep that in mind because that's going to be the answer to your questions which you probably have asked yourself for the past couple of days of how do I achieve global connectivity with this thing that's now available. So I mentioned private virtual interfaces. You also have transit virtual interfaces that connect to the Direct Connect Gateway that on the other side has a Transit Gateway or Cloud WAN, or public virtual interfaces that allow you to connect directly to AWS public endpoints.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/490.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=490)

Now we've  talked about Direct Connect, a couple of more in-depth details about it, because it is going to be the foundation based on which AWS Interconnect is built. You can have dedicated connections or hosted connections in Direct Connect for virtual interfaces, and you can create highly resilient architectures, either three nines or four nines of availability depending on your workload types.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/520.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/530.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=530)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/550.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=550)

### The Multicloud Connectivity Challenge: Current Solutions and Their Limitations

Now we are talking multicloud connectivity, right? So  let's talk a bit about the background and  how customers, how you all used to build that multicloud connectivity so far. So the background is the customers want this, right? You want this ability to connect privately, securely your workloads deployed in AWS with your workloads deployed in other clouds, and that needs to be scalable, right? Let's see how you used to do this first using site-to-site VPN, right? We've talked about this. We've seen how it works. Now,  that site-to-site VPN, the customer gateway side of it, instead of living in your data center being a router, a physical router, could be on the other side, the other cloud service provider service, right? So you can have this natively integrated.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/570.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=570)

There was a nice comment  that came as we launched AWS Interconnect that somehow thousands of VPN connections were a disturbance in the force. So I don't know if that person who made the comment is in the room today, but I really loved it, and yes, they're pretty challenging in maintaining and making sure they're highly available and resilient.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/590.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=590)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/600.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=600)

The next option is routing through a colocation  facility using Direct Connect and the other cloud service provider analogous of that, right? So you would have a  point of presence where you aggregate Direct Connect. You aggregate whatever the other cloud service provider supports, and you essentially do layer 3 routing.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/620.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/630.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=630)

by hairpinning through either your router or a partner router, right? Pretty straightforward. How many of you folks are using this? Okay, cool. It comes with its own challenges, right? We're going to talk about the challenges  in a second. So one of the most important parts here is that you do have to have a cloud service  provider alignment, right, so they need to come in that same point of presence, or you need to have a backbone to route between whatever you aggregate, AWS to whatever you aggregate other cloud service providers.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/650.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=650)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/670.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=670)

Now the third option is using a third party fabric through Direct  Connect, so using a partner that provides you with a fabric that gives you layer 3 connectivity and you don't have to worry about maintaining your own backbone. Pretty standard option. Now last but not least, I wanted to bring up the overlay solution because I've heard this from a lot of you in the past days.  It's pretty common for customers to build overlay solutions on top of a layer 3 connectivity option, so it's not in the same bucket as the other three, right? It's an overlay that runs on top of whatever layer 3 connectivity exists there. And you can do this with third party partners that provide appliance based solutions. Reasons for doing this are not in the scope of this talk.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/700.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=700)

But we know for a fact that there are challenges, right?  And this is what we started working backwards from. We have scalability, management overhead, additional points of failure that you have in the path that you need to monitor and manage, support and troubleshooting. Who's running multi-cloud networks here knows that if A in one cloud talks to B in a different cloud and all of a sudden that flow doesn't work, usually the network team is called and it's like, oh it's the network, and then you start troubleshooting and trying to figure out where on that path is the problem and you start opening support cases to all the cloud providers including to the third party that you're using to connect them in between.

And consistent global coverage. You may be using partners, you may be using your own backbone, and you have to always make sure you have that consistent presence in the regions that you want to expand to or connect. So I think it's pretty clear what the challenges are, right Santiago, but how did we solve for this? Yeah, very good question. Thank you, Alex.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/770.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=770)

### Introducing AWS Interconnect Multicloud: A New Vision for Cross-Cloud Connectivity

So, as Alex mentioned, multi-cloud networking, well, it's hard.  On the previous slide we saw some of the challenges. The biggest one is actually geography for folks who are in multiple regions, multiple geographies. Sometimes you have a great setup with a partner in EMEA and a great partner in APAC. Now you need to even get those two to talk to each other. It's very hard. So, all these problems were sort of pointed out to us through many, many customer conversations, so we work backwards in AWS so that's our problem, we need to solve that, right? So how did we do that?

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/810.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=810)

So we had a vision. The vision was, this shouldn't be hard. It is very hard, but it shouldn't.  This architecture you see in the slide, that's the ideal. It should be an attachment between two things. Folks here doing like cross-region peering, you're probably familiar with this. We don't ask you to reason about routers, but I can promise you there could be 100 routers between two regions, but we don't ask you to reason about them. We just, you know, say VPC attached to the other VPC and you're done. So, why not, right? So, that's our North Star.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/840.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=840)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/870.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=870)

Now,  Alex walked you through a lot of sort of reference materials and foundational materials, because we want you to understand where we're coming from, right? So you understand our thought process here. So, remember this architecture. Your goal is to somehow communicate our cloud to another cloud, right? So, let's say you're using Direct Connect, you need the assured resiliency, the bandwidth, et cetera. So, okay, you have 2 locations, right?  This only gets you, well, 3 nines, but you still need to manage routers.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/880.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=880)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/890.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=890)

Okay, actually you need 4 nines, well, you need to do that twice, right? And you need 2 locations with,  we use different vendors, so you need those relationships, right? And those of you folks who are running a global network, well, you get the point. You need to do this  globally now, so, it's hard. Now, that's our vision, that's our problem statement. So we're very, very happy to introduce AWS Interconnect Multicloud. And we're also extremely happy

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/910.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=910)

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/920.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/930.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=930)

to launch this in partnership with Google,  and we're going to talk a little bit about how that played out in a moment. Now, this vision is where we landed.  AWS giving you an attachment to Google Cloud, and that's it. No. There we go.  On the AWS side, our attachment object is the Direct Connect Gateway, and this has a lot of benefits. It allows you to immediately have compatible networking with all AWS networking services, so it doesn't matter if you're using Virtual Private Gateways, Transit Gateway, or Cloud WAN.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/960.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=960)

On the Google Cloud side, the attach point is the Cloud Router. And this is all you see, that's it. You're done. Behind the scenes though, there's quite a bit going on. So,  the way we architected the specification for this is you are going to be having multiple logical redundant connections across at least two physical facilities across multiple routers. So, we'll talk about that in a moment.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/980.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=980)

### How AWS Interconnect Works: Simplified Creation and Activation Process

So how it works.  I will need to ask some of your patience here. This is going to be a little bit unusual, but I'm going to explain to you how it works from the customer side, then I'm going to show you how it works under the hood, because my goal is to give you confidence on why this is a product for you, but also so that you don't need to think again about this. That's the whole point. You don't need to think about routers or BGP or IP addresses, nothing. So, we're going to talk to you sort of once and then you can forget about it.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1020.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1030.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1040.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1050.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1050)

So, how it works. You as a customer  can request to AWS or Google Cloud. It's fully symmetrical, so you can use whatever console you want. You can request a new interconnect. So we receive your request  and then pass it on to Google and say, this customer needs ten gigabits per second in North Virginia. So then they ask you to confirm that request.  So we have two main actions, create and accept. Those are your flows. You create on one cloud, the cloud of your choice, and you accept on the other one.  And then once you accept, basically we start talking to each other and leave you with your one single attachment.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1060.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1070.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1070)

Let's talk how that looks like on the AWS site.  We're going to use the AWS console for this example. So, where you're going to start by selecting your partner, so you want to create a new interconnect, you select your partner, Google Cloud.  You're going to select your AWS region, and that will immediately populate the second option, which is the Google Cloud region. So we're creating massive amounts of capacity and we're basically interconnecting the two regions that are closer to each other because we found through customer conversations that's the most common use case. London to London, Virginia to Virginia, Tokyo to Tokyo, and so on. And this is all you need.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1100.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1100)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1110.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1110)

You set up your speed. For public preview, we're offering one gigabit connections at no charge for the duration of preview, so let's  go with that. You pick a name, you select your attach point, which in our case is Direct Connect Gateway, you can tag.  And you get this sort of review screen and you're done. That's it.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1120.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1130.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1140.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1140)

Now let's see how this looks on our flow. You created a connection. Again,  remember, create flow. So you created your connection. And we get in touch with Google and now you need to activate. Let's see how that works as though you had started the flow on Google.  That is a very long key. Basically that's all you need to activate. You paste that key and we immediately sort of confirm details. Again, you select your Direct Connect Gateway,  confirm the connection. And that is all. You're left with whatever capacity you requested as a single attachment between the two clouds. There's no BGP, no Layer 3 connectivity configuration, VLANs, nothing.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1160.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1160)

So, with that,  the API that we created for this is very cool. How we got away with this is that we cabled very, very large pools of capacity between these pairs of regions. So then we can just sell slices of that to you via API in a very short time. You don't need to have someone go with a cable, nothing, all that goes away. But the interesting thing is that because now we're on a multiple router configuration and you don't see it, we can move you around pretty easily. So we'll see what that allows us to do.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1200.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1200)

### Under the Hood: Multi-Router Architecture and Built-In Resiliency

Let's talk about the architecture behind the scenes. This is what your connection looks like to you.  It's sort of the pink line in the middle there.

But behind the scenes, for every single interconnect, we're provisioning on multiple different devices, multiple logical connections, and each one of those is a valid path. So essentially, you have the maximum resiliency configuration that Alex talked to you about a moment ago by default on every single connection, but you don't see it.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1230.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1230)

Now, here's where it gets really interesting, and again, the point is to give you confidence on how we're building this so that you never need to think about this again.  The API calls for a new logical object between the providers. You don't see this. These new logical objects can expand across buildings. It understands routers, it understands connections, it understands link aggregation groups, or LAGs, these very large pools of capacity. It knows about all of that.

So what this affords us is that immediately we can ensure that your connections are placed in four different routers. So immediately we get rid of that pesky thing where unfortunately two connections turned out to be on the same physical device. Anyone experienced that? I hope not. Well, that's pretty bad because if that router goes down for maintenance, you lose both of your connections. So immediately we get out of that situation.

But this is also what allows us to create the flow of placing your connections, because when you tell us, give me 10 gig to Google Cloud, we basically now tell Google, hey, we're thinking about using this interconnection point here, what do you think? And they say, yeah, sure, go ahead. And then our new specifications start building your logical connectivity until all checks are clear, and then you're presented with this new abstracted single object that actually has built-in resiliency.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1320.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1320)

Now, it gets better. When we run out of capacity, because we're doing great, you love the service, you're consuming all of our capacity, so we need some more.  So we started with the purple one, right? Now we need to put some more routers there. So we put in this new logical object, so now we have two, we have the green one. The way we're going to provision your connections is the criteria that we call least utilized interconnect, and yes, we named that logical object interconnect, and yes, we did run out of names when we were building this thing.

So your connections are going to start to get spread across multiple routers by default. You don't need to do anything, because we're always going to use the least utilized interconnect. So if your marketing department wants a 1 gig connection, and your production workload wants a 10 gig connection, and your other critical workload needs a 100 gig connection, those are going to start over time getting spread out across sets of four different routers.

As it turns out, there are very, very few customers who are big enough to afford this level of resiliency. No one has 12 routers in a location, enterprise routers connected directly to our location. So the more you use the service, the more resilient it becomes.

Another cool thing now, remember that I said that we can move your connections around. Say you landed your first connection on the purple object, and all of a sudden you need more capacity. You tell us, hey, my workload grew like crazy, I need 50 gigs or something. And we don't have more space on the purple one, well, we just create more logical connections to the green one. Then we move you, all the checks are green, and then we take you out of the purple one.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1440.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1440)

So you can scale these connections up and down as needed, and because we have these very large pools of capacity, we should be able to get you the capacity you need whenever you need it. It gets better though. This new object actually allows us  to understand what's going on on both sides. So there are things that now we can do to ensure that this resiliency stays even through maintenance.

So that situation you see in the slides would be very unfortunate. Say we take down a router for maintenance at the same time that Google Cloud was doing maintenance on their router, so now we lost half of our redundant capacity. So that's bad. So now we can agree with each other, don't take down that router over there while I'm working on this one over here, please, so that you never lose half of your capacity because we just didn't talk about maintenance in a timely manner. So that's pretty good.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1480.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1480)

### Open Specification and Partnership: Building the Future of Cloud Interconnection

Now, here's the thing though. As you probably gathered,  we couldn't have done this with our existing APIs, and when we were designing this, we looked at all the approaches to build multicloud connectivity, and the conclusion was they're not enough. We just cannot operate the service the way we want to, and we cannot simplify the service the way we want to with the existing approaches, so we need something new. It was very, very important.

for us that as we onboarded other clouds, whenever you get an interconnect to Google and an interconnect to whoever comes next, that thing looks and behaves and is equally resilient with everyone. We didn't want to have one approach for cloud A, B, C where it doesn't work the same way, the API doesn't give you the same object, the routing, the resiliency. That was unacceptable. So it was very important for us at AWS to say, when a customer gets 10 gigs to any cloud, that thing needs to act the same always.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1560.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1560)

We sort of found ourselves in a problem here. We need a new specification, but we need other folks to adopt it. If you come up with this great spec and no one uses it, well, it's a problem. So we thought, wouldn't it be great if we came up with the spec so that actually everyone uses it.  Your customer expectation should not only be that when I get a connection from AWS to another cloud, it should behave in this way. It should be, actually, you know what? I want all clouds to talk like this. Make it easy for everyone. Why am I thinking about routers again? It would be great, right? Yes. I see people nodding, thank you.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1590.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1590)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1610.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1610)

So it would be great. You know what would be even better? If the spec allowed folks to  essentially connect clouds with each other in the same way, even if they don't talk to AWS yet, maybe they're in a different region or what have you. But it would be awesome that they adopt the same specification so that when cloud pink up there at the top then becomes ready to talk to AWS,  well, they can use the same spec and they're done, right? They don't need to build it again. You just build once and use the same specification many times over. That would be great. So that's what we did.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1620.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1620)

 We opened the specification under Apache 2. So there are no patents. You can fork it. You all can go in and fork it right now. So it's open, it's on GitHub. And the idea here is to give now confidence not only to you customers but also to our partners that this specification is not going away. This is not something that we will deprecate tomorrow or anything. It's open, you can fork it, you can use it with anyone you want. So it's out there.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1650.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1660.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1660)

 Now, as I've been sort of building toward this, these are the features and you can take pictures of this.  Essentially, this is what you are left with. These are connections that can scale up and down dynamically, that have built-in resiliency. You can count that all the physical links between the AWS router and the Google Cloud router are encrypted using MACsec. And it's pretty cool because MACsec has this mode of operation called, well, depends on your vendor, but must secure, depends on your vendor. But the mode is that the link doesn't come up if the MACsec session is not up. Another way of saying that is, if you don't have encryption going, you cannot transmit customer data. So you can be assured now that any data you are pushing through this new service between AWS and Google will be traveling over encrypted links. That's very important.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1720.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1720)

 So let's talk a little bit about the preview. For public preview, we are in five regions and more to come. And you will be able to create at no charge one gig connections to GCP. Now, here's the interesting part. We wanted to give you the tools to start learning about the future because, well, it's going to take us some time to be everywhere, right? But now you can create your connections and start kicking the tires, learn how to use the service, so that when we go GA you're ready to go.

Another thing that we're doing is that for the first time we're creating a service that's really going out of AWS, out of the AWS global backbone. So monitoring is an area that, quite simply, we lose visibility. We hand the traffic to Google Cloud and we don't know what's going on on the other side. So with every new interconnect, we're also including CloudWatch Network Synthetic Monitor. So you will get one synthetic monitor for every new interconnect. Now, you still need an endpoint on the other side that responds to the probe, TCP or ICMP, but you will be able to get signal from wherever you're going. You'll be able to tell how your connection is doing, its health.

The last feature, and this is new as I mentioned before, is that these connections are scalable. You can scale them on demand, up or down as you need them. Maybe your test environment needs a tiny connection, but then it starts to grow, so you grow it. You have a seasonal workload and you need to serve traffic spikes, so you just get more capacity for busy season. That's fine.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1830.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1830)

 Now, I do want to talk a little bit about the partnership and I'm going to do that in a minute, but for now I'm going to hand this off to Judy, and I just want to finish with one thought. As we built this thing, it was very important to find a partner that shared our vision. It should be simple. We should take care of the infrastructure, the support experience, the whole thing, right? For us, I'm very thankful for the partnership we found in Google, someone with that vision, and basically that's the product we're launching today.

### Google Cloud Interconnect: Private Connectivity and Cross-Cloud Solutions

Thank you. Likewise, thank you for having me. I introduced myself as the Cloud Interconnect at Google product manager. I'm very happy to speak to an audience of AWS enthusiasts. How many of you here know Cloud Interconnect and have worked with Cloud Interconnect at Google? Wonderful. Awesome. So for the folks that don't know what this does, essentially maybe we'll spend a minute level setting on what this actually is.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/1890.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=1890)

Essentially, Cloud Interconnect at Google is  our private on-ramp into Google Cloud for customers that want private and secure connectivity from their on-premises environments into Google Cloud. We deliver this from colocation facilities. We call those Interconnect POPs, points of presence, and in many cases those Interconnect POPs are places where we colocate with Amazon or other cloud providers as well to offer that connectivity to customers. That's been a model that's been adopted in the industry for a while. This is how cloud providers peer publicly as well to deliver public connectivity at scale to their customers.

As Alex mentioned earlier, we've seen a lot of patterns where our customers want to use this product for connectivity to other clouds. This is not only used for hybrid connectivity into my own premises. I also want to use it to another cloud because I like the fact that it's private. I like the fact that it's secure. I like the fact that it's dedicated to me. I'm not sharing a channel that is over the public internet, essentially.

We recognized those challenges a long time ago and we introduced a product called Cross-Cloud Interconnect. We added it in 2023 and we've been seeing a lot of adoption on this product. Essentially what we did with that is that we've extended our footprint from our edge router into the edge router of the other cloud provider. By virtue of the fact that we colocate with Amazon in many of those colocation facilities, we could just extend our port with a cross connect into the port that's on the AWS side, the Direct Connect.

So the customer buys a Cross-Cloud Interconnect port from us, a Direct Connect from AWS. They give us the LOA and we go terminate that cross connect on their behalf. No more on-premises footprint. No more having physical routers in the colocation facilities. Happy days. I can get my 10 gigabit or 100 gigabit connection between Google and Amazon in almost no time. There's still a tech that's dispatched to go terminate that connection because it's actually built to order and it's dedicated to me, but I can achieve my cross-cloud connectivity goals in terms of privacy, in terms of scale very easily.

I can get all the way to four lines of availability without having a single relationship with a third-party provider like a colocation facility or a partner provider, for instance, which means that I can lower my total cost of ownership and I can minimize the amount of vendors that I need to talk to when there's an incident. Cool stuff.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2040.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2040)

Let's build on top of that to start getting to that resiliency model with Interconnect. We've invested a  lot in building resiliency in layers, so similar to constructs in AWS where you get maximum resiliency, we introduced that at the physical layer in our Interconnect facilities by introducing a construct that's called EADs or Edge Availability Domains. Those are essentially separate fate domains within a colocation facility. You can think of it as a collection of hardware that doesn't share power, that doesn't share uplinks up to the region. They just don't share fate in a way that we know that if you're connected to both of those, you're not likely to have an incident that impacts both. We're never going to schedule maintenance in both of those, so that's how we kind of guarantee that we have that physical separation.

Building those layers up to the regions, adding some virtualization there, we have a construct called VLAN attachments that allow you to virtualize your Interconnect into multiple VLANs that have different bandwidth essentially and attach those to cloud regions. Those also talk to a construct called Cloud Router that orchestrates your routing.

It propagates routes from your on-premises environment or your other cloud into Google Cloud, and that engine is also redundant from the control plane perspective. So you're getting that physical redundancy and you're getting that logical redundancy. Awesome, good stuff, right? Story ends.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2120.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2120)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2130.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2130)

### Addressing Challenges Through Collaboration: Google's Perspective on AWS Interconnect

But  these challenges, we're seeing a lot of adoption with this product. Hundreds of our customers have adopted this to do cross-cloud interconnects, not just with  AWS, but with a number of other providers that we happen to co-locate with and support this product with. There's a few things here. It's a dedicated link, so the minimum link capacity that you can buy is the line weight of the interface. In our case, the minimum is 10 gigs. We're building that to order, so we're dispatching someone to go connect that on your behalf. There's some lead times associated with that, which means that you're probably overbuying capacity because you want to make sure that that capacity is there when you need it. So not great, but we do have a solution.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2170.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2170)

Then what happened was that some customers forget that they need to communicate  the context of reliability from one side to the other. So while we invested so much time and engineering efforts in building resiliency in our stack, the customer goes and builds these things into availability domains, but then they forget that they need to do the same in the other cloud, and then they go connect two routers here to the same router on the other side. Not great if you want reliability.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2200.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2200)

Even worse, something that they probably miss oftentimes is that  sometimes even though I make sure that I don't have overlapping maintenances between my two Edge Availability Domains, that needs to extend to the other cloud. So the customer needs to notify me if I happen to have a planned maintenance next week that happens to overlap with their Amazon router that's also planned for maintenance next week. So customers sometimes miss that they need to provide the same context to both clouds. They need to coordinate between both clouds if they want things to work.

So that got us to pause and rethink our approach a little bit. How can we do better? What can we do if we wanted to provide that resiliency to customers without them having to be engaged? How can we address some of the limitations that we have in the service? So we started thinking about what's that North Star and how can we get there.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2250.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2250)

What if essentially  we could pre-build large pools of capacity, pre-cable them to a number of providers, and virtualize the connections in a way that you don't have to wait for that tech to go and connect something? What if we also encrypt it by default so you don't have to worry about what's happening in that segment that's outside of my circle of trust? What if we can deliver reliability out of the box so that you stop thinking about all these things and coordinating maintenance and things that nobody wants to spend time focusing on? What if we can make it scalable in a way that you could scale up, like Santiago mentioned, or scale out by adding more connections that could be turned up on demand in minutes? What if we could abstract all that BGP stuff that Alex loves, but also nobody wants to worry about, and not have that be your problem, so you don't have to replicate policies across four routing sessions and make sure that they are symmetrical and all that lovely stuff?

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2310.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2310)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2320.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2320)

And we thought a lot about this and honestly  we thought of a number of ways to do that. We could potentially go buy a lot of capacity from Amazon and try to virtualize things. We could do all sorts of stuff, but  all of those approaches have holes and all roads essentially led us to Rome, and Rome is we have to collaborate with other cloud providers. The only way this is going to work is if we actually talk to one another. But that's impossible. Why would Google talk to Amazon or why would Amazon want to talk to Google?

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2340.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2350.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2350)

But then  we're here, so it turns out it's actually pretty possible and it's actually here. So the announcement that we made on Sunday that we're all extremely excited  about is that we have been collaborating with this amazing team to try to come up with a solution to all these challenges that would essentially check all the boxes for our mutual customers so they can be successful adopting both platforms without worrying about the infrastructure layer that does not add any value to their workloads.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2370.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2370)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2380.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2380)

So essentially  I'll talk a little bit about our perspective of how this became possible. We started working together about a year ago maybe on this East-West  API that we have between one another. And it's essentially being able to encode all those collaborations that we were expecting customers to do to make sure maintenance doesn't overlap, to make sure resiliency is well met across both places, to be able to orchestrate the creation of all these resources. The best way you can do all of that is that you can put some amazing engineers from Amazon and Google in the same room to come up with a spec that can automate all of that.

That's what we've encoded in the East-West API that we've co-developed and opened so any cloud provider can implement this, or any private peering provider can implement this. It doesn't have to be a cloud provider. We've made this East-West API available and we've also implemented it in a number of locations. I think we're in about four or five regions now, and we're looking to expand that globally for global coverage.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2470.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2470)

We've also invested in those North-South APIs, if we want to call them that, and that's what we expose to our customers to be able to interact with our tools. As Santiago mentioned, those are symmetrical. If you come from an AWS environment, you're familiar with those tools, you can go to the AWS console, configure whatever you want, and everything happens behind the scenes. All you need to do is acknowledge on the Google side. If you're more familiar with our APIs on the Google side, you can also use our console and then acknowledge on the AWS side, which makes it very scalable, very adaptable to any cloud provider, to any customer that's coming from any  cloud background, essentially.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2480.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2480)

Santiago popped the hood on the AWS side, so I'm going to spend some time doing the same on our side.  From the physical side, we're using two different facilities to be able to map two different regions. If we take Northern Virginia as an example, we use two facilities where we happen to co-locate with Amazon, and we cable different routers within those facilities to be able to meet the maximum resiliency targets that we both have. From a virtual perspective, we're building those VLAN attachments that I mentioned earlier to virtualize the connections. We're building Cloud Routers and we're putting those in a managed project in a managed VPC that Google owns, so we're orchestrating all those resources that we were asking you to create before. We're doing whatever you need to be doing to be able to have that end-to-end connectivity from Google into Amazon.

We're doing BGP, we're negotiating VLANs, IP addresses, all that lovely stuff that everybody wants to spend their time doing. Essentially, all of that is put in a Google managed project, and the way you can communicate from that project into your own environment is with two approaches essentially. In preview today, you can do what we call VPC Peering. VPC Peering is essentially a one-to-one relationship between the managed VPC that we own and the VPCs where your resources are. This is perfect if you want to do plain vanilla VMs that need to talk between AWS and Google. You have that connectivity up and running. Even if you want to use Google APIs, for instance, and you want to be able to call them from AWS, you could do that with the construct that we call Private Google Access and advertise those ranges into AWS to be able to access those APIs safely and privately.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2580.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2590.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2590)

In addition to that,  right after preview, we're looking to introduce support for what we call Network Connectivity Center.  You can think of VPC Peering as a one-to-one relationship. Network Connectivity Center is essentially a hub that allows you to attach multiple spokes to it. You can think of our managed VPC as one spoke, and then you can attach a number of other spokes to that hub so that they can all be able to leverage that connection and talk to AWS. This is perfect if you also want to use that for plain IP connectivity between Google and AWS, or if you have some other types of applications that require some type of exposing an endpoint through a producer-consumer model, for instance, or other types of private endpoints within Google through a service we call Private Service Connect.

If I want to summarize all that, regardless of what you want to do, we're making sure that we have that transport in place, we have that underlay in place for you to build any application on top of that. Our goal is to abstract building that transport the same way you get when you publicly peer between providers. We're doing that for private peering, and we look forward to seeing what you will do with all of those innovations. With that, I'm going to hand it back to Alex to talk about what you can do if you wanted to scale that beyond a single region.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2680.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2680)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2700.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2700)

### Reference Architectures: Single Region, Multi-Region, and Route Advertisement Patterns

Thank you. Let's see if we're completely rid of BGP or not. So let's see how you folks can use these. We've looked under the hood, we see how this works. At the end of the day, you get an interconnect. What can you do with that interconnect? Well, there are some reference architectures here we should look at.  First is single region, single interconnect. On the AWS side, you would have a single VPC, usually. On the Google Cloud side, you would also have a single VPC. We've seen in the intro how the VPC looks like for those of you who are not familiar with the subnets and the Virtual Private Gateway that's attached to the VPC.  Of course, it supports both IPv4 and IPv6. I'm very grateful to my partners here for that, so you can run both stacks across the clouds, right?

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2720.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2720)

So you get your interconnect, you attach it to your Direct Connect gateway, you attach it to the Google Cloud router on the right hand  side. Let's look at route advertisements. This is going to be the standard for how we are looking at these architectures.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2740.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2740)

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2750.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2750)

So first, the Virtual Private Gateway picks up the VPC prefixes, both IPv4 and IPv6, and advertises them to the Direct Connect gateway automatically. You don't have to do anything. Then the Direct Connect gateway automatically advertises these prefixes to the Google Cloud router  in BGP that you don't configure, right? It's already there, it exists from the Google Cloud router perspective.  It advertises the VPC subnets that you have in your VPC, either in just this region or globally if you have a VPC that has multiple regions. So it advertises them to the Direct Connect gateway. The Direct Connect gateway sends them to the Virtual Private Gateway, and if you have route propagation enabled on the subnets route tables in your VPC for that Virtual Private Gateway, those routes are going to be automatically populated in your route tables, right? Pretty straightforward so far, right?

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2790.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2800.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2800)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2820.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2820)

Let's look at the route tables. So from the VPC perspective, VPC prefixes are local  to the VPC, right? Those are routes that you are not going to override. And then all the other routes that were learned dynamically go to the Direct Connect gateway, and then the Direct Connect gateway  is going to have, as you may expect, routes pointing to the Virtual Private Gateway for the VPCs attached to that Direct Connect gateway and towards the AWS Interconnect for what is on Google Cloud. Google Cloud router will have Google Cloud routes  that are going to be sent onwards to the subnets that you have in the Google Cloud VPC and automatically populated, so you don't have to do anything. This will work, right? Pretty straightforward.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2840.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2850.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2850)

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2870.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2870)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2880.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2880)

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2890.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2890)

Now, if you have multiple VPCs on AWS,  we've talked about ways to aggregate that, and if you are in a single region, we are still in single region single interconnect, right, you can use an AWS Transit Gateway that you can of course attach to  your Direct Connect gateway. Now, ideally you would use VPC prefixes that are easy to summarize so you can have a nice summary route that is sent from the Transit Gateway to the Direct Connect gateway. And this works today through a prefix list that you  get to configure, so the more compact that prefix list is, the nicer your route tables are going to look like. Now the same process happens  from AWS to Google Cloud. Routes are advertised through BGP and the other way around. Routes are propagated into the Transit Gateway route tables,  right?

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2900.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2900)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2920.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2920)

Now from a route table perspective, if we look at the VPC route tables, here is where you have to configure routing, right? The VPCs are  still the owners of routing decisions in AWS, so there's no direct route propagation from the Transit Gateway to the VPC. So here is again where summary prefixes are very useful because they keep your VPC route tables small. On the Transit Gateway, all routes are dynamic,  and they point, as you may expect, either through VPC attachments or through the Direct Connect gateway. The Direct Connect gateway route table looks very similar to what you folks have seen previously.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2950.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2950)

The Direct Connect gateway is again a global construct. You can attach multiple Transit Gateways from multiple regions to the Direct Connect gateway. Keep in mind that cross-region communication needs to go through Transit Gateway peering. So that's why Cloud WAN is there.  Now on the Google Cloud side, again routes are learned, those prefixes that you configure in the prefix list of the Transit Gateway attachment to the Direct Connect gateway, and routes are propagated all the way to the subnet side. Animation is complete, so you can take a photo.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2970.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2970)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/2980.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=2980)

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3000.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3000)

Now let's get to more complex scenarios, right?  The first one is multi-region, right? And multi-region on AWS, you could have multi-region on Google Cloud as well, and you still start with a single interconnect, and you may ask yourself, well,  how will this work, right? I have this Direct Connect gateway that's a global construct. I have maybe Cloud WAN or Transit Gateway northbound of the Direct Connect gateway that are attached to it in all regions. So how will this work? How will routes be advertised? Well, pretty simple.  The Direct Connect gateway is a global construct, and whatever routes it learns, it advertises to all its global attachments, right?

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3010.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3010)

So let's look at routes and route tables.  First of all, the Cloud WAN attachment to the Direct Connect Gateway is again a global construct, and under the hood by default, the Direct Connect Gateway gets attached to all core network edges in Cloud WAN. These small bubbles here are core network edges, like those Transit Gateways under the hood that are your routing hubs in AWS.

Now these core network edges advertise all their prefixes to the Direct Connect Gateway automatically. You don't have to configure anything. There's BGP that's running under the hood. Keep in mind that every single core network edge is going to advertise the prefixes that it knows regionally to the Direct Connect Gateway, so it's not going to advertise a summary route unless you tell it to do so, and we're going to look at how that happens with Cloud WAN routing policies, which is a recently launched feature. But by default, every single core network edge advertises its attached prefixes, its local prefixes, to the Direct Connect Gateway. The Direct Connect Gateway will take these prefixes and will advertise them over to the Interconnect. Pretty straightforward so far.

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3090.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3090)

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3100.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3100)

From the Google Cloud Router side again,  the subnet prefixes in Google Cloud tenant or customer VPC are going to be taken by the Google Cloud Router and advertised over to the Interconnect, automatically learned by the Direct Connect Gateway  and then sent forward to the core network edges automatically. So it works pretty straightforward. You don't have to do anything for all of this to happen.

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3120.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3120)

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3140.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3140)

Now if we are looking at the route tables, again from a routing perspective, the VPC is still the owner of the routing  in AWS, so you have to configure VPC routing and decisions at that level. Summary prefixes are amazing again, so keep in mind that route summarization is important. From Cloud WAN perspective, every single core network edge  has its own route table. In this case, all core network edges' route tables are going to look the same. Each core network edge will know its attached prefixes for VPCs or other types of attachments that you have in that region and will know the Google Cloud IPv4 and IPv6 prefixes from the attached Direct Connect Gateway. Pretty straightforward.

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3170.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3170)

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3190.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3190)

And the Direct Connect Gateway will have in its route tables the routes for the Google Cloud environment in its route table.  Same for the Google Cloud Router route table. It will learn the AWS prefixes and populate its routes in its route table. Seems even too easy to be true. From the subnet perspective, again, routes are being automatically propagated. You don't have to do anything. 

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3200.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3200)

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3220.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3220)

### Advanced Multi-Region Scenarios and the Road Ahead for AWS Interconnect

Now the most interesting one is when you have multiple  regions and you have multiple interconnects. Now option number one here is where you have full region overlap between AWS and Google Cloud. So let's say you deploy in AWS in Virginia, you deploy in Google Cloud in Virginia, you deploy in AWS in Europe, you have in Google Cloud in Europe. Now this is a scenario that's pretty  straightforward, pretty common. However, you may grow into a place where not all regions overlap, and we're going to look at that next.

So recommendation is to have a Direct Connect Gateway per Interconnect because it allows you to have full flexibility into how routing decisions are being made. Now you're getting close to maintaining a global backbone, and you have to kind of know a bit about route preferences and you have to know how do you route to certain prefixes in the other cloud provider environment. So again, I was mentioning that by default when you attach a Direct Connect Gateway, it gets attached to all the core network edges in Cloud WAN, so you don't have to overthink this too much. Now each Direct Connect Gateway hosts an Interconnect, has its own BGP session that's already running, so let's look at route advertisements.

[![Thumbnail 3280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3280.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3280)

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3300.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3300)

Core network edges advertise  their local routes to all the attached Direct Connect Gateways by default. So now each core network edge will send those local routes to both Direct Connect Gateway A and Direct Connect Gateway B. And each of these Direct Connect Gateways is going to send these prefixes to the Google Cloud Router. 

[![Thumbnail 3320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3320.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3320)

Now in each region, the Google Cloud router is going to advertise all the Google Cloud prefixes across all regions from the customer VPC to both Direct Connect gateways, so each Direct Connect gateway will know about all the routes that you have for all the subnets in Google Cloud.  And what will the Direct Connect gateways do? Well, exactly what you expect them to do. They will send the routes to the core network edges automatically without you having to configure anything. So this is how route propagation looks like.

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3350.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3350)

Let's see how route tables look like. From a VPC perspective, nothing changes. From a core network edge perspective, here is where you can start doing  the starting point of traffic engineering on AWS, where you can decide which paths your traffic should take depending on your available egress points. In this example, I've configured using Cloud WAN advanced routing policies a preference for Direct Connect gateway A for the Google Cloud prefixes in US East 4, and Direct Connect gateway B for the prefixes in Europe West 3. You can do that however you want. You can use AS path, you can use local preference, you can prepend, you can play with all the BGP constructs that you may or may not be familiar with. You should probably be on Cloud WAN, and you can decide how Cloud WAN routes. So Cloud WAN is your global backbone on AWS that you can manage however you like.

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3430.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3430)

Now the Direct Connect gateways route tables are going to be pretty straightforward. They will know the prefixes attached to every single core network edge. The Google Cloud routes for the Cloud Routers are going to look very similar to before. And then from a subnet perspective, each subnet on the Google Cloud side is going to select,  of course, the Google Cloud router in its region to route to AWS prefixes. So even though each subnet receives the routes from multiple Google Cloud routers, it will select the regional one. Pretty straightforward, right?

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3450.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3450)

And last but not least, at the end of this,  it's probably one of the most common scenarios but also one of the ones that requires most routing knowledge from all of you, which is not full region overlap. You may have regions where you overlap and you can create these interconnects, and you may have regions where you don't overlap between clouds. But you may still want to say, well, I have this workload here in AWS that needs to talk to an API endpoint or whatever data store that I have in Google Cloud, and it needs to do so across region. So can I leverage Cloud WAN as a global backbone? Yes, I can.

[![Thumbnail 3500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3500.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3500)

[![Thumbnail 3520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3520.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3520)

[![Thumbnail 3530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3530.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3530)

[![Thumbnail 3540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3540.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3540)

So route advertisements again, every single core network edge advertises to the Direct Connect gateways it's attached local routes, so pretty straightforward. Nothing changes here.  The Direct Connect gateways send the routes to Google Cloud routers over the interconnect, and that is pretty straightforward. I don't know if you folks have the time to take a photo if you want to take a photo. We're going to look in the next slide on the other direction,  on the Google Cloud side towards AWS.  The Google Cloud router again in each region advertises all the routes from the customer VPC. These routes are propagated to the core network edges and they're of course learned, and here's where you can again apply traffic engineering to understand  what the next hops should be. This one just shows you an example. For example, core network in US East 1 in AWS prefers Direct Connect gateway A for prefixes in Google Cloud North Virginia, and then prefers Direct Connect gateway B for prefixes in Africa South 1. It's totally up to you how you want to route those depending on how you think about how your workloads are deployed, what are the connectivity options you may have.

[![Thumbnail 3600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3600.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3600)

You could also say that the core network edge in US East 1 routes towards everything in Google Cloud using Direct Connect gateway A. Totally good. Just keep in mind to have that observability set on these interconnects to understand utilization bandwidth when you need to scale up, which is now very easy, or if you need to scale down. From the Direct Connect gateway perspective, everything stays the same,  and on the Google Cloud side again, all routes are being automatically propagated. And now for the last 30 seconds I'll give it back to Santiago.

[![Thumbnail 3610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/34573d48d1c2faff/3610.jpg)](https://www.youtube.com/watch?v=yfxS9Lizu5M&t=3610)

 Thank you, Alex. So this is our product. We're super happy to have launched it. We think it's going to be great for your products. The idea is that you take this and build whatever application you want to build that requires connectivity across clouds. We're also working with Azure to bring you the same product in 2026 with them. And the last thing is, please bear with us as we roll out this globally. Turns out it's a pretty big globe, and sometimes we are in the same facility as the other providers, sometimes we are in the same campus, sometimes we are in the same city, sometimes we're not in the same city. So it's going to take us some time to be everywhere. We're investing heavily in this. We're building new facilities to be able to provide you the best possible experience. So please bear with us. We're going to be using our documentation to signal to you folks this number of regions are going to come in this time frame, this other time frame, this other time frame to keep you informed so that you can plan your products around AWS Interconnect. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
