---
title: 'AWS re:Invent 2025 - How TE Connectivity Transforms Product Engineering with Agentic AI (IND369)'
published: true
description: 'In this video, AWS Senior Solutions Architect Mrunal Daftari and TE Connectivity''s Girish present TELme, an Agentic AI platform serving 40,000 users with access to 10+ million technical records across 50+ data sources. They detail how TE achieved 99.99% uptime, 75% engineering adoption, sub-15-second response times, and breakthrough 90%+ accuracy in 2D engineering drawing analysis versus the 40-50% industry standard. The session covers their architecture using Amazon Bedrock, OpenSearch, and Redis streaming, cost optimization reducing expenses to under $1 per user monthly, role-based security implementation, and lessons learned including user enablement challenges, data quality management, and the journey from 8,000 to 40,000 users with 10,000 monthly file uploads.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/0.jpg'
series: ''
canonical_url: null
id: 3093163
date: '2025-12-08T20:55:19Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - How TE Connectivity Transforms Product Engineering with Agentic AI (IND369)**

> In this video, AWS Senior Solutions Architect Mrunal Daftari and TE Connectivity's Girish present TELme, an Agentic AI platform serving 40,000 users with access to 10+ million technical records across 50+ data sources. They detail how TE achieved 99.99% uptime, 75% engineering adoption, sub-15-second response times, and breakthrough 90%+ accuracy in 2D engineering drawing analysis versus the 40-50% industry standard. The session covers their architecture using Amazon Bedrock, OpenSearch, and Redis streaming, cost optimization reducing expenses to under $1 per user monthly, role-based security implementation, and lessons learned including user enablement challenges, data quality management, and the journey from 8,000 to 40,000 users with 10,000 monthly file uploads.

{% youtube https://www.youtube.com/watch?v=xn-GlgWNqhs %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/0.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=0)

### Introduction: TE Connectivity's Ambitious Vision for Enterprise-Scale Agentic AI

 Good afternoon, everyone, and welcome to Industrial 369 breakout session on how TE Connectivity transforms product engineering with Agentic AI. My name is Mrunal Daftari. I'm a Senior Solutions Architect here at AWS, and I've had the privilege to work closely over the last couple of years with TE Connectivity to help them architect and implement what I believe is one of the most impressive manufacturing Agentic AI workloads out there.

Today's session is going to be unique because you will hear the story directly from our partner TE Connectivity. I will be joined by my partner at TE, Girish, who will walk you through this breakthrough transformation journey and how the solutions were built. When TE Connectivity approached us, they had an ambitious vision of serving 40,000 employees with a Gen AI platform that could intelligently access over 10 million records and 50 plus data sources while maintaining enterprise-grade security, generating low latency responses, and maintaining cost per employee ratio to as minimal as possible. So let's go through today how TE Connectivity was able to achieve them, because most companies fail to do one or two of them while TE Connectivity was able to achieve all of them.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/100.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=100)

 So here is the agenda. We will start with what we see in the manufacturing AI landscape, why 73% of enterprise manufacturing AI workloads fail to reach production, and what are the challenges that engineering document processing requires specialized solutions. Next, we will go over how TE Connectivity transformed the story from the concept of serving a few thousand product engineers to 40,000 plus users in today's latest version of TELme. How they came up with 99.99% uptime and a breakthrough in how 2D model readability accuracy was improved versus the industry standard of 40 to 50%. The core of our session will be led by my partner Girish, who is going to demonstrate TELme's Agentic AI capabilities, architecture, and real-world impact with 75% adoption in engineering departments. And of course, TELme has now grown to over 40,000 users from the initial 8,000 pilot user target.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/170.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=170)

### The Perfect Storm: Why 73% of Manufacturing AI Initiatives Fail

 We will wrap up the session with lessons learned, cost optimizations we did, and what it means for the future of manufacturing AI at TE Connectivity. So manufacturing faces a perfect storm of data challenges that makes AI solutions struggle or fail in most instances. There are three critical challenges: volume, access, and complexity. As a manufacturer, they generate thousands of petabytes annually, thousands of technical documents scattered across systems with millions of records requiring more and more processing. Most digitization is still manual, making it costly and time sensitive.

But volume isn't the real problem. It's the complexity within those documents. We are dealing with engineering drawings, technical schematics, and manufacturing specifications that your typical or simple AI solutions cannot understand. Knowledge gets trapped in unstructured content that traditional systems fail to understand. Even when you have the data, engineers spend hours hunting for that information. We are seeing massive knowledge loss as experienced workforce is retiring and taking that essential knowledge with them. So when you add the enterprise security requirements on top of all this, you have inaccessible critical information exactly when your product team members really need it the most.

So what is the true manufacturing AI reality? Here's the sobering truth: 73% of Gen AI initiatives never reach production. In manufacturing, it's worse because standard GPT-style solutions cannot understand your CAD diagrams, cannot understand your 2D drawings, and navigating these complex engineering workflows is challenging. This is exactly what TE Connectivity faced when they set out to serve over 8,000 users from their product engineering department to access over 10 million technical data points. So the question wasn't whether AI could do that for them or not. The question was what sophisticated AI system we can build to help them accelerate their product engineering workloads and efficiency.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/310.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=310)

for their product engineering departments.  That is where our true partnership began. So let me show you how these challenges could become advantageous to your company when you do things the right way.

### Breaking Through Industry Benchmarks: TELme's Remarkable Results

We got some 15-second responses across millions of documents. Think about that. Sometimes for engineers, just to open your drawings can take longer than that. Your engineers can now query 10+ million technical records and get accurate responses faster than ever because they have access available in sub-second latency. High adoption rate in engineering departments brought great efficiency, because in an industry where most initiatives never reach 20 or 30% adoption, TE Connectivity was able to achieve more than double those industry benchmarks. That's the proof that when you build AI that actually works for manufacturers, people will come and use it.

Accelerated product design and engineering cycles help innovation move quicker because when engineers can instantly access decades of data at their fingertips with the right security access, they can propel their efficiency and their new production workloads at excellent speed. Now the biggest challenge we face in manufacturing is a retiring aging workforce, which carries away preserved expertise despite a lot of documentation being available. So when you have the next generation of engineers ready with such system access available to them, your growth and knowledge retention would remain intact.

And finally, extremely low cost per user per month. At an enterprise scale of 40,000 users, that's remarkable because even a $10 to $20 per employee per month is a huge cost, and TE Connectivity was able to achieve that cost as a fraction of that $10 to $20 benchmark across the industry. So these aren't just metrics, they represent a fundamental shift in how manufacturing organizations can leverage their intellectual capital, and that's what TE Connectivity didn't just build as an AI platform, but they built the future of manufacturing intelligence for their product engineers.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/450.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=450)

### Architecting the Solution: AWS and TE Connectivity's Strategic Partnership

So how did we architect this solution  that could meet these demanding requirements? I think this is where true partnership between AWS and TE Connectivity came into play. We brought the right tool sets and technologies. TE Connectivity brought the right experience from the industry and the datasets accessibility for us. We didn't just implement another AI solution, we built a modern industrial data strategy from the ground up. This meant rethinking how manufacturing data flows, how it's indexed, and how it's made accessible to AI systems at enterprise scale.

The technical challenge was immense, serving thousands of active users simultaneously with sub-millisecond latency while maintaining uptime. This required innovative architecture, and that's why we used Amazon Bedrock for the AI needs and Amazon OpenSearch for the lightning-fast vector search for searching across 10+ million documents. We had a phased approach with measurable outcomes. We took a disciplined approach where each and every phase has a measurable outcome, a planned launch, and a success criteria with measured adoption rate.

The groundbreaking thing in this entire solution is the accuracy of 2D model understanding. This is where TE Connectivity pushed the boundaries, and they were able to achieve greater than 90% accuracy for technical drawings versus typical industry standard of 40 to 50%. This represents a greater than 50% improvement, absolutely game changing for their engineering organization dealing with complex mechanical and manufacturing drawings.

One of the breakthrough things we did was integrating knowledge graphs within Gen AI. This allows TELme to understand not just the individual documents, but the relationships between engineering diagrams, manufacturing processes, and design decisions, and it enabled the platform to provide contextually intelligent responses. On top of everything, enterprise manufacturing data is incredibly sensitive. So we built a sophisticated role-based access system. That means that users with the right access can see everything what they are supposed to.

However, AI can reason based on that entire database to provide a better and accurate response within the security boundaries set by your organization. And finally, the most important thing we achieved is remarkable cost optimization. As I mentioned earlier, after its third phase, we launched optimization techniques within the TELme solution, which brought costs down significantly and was able to help prove that sophisticated manufacturing AI can be cost-effective and bring more efficiency and productivity boosts to your organization. So this was the classic advantage of doing a partnership between AWS and TE. Results speak for themselves.

Last year at re:Invent, we did a presentation on how TELme was serving up to 8,000 users, and today TELme serves over 40,000 users with 99% uptime and an economical per user per month cost with breakthrough engineering drawing understanding capabilities built in-house for remarkable accuracy. So you didn't come here just to hear all this from me, but I would hand it over to Girish. He will walk you through the entire transformation journey, dive deep into architectural patterns, and why TELme stands out in the manufacturing industry.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/700.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=700)

### Girish's Journey: From 100 Experts to 40,000 Users in One Year

Girish, thank you, Bal. So TE Connectivity is a leading connectivity solutions and sensor design and manufacturing company.  So as you see here, we are close to around 100 factories, around 90,000 employees, and around 9,000 engineering workforce with $740 million invested in our research and development engineering. So my name is Girish. I'm part of the TE Connectivity Singapore AI Hub. I'm Senior Software Development Manager. I'm leading a generative AI team there. We are around seven people, five in Singapore and two in India.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/770.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=770)

I would like to go through the journey of how generative AI can transform a manufacturing company like TE and what is the benefit the user can see with a remarkable 10 million data set augmented to an LLM plus a cost which is less than $1 per person per month. So that's the story. So we're going to start in depth through architecture as we go further.  So before, how did the journey begin, right? So the question is, what is the fundamental behind this generative AI program? You have something like off-the-shelf tools. You have off-the-shelf many solutions we can hook up. Why another generative AI tool, right?

So here's the story. We started sometime last year. What we did is, TE Connectivity is a big company with multiple business units, multiple segments, each autonomously working and developing multiple products. So what we did is we gathered 100 plus experts and subject matter experts, AI enthusiasts, domain experts, and we said, hey, what do you want to do? What do you want to see in generative AI in the next two to three years? How do you think generative AI can apply to your domain, right? This is the fundamental block of our story.

So we came through a lot of ideas that were generated by domain experts, then the people who are AI enthusiasts. They knew what could be achieved in AI. And then we said, okay, we got a list, a bunch of lists on what they want to achieve in generative AI. Then we sort of segregated them and said, okay, this is the first year. We have a huge number of documents. Let's build a question and answer tool. Anybody gets in, 40,000 people in the company, they ask a question, they get an answer. That's the fundamental thing, right? So that's the fundamental point.

Then they said, okay, our journey starts from there. Once you have a lot of data, what we do is we derive insights from those data. Then we generate workflows, we automate. So that's the timeline that we do. Within a one-year timeline, we have millions of data brought in, and we're going to generate insights. We're going to generate an intellectual response based on all the content that is available.

We didn't expect that AI was going to do that. When we brought in the data, we were surprised by the way generative AI was able to seamlessly bring insights from so many disparate systems where the data is scattered across multiple places. Once you bring in generative AI, it's able to synthesize the responses that we couldn't think of. We were genuinely surprised. So that's the journey of the first year. In 2025, we launched it to 8,000 people, and then we saw tremendous enthusiasm from the people.

Then our next story was that by next year, we want to generate something that can do specific functional roles, maybe something like a design review, design failure mode analysis, checking your quality audits, or checking your compliance. So that's what we said. By the next year, we're going to reach a goal where it's able to automate some of your tasks within a function. And then we said it should be able to create, format, and generate engineering content based on the information that's available. Plus, we said we should be able to use internal data that we have and bring external knowledge from the large language model and augment that and amplify the insight. Those are the targets that we had.

In September 2025, with the launch of our own Gen AI platform based on AWS, we are able to hit all those targets that we have. Where do we want to go in the future? So that's the future we envision. In the next one to two years, we want to do complete automated end-to-end tasks as much as possible for a role. Maybe 70 to 80 percent. We want to identify roles and find out how much the role and the functions can be automated. We want to do that automation. Then, generative product design. We are working on trying to find out how we can use generative AI in the product design itself.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/1080.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=1080)

And then agentic mesh. We're all seeing about agents right now. I think AWS re:Invent is all about what we're hearing is agentic AI. So we are thinking the next step is an agentic mesh where our agents perform multiple things for a user, solving complex scheduling, planning, and whatnot. That's the future we envision. We are at a point where last September, we met most of the things that we envisioned this platform to do. So these are the capabilities and examples,  but very limited ones.

The biggest appreciation for us is not the capabilities and the outcome that we see. It's when people come and say, "Hey, we are using TELme, and it created an impact." Often we hear from our sales and product marketing folks, and our customer service folks. They say, "We used to spend a lot of time talking to engineering, talking to product marketing to find out various information about products. We no longer need those kinds of conversations. We get everything from TELme." That's because we have all the product catalogs in TE, all the data sheets in TE, all the design notes, and all the technical information fed into TELme, and it's continuously updated.

So the biggest appreciation for us is when somebody comes and says, "Hey, it made an impact for us." Generally, typically, it used to take a week plus to talk to your engineering, talk to your marketing, finding out what we should respond to customers. That's no longer the case. Our sales teams, at least whatever I heard from our sales teams, were really excited. They say, "I no longer need to reach out to many, many people to get an answer. I have TELme." So that is the fundamental shift we are seeing, where the information that's being fed is transforming the user's day-to-day work.

### Technical Deep Dive: Architecture, Capabilities, and Infrastructure at Scale

So what are the capabilities of this tool? You can do single file upload and ask for insight. You can upload multiple files, up to 20 files in a session, and ask for insights. Then you have internal data that is basically around 10 million records, both structured and unstructured, and then get information from them. Then there is an external mode. It typically works like any other generative AI tool. It makes direct calls to the large language model.

Then AI agents. The large language model enables AI agents where people can create their own agent. Any of the 40,000 users in our company can log in, create their own GPTs and agents, and share them with their colleagues at no cost and no license fees. They can just create and share. This is the way we are seeing work transforming. People are being very creative. If you ask me what the use cases are, I might be able to think of only four or five, but now with the power to the users, they are building their own agents. The imagination is the limit. That's the transformation we are seeing where everybody can create agents, share agents with the entire 40,000 people in the company at no cost and no license fees.

Then there's a gamified teaching assistant for prompting. This is the biggest challenge, and I'm sure most of you agree. People are still in a Google search era where they type one word and expect the answer to come through, but generative AI is not like that. It depends on how good the prompt is. So we've got to shift our entire user base from the Google search era to a generative AI era, and it's not so easy. I'm sure you would have seen in your companies that it's not so easy to shift that mindset. So what we thought is, why not gamify this?

What we built is a gamified kind of agent where people can log in and say, I'm a product manager, I am at a beginner level or expert level in prompting. It will start giving you questions, multiple choice questions, and give you points if you answer right. If you answer wrong, it will give you an explanation of what should have been done better, why, and what prompting principles you should correct. So with that, it becomes an interactive teaching experience. At the end of the month, we collect all the points and publish the winners. This is where we're trying to increase the awareness of prompting in the generative era through gamification.

Then SharePoint integration is basically ensuring that agents have the right access. People who are using the agents and the knowledge store have the right access to SharePoint so that nobody can upload files which are arbitrary and don't have any security control. This is a security enforcement integration that we have done for SharePoint.

We have almost 99.9% availability. It has never gone down from the day we launched in September last year to this year, not even a single day or single minute, except when we want to upgrade the system. It has never gone down, never failed, not even one component. Fifteen seconds time for first token across the globe, typically around six seconds. That is searching across 10 million records and responding to users, typically in six seconds. With OpenSearch, it's possible. I'm sure you would have probably seen this kind of question: what would be the latency if I have so many datasets in my company? With OpenSearch, it's possible to get a response in six seconds.

We don't make just one OpenSearch call. For each query, we split that into 20 to 30 calls to OpenSearch, multiple parallel calls. And all that happens within a second or two from OpenSearch. That's the kind of performance that amazes us. We always say we are thankful for the AWS folks because it's very robust. Accuracy is pinpoint. We don't use any re-ranker because the plain out-of-the-box solution from OpenSearch is very good. The latency with 20 to 30 calls going in parallel for a single query to OpenSearch is less than one to two seconds response from OpenSearch. So those are the powerful features that have enabled us to reach out to 40,000 people.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/1490.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=1490)

So now the architecture piece of it,  sorry, we are not sharing that with very much clarity. If you see the left-hand side of it, it's all about security, logging into the ID single sign-on.

And once the user logs in, the system has to know the user. Suppose if I am a Class 1 user, I have access to Class 1 documents, so I have to see only responses from Class 1 documents. Out of 10 million documents, I should only get responses for Class 1 documents. If I'm a Class 2 user, I need to get only responses from Class 1 and Class 2 documents alone, nothing else.

So that's the second piece on the security side, where we have role-based access control implementation. When the user logs in, it not only authenticates the user, it also gets what is the user's security role, whether he's a Class 1 user, whether he's a Class 2 user, or a Class 3 user. So we take that information back and internally we process with OpenSearch database with the role-based access control mapping. Suppose if somebody like a Class 1 user logs in, it will give him an answer saying, "Hey, this is the answer for you from the Class 1 data." And at the end, it says, "You have more information across 30 documents, but you don't have access to them. Please reach out to your colleagues or your supervisor if you need more information."

That's the power of security that we have built in, right? This is where most of the organizations struggle with data security. So our security-first design principle and our forward-looking security posture have always made this product a success, where only people with the right access and need to know will get the information, no one else.

On the right-hand side, what you see is our data pipeline, where we have millions of documents brought in. Then whenever documents get updated or deleted, it is checked out and then refreshed. That's our data pipeline. The centerpiece is about all AI agents, the GPTs, the Lambda functions, and all that stuff.

So typically, what do we have? We use OpenSearch Vector Database, which I said is one of the best I have seen. Then we have Aurora DB for general logs and injection pipeline management. Then we use Qdrant Vector Store. That is because when the user uploads, say, 20 files to ask a question, that goes to Qdrant Vector Store. So we dynamically push the data to the vector store in that session. We bring those 20 files back and put it to Qdrant vector search, and then we generate the result.

Then, of course, we use Bedrock API. This is our fundamental shift that has happened over the past one year. We harmonized all our API calls with Bedrock API so that any moment we need to switch a model for our agents or for our use, be it Amazon Nova, be it GPT 128B open-weight model, or be it Claude, we just change by one word, right? So that's the harmonization that we did to bring Bedrock API standardization calls. Then, of course, Amazon Titan we use for embeddings.

Qdrant streaming. This is a major, major thing. This is where we had a strong partnership with Mrunal. So when we started, it was supposed to be something like 150 people kind of POC. Then our management said, "Hey, we need to go to 8,000 people, not just 150." And that was a 14-week end-to-end delivery, right? And then we went to Mrunal and we came to AWS and said, "Hey, we got a request to make it available for 8,000 users." And that's a big ask.

And if you have developed a large language model application like this, there are multiple trade-offs: tokens per minute, requests per minute, then context length, how much you can serve a user, right? So you have certain limits. Within a minute, you can only do 2 million tokens. If your 10 users are consuming, talking, the rest will be throttled. So how do I support 100 people, 200 people? So these are the architectural bottlenecks that we see when we want to build an enterprise-grade generative AI tool.

So this is where we spent a lot of time with our AWS folks trying to figure out how a robust enterprise generative tool can be built.

This is where we came up with the idea of using Redis streaming, a WebSocket streaming approach rather than just using a Lambda function to stream output. We had Redis, one of the fastest and most reliable streaming solutions. We use WebSocket streaming there. It serves over 100 concurrent users at a time without any glitches. Then, of course, we use EC2, ECS, API Gateway, S3, Lambda, and most of those standard components. Control-M, as you know, is one of the standard tools we use for pipeline management and backend software. These are the core elements, the fundamental building blocks that we use today. And of course, you'll probably see an addition of Agentic Bedrock as we move forward, which is something very exciting that we see to date, so that gets incorporated as well.

### TELme 3.0 in Action: User Adoption, Advanced Features, and Engineering Drawing Accuracy

Then we had a partnership in the beginning. Not everybody got the token limits or the rate limits as they wished. Everybody was developing, the cloud was developing, and they had their own limitations. We had very limited tokens, something like 1 million tokens per minute, but we had to serve over 100 concurrent users. With all the token limitations with large language models, we could not move forward with an enterprise-grade software. So this is where we worked with Mrunal and the cloud team for extending those limits for us. That's pretty much about the architecture.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/1900.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=1900)

 So now, this is how TELme looks like. You can see a Teach-Me agent there. So we have 40,000 users who have access to this tool. We have 7,000 to 8,000 active users every week, close to 800 plus active daily users using the system. We have over 10,000 file uploads every month, which is growing. We are hoping that it will keep doubling every two to three months. 10,000 file uploads means 10,000 files that people are uploading and asking questions about, generating insights. It's not a small thing for an enterprise generative tool. That speaks volumes. Then, month on month, we have 60 to 80% user retention. I mean, 80% of the people who logged in the previous month log in the next month. These are the metrics, and of course, the adoption which Mrunal talked about, close to 74% adoption in engineering, which is significant.

We also have active feedback collection. You can see there is a prompt library because this is where we want to teach people about standard prompts, recommendations, and how to use them. And this feature, the fusion of internal data with static large language model knowledge, if you see any conversation at the end, we have TELme leading the user, asking, "Hey, do you want to also see external content on the search result you got?" So we lead users not only to take internal data but also to gather any external information that is available from static large language model knowledge.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/2020.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=2020)

Quick insights.  So TELme 3.0, which we launched in September, has all these capabilities. Something like 10 million records and documents indexed across the platforms, then 40,000 users. File processing, multi-file upload capability, and as I just spoke about, 10,000 file uploads per month, and it's growing constantly. The internal mode and external mode fusion to bring the fusion of internal data with the external static language model data. Then, a key security-first posture. Having a role-based access control is the key for any generative AI or enterprise-grade software because I'm sure many organizations have data security teams. They are worried about the amplification of data when it comes to AI. So they need to ensure that security is built in. So our posture of having a role-based access control, security first, has always made this platform secure.

It reads engineering documents and engineering drawings. We have built our own advanced model. Traditionally, if you ask a large language model, be it Claude or be it ChatGPT or be it any other large language model in the world, if you upload a 2D drawing and ask for insights from it, most likely you will not get a good response.

If it is a plain, simple one, yeah, most of the large language models will do, but if it's a complex drawing where you have a huge number of dimensions, up to 100 and 200, 200 plus dimensions in a drawing PDF, most of the time they are rotated 45 degree angle turned. Some of them have the bubbles for markup. Most large language models fail to meet even basic accuracy of 30%. So this is where we built our own models, fine-tuned our own vision transformer models to enhance the accuracy of what we see today in large language models.

We have our own models now generating an accuracy of close to around 70 to 80% for complex drawings. For normal drawings, it's always now up 90, 95%. So this is a significant transformation because a manufacturing company like TE or any industrial company, the fact when whenever you have 100 plus factories, the design is basically your CAD data in 2D drawing. So it's something like the way you read PDF. Our tool is now able to read the design drawing. So that's the power that we brought, of course, with significant help with Amazon folks and infrastructures to train those models and other stuff.

Then agent development, which we spoke about, we probably will see as we move forward. You can see in here the right side, the GPTs. ChatGPT calls them GPTs, Copilot calls them agent, LangChain Studio also calls them agent. Right? So it is basically all of that in TELme, where people can build their own GPTs or agents and share with others. So that's the democratization. Basically, anybody can build. We are bringing the power to the 40,000 users.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/2250.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=2250)

So this is how our GPT looks like. You have basic things like  your GPT name, you have the instruction for the agent to follow. This instruction, what you see is a small one. We have something like 200 to 300 pages of instructions written there for agents to follow. Then we have tool selections, be it a web search, be it internal search, external search, or file automatic file generations. So all the capabilities as we build tools, the GPTs get the capabilities, so people can attach the tools that they can run.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/2290.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=2290)

And then we have something implemented, something like a smart document analyzer, which is another significant step. If you have two files and if you ask a question,  because of context limitations that we see generally, we don't get a good response. So we have something like a smart document analyzer, which will do multi retrievals to help the accuracy when you have more than 20, 30 files in the system. So this is now something like last month we launched. People are very creative. They're building and sharing their own agents across the company. So that we can't imagine how much productivity it brings in to the company.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/2330.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=2330)

Key outcome,  40,000 users experiencing generative AI at a minimal cost, less than $1 per person per month. That is significant. That also tells us probably what is the real price of the commercial tools, right? So with the partnership from AWS, of course, it's not that we are in this position, $1 less than $1 per person per month, from the beginning. And we had a huge cost when we had the 10 million records, 40 million vector indexes in OpenSearch. The cost was huge.

So we tirelessly worked with AWS for six months, for six months, to get the cost to one tenth. You can't believe it, right? One tenth of what it used to be. It used to be higher, 100 higher on something like 50 cases, which came down significantly, one tenth of it, with rigorous optimization on embedding reductions. Then using something like a non-HA infrastructure, then packing the information into each vector index.

Most of the time, if you follow the general guidelines available online or get information from any core development tools, your chunking strategy won't be optimal. Titan embeddings can support up to 30,000 characters, but when we looked deep into each vector embedding, it was hardly something like 10 to 15 words in one vector embedding. The chunking strategy that you see on the web is not the real one. The real one comes through engineering optimization. That's why we worked closely with Amazon. What we saw initially was something like just 80 words or 50 words used to be a vector. We worked rigorously with AWS folks on how to optimally pack enough information in one vector index. We took that to close to almost 8,000 words in a vector index compared to 80 or 100 that there used to be. So that is a significant reduction that we brought in.

With all that optimization, the cost of our OpenSearch for 10 million records is one-tenth of what it was in the beginning. Our costs are less than one dollar per person per month. This is a significant achievement, I would say, in this entire story. As I speak, it's a one-stop solution for our customer service and sales folks because all the product database, product information, and catalog are there. Even if they just enter the product code, some numericals like an 8 to 12 digit character, they can get all the details of the product, including its specifications.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/2570.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=2570)

All they have to do is just enter the part number or the ordering code, which is all just some numbers. We didn't feed that into TELme separately. We just fed the document and used the right embedding, the right tokenizers, and the right PDF extraction tools. People just enter some 12 or 13 digit numbers, and they get all the insights about ordering code, product specification, product key information, what are the key applications, everything that comes in the blink of an eye. That's the power that we have. 

### Lessons Learned: Cost Optimization, User Enablement, and the Path Forward

So what are the lessons learned in this journey? Cost optimization is a way of life. Don't get scared if you see the costs are ballooning in the initial stages. It will be. Continuous optimization is the way of life. Today, we are in a far better place. If anybody is saying there is 10,000 document file upload and there's 10 million records with sub-latency of something like less than 15 seconds at a cost of less than one dollar per person per month, it is a significant achievement. That has happened not in one day, but through the journey of the past five to six months with our Amazon folks.

User enablement is the biggest challenge most of you would have been facing. How to train our people? Prompt trainings are essential. We have created curated content. We have trained people. We have enabled multiple training sessions, close to 20 plus last year, to help people understand five to six prompting principles and how they use them. We continuously publish this information. We have our own internal Viva Engage, where we have recently 1 million active views on those portals. We teach people quite often. We keep teaching, but it's not enough.

We hope this is a journey, like our cost optimization journey. There's a point where people will become expert, but if you are in the journey, this is something that fundamentally we have to pay attention to for sure. Technical complexity is another consideration. The basic assumption people will think is, okay, it's a wrapper around Claude model or a wrapper around OpenAI model.

It is not. Real engineering gets into a lot of depth to achieve what we have done, including balancing the context windows, balancing the token constraints that we get from Claude or any other generative model, and the response quality. So all those things, the complexity is huge. You need an expert AI team for sure to have that.

Data quality, right? So this is the biggest challenge that everybody will face. More LLM knows, it's also more problematic. The reason being you start trying to pick a pattern from your data and start answering the questions. So sometimes we think it's hallucination. We ask a question back to the large language model. Hey, why did you give this response? It's not right. Are you hallucinating? It tells me, no, no, I picked it because there is a pattern of this data with respect to some other data set that I have seen. I'm seeing a pattern. That's the reason I felt this is the right answer to do.

Now, you can control hallucination because you have prompting techniques. You can fix hallucination, but how can you fix pattern picking up by LLMs? It gets tough. So the real engineering begins when you have more and more and more data, where you have to control the large language model and its behavior. That's what people face when they start from small to become like an enterprise-grade software. That's where people start learning, and this is where we are learning how to make this better.

Accuracy. This is the biggest challenge and for the future. I'm sure with the Bedrock Agentic Core coming in with the input validation, it's a big step. Often we get questions from users. Is it accurate enough? How can I trust this? Right? So what we do, we do an internal validation step before the answer. Even with that, we see 80, 86, 87% accurate, right? How do you make sure that it's very much 100% accurate, especially with the agentic AI when you have to make a decision? These are not good enough numbers. 80% are not good enough numbers. You've got to be very close to 99%.

So this is the challenge that we are learning, and what are the evaluation frameworks for generative AI responses, be it RAGAS or be it something new evolving. So we're trying to evaluate all those changing landscapes in the technology. We try to bring them in and try to learn what's the best. And it was encouraging to see Bedrock Agentic Core with a validation thing coming in, right? Hopefully, that will solve our problem. We don't need to go out and then bring an open-source framework to do all that validation. With that Bedrock agent, we should be able to solve the problem now.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a02e96451cfe98f3/2950.jpg)](https://www.youtube.com/watch?v=xn-GlgWNqhs&t=2950)

Now, again, what is the agent effectiveness? So what is the ROI? So these are the frameworks that we need to build, and we will probably mature as we move forward in this journey. So with that, I would like to say thank you for listening to us, and we are looking forward to have meaningful conversations with experts like you. I would love to learn from what is going on with your organization, with yourself in terms of generative AI. We would like to learn more and probably share more. This is where we can grow together. And thanks for being here. I  much appreciated. I'm handing over to.


----

; This article is entirely auto-generated using Amazon Bedrock.
