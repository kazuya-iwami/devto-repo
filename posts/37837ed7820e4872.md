---
title: 'AWS re:Invent 2025 - Building an AI-powered waste classification using Amazon Nova & IoT (AIM256)'
published: true
description: 'In this video, Gonzalo V치squez and Rodrigo Monge present an AI-powered waste classification system deployed at AWS offices in Santiago, Chile. The solution uses a Raspberry Pi, webcam, and Amazon Nova Pro model on Bedrock to classify waste items in 3-5 seconds, guiding users to proper recycling containers. The hybrid architecture combines edge processing with AWS IoT Greengrass and cloud services including Lambda, Step Functions, and DynamoDB. At $350 hardware cost and $4 per 1,000 images, the system achieved 95% accuracy and increased recycling efficiency by 52% over six months. The solution is expanding to Buenos Aires, Bogot치, and potentially S칚o Paulo, Arlington, and New York, with plans to open source the code on GitHub AWS Samples.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/0.jpg'
series: ''
canonical_url: null
id: 3093256
date: '2025-12-08T21:55:26Z'
---

**游붃 Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


游닀 **AWS re:Invent 2025 - Building an AI-powered waste classification using Amazon Nova & IoT (AIM256)**

> In this video, Gonzalo V치squez and Rodrigo Monge present an AI-powered waste classification system deployed at AWS offices in Santiago, Chile. The solution uses a Raspberry Pi, webcam, and Amazon Nova Pro model on Bedrock to classify waste items in 3-5 seconds, guiding users to proper recycling containers. The hybrid architecture combines edge processing with AWS IoT Greengrass and cloud services including Lambda, Step Functions, and DynamoDB. At $350 hardware cost and $4 per 1,000 images, the system achieved 95% accuracy and increased recycling efficiency by 52% over six months. The solution is expanding to Buenos Aires, Bogot치, and potentially S칚o Paulo, Arlington, and New York, with plans to open source the code on GitHub AWS Samples.

{% youtube https://www.youtube.com/watch?v=2n8c80jy90w %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/0.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=0)

### The Smart Recycling Challenge: Deploying AI-Powered Waste Classification at AWS Santiago

 Hello everyone. I guess some of you might be returning from lunch. Suppose you've had pizza for lunch and you're at the office and you have several containers to put this box in. That's the usual issue when you have recycling, landfill, compost, and the like. If you see it, it would go into cardboard, but if you open it, there's a whole different story. That's kind of the question we're having at the AWS office in Santiago, Chile, and that's what we're going to present to you today as a colleague produced a solution for this issue we were having. My name is Gonzalo V치squez and Rodrigo Monge is in the session.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/40.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=40)

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/50.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=50)

This video shows exactly what we have deployed  right now in Santiago, Chile. I just pushed the button, there's a camera, took a picture of the plastic bottle  and showed you a plastic bottle sign. That signage is exactly what we have at the office. When you see it's a plastic bottle, you push it into the corresponding container. We have several containers, and that's why we have such questions every day from all Amazonians and all customers that visit our office.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/70.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=70)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/80.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=80)

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/100.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=100)

 A little bit of context within Latin America where we're based.  There's a huge increase projected in waste by 2050, but we have such low recycling metrics that we decided to push this and go farther and improve our Earth ecosystem. Specifically in our country in Chile,  most of the waste ends up in landfills. Only just a few portions of it get recycled, and we are generating 17 million tons each year.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/120.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=120)

What do we have at our facility, and by the way, it's also in the Venetian Expo where we've been  the whole week showing it for you. It's by the sustainability kiosk. We present an item, whatever you have, that's a banana in the picture we were usually using for demo purposes, a banana peel or whatever you like to show, a plastic bottle, the pizza box. You push the color-coded button, the green one over there, and the system analyzes it. What analyzes it? Amazon Nova Pro model running on Bedrock actually does the inference for you and provides output showing you the feedback on screen of the guidance of what kind of signature leads you to the proper container. On average, the solution takes 3 to 5 seconds to show you the result where you should put the waste item.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/170.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=170)

As an exploded kind of diagram, it's very easy, the parts,  the button to trigger the photo, a Raspberry Pi inside that 3D printed box. It's a webcam. Any webcam would do. Actually we compress the image, we resize it to make it even more efficient in the process. We don't need a full quality camera, and we have a Fire tablet to actually show what's going on and what the results are and where to put the item.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/200.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=200)

On the tablet itself you get the photo that's been taken.  You get the image of the classification. Beside it we're providing extra reference for the user as labels, and below we're showing you the statistics of what has been going on within the time set that we have specified within that specific setup.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/220.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=220)

We don't  only have a pilot at the office, but we actually went to production with this at an event. It's our local version of summits that you might know. We had that at AWS Experience this year. We had three recycling sites deployed within the venue, and most of it got properly classified as papers. That was mainly what was available there without any contamination incident reported from the landlord.

This is installed today in Santiago, Chile. Rodrigo will be deploying it in Buenos Aires. The device that's on the demo booth is going back to Bogot치, Colombia, and we've had some fellow Amazonians that got interested in this this week, and we might get it deployed in S칚o Paulo, Arlington where HQ2 is, and the New York office. Even some customers got the idea to deploy it in their venues. Now I'll pass on to Rodrigo so he'll explain the architecture.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/300.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=300)

### Building the Solution: Hybrid Architecture with IoT Greengrass and Amazon Nova Pro

All right. Let's build this architecture step by step. I'll show you how each component appears and connects to the rest of the components to  build the complete solution. This solution uses a hybrid architecture. We call it hybrid because we have some components running at the edge like the Raspberry Pi, the webcam, and the button and the display where the user sees the waste classification result.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/310.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=310)

 In the cloud, we process the images using AI.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/320.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=320)

Now let's dive into the edge processing layer.  We have AWS IoT Greengrass, which is an open source runtime that runs on edge devices. It enables edge devices to process data locally and connect securely with other components and AWS services. And IoT Core is designed to help you connect your edge devices with AWS services and other components using MQTT, HTTPS, and LoRaWAN protocols.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/360.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=360)

What components do we have with this  solution in IoT Greengrass? We have the webcam to capture the images. Then we have a component nucleus, which is the Greengrass runtime. Think of it as the operating system for the IoT components. It manages the data processing locally and the connection to AWS services. And finally, our Smart Bin component, which contains the custom business logic. Here we have, for example, a Python script to reduce the size of the images for efficient transfer to the cloud.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/420.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=420)

So when someone wants to get an item classified,  the person approaches the station, holds the object in front of the webcam, and presses the button. Then the workflow is triggered. Stream Manager is the one in charge of uploading the image to the cloud, so this component is very critical for reliability as it handles local buffering when the network connectivity is poor or does the retries when an upload fails.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/460.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=460)

So once the image is uploaded into Amazon S3,  Amazon EventBridge captures the S3 upload event and it has a rule to route it to Step Functions pipeline. And this service, Step Functions, is a serverless service to orchestrate the AI workflow. It starts with an AWS Lambda function that preprocesses the image and calls Amazon Bedrock to determine the waste classification type and the confidence score. We are using Amazon Nova Pro, which is a multimodal model that analyzes text and images simultaneously.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/520.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=520)

Let's break down the prompt that is actually being used at the AWS office.  We have to set the role first, and here we define the AI as an expert in waste classification. We set the expertise level, the context, so the AI understands we are talking about a waste management classification solution and not just a general object detection solution.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/550.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=550)

 Then we define the task. The task has a clear step-by-step set of instructions where we analyze the image. We determine the waste classification based on the recycling rules that I'll talk about in a bit. Also, it returns a confidence score between 0 and 1 and a structured output using a JSON format.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/590.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=590)

These are the rules  which are particular for your facilities. In this case, that's the AWS office in Chile that has these rules.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/630.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=630)

Here we define our knowledge base. We define one category that should match the basket that you have at the facility. You have to describe a brief description of each category. For example, in this office, not any kind of plastic is recycled, just plastic bottles. 

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/660.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=660)

So at this point, we determine the result, but we need to store the results somewhere. AWS AppSync provides a GraphQL API layer. This API stores the results into a DynamoDB table and also enables querying the data in real time. Finally, once it is stored, we need to display the result. 

That's why we use AWS Amplify, which is a service that provides hosting and deployment capabilities for the web application. The web interface is accessed through a tablet and not only provides the waste classification result but also has a dashboard with metrics for recycling efficiency at that facility. As you can see, the user experience is seamless. The person approaches the station, pushes a button, and gets immediate feedback with the waste classification result, so the person can drop the item into the right container.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/720.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/730.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=730)

### Measuring Success: 95% Accuracy, 52% Efficiency Increase, and Plans for Open Source Release

So now let's measure the impact of this solution at the AWS office in  Chile. The goal was not solely to provide an answer for where to put things. It was to actually get our recycling process better and improve our efficiency.  Right now we've been able to achieve nearly 95% accuracy of all types of classifications, so that's pretty nice according to what we previously had, where we had a real mix of waste put into different containers.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/750.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=750)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/780.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=780)

Cost-wise, the solution in  Chile and other countries might be a little bit different. It cost us $350 to put it in place. That's only the hardware setup cost, so it's quite cheap compared to other solutions that we've been seeing around. Regarding usage, every 1,000 images costs $4, as Rodrigo presented. The solution is entirely serverless, so when nothing's going on, there are no costs involved or nearly no cost because we have this S3 bucket storage, but it's minimal. 

Regarding environmental impact, remember we located them on the sustainability kiosk. That's why it's related to this. Our efficiency in recycling within the office has increased 52% with the six-month evaluation period that we have had. This metric comes from our landlord. The owner of the building provides monthly metrics for us regarding the weight of the different types of containers and different types of materials being classified, and that's the metric that they provided to us. Those numbers really matter in terms of environmental impact.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/820.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=820)

Why all this matters to us? We can make it scale. We were supposed to be  scaling to just two other Amazon offices, but since we've been here, already three more are interested, as I've said before, and even more customers. Rodrigo showed you the prompt. Updating the rules is as easy as changing the last part of the prompt in natural language, depending on what you have at your venue, what you have at your office, or maybe even what you have at home if you like to do that. Cost-effective? Check. Measurable results? Definitely. It's even easier to measure when you have a landlord that actually provides you the metrics, but it can be done.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/880.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=880)

Open source with a question mark? No, it will be done. We've already started the process to open source it for you. It should be available in the next weeks, hopefully, in GitHub AWS Samples. The code and resources should be there. As I've said before, we are in the Venetian at a demo. If you'd like to read a little bit more, we have the original blog post,  which was in Spanish, but we've translated it for you for a wider audience. The QR code below is an English version. I'll wait a second.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/900.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=900)

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/37837ed7820e4872/910.jpg)](https://www.youtube.com/watch?v=2n8c80jy90w&t=910)

And finally, if you  want to learn more about what's going on at re:Invent, what has been announced, and all the services involved during re:Invent, please check out that code. Thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
