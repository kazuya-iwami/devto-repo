---
title: 'AWS re:Invent 2025 - GenAI-Powered Contextual Security Analysis & Remediation (ISV315)'
published: true
description: 'In this video, AWS Solutions Architects Ashwin Vasudevan and Sahil Thapar demonstrate a GenAI-powered security monitoring solution for multi-tenant environments. They present a prototype using AWS services including Security Hub, GuardDuty, Inspector, Security Lake, OpenSearch, and Neptune graph database to ingest and analyze security telemetry at scale. The solution leverages Bedrock and AgentCore to create "Security Assist," a chatbot that enables SOC analysts to query security findings in natural language. The demo shows how the system processes over 4,000 security findings across three tenants, using Neptune to map resource relationships and OpenSearch for OCSF-formatted security data. They conduct a detailed code walkthrough demonstrating AgentCore deployment, prompt engineering techniques, memory management, and observability features. The presentation includes extending agent capabilities using MCP servers for cost optimization and discusses evolution toward automated threat remediation using multi-agent architecture.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/0.jpg'
series: ''
canonical_url: null
id: 3088712
date: '2025-12-06T11:13:37Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - GenAI-Powered Contextual Security Analysis & Remediation (ISV315)**

> In this video, AWS Solutions Architects Ashwin Vasudevan and Sahil Thapar demonstrate a GenAI-powered security monitoring solution for multi-tenant environments. They present a prototype using AWS services including Security Hub, GuardDuty, Inspector, Security Lake, OpenSearch, and Neptune graph database to ingest and analyze security telemetry at scale. The solution leverages Bedrock and AgentCore to create "Security Assist," a chatbot that enables SOC analysts to query security findings in natural language. The demo shows how the system processes over 4,000 security findings across three tenants, using Neptune to map resource relationships and OpenSearch for OCSF-formatted security data. They conduct a detailed code walkthrough demonstrating AgentCore deployment, prompt engineering techniques, memory management, and observability features. The presentation includes extending agent capabilities using MCP servers for cost optimization and discusses evolution toward automated threat remediation using multi-agent architecture.

{% youtube https://www.youtube.com/watch?v=DZ6MMauVrZg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/0.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=0)

### Introduction: Key Requirements for Multi-Tenant Security Operations Monitoring

 Welcome all to today's session on GenAI-Powered Contextual Security Analysis and Remediation. I hope you're all hydrated and energized after lunch. In this session, together, we are going to explore and find out how to secure your multi-tenant workload. I'm Ashwin Vasudevan, a Senior Solutions Architect at AWS, and with me here today is Sahil Thapar.

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/30.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=30)

Thank you, Ashwin. Once again, welcome everyone. My name is Sahil Thapar. I'm a Principal Solutions Architect. We're really excited to have you all here right in the middle of what we hope has been  a really exciting week of learning and innovation for you all. In the session today, we're going to take you through a journey, a journey through four key parts. First, we're going to talk about some of the key requirements that one needs to consider to design an effective security operations monitoring solution for a multi-tenant environment.

Then we'll look at the solution architecture where we'll walk you through the various pieces of the tech stack that we have used, focusing on the GenAI approach that we have used to solve this problem. Then we'll run through a quick solution demo, and following that we'll open up the code to do a detailed code walkthrough, again focusing on the GenAI tech stack components like Strands and AgentCore to show how you can implement something similar in your environment. And then finally, we'll close the session with Q&A where we look forward to answering your specific questions and your specific scenarios.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/80.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=80)

 So let's jump in. Let's get started with looking at some of the key requirements that you need to consider when you're designing a security solution for a multi-tenant environment. The first thing that you need to look at is designing for robust data ingestion at scale. What this means is you need to design a solution that can ingest and process security telemetry from multiple tenants, and this security telemetry data can range to millions of events per day. So you have to make sure that the ingestion solution meets this kind of scale.

Second, in a multi-tenant environment, the tenant context is very important because every security event needs to be analyzed in a specific tenant's context, its security posture, as well as its compliance requirements. So that is one important thing in a multi-tenant environment that needs to be considered. And then, in a multi-tenant environment you have multiple resources, so you need to clearly understand what is the relationship of the resources both within the single tenant as well as across the tenant. How are the resources interconnected?

This is important because you need to analyze if a security issue impacts a single tenant, does it propagate to the multi-tenant environment or not. And finally you need to have a differentiated approach to find out if a tenant is impacted in the multi-tenant environment, is the security issue capable of propagating across the environment. So that is another thing that you need to consider. So we have considered these three main requirements to design the prototype that we have built out here. Ashwin.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/180.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=180)

### Solution Architecture: Integrating Security Hub, OpenSearch, and Neptune Database

Thank you. So  this is the high-level architecture of the prototype. So what we have here is the design of the SaaS control plane and the data plane, and I'm going to explain how this all goes together. So the highlighted one is the data plane. So we have three tenants for the prototype, and the way we have set this up is in order to capture the alerts, the telemetry data, we're leveraging native AWS security services, which includes Security Hub, GuardDuty, and Inspector.

And we have also leveraged the OCSF for OpenSearch open source project to actually build a pipeline and ship the telemetry data from Security Hub to Security Lake and OpenSearch. And once this data is all available in OpenSearch, it's all in OCSF format, which makes it really easy to query and understand the security risks in the platform across all the tenants. And we have another path as well where we want to actually understand the relationship of the infrastructure components within each of these tenants, and that's the reason we're actually using the Neptune database.

So, would you be able to tell our audience like how we are leveraging Neptune database and what's the significance of Neptune database in this entire design? Sure, so Neptune database is a fully managed graph database solution. Like I mentioned, in the multi-tenant environment, we need to understand how resources are interconnected both within the tenant boundary as well as across the tenant boundaries in the multi-tenant landscape.

What we have done here is we have implemented AWS Config aggregator across the multiple tenant accounts, and this aggregator collects the resource information, resource identifiers, and the resource relationships, and we've implemented a backend data pipeline that reads this data from the AWS Config aggregator and it processes it and transforms it into a graph structure.

In the graph structure, the resources get transformed into graph nodes or vertices, and the relationships that the resources have with each other become the graph edges. For example, in the case of an EC2 instance, the EC2 instance itself becomes the graph node or the vertex, and its relationship to the VPC, security groups, and other components become the graph edges. That's how we designed the complete relationship map of the resources within the multi-tenant environment.

Once this data is transformed, we use Gremlin queries in batches to insert this data into the Neptune database. As a result, we're able to create a complete multi-tenant relationship map for the multi-tenant environment that we have. So now that we have all the telemetry data, we understand the relationships between the different infrastructure components within each of these tenants.

### Agent Core Runtime and Security Assist: Enabling Contextual Security Analysis

Now let's see how a SOC analyst or an admin of this infrastructure can actually analyze the threat and understand how to analyze and also plan to remediate some of these security risks. The SOC analyst first logs into the chatbot, which we call Security Assist. I'm sure Sahil is going to give a demo of what it looks like. Once they put in their chat message, for example, they would want to see the list of all the severity one security risks on tenant one or across the platform, and also understand what is important, what is significant, and which one should they prioritize and remediate first.

There are a number of questions which they can ask. Once they do that, it goes to the Agent Core runtime. Before I get here, I just want to understand how many of you are experimenting with Agent Core or are actually actively using Agent Core at the moment. Nice, I can see a few hands raised. Agent Core basically does undifferentiated heavy lifting, and it's very easy to deploy to production. It's got a number of features packed into it, including observability and memory management, and we have used some of these features for this solution prototype.

This Agent Core runtime, we have created a simple one for the purpose of this demo, and we have some tools associated with it. As you can see here, the backend tool actually queries the OpenSearch because, as I said, all this OCSF data that got ingested from Security Lake sits in OpenSearch, and we want to give our agent access to these tools. We want to give the agent access to the tenant information because it needs to understand what are the different tenants so that it can find the risks or security events associated with those tenants. Then we have a remediation tool just to give you a recommendation on the ways to remediate.

There's a lot more that you can add in terms of the tools and also in terms of how you can design this. For example, if you want to go in the path of graph or if you want to go with the path of swamp, there's a lot of ways to expand on this particular solution, and it can be used as a reference architecture. One of the most important things is that when a SOC analyst or an admin interacts with the Security Assist, they would like to ask questions which are quite contextual, wherein we need the agent to understand what was discussed in the past and respond accordingly. That's the reason we have deployed agent memory as well. We will take you through this in detail during the demo and also while I show you the code.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/540.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=540)

### Live Demo: GenAI-Powered Chatbot for Multi-Tenant Security Analysis

 Okay, so this will be a quick demo of the prototype application that we have built. Again, this is a prototype. By no means is this a production implementation reference that we're talking about here. We have kept our approach very simple. What we have designed is a single page lightweight React application, and on the backend it is getting powered by API Gateway and Lambda function.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/580.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=580)

It's very simple and straightforward. What we have done is we've implemented five tiles out here. Typically, as we mentioned, a security analyst would log into this application, and to get a sense of what is happening in the multi-tenant environment, the analyst will look at all tenants.  If you look at it, what we've configured here in the prototype application is just three tenants. With the three tenants, you can see the number of security findings that have been flagged are upwards of 4,000.

The very scale of these findings itself is something that we refer to as alert fatigue. The teams are usually not having so much time to sift through all the 4,000 findings to find out which is the alert that needs to be focused on.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/630.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=630)

In such applications, there always needs to be a mechanism to filter the most important alerts that the security team should look at first. What we have tried to do here is provide a filtering mechanism at level one where the analysts can look at what specific tenant they would want to focus on. For example, in this specific case, if the Data Plane One tenant is the production tenant which is most critical, the analyst would go and look into that specific filtering tab.  From 4,000 alerts, it comes down to upwards of 900 alerts for this specific tenant.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/650.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=650)

At the second level, what we have tried to do is filter it again by critical, high, and medium buckets. Now, supposing the security analyst needs to look at high alerts, in the second tile,  what we have done is aggregate the resources which have been flagged with the findings. We have given a number of findings without trying to overload the analyst with all the finding information in the second screen itself.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/670.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=670)

Now, supposing the analyst wants to look at why this resource, this EC2 instance, has been flagged. There are five findings for this EC2 instance.  The third tile here is what I was talking about regarding the importance of the relationship map of resources within a tenant, and this information is coming from the Neptune database, the graph information that we had captured in the Neptune database. What this highlights is that this specific EC2 instance is connected to various other resources within the tenant. We have not implemented the cross-tenant mapping, but within the tenant itself, there are other findings that have been highlighted which are getting impacted by this specific EC2 instance. For example, there's a security group here, and there is a role mapping here which has elevated privileges. Those are the kinds of things that give a clear picture of what the security analyst needs to focus on.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/720.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=720)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/740.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=740)

Moving on from here, if the security analyst wants to find out what the five findings are, the fourth tile is where we've tried to give a high-level summary of  what the issues or the findings have identified, and this data is coming from OpenSearch. This has typically been the approach of security analysts using this kind of dashboard. What we have tried to do is simplify this by giving a GenAI approach where we have created a chatbot which is powered by Bedrock and AgentCore on the backend. Here, what we've given is  an interface for the security analyst to ask questions in natural language.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/760.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=760)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/790.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=790)

Supposing here, if you want similar functionality, you can say list all tenants. This can be the primary interface for the security analyst to look at, and the same functionality that we were doing here manually, such as how many tenants are there,  the analyst can ask in this chatbot to list all the tenants. What we have done here is use HTTP protocol, so this is going to time out under 30 seconds. We didn't want to use WebSocket because we didn't want to wait for this demo, but typically you should be looking at WebSocket for this kind of application. They should come back with a response quickly. It's timed out, but I will go through some of the questions that we have  used before.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/810.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=810)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/830.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=830)

The question here is, how is the security posture of the multi-tenant system, and is there anything that needs immediate attention? The similar work that the security analyst was doing by looking at the dashboard and trying to sift through each tenant, here's a simple question: tell me, is there anything that needs immediate attention in my multi-tenant environment? Let's see how  quickly this can get back. Here, if you look at it, it tries to summarize the information, saying that there are overall 61 critical findings. It gives what are the overall critical findings that we  saw in the dashboard, and then it's trying to break it down on a per-tenant basis.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/850.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=850)

If you look at it here, Data Plane One, for our example we assumed that was a production tenant, it says that there are 29 critical findings and 233 high findings. It tries to summarize the information in an easily digestible manner and provides quick information. This is the value add here. It clearly highlights what are the resources  that need immediate attention. It says that there is an internet-exposed security group, which is a critical finding, and multiple security groups have unrestricted internet access. You see here the problem statement has been simplified to a larger extent where the security analysts can clearly focus on resources that need immediate attention.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/870.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/880.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=880)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/890.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=890)

Another question that we typically see is,  is my tenant vulnerable to any kind of DDoS attacks? Let's see how this responds.  It clearly says that yes, Data Plane One tenant has a few security groups which are open to the internet.  There's an EC2 instance that is publicly exposed without AWS Shield protection enabled. You see the value here, right? It quickly gives you the actionable items that you need to look at to fix the issues.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/930.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=930)

On the back end, the system retrieves information from Neptune to build the resource map as well as from OpenSearch where it has all the findings. It reads that data and through AgentCore it summarizes it to give you very actionable information without wasting a lot of time. 

### Code Walkthrough: Implementing Agent Core with Prompt Engineering and MCP Tools

Let me take you through the code. While Sahil was giving the demo, you would have noticed that sometimes it takes time and sometimes it's really fast. I'll walk you through how it's deployed and some of the nuances within AgentCore, including what you can do to make it actually fast and more accurate in terms of the response that you receive. When we were tuning this even for the prototype demo purposes, we iteratively had to go through a number of things when it came to the prompt.

The main challenge that we were running into was that we wanted to get a response time under 30 seconds because this is a chatbot and nobody wants to wait for more than 30 seconds. The initial approach was to look at the system prompt and make it very specific, giving exactly what is asked from the agent. Then we tried to implement few-shot prompting as well as role-based prompting where we gave an example of what exactly needs to be looked at and what are typical kinds of issues that the agent can find out and highlight quickly. For instance, we gave a highlight saying to look at the security groups and look at the S3 buckets which are publicly exposed. These are some of the steps that we used with prompting to optimize the performance.

As you can see here, this is the main deployment file, and this is how AgentCore actually deploys the agent in the AWS managed account. What we have here is the handler that's the entry point of where the call first goes once the chat request or the prompt comes in. Then you have to define the role for the agent because now there's a lot of interaction that happens. For example, it interacts with OpenSearch in order to retrieve the security information, and it also talks to our other data source which has information about the tenant because we did talk about the three tenants which I showed you in the beginning. All that information needs some rules, and this is where we define permissions and roles.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1090.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1090)

Again, there's auto ECR, which means that an image is created when you run this. This image is created in the ECR sitting in your AWS account, and once you build and deploy this, what happens is this gets pushed onto the AWS managed account and runs in a Fargate environment using containers. So this is one part of it.  There's another configuration that I have actually introduced because in my case I did not want the communication between the agent and my infrastructure to traverse via the Internet.

Here what I wanted to do was leverage the VPC to which this OpenSearch is actually deployed, and this is basically the configuration. What basically happens here is it configures such that it creates ENIs in these VPCs and makes use of these ENIs for the communication with OpenSearch. That's pretty much the network configuration, and this is optional as well.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1150.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1150)

 As I said, here is the entry point, and this is where the action actually happens. What it does here is the agent actually creates the object and returns it, populating it with all the attributes as required. For example, when the agent actually gets created, we have a few things that we need to pass. One is the model of choice. We need to keep this a bit open because not all models suit all use cases, so this is open to any model that you want to use. Here, as you can see, I have leveraged the trans agent framework, but again you could use anything that you want. There is no such limitation with AgentCore, so feel free to use what you're familiar with.

In terms of tools, as you can see, there are a list of tools that I have used, and this is an example that I created for this session just to show what else you could do.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1210.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1210)

In this example, I've got one agent with all tools, but that's not the only pattern. There are a number of  different patterns, and you have to optimize based on your requirement. In this case, let's say that you want to understand what the Cost and Usage Report shows, for example, as a SaaS administrator. You might want to optimize cost on your tenant, and in order to do that, you need to first get access to the parent payer account, and underneath you need to see how you can actually reduce cost. This is where we leveraged the MCP server for billing and for billing cost and usage report from AWS Labs. All we need to do is just add the billing MCP tools to the tool parameter of the agent.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1260.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1260)

 Here I have done dependency injection. As you can see above, the billing MCP tool has got this self.billing_service.get_mcp_tools where it's got all the definition, and once it comes up with the list of tools, it basically unpacks all the functionalities that are available within the MCP tool. Now all of this is available for the agent to query. Before the session, I actually did a deployment of this entire solution with this new capability by adding this additional feature, and I compiled it. What basically happens is this deploys, as I said, the agent to the runtime, and not just that, it also comes up with all the information.

One of the key advantages here is when it comes to performance and tuning. As you saw, sometimes there's a delay. AgentCore has this capability for observability built within it that actually shows you clearly what are the different paths through which the interactions or communication happened, and you can see the latency at each of these paths and optimize accordingly. Here it comes with the information necessary for you to tail those logs. Now, let me quickly show you an example of this.

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1360.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1370.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1380.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1380)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1390.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1390)

The first place to actually test or see once this agent is deployed, whether it's functioning properly, whether it is as per your expectation, whether you need  to tune this further, would be to go to the agent sandbox. Here I have,  since I just deployed that, you can see this demo and it comes up with the  endpoint as well.  I have some questions that I've prepared. For instance, if you want to understand, will savings plan help optimize the cost for my tenant workload? Let's run that and see what happens.

So while Ashwin is doing that, what he has done is he's extended the agent's capability by adding a tool which is calling the MCP server. Ashwin, what was our thought process? If you can just let everybody know, if you want to extend the capability, is adding a tool the approach, or are there other approaches as well? There are several approaches. As I said, AgentCore is packed with features. For now, just for the sake of simplicity, I have one agent having a lot of tools, which may or may not be the right thing to do.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1450.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1450)

For example, when you're actually using MCP server, you can also convert your existing Lambda functions to be an MCP server.  There's a gateway available as part of AgentCore. You could leverage gateway, and gateway has got a lot more things because it keeps track of all the tools that are available for the agent, and it provides, it gives, it lets the LLM pick the right tool for the right prompt. For example, here in this case, it used the MCP server, and what it had to do was it had to talk all the way to the payer account to understand which tenant we're talking about, and then it came up with this detail.

Again, for now it's used two tools. For example, it used the MCP server and it also used the tool on tenant information. You can now have, if you have to optimize this, or if the solution has to be evolved to fit your need, you need to take into consideration what would be the right pattern. Should I have a graph? Should I have specialized agents? Should I have a supervisor, and then have specialized agents for different tasks?

All of this would finally help optimize the performance. The most important thing, at least what I observed during my time building this prototype, was prompt engineering. The system prompt has to be accurate. The system prompt has to be as close as possible to how you want this agent to respond. So in this case, what it basically says is it's asking you, when you mentioned tenant 2, are you referring to one of the specific AWS accounts? So it's asking us back the question, and maybe we could rephrase this question.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1590.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1600.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1600)

While this is running, I'll also show you how I have actually configured this. So I wanted everything to be configured,  everything to be easily configured and changeable as well, because I wanted the flexibility when it comes to my model, when it comes to  adding more details to my MCP server, or if I want to, for example, for memory as well. Now I'm using, I wanted to use just one single memory, and I wanted to use that memory across all the sessions because this is just a demo and that would serve the purpose, and I wanted it to have the context of all the previous discussion.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1660.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1660)

So I initially created the memory here, and I have all the configuration associated with the memory. The memory strategy, as you can see, it has a flat structure. It has the namespace pattern, and so it tracks all sessions for the actor. Here the actor is basically the user ID with which Sahil logged into the demo portal, and these are the environment variables required for the billing MCP server to work. And these are the OpenSearch settings, which are required for the agent to talk to the backend. And model, as I  said, the model should be something that you should have the flexibility to switch between models and see which one works for your specific use case. So again, everything is configured using the config here. I think it's still running.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1680.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1680)

### Performance Optimization, Future Evolution, and Q&A Session

Yeah, just to put in context  the evolution of this solution that we were working towards. Right now we're doing with this single agent solution basically threat detection. The evolution that we are working towards was threat remediation as well. So we were thinking of building another agent which can take the input saying that the issue that was identified, for example, a security group providing open Internet access. A second agent in a multi-agent architecture that we were thinking as an evolution would take input specifically with bounded privileges, seeing that we can instruct the agent to go fix the security group according to the best practices in a specific tenant account, and that is a multi-agent architecture that we were thinking in terms of evolution of the solution.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1720.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1720)

 Yeah, so it did take a lot of time. As Sahil said, while we were doing this demo, we were contemplating whether we should be using WebSocket or HTTP because sometimes it makes sense for the delay in the response. In some cases you want it really fast depending on what the use case is. Now it has come up with more detail because what it wanted to do was it wanted to reason out and it wanted to clarify, hey, you're talking about some tenant, which tenant are you talking about? Then I said, yeah, this is the tenant I'm talking about. Now it's come back and it said, does it make sense for me to have a savings plan or not? So this is one way of actually using this.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1790.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1790)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1800.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1800)

And another feature that I actively used when I was actually developing the prototype was the observability. So as you can see, the observability basically, you know that  I recently launched the VPC demo agent, so you can see that there are a number of traces associated with this, and you have the information regarding  the traces and the session details. So this actually gives me a way to look at, hey, how do I actually optimize this?

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1810.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1820.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1820)

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1830.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1830)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1840.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1840)

We used this approach when the response was taking more than 30 seconds.  When we looked at the traces, we saw that it was being very chatty, and that's where we tried to prompt it accordingly so that it knows what exactly is being asked for, and the response was able to get under 30 seconds.  This basically tells me how long it took, which is quite a bit of time, and we can start diving deep into what this actually means.  It gives a breakdown of all these calls and where this chattiness happened, and then based on this, I have a lot of information here.  Based off this, I can start to troubleshoot and optimize the performance. I found this pretty useful while developing the overall model.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1860.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1840adb8d790aa05/1870.jpg)](https://www.youtube.com/watch?v=DZ6MMauVrZg&t=1870)

We designed that as a control table, which is a mapping of all the tenants. In typical SaaS environments, in multi-tenant environments, you'll have a tenant ID, the tenant account number, tenant priority, whether it's production, non-production, and things like that.  That is the control table, and that's what this tool is referring to.  What I did here was I gave one of the tools called list_tenants. What it has is tenant information, and when I say tenant information, it has the account ID, the name, and whatever additional metadata you want to add to that. If you provide this, now the LLM knows where to go for this information. That's how it was actually able to relate in the example that I showed you. When I said tenant 2, it actually understood what I meant.

Do you use the reasoning and acting mode? Yes, reasoning and acting. Okay, cool, thank you. How is the Neptune graph database updated? Is the agent doing that updating too, or how is that being handled? No, that's a backend pipeline that we've set up. The AWS Config aggregator reads the information continuously, and there's a backend pipeline that runs every 15 minutes. It reads the information from the Config aggregator and uses Gremlin queries to insert that into Neptune. It's not exactly real time, but we do it every 15 minutes. The backend pipeline information is there in the Git repo, so you can have a look at it.

You showed one error message in the multi-tenant demo when we were running that, right? There was some observability that some of the ports were open. Where is that information coming from? Is it from CloudWatch? Because you've enabled the observability, you will see that error message in CloudWatch, and you can also view that from within the agent. Are you talking about a specific error message or the message where the ports were open for the security group? That's a finding. That's a finding that has been highlighted from the initial architecture that we showed. We configured Security Hub, GuardDuty, and Inspector. Those are the services that are monitoring the specific AWS accounts. The findings are flagged from there. Those are the findings that get into OpenSearch after the processing of OCSF format. I thought you were talking about the agent. Thank you all. Thank you for your time.


----

; This article is entirely auto-generated using Amazon Bedrock.
