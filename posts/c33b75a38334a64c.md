---
title: 'AWS re:Invent 2025 - Reimagining large-scale migration planning with agentic AI (MAM226)'
published: true
description: 'In this video, Dr. Tushar Saxena and Dr. Jonathan Shapiro-Ward from AWS Transform demonstrate how agentic AI revolutionizes large-scale migration planning. They explain the importance of defining migration goals and metrics before starting, then showcase AWS Transform for Migrationsâ€”a service that automates discovery, TCO assessments, dependency analysis, and wave planning. The live demo illustrates how the system ingests diverse data sources (RVTools files, meeting notes, wiki pages) and uses AI agents to identify applications, analyze dependencies, and generate executable migration plans through natural language interaction. They emphasize treating migrations as agile projects and highlight how AWS Transform reduces planning time from months to hours while eliminating manual spreadsheet work.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/0.jpg'
series: ''
canonical_url: null
id: 3087509
date: '2025-12-05T20:33:36Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Reimagining large-scale migration planning with agentic AI (MAM226)**

> In this video, Dr. Tushar Saxena and Dr. Jonathan Shapiro-Ward from AWS Transform demonstrate how agentic AI revolutionizes large-scale migration planning. They explain the importance of defining migration goals and metrics before starting, then showcase AWS Transform for Migrationsâ€”a service that automates discovery, TCO assessments, dependency analysis, and wave planning. The live demo illustrates how the system ingests diverse data sources (RVTools files, meeting notes, wiki pages) and uses AI agents to identify applications, analyze dependencies, and generate executable migration plans through natural language interaction. They emphasize treating migrations as agile projects and highlight how AWS Transform reduces planning time from months to hours while eliminating manual spreadsheet work.

{% youtube https://www.youtube.com/watch?v=8PBNaDa0TUw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/0.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=0)

### Reimagining Large-Scale Migration Planning with Agentic AI

 Hello. This is the "Reimagining Large-Scale Migration Planning with Agentic AI" session, so hopefully you're all here for that. So what do we mean by reimagining migration planning? Migration planning has traditionally been a very manual process. Migrations take a long time, there's a lot of information in various places, and you create large teams to share information with each other, write scripts, and figure out in Excel spreadsheets how to do migrations. But with the advent of generative AI and agentic AI assistance, things are changing quite a bit. The assistant, the human being that you used to interact with to figure all these things out, can now be an agent. So you could use agentic AI to do your migration planning a lot more seamlessly, and that's what we're going to talk about here today.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/90.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=90)

My name is Dr. Tushar Saxena. I lead the AWS Transform migration service team. And my name is Dr. Jonathan Shapiro-Ward. I lead discovery, assessments, and planning as part of AWS Transform for migrations. So our agenda today, we're going to talk a little bit about why you want to migrate. That's an important piece before you begin a migrationâ€”you need to know why you want to migrate. Then we're going to talk about migration planning itself.  When I say migration planning, you might have a 5,000 or 10,000 server on-premises estate with all kinds of complex network technologies and 600 to 800 applications. You need a solid plan before you embark on migration. So how do you develop that plan with agents? We're going to talk about that.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/130.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=130)

Then Jonathan's going to talk about AWS Transform, which is our new agentic AI service to help you not only create effective plans but also help you migrate all the way end to end, from discovery all the way to rehosting. And then we're going to talk about some best practices for accelerating your migration.  Finding out your why before you even start your migration is important. You need to know why it is that you want to migrate. If you're not absolutely sure about your motivation for migration, more than likely your migration is going to fail. Not because something went wrong with the migration itself, but because you don't know where you're headed before you embark on the journey.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/160.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=160)

### Finding Your Why: Defining Migration Motivations, Metrics, and Constraints

There are many reasons why you might want to migrate. As we've worked with customers over the last decade on migrations, cost comes up a lot.  Customers want to reduce the operating cost of their workloads, and so they want to migrate to the cloud. That could be one reason. There might be some temporal reasons as well. You might have a data center exit you want to execute within the next six months. There might be some licenses that are expiring in the next few months as well, which might create urgency for you to migrate. Or you might just want to have the capability to innovate a lot more. Having a legacy system on premises has its own technical debt and it prevents you from innovating with the latest technologies such as generative AI. So you might want to move to the cloud so you can innovate faster and quicker as well.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/230.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=230)

There are many different reasons why you might want to migrate, and oftentimes it's not one or two reasonsâ€”it might be a bunch of reasons why you want to migrate. So you want to be sure upfront about why you want to migrate, and you want to have buy-in from your leadership as well as from your colleagues within the company to ensure alignment on the reasons for why you want to migrate to the cloud.  Once you know why you want to migrate, it's important to also know that most of the whys can be tied to some kind of metric. So you want to be able to define metrics that can tell you how successful you are being at migration. Migrations can take a long time. Priorities can change. It can take months, and hopefully with agentic AI, weeks. But even so, as the migration is proceeding, you want to make sure you have metrics that tell you whether or not you're on the right path and whether you're being successful or not. So these metrics are going to be important.

There are many different types of metrics. Cost savings, as I mentioned, might be an important metric to track. So as you are developing a migration plan, you might want to weigh how your costs would change if you were to do this plan versus that plan. If I were to use these EC2 instances to host my applications versus that, how would my cost change? You might want to lower the time to market. Say you have competition that's coming up with innovative services and you want to move to the cloud quickly so you have a market mover advantage. So there might be timeline reasons why you want to move, and you might want to measure how much time to market you want to lower as you move to the cloud. Or you might want to increase the amount of staff that is working on innovative things.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/330.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=330)

Once you move to the cloud, there are fewer staff members you need to focus on infrastructure and non-innovative work because the cloud can take care of that. You might want to increase the percentage of staff focusing on innovation as well. There can be many different reasons, but it's important to have these metrics defined upfront. 

Most importantly, does your leadership understand why you want to migrate? Most successful migrations require strong buy-in from leadership. This is an important move for a company. You're taking your on-premises workloads over to the cloud, and almost everyone in the company will be impacted by it. You want to make sure not only that you have buy-in from leadership, but also from all of your business units and colleagues as well.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/370.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=370)

These are the ingredients of a successful migration. Leadership buy-in is the first precondition, but it alone won't make the migration successful.  There are constraints as well. If migrations have too few constraints, they can fail. For example, if you're not time-bounding your migration, it can go on for a long time. Jonathan likes to remind me of Parkinson's rule, which states that if you increase the time, work comes in to fill up that time as well. You'll have people doing busy work that's not contributing to your migration.

You want to make sure you're aware of what the constraints are within which you want to execute the migration. Once you have the why, once you have the metrics, and once you have the constraints with which you want to migrate, as we get into agentic AI migration planning, these are non-structured, human-understandable concepts that you want to feed into the agent. The good news is that with generative AI and agentic AI, these concepts of why you want to migrate, what metrics you want to target, and what your constraints are can be interpreted by the agent and incorporated into the migration plans that it proposes to you. You can iterate on that plan as well.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/470.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=470)

### The Four Pillars of Migration Planning: Discovery, Assessment, Wave Planning, and Agile Execution

With that, I'm going to hand it over to Jonathan to talk about migration planning. Those are the preconditions for your plan to succeed, but ultimately you need a plan. The plan is fundamental for your migration to be successful.  It sets the timeline, the outcomes, and the goals that you are seeking, and helps your migration stay on track.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/500.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=500)

We're going to talk about the general principles of building a plan, and then we're going to go deeper into how you can accelerate building a plan with AI agents and specifically with AWS Transform for migrations. At a general high level, the basics of building a migration plan consist of roughly four high-level steps.  To begin with, you need ground truth. You need discovery data, ultimately. That means uncovering information from your on-premises environment. This includes a combination of both technical and non-technical data, which serves as the information that feeds the rest of your migration plan.

Once you have an initial set of discovery data, it then becomes a case of assessments. Assessment can mean many things, but typically an assessment means a TCO assessment, a total cost of ownership assessment. This ultimately provides the financial basis for your migration, working out what the current state is on-premises and what the cost will be on AWS, not just the pure cost, but the optimized cost. This ensures that you are managing your costs and realizing cost savings as part of the migration. That typically serves as the basis for the overall financial model and one of the key metrics for success for the migration.

Once you have the basics of your assessment, it then becomes a case of wave planning, which will be a topic we'll get into throughout this talk. Wave planning, put simply, is the process of taking everything that you have on-premises that's in scope for migration, analyzing it, and then ordering it into discrete groups that can be moved independently and executed. It serves as the basis for your migration execution. Lastly, there is agile planning, because ultimately, something that I really want to stress and that will come up a few times in this talk is that a migration is an agile project. You should treat your migration as you would any software engineering project and apply the same tenets and principles that you would to

the majority of software projects. Think agilely and consider how to enact your wave as an agile project, which involves setting up all of the ceremonies, managing your roadmap, your backlog, doing retrospectives, and thinking about how to employ the mechanisms that will keep your migration execution on track. Critically, this is not a one and done process; it is a cyclical process.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/630.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=630)

When you start with discovery, that is uncovering some initial information. Often we see customers start with something like an RVTools file. If you're not familiar, that is a basic export from a vSphere VMware environment that contains basic inventory, or it could be something like a simple CMDB, change management database export. This provides enough information to get a basic assessment going and to do some basic planning. 

However, typically as your migration progresses, maybe you start doing a few proof of concepts or move one or two simple POC waves, you start to need greater detail to do more detailed planning and to get more detailed understanding. Realistically, non-trivial applications are, by definition, complicated. They have complex dependencies, complex communication patterns, and complex RTO and RPO requirements. To understand that, you need more data. So you go back to discovery, getting more data, and then as you get more data, you have more data to fuel your different assessment types with your TCO and other assessment types.

You are fueling that information to your wave planning, which then feeds into your agile plan. You update the stories that you are working from, you change your estimates, and so forth. At a high level, that is the general process. I will go into that process in a little bit more detail, and then we will talk about how you can use AWS Transform to accelerate the execution of this process. This process that we see here is something that customers are doing currently in months and years if they are doing it manually. Very commonly we see this done as a manual program involving consultants, program managers, and engineers executing this at scale. What we want to investigate here is how can we make this go faster and how can we get the time to value for your migration as down as possible.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/760.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=760)

### TCO Assessments, Right-Sizing, and the Challenge of Dependency Analysis

Before we get onto that, let us go a bit deeper and understand these steps in a little bit more detail.  I have mentioned assessments, and I have dwelled a little bit on TCO assessments. That is typically the absolute essential assessment type because without having a good understanding of your costs, there is a major risk to your migration. After the first risk, which is not understanding your why, your reasons for migrating, lack of cost management is probably the most common reason we see for a migration getting derailed.

When we see costs exceed expectations or costs are not managed and there is no baseline expectation, that creates risk for the migration. Performing an assessment to understand those costs and then to track them throughout the migration is indispensable. It is an essential component of every migration. However, there are other assessment types as well that are important, and these typically vary by customer to customer and business to business. Many customers need some form of specialized assessment. For example, we typically see large enterprises often have some sort of compliance need or some security review. These are assessments that are not entirely related to the TCO and may have no cost component at all.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/880.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=880)

Often these are done with an AWS partner or AWS Professional Services, and they help provide additional supplementary insight into your current disposition on premises and inform your migration strategy. The third common bucket of assessment type is what I call a people and process assessment, where you want to analyze your current organizational strategy. You want to look at potentially things like your change management process, your finance, your operating models, analyze those factors, and figure out how to transform them for better operating on the cloud. You may need all of these. You may need a TCO assessment, but it depends on your particular challenges that you face on premises. 

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/890.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=890)

One thing that I do want to emphasize is that the TCO assessment is really essential for understanding costs.  Being able to track those costs and reduce the costs are particularly important. That reduced cost angle is one of the things that is really important for that cost tracking.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/910.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=910)

On-premises workloads are typically heavily overprovisioned.  What we observe most commonly is that on-premises machines are provisioned for peak demand. We see this most commonly in places that have any sort of seasonality in their workloads. Most significantly, you can see this in the retail sector where you provision infrastructure for peak utilization. You think about your Black Friday demand or your Christmas sales, and you're provisioning your infrastructure to meet those peak demands.

Now if you were to simply migrate that infrastructure as is to AWS, what you'll typically find is that your costs are in excess of what would be desirable. Instead, the best practice is to right-size. Right-sizing involves matching your infrastructure to the demand curve of your actual utilization. If you are only needing 200 cores of CPU compute, you size that to the appropriate number of virtual machines, EC2 instances, Lambda functions, or what have you. Then if you have seasonality, you use the inherent elasticity of the cloud through things like auto scaling groups to manage that demand and meet the demand curve as it fluctuates.

You can do this manually, but it is particularly challenging to do so. Typically that involves a lot of manual analysis and spreadsheets. Being able to do assessments in an automated way is particularly valuable. We'll talk about exactly how to do that in just a moment. Once you've done the assessment, something that typically straddles the line between the assessment and planning stage is dependency analysis.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1010.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1010)

This slide is moderately difficult to interpret.  It's very hard to make out what's going on there, and that is somewhat the point of this slide. What this is, is a visualization of a TCP trace from a semi-real environment where we're looking at the communication of different machines within the environment. It's very hard to make heads or tails of this, but understanding the communication patterns and the dependencies of everything in the environment is incredibly important.

Ultimately, what you don't want to do is break apart some of that communication and then find that your critical application, your mission-critical system no longer works. Understanding these dependencies is essential, but trying to do this manually by hand is a huge challenge. I've seen customers try to do this in spreadsheets through manual analysis, and it is possible through graph analysis using all sorts of manual tools. However, it is a huge time-consuming slog and it's error-prone. You might miss a dependency, risk wave failing to complete, and then you have to roll back.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1090.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1090)

### From Applications to Move Groups: Building and Prioritizing Migration Waves

This is a huge opportunity for generative AI to help us with. We can use large language models to solve this problem instead of doing it manually. Nevertheless, dependency analysis is a critical step which powers the migration wave plan. Once you're doing your dependency analysis, what you're attempting to do is then identify applications within your environment.  Now those applications might be predefined. You may have a good CMDB, you might have a good application inventory. That does happen, but very commonly what we'll see is that customers have had many generations of technology investment.

Some of their applications could be decades or more old, and being able to actually get a full handle on what's in the on-premises environment can be a challenge. Things like CMDBs or wiki pages or logs could be significantly out of date. So actually developing that application portfolio and getting a full handle on what's in the environment is a step that needs to be done. You can't just trust the data that's available. So the first step is building that application portfolio and understanding these are my applications.

By applications, I don't inherently mean a single application. It often is simply an app, but really what I'm meaning is a unit of business value, something that underpins a business function. Typically that's an application, but whatever it is, that is the unit of commonality that you care about in your environment. Once you've established that, which we'll call an application for simplicity's sake, the question then becomes what are the dependencies between those applications. That's where your dependency analysis starts to feed into this.

What I want to know is that my application over here has a shared database with my application over there. I can't move these applications independently because they both have dependencies on stored procedures in this database. If I move them apart, something will break and critical business functionality will go away. So I need to move those together. Well then, that database has a backup over there and it's got a replica over here, so I need to understand the full extent and the boundaries of my system such that I can understand what has to be moved together.

Likewise, when I'm trying to resolve these dependencies, there's also the question of what doesn't have to be moved together.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1250.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1250)

For example, database dependencies are critical. However, something like an HTTP dependency or a web API is typically more latency tolerant and doesn't have to be as tightly coupled as a database. Often you can split apart a web app, and that can go over a VPN or a direct connect between on-premises and AWS, so it's more tolerant. Understanding the limits of your application and those dependencies is the next important step. Then lastly, you take your application groups, typically what we refer to as the dependency groups or move groups, and order them into waves. That is ultimately  the wave planning process.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1300.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1300)

To clarify quickly on terminology, I used some terms there. Applications are units of business value, whatever you want to call them. Ultimately, those are the components that you care about and the things you want to move from on-premises to AWS. You then have all of the communications between your apps that depend on each other, such as database dependencies or web app dependencies. That's what we call the move group or the dependency group. I'll be using these terms in the demos, and they're useful to keep in mind. Then lastly is the concept of a wave, which is simply a prioritized, ordered list of dependency groups or move groups. It's taking from my applications to my move groups, and move groups to waves. Put simply, or more visually at least,  this is what that looks like.

You start off with servers, I'll say for the sake of simplicity, but it could be servers, networks, or storage arraysâ€”the underlying physical or virtual infrastructure that underpins your apps. It's then a matter of identifying how this infrastructure underpins your apps and identifying what composes an individual application out of that set. Once you've defined your apps, you determine which apps have dependencies on one another, such as shared databases. Once you've identified each of those move groups, you can see the colors correspond between move groups and waves. You then take those move groups and group them together. For example, you might want to move ten move groups in one wave or one move group in one wave.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1370.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1370)

As you gain confidence in your migration, you start off with, say, one move group of ten servers for your first wave as you dip your toes in the water. Then as you advance and get to your nth wave, you might want to be moving up to say 150 servers per wave or ten move groups per wave, getting larger and more ambitious. This feeds into  the plan for your migration. You end up with something that looks like this, effectively visualized in a Gantt chart form, where you have an overall set of weeks. You can imagine this is sprints effectively, and then you have the tasks that correspond to each wave being executed in series. You are thinking about how to go from your infrastructure to the wave plan and then to an execution of that wave plan.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1410.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1410)

Something worth thinking about, just giving some guidance here, is that as you do that planning, you want to think about your why and the reasoning for your migration. This is  typically how you put together your wave plan if you're thinking about how to achieve your objectives. Think about your why and your reasoning. Generally, I start off with something that shows initial value. I want to start off my migration plan by demonstrating value, because recall that the migration plan is not a one-time effortâ€”it's a perpetually moving and evolving plan. I want to start off my plan, execute some of it, and show value to my leadership to demonstrate that my migration plan is actually feasible and indeed works.

Typically, you start off with some small but meaningful applications. You don't typically want to start off with purely dev, test, or throwaway applications, because that doesn't necessarily prove anything. Anybody can move a server that doesn't have any users or an application that has no data. You want something that is small but actually has meaningful business value to it. Ideally, those are applications that have fewer dependencies, so if your dependency analysis did miss something, you're not going to break anything. And ideally, something that's maybe more latency tolerant or can tolerate some more downtime, so lower RTO and RPO or higher RTO and RPO requirements.

Then, as you gain confidence in your migration and start to prove its validity, you start to get into more complex apps, generally getting into the weightier parts of your application portfolio. Before then, as you really get moving in earnest, you get onto those core mission-critical appsâ€”the things with low RTO and RPO, the things that are more latency sensitive, and those with more technical limitations.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1520.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1520)

These should be addressed as you progress until you have completed your migration, evolving your migration plan as you gain insight and learning from each subsequent wave. You always go back and change the waves as you learn, and that is key  to understand because ultimately, as I have said and will emphasize, migrations should be treated as an agile project. Something I have seen many customers do is get into the thinking that a migration should be a waterfall project, that they are moving on-premises to AWS with a number of steps executed in serial as a linear project plan. But absolutely, that is a good way to run into challenges because you need to be able to evolve your plan as things change and as you gain new information. It is important to think of this as a cyclical agile process.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1560.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1560)

### Introducing AWS Transform: Generative AI-Powered Migration Assessments

With that in mind, one of the biggest challenges that comes with migration planning being an agile project is  that this is how we got used to planning projectsâ€”ultimately through staring at spreadsheets, Gantt charts, Jira boards, and so forth. The headache here is that it is so much manual work to track this. Think about that slide with all of the network dependencies, tracking that by hand, breaking that down, and creating different tabs in a spreadsheet. I have literally seen 50-tab spreadsheets being used for large migration planning. What is worse is that you end up emailing them around, so you end up with John's version 1, Susan's version 2, and so forth. Version tracking and maintaining all of that becomes a huge headache.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1610.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1610)

In the era of generative AI, we can absolutely do better to plan and execute large migrations. That is where we get to AWS Transform for migrations.  Just as a quick show of hands, who is familiar with AWS Transform? Has anyone had an opportunity to play with it or explore it? It is a relatively limited show of hands. Hopefully by the end of this talk, you will all be familiar with AWS Transform and you will be inspired to go off and experiment with it because you will have seen hopefully a lot of the announcements that came yesterdayâ€”a huge number of launches of AWS Transform, our best-in-class generative AI capabilities for accelerating migration and modernization.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1650.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1650)

AWS Transform is an agentic AI service that encompasses a wide number of  capabilities. It includes migrations, which is going to be the thrust of what I am going to be talking about, but it also includes full stack Windows modernization, so being able to transform Windows, .NET, and SQL Server applications. It also includes mainframe modernization, so decomposing COBOL applications and moving them to Java, and then also launched yesterday is custom transformations, so the ability to do almost arbitrary transformations, things like programming language upgrades and framework upgrades using custom transformations and database transform. I would highly encourage you to check out all of these suite of capabilities that really help you both with the migration side of things, but then also once you are on AWS to modernize your environment.

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1700.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1700)

Where I am going to focus is really on the migration side, starting  with AWS Transform Assessments. AWS Transform Assessments is a generative AI capability for generating a TCO assessment. Has anybody ever done a Migration Evaluator assessment or an OLA assessment? Does that ring a bell with anybody? Perhaps not. These are ways that AWS has had for a number of years now, and they are capabilities that are human-led programs for building migration assessments. They give a detailed assessment, but they are ultimately led by humans and take typically a week or so to get a result. AWS Transform uses agentic AI to automatically build an assessment, a TCO assessment.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1750.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1750)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1800.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1800)

What it does under the hood is generate  code and programmatically perform the assessment and then generate you an assessment output in PowerPoint, PDF, and Excel format. You can then take them to other tools, analyze them, and so forth. It is the fastest way in AWS to get a migration assessment, actually the fastest way pretty much anywhere to get a migration assessment. It currently covers any x86 compute, attached storage, object storage on-premises to S3, and SAN storage to FSX. We have a broad set of capabilities there, and we are expanding and adding more and more assessment capabilities. We can have a quick demo of what this looks like. This is the AWS Transform console. 

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1810.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1820.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1820)

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1830.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1830)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1840.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1850.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1860.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1860)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1880.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1880)

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1890.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1890)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1900.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1900)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1910.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1920.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1920)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1930.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1930)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1940.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1950.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1950)

I've created an assessment job that's asking me to upload data. I'm going to use an  RVTools file. It accepts a variety of different formats, but an RVTools file is a nice and simple format that everybody can get very early in the migration journey.  I'm specifying the region I'm going to be in, and I'm going to choose Oregon. Then I'm going to generate an assessment. I'm going to skip ahead a little bit in this recording, but it takes about 10 minutes to generate the assessment programmatically under the hood.  After we wait a few minutes for it to generate, I'm going to click and then click review.  What this is showing here in the right-hand pane is the assessment result. This shows in the console the insights, and you can see the compute, the storage, the annualized cost, the cost with reserved instances, and it goes through all the assumptions that I've generated.   It goes into the detail about how it was calculated. Something that's piqued my interest here is that it's using dedicated hosts. I'm wondering why it used dedicated hosts, as that's not super common. So I'm just going to ask the chat why it recommended dedicated hosts.  What it's going to do then is use a knowledge base built out of all the assessment data, the data that you provided, and it will go through the math to figure out why it recommended dedicated hosts.  What it's going to explain is that you had a set of Windows licenses that looked like you could optimize the costs using dedicated hosts, and then it did the math to bin pack those Windows virtual machines onto a dedicated host to get you the best cost effectiveness.  That's what it's explaining there. You can ask it a number of different types of questions. It's explaining the BYOL, bring your own license, math that's there.  Once you've done that, you can look at the exports that it generates.  This is the same data, exactly the same thing, just in PowerPoint format. I'm just skipping through it very quickly here, but the same numbers and math are there. If you want to show that to your stakeholders or to your leadership, you've got nice collateral generated there that's easy to take.  You also have a number of other different formats. The other one that's most useful is spreadsheet outputs. You can take all the math, throw it into other tools, visualize it, do whatever you want with it, so you can see all the raw assessment data.  And then lastly is PDF.  The last step that it gives you is that you can then transition to migration execution. What you just saw there that went by quite quickly is that you're now in a welcome to AWS Transform trials forum where it says I can help you migrate to EC2. What just happened there is it takes the data from the assessment and then populates it into a migration job. So then you can go into more detailed planning and do all the dependency analysis and migration planning that I'll show you next, but it just nicely feeds through to the next step.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/1980.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=1980)

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2000.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2000)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2030.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2030)

### AWS Transform for Migrations: Discovery Tools and Data Ingestion

So what comes in the next step is AWS Transform for Migrations. Yesterday on Monday, we launched a huge series of new capabilities for AWS Transform for Migrations. If you've ever used AWS Transform previously, we've hugely overhauled and completely replaced the existing migrations experience to add brand new capabilities that we are very excited by.   This is what the AWS Transform for Migrations flow looks like. It starts off, and perhaps unsurprisingly, this correlates quite closely to what I've described earlier. Indeed, it was built that way. You start off with discovery, taking the on-premises data, you go through migration planning, and as the focus of this talk is migration planning, I won't get super detailed into the later steps, but you have network migration, so you can take your on-premises networks and move them to AWS, and ultimately execute your migration.  This is a bit more of a detailed explanation of what each step does in the AWS Transform for Migrations capabilities. Something that's particularly novel and isn't going to be a focus of this session, but I'd encourage you to look into is the network migration piece. Quite commonly, you have to migrate a network from on-premises to AWS. You're not building brand new networks; you're wanting to move your existing applications and your existing networks over. What we can do is take existing networks on-premises via VMware NSX, through basic RVTools networking that includes vSwitch information, through Fortigate, Palo Alto, and through Cisco ACI. We can take exports in those formats, analyze them, and then generate VPCs, subnets, security groups, route tables, transit gateways, and all of that infrastructure from the on-premises network. Rather than having to manually translate all of those networks, you get that done for you with agentic AI, which is super powerful. You can also edit those CDK, generate CDK, CloudFormation, and Terraform. You can edit those manually. You can pipe them into something like AWS Developer or into Kiiro, for example, and further modify them.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2110.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2110)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2170.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2170)

Now let's look at the capabilities in AWS Transform for migration and see how they can help accelerate your migration plan.  We'll start with discovery.  Here are some examples of discovery data that you will typically want to use. The best practice is to use a combination of different data sources. You don't want to rely purely on technical sources. Instead, you want a combination of technical and business sources because ultimately you need to understand the why and the meaning behind your environment. I might understand that these are servers communicating on port 80 and port 443, but why do these servers exist? What function are they underpinning? What's their criticality? How should I reason about them as I go through my plan? That business context and business logic that informs why they exist is important to incorporate, and we can take that through formats like CMDB data, meeting notes, or wiki page dumpsâ€”anything that gives that kind of context we can adjust.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2210.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2210)

What this looks like is that you have an upload pane in AWS Transform where you can throw data into, and it's literally that simple. You take your data, throw it in there in a wide variety of formats, and we will ingest that data. One thing I want to highlight is that getting technical data can be challenging. Obtaining things like process data, inventory data, and networking data may rely on third-party tools or a combination of different tools. We've made that simple by creating the AWS Transform Discovery Tool. This is a network-based collection tool installed as a virtual machine into your on-premises environment.  It then collects using a variety of protocols like SNMP, SSH, and WinRM data from your environment and maintains it centrally, then outputs a file that you can upload to AWS Transform. Critically, that file is independent of any network traffic, so you can run it in a completely isolated network environment. No route or communication is necessary back to AWS, so it can run in isolated environments.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2240.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2250.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2260.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2270.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2270)

Let's look at the Discovery Tool very quickly.  This is the Discovery Tool running on-premises in a local environment, and this is its local web UI.  It's a very simple UI, and what it shows here is the data that I'm giving it.  I'm providing it with a different set of SNMP credentials and SSH credentials that it's using to pull from the environment. This is just a test lab that it's running in, but what it's doing is pulling from this environment approximately 1,000 servers.  You can see that it only collects database data from Windows SQL Server right now, so that's not supported there, but you can see the majority of servers it's pulling network connections from. It has a few errors that I need to resolve, but it reports that information and then gives you an inventory file that you can download and then throw into AWS Transform.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2290.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2290)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2300.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2300)

The process of setting up this by deploying a VM and then downloading the file takes about an hour end to end to get basic information.  Here I have a spreadsheet filled with rich network data and process data.  I get process-to-process information, so I understand for example that I have Curl running on this server communicating over HTTP to another server where HTTPD is running. This gives me a rich network trace to inform my dependency analysis, and this is available without charge, as indeed all of the AWS Transform migration experience is. It's a very powerful tool for getting data in.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2330.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2330)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2340.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2340)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2360.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2370.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2370)

Once I have data either from the AWS Transform Discovery Tool or from alternative sources, it then becomes a matter of getting into AWS Transform.  What I'm looking at here is the beginning of an AWS Transform job.  This is a migration job, and for the purpose of this session, I'm going to do migration and planning, specifically discovery and planning. I can do an end-to-end job or just network migration, a whole host of different permutations, and you can add and remove job steps at any time.  I'm going to throw in a whole bunch of information here. These are my two steps: import of inventory and then build migration plan. I'm saying it looks good, and it's going to create the job plan, which you can see on the left-hand side.  What it's now going to move forward into is awaiting user input, so it's awaiting my data. It takes a second or so, and now I can upload data using this file upload. I'm going to click that file upload, and then there is the upload dialog box. What I'm doing is taking all of these files that I have sitting on my desktop and uploading them. We'll get into what those files are in a second, but they're just a whole random collection of files.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2410.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2420.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2430.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2430)

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2440.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2440)

 We'll jump onto that in just a second, but I told it to look good and go forward with it. What's going to happen is that a combination of traditional parsers  and AI agents are going to analyze all this data. You can see we have some data from the Discovery tool here, an RVTools file, which is arbitrary CMDB-style data  mapping apps to cost centers and users. This is a file that's a Zoom meeting recording with a whole bunch of people chatting about what their migration is and what they're going to doâ€”completely unstructured. This is an export from a wiki that I put into markdown format, and I'm just throwing it up there and asking the AI agents to analyze this data and make sense of it all.  I don't have to mix formats and manage spreadsheets. I just ask the AI agents to figure it out, and that's what they do.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2450.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2450)

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2460.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2460)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2470.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2480.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2480)

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2490.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2490)

You can see once I've uploaded a quick summary, I have about 1,300 servers from this import.  This is my work on my operating system. This is my app catalog that's been figured out. These are my common network portsâ€”I can see a lot of SSH collection, connections, I can see MongoDB in there, HTTP, and I can identify some apps. It gives me a view of all of my applications.  From those meeting notes, it's figured out a bunch of apps that are in focus. From the Discovery tool, it has all the network connections and process data, and there's my raw server list that I can get and export if I want.  Literally, I've built up very quickly a pretty detailed view of my environment from just arbitrary files that are pretty easy to get.  Once I have that, I can then start the planning process, and I can come back to this at any time. But let's move on to the next step, which is planning.  You can see now I've moved on a little bit, I've added some additional data, and I've skipped to building migration plan. What I'm doing here is trying to build up my app catalog and understand what apps exist in this environment.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2530.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2540.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2550.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2560.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2560)

### Live Demo: Chat-Based Migration Planning from App Grouping to Wave Generation

What I'm doing here is looking at my inventory as it exists right now, and it's giving me similar to what we just saw, but a little bit more detail with additional servers.  I've noticed somethingâ€”for example, I uploaded a bunch of data that had a whole bunch of powered-up servers because the data was messy. Now, the agent didn't want to just remove those servers because that could actually be important to me. But what I'm going to say is, here's why I want to get rid of some of this dataâ€”it's showing me questions you should think about.  For example, should I get rid of powered-off servers? Should I normalize the data? What I'm saying is those powered-up servers are just messy data. Let's get rid of that.  So it's going to clean up that data for me, and then it says I've gone down from 5,000 servers to around 1,200 servers, filtered it all down and cleaned up my environment. You can do any arbitrary data processing that you want here. You can say I want to remove servers based on a hostname pattern, or I want to remove servers based on being in this data center or being on this subnet.  I want to add more serversâ€”here's a list of the servers I want to add, or here's another file I want to add to them.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2580.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2580)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2600.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2610.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2620.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2620)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2630.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2630)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2640.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2650.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2650)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2680.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2680)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2690.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2700.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2700)

You can see here it's saying I want to go to app grouping now as my recommended next step now that my data's a little bit cleaner. But before I go into app grouping, I'm going to ask you some questions.  One question I'm asking is, for example, describe my network architecture. It's saying here are some notable things. I can see LDAP is common in my environmentâ€”everything is speaking to the LDAP server. You have one single LDAP server, and you might want to make that more fault tolerant. AWS Directory Service would be a good match for that.  You have four primary database types in my environment. Notably, you have a single SQL server in the environment, which is a little bit odd. You can see that this is my network topology at a high level.  You have some key subnets in there. It's generating some initial insights into what things my waves could look like, and it's asking questions about how I want to go about application grouping.  Now I saw that LDAP server getting mentioned, so I'm going to say, what is that primary LDAP server? You mentioned everything has a single point of failure.  It says, well, here is your LDAP server. It's ldap-primary.testlab.local, my test lab server, and it gives me information about its role in the environment. I can ask pretty much any arbitrary question as long as I have the data for it.  If you have a software catalog saying this is my SAP environment, or you have inventory about what is running on each web server at a web app level, you can ask questions about any of this.  After I've done that kind of data curation, then it goes on to app grouping. What I need to do is ultimately get my servers into apps.  What it's saying here at the start of this is that I can notice you have a common set of hostname patterns. You have common dev, test, prod names in here. I can see from your meeting notes and your calls that you're talking about different environments.  So it's asking some information to figure that out. I'm going to paste a whole bunch of things in there that answer these questions. For example, I have two primary facilities, one in Vancouver and one in Seattle, and they have different hostname patterns. 

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2710.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2710)

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2720.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2720)

This is how I identify whether it's dev, prod, test, and so on from its hostname convention.  I'm also asking it to consider network traffic. For example, I'm asking it to group anything together with tight coupling like database dependencies, but then separate anything that has more latency-tolerant loose coupling dependencies.  I'm saying don't split app tiers. You might want to have your database and web tier separate, but I don't. It's one app to me.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2730.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2730)

It asks some clarifying questions to try to execute what I'm working on. I noticed some things like some servers have hostname patterns with uppercase or lowercase characters. I mean, just ignore that. I don't care about it. But I could say actually it means something semantically significant, so I can inform that.  It's asking about unknown prefixes. Should I create new apps for prefixes that don't correspond with anything I talked about in my meeting notes or the wiki page? The meeting notes I gave mention certain apps, and it's correlated the hostnames to those apps, but there's some that don't match. So what I'm going to say is to just create a new app for it. I'm saying other things like if it's an app server and a web server, just treat it as an app server. In my world, app and web means the same thing. So I'm just giving it some additional context, and it's figuring out all the intricacies of my environment.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2780.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2780)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2790.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2790)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2820.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2820)

So it's a purely chat-based experience.  What it's saying here is that I've done my app grouping and I've ended up with 486 apps, which is a nice number.  I can break that down more. It's identified quite a few single-server apps because they don't have network traffic, so they look like single-server apps, but I can break that down further with more information. It's asking how to prioritize those apps. So do I want to assign a priority to different apps? Are all apps equally prioritized? Probably not. So what I'm saying is that I want to prioritize environments. For example, I want to move my dev and UAT environments before my prod environment, because I want to make sure that those environments work correctly first.  I'm saying that I have some things like the actual data collectors that gather the data in that dataset, so just remove those because they're noise. You can change the scope at any given time. I'm just answering some of the other questions it's asking about complexity and so forth. So I'm giving it those answers.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2840.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2850.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2850)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2860.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2860)

What it does then is basically produce a final app list.  It's saying app structure is complete. Here's my list of apps. It's happily named most of them because it's got the names from all the meeting notes that I submitted.  It's figured out some other names that we don't have good data for, like test cluster. It's kind of made up the names, but you can say actually that test cluster is called this, and it'll appropriately rename them.  Then it's building out an incremental kind of plan that it still needs more information to refine, but it's getting closer to a final plan.

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2870.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2870)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2880.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2880)

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2900.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2900)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2910.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2910)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2920.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2920)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2930.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2930)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2940.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2940)

What I ask next is move groups.  That is, as you recall from earlier, move grouping is that dependency analysis phase where we group all the apps together.  This is what it's doing. You can see I regenerated it. I skipped ahead a little bit, but I basically told it to group everything together on network dependencies and do that analysis for me, and it's done an initial grouping. But one of the things I noticed is that it's saying, and one of the things that I gave it in a bit of an earlier conversation was some information about my teams and my capacity for delivering my plan. I'm noticing that some of these numbers don't look great to me. You've got weeks one to six, you've got weeks seven to eight, and you're saying there's certain timelines implied here.  I don't love that because I'm on a timeline. One of my constraints is timeline constraints. So I'm asking how do I change my move groups to move faster?  It will go forth and do that analysis and figure out well, actually, given your timeline constraints, I could change some of my move groups, balancing risk and speed, and make some changes.  For example, saying combine dev, UAT, alpha, and beta because they're all functionally reasonably similar in terms of their behavior. Group that together and make the groups larger, parallelize them out, and then you could actually meet your migration target within the timelines.  So it's coming up with ways to accelerate your migration plan, changing its strategy dynamically on the fly.  You can ask it about any constraints that you want to employ on your migration here to meet those constraints.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2950.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2950)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2960.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2960)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2980.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2980)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/2990.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=2990)

I was asking some other things here that we'll skip through a little bit, but I was asking you to tell me more about your timelines, your risk tolerance, things that you want to focus on.  I'm saying ultimately I want to focus on six weeks. I want to balance risk tolerance, and it goes forth and figures that out. But in the interest of time, I'm going to skip forward on that one.  Once it's generated the final move group, so analyzed all those dependencies and come up with a plan, what it then generates is the wave analysis, the wave plan.  That is ultimately taking those move groups and then prioritizing them into an ordered, prioritized set of waves.  So what it's going to ask for here is information on the business prioritization, the business logic in here.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3010.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3010)

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3020.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3020)

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3030.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3030)

Here is information on the business prioritization and the business logic. I've been able to glean a lot of this from the information that was provided. The more information you can submit,  the more it understands and the less it has to ask. I'm giving it some general information here. I want to start with minimally complex apps.  I want to save my products to the very end. LDAP needs to move first because we knew LDAP is a critical dependency, and it comes up with a general high-level plan  for ordering these waves. It says these are my phases to move each wave in, here are the servers within each wave, here are the move groups within each wave, and it breaks it all down.

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3040.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3040)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3050.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3050)

What you can see at the bottom there, if you want to scroll down,  is some of the key metrics. It's achieving the six-week timeline,  and it's saying these are the resources that I use. I told it about the people I have to execute my migration. It's talking about the max services per wave, the time savings, and so forth. I can ask it for arbitrary breakdowns of the grouping, I can change its priorities, and I can do anything I want really to change my plan.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3070.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3070)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3080.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3080)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3090.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3090)

Ultimately,  what I can do is finalize this plan to execution and go forth to my network migration and rehost. But at any time during my execution, I can come back to this plan.  I can say, well, actually, wave eighteen didn't work out so well. It turns out I missed a key dependency and I want to go back there. Or my leadership is saying go faster,  so I actually need to come up with a different parallelization strategy or I need to find other ways to accelerate. So it's an iterative process. You can at any point go forth and refine the plan.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3100.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3100)

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3110.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3110)

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3120.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3120)

Lastly, I'm asking for some backlog that I could populate my sprints with,  and it generates some ideas as to the tasks that you need to do to actually execute this. You can then basically take your wave plan, and there's the final step of planning and populating the agile plan,  and then begin to set up all of that mechanism and process to then start delivering your migration. 

So we've got a few minutes left. That has been quite a whirlwind tour, but that is the discovery and planning experience within AWS Transform. That is effectively going from high-level, rough data in many different formats through analyzing my environment, doing the dependency analysis, applying business logic to prioritize, and then producing an executable, ordered wave plan, and then generating ultimately a set of very high-level stories that I could then start to use to inform my agile plan. This typically is something that we see customers execute in months. It's a long-term process. This is something that you can now move to literally building in minutes or hours. But in a real-time scenario, this is something you always come back to and we're always refining, but still something that you're doing orders of magnitude faster than what you would be doing if you were doing this by spreadsheets and manual analysis. So we think this is really changing how migration planning will be done in the future.

[![Thumbnail 3210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3210.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3210)

[![Thumbnail 3280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3280.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3280)

### Best Practices and the Future of Migration Planning with Agentic AI

And now over to you for the conclusion. Yeah, so you saw the advantages of using generative AI and an AI agent. This is when I knew a couple of weeks back that we really built something  very useful to customers. What was shown is that the agent was being asked a lot of different questions. None of those are questions that we've programmed the system for at all. It's figuring it out on its own. So this is when I knew a couple of months back when a customer was using this on a trial basis with this wave planner, and they randomly asked, hey, group all my applicationsâ€”and they had like hundreds of applicationsâ€”group and create a wave so that all my security-related applications move together. That's not something we've programmed for, but when that question was asked in front of the customer, it figured out which applications related to security, figured out which servers had dependencies on those applications, and generated a wave group that was appropriate for that particular request from the customer. So that's the advantage here. When you use AWS Transform and this wave planning agent, you don't have to wait for us to add new features that meet your use case. Now it's intelligent enough to listen to your use case, take your own data, and figure out how to solve it on the fly instead of waiting for the service team to launch new features. That's the advantage of generative AI here. 

Best practices for migration planningâ€”we've covered it all. I'll recap it one more time. Again, automate with agentic AI. Don't try to manually plan. Spreadsheets are dead. You don't need them anymore. We've got artifacts stored within AWS Transform. The agents, as they work on your data, will generate assets.

If you want those assets, you can download them and take them with you, but you don't have to be the one working with those assets, spreadsheets, and CSVs. The agent will do it for you. Iterative migration is never a one-shot deal. Large, complex migrations take time. A customer mentioned that what was shown live in the meeting would have taken them weeks to accomplish manually, yet it was completed by simply asking the question. That's the advantage of iterating with an AI agent and generative AIâ€”you can accomplish so many things so quickly because the agent can think on your behalf at high speed.

You can iterate quickly without worrying about getting everything right the first time. You can go along, come back, restart, reset, and iterate over and over until you see something that you like. You don't have to be bound by the kinds of files this tool accepts. Jonathan showed how you can dump all kinds of meeting notes and various data into the agent, and it was able to extract information and use that to generate your migration plan. Just throw everything you have at it and see what happens. It may not get everything right the first time, but that's where iteration comes to your rescue.

Understanding technical and business dependencies combined means you don't have to worry about software that only accepts data in specific formats. For the first time, you don't have to feel constrained by file type limitations. Start with low-risk waves, moving small numbers of servers, and as you gain confidence, increase the wave size. We focused on discovery, assessments, and migration planning today, but AWS Transform allows you to generate networks, move your servers, and execute the entire migration end to end once you have the plan.

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c33b75a38334a64c/3430.jpg)](https://www.youtube.com/watch?v=8PBNaDa0TUw&t=3430)

You should consider using it for a small dataset and doing an end-to-end migration once you return from here. With this kind of agentic AI,  you can migrate large enterprise workloads at least 4 times faster. You don't need to use spreadsheets or any kind of files anymore. Just use natural language to interact, and the agent will create the files for you. Take the undifferentiated heavy lifting out of your migrations and de-risk by automating dependency analysis and wave planning instead of doing it manually.

Anything you do with this agent can be documented. You can ask it to generate a detailed report for auditing purposes internally to your IT department, leadership, or legal team. The agent will give you a detailed report of the plan, or if you choose to execute what it did with network migration or server migration, you can take those reports, print them out, and use them for auditing without having to do all this manually yourself. With that, we'll open it up for questions.


----

; This article is entirely auto-generated using Amazon Bedrock.
