---
title: 'AWS re:Invent 2025 - Deep dive into Amazon DocumentDB and its innovations (DAT444)'
published: true
description: 'In this video, Principal Solution Architect Cody Allen and Principal Product Manager Vin Yu present Amazon DocumentDB''s 2025 feature releases in a 400-level deep dive session. They explain DocumentDB''s architecture with compute-storage separation, six-way replication across three availability zones, and 128TB scaling capacity. Key announcements include Zstandard dictionary compression achieving up to 8x improvement over LZ4 (reducing 13MB documents to 90KB), Graviton 4 instances delivering 30% performance gains over Graviton 3 with 100%+ price-performance improvements, and DocumentDB Serverless with DCU-based scaling from 1 to 64 CPUs. The new DocumentDB 8.0 query planner optimizes index selection for negation operations and simplifies aggregation pipelines by combining $lookup and $unwind stages. Additional improvements include 90% faster global switchover times (from 300 to 30 seconds) and MongoDB 7.6/8.0 API compatibility.'
tags: ''
series: ''
canonical_url: null
id: 3085286
date: '2025-12-05T04:40:09Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Deep dive into Amazon DocumentDB and its innovations (DAT444)**

> In this video, Principal Solution Architect Cody Allen and Principal Product Manager Vin Yu present Amazon DocumentDB's 2025 feature releases in a 400-level deep dive session. They explain DocumentDB's architecture with compute-storage separation, six-way replication across three availability zones, and 128TB scaling capacity. Key announcements include Zstandard dictionary compression achieving up to 8x improvement over LZ4 (reducing 13MB documents to 90KB), Graviton 4 instances delivering 30% performance gains over Graviton 3 with 100%+ price-performance improvements, and DocumentDB Serverless with DCU-based scaling from 1 to 64 CPUs. The new DocumentDB 8.0 query planner optimizes index selection for negation operations and simplifies aggregation pipelines by combining $lookup and $unwind stages. Additional improvements include 90% faster global switchover times (from 300 to 30 seconds) and MongoDB 7.6/8.0 API compatibility.

{% youtube https://www.youtube.com/watch?v=M75JFvwhjtQ %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/0.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=0)

### Introduction to DAT 444: A Deep Dive into Amazon DocumentDB's 2025 Innovations

 This is DAT 444. We like the number 4. D is the fourth letter of the alphabet. We're data geeks, so we geek out over this, but this is a deep dive into Amazon DocumentDB and its innovations. Today we're going to talk about all the big feature releases that we've announced for DocumentDB in 2025. DocumentDB is a purpose-built database for managing your document database workloads at enterprise-level scale. This past year we've released a lot of really cool features, including our new compression ratio and improvements to things like global failover and global switchover. We're going to tell you about these features and get deep into them. Since this is a 400-level course, we'll show you how you can improve your workloads using a lot of these new features on DocumentDB.

I'm Cody Allen, a Principal Solution Architect with the service. With me is Vin. Hey everybody, my name is Vin Yu. I'm one of the Principal Product Managers with the DocumentDB team, and I'm super excited to share what we've launched this year. So we're going to start off by talking about the architecture.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/70.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=70)

 Before I start, who's never been to re:Invent before? Who is this your first year here? We had a session earlier and about 80% of the attendees were brand new, so that's awesome. I'm glad that you're here. Anybody been here five, six, or seven times? That's awesome. I don't know how many years re:Invent has been going on, but 56 years in a row is awesome. The reason I ask is because we have names and numbers for sessions. This one's DAT 444. DAT means it's a database session. The 444 means it's an expert level. An expert-level session means that we expect you to know a little bit about DocumentDB. You've probably been hands-on with it and are using it in a production environment. That means we're going to skip over the basic stuff and go straight into expert-level materials. But we are going to start off with the architecture. I want to set a foundation for us because DocumentDB was purpose-built and one of the key features of it is its architecture.

We're going to spend some time on that and then get into the fun stuff. We're going to go over things like enhanced compression, serverless, and Graviton 4. We were so excited about Graviton 4 that we skipped Graviton 3. We came out with DocumentDB 8.0. Again, we just skipped 6 and 7 and went straight to 8. We're going to talk about the new query planner, one of the new features that you get with DocumentDB 8.0. But like I said, this is a 400-level session, so we're going to skip over the marketing slide. We're not going to spend time on the fluffy stuff that you can read on our web page. You know what you're talking about.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/160.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=160)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/170.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/180.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=180)

  We're going to skip over the marketing slide. Sometimes you'll see NASCAR slides where they say, "Oh, look at Aurora and look at all of its customers."  We have to justify ourselves, and they'll segment out their customers by saying, "Oh, look, we have customers here in the marketing department." We're not going to talk about that kind of stuff. Let's jump right into the cool architecture.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/200.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=200)

### DocumentDB's Purpose-Built Architecture: Separation of Compute and Storage

 DocumentDB is a purpose-built database, and the main feature here is a separation of compute and storage. The reason we did this is so that you could scale your workloads based on the resources that you need, either at the compute layer or the storage layer. The storage layer is actually broken up into two different pieces. We have the distributed storage volume that holds your data, and we have your backups that go to S3. We'll dive deep into that here in a little bit. Your data is automatically replicated six times across three availability zones. That means you're in multiple physical data centers, and that gives you enterprise-grade durability.

What's really cool about this durability is you get six copies of it even if you have a single compute instance. It's not dependent on how many compute instances you have. You can even run headless. You can run with no DocumentDB instances whatsoever and still have six copies of your data. Now you can't interact with it because you do have to have a compute instance to actually query your data, but you could do that because this data durability is handled at the storage layer. You don't have to worry about the compute layer at all.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/240.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=240)

 We scale up to 128 terabytes of data for you. You don't have to worry about it, and that means you're not going to hit any kind of compute layer size restrictions that some database systems have because the compute and storage is co-located together. If you hit a certain amount of storage that you're using, in order to get more storage, you have to scale up. You have to add more vCPU to it. You have to add more RAM. DocumentDB lets you scale independently. You don't have to do this. This also eliminates the need to do things like sharding where you have to split your data up into certain segments on different clusters.

All of this is going to reduce that operational overhead and reduce the risk of having data hotspots. Because your data is replicated with six copies across three availability zones, no matter how many instances you have or what size you have, it means that it's going to grow with you. It's going to grow with your workload.

Document databases and NoSQL are still relatively new compared to most legacy systems that were built on relational databases like Microsoft SQL Server or Oracle. Document databases are really a developer's database, so you're building from scratch. When you do that, you don't really care about durability or availability. You just want to develop something, see if it's going to work, and then it quickly grows as you have success with it. We all know about the server under the developer's desk that's running a production system because they developed it, it looked great, they sold it to leadership, and now it's still sitting under their desk in the shadows.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/380.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=380)

With DocumentDB, you get enterprise-level durability, availability, and security from start to finish, no matter what size instance you have as you scale up. The data  in DocumentDB is log-structured. Most traditional databases use a block storage device designed around database interaction, so it doesn't have to concern itself with that underlying storage layer whatsoever. Data is going to continuously stream to S3.

Most database systems require you to do a full backup and then incremental backups that all run in the compute layer. You have to use compute layer resources in order to do those backups. But because of the log structure of DocumentDB and because we stream this from the distributed storage volume, that happens outside of the critical path. That means your applications are not affected by any of these backups whatsoever. It all happens seamlessly behind the scenes.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/430.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=430)

The blocks of data that we store  are about 10 gigabytes in size, and this is called a protection group. These are the things that get replicated across six different availability zones. This is why your storage grows in 10-gigabyte chunks, and this is why the minimum you're charged for storage is 10 gigabytes, because that's the smallest block of data that we have in DocumentDB.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/450.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=450)

The storage layer  is designed around handling failures. It can lose an entire availability zone and a block of data in another AZ and still be accessible. We have a quorum model that means in order to have a successful write, we have to wait for an acknowledgement of four out of those six blocks. Basically, your application sends a write, we write to four blocks, and as soon as four blocks have acknowledged it, we tell the application you're good to go. We have the data, and at the same time we write those last two blocks to give you those six copies of data.

For reads, we're just looking for three out of the six blocks, and honestly for recovery purposes, we're really just looking for one healthy node to come back and give us that data back. In this situation with that worst-case scenario and failure situation, the database is still available because of this quorum model. The storage volume has a self-healing architecture that means if a block is down, failed, or under maintenance, the storage nodes actually use a peer-to-peer gossip protocol where they're able to talk to each other and rebalance that data among themselves while it's getting those other data blocks back.

With traditional database systems, if you have 10 terabytes of data and that node goes down, you're recovering 10 terabytes of data, which can take a minute or two. Our storage blocks are 10 gigabytes, so the mean time to recovery is much faster. It takes a lot less time to restore that 10 gigabytes of data. All of this happens without having to do anything. This is all handled for you in the backend.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/560.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=560)

The last thing I want to talk about is the checkpointing process. Databases  use a regular checkpointing process, and nearly all of them use a single-threaded process where the writer writes block after block after block. With DocumentDB and this distributed storage model, we can actually make these run in parallel, and we can also recover in parallel, and that makes that recovery process a whole lot faster than most traditional database systems.

You would think that with traditional database systems, readers would have to go all the way back to that distributed storage volume to get new data as data is updated, and that would make reading slower. But with DocumentDB, what we've designed is we send updates to the distributed storage volume and then we send them directly to the reader instances via write-ahead logs. So the readers will look at those changes.

The readers examine those changes and apply them if they have the corresponding documents in their storage. If they don't have them, they ignore them. For example, if I have a document that says Cody's favorite color is purple and I perform an update, that update goes to the writer instance. The writer receives it and says, "Cody's favorite color is now red," and writes that to the storage volume. The writer then notifies the secondary instances, saying, "I have an update to Cody's document. If you have that in memory, update it." The reader instance looks at it and says, "Yes, I have that. I'm going to update my information in my cache." If it doesn't have it, it simply throws it away and ignores it. This approach lowers your replication lag and demonstrates that we don't dirty the cache of reader instances from the primaries. We keep these caches completely separate from each other.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/660.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=660)

### Enhanced Compression with Zstandard Dictionary: Storage Savings and Performance Gains

Now that we have covered the architecture background, let's discuss these features, starting with enhanced compression.  Two years ago, we released LZ4 compression with DocumentDB, which made a significant impact on your storage volume because we didn't have any compression before that. This was the first compression we released. This year, we've released Zstandard dictionary compression, which represents a very significant step forward in storage savings with DocumentDB. With LZ4, the biggest gains were with large documents. The larger the object, the better it compresses. With Zstandard, we're seeing significant gains even in smaller documents.

LZ4 has to analyze each document independently. It doesn't look at a broad range of documents; it examines each one as it comes across it. With Zstandard, we pre-analyze common fill patterns and create a dictionary from these patterns. This results in highly efficient compression when you have repeated field names and repeated schemas, which translates to significant storage cost decreases and a couple of other performance increases that we'll discuss in a moment.

Now, what does this mean in practice? Here we have a sample schema. Dictionary compression replaces frequently occurring values with references to a dictionary. In this example, we have very verbose field names like customer_identification_number and product_description_text. These are really long. I want to pause here for a moment because I want to take a detour before we go down the compression route. Does anybody see an issue here, or maybe that's not the right word? Does anybody see an anti-pattern or something that we can improve even without compression?

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/800.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=800)

JSON is a fantastic data model because it's very easy to parse and human readable. We can all look at this and determine what it's doing and what the data represents. However, you have to remember that we're taking up a lot of extra space with these verbose names. Every single character in here adds more bytes to our document. Simplifying field names can save us a ton of space.  For example, if we shorten customer_identification_number to cust_ID, we're saving 23 bytes per document just on that field name. If we shorten product_description_text to 4 bytes, we just saved 20 bytes per document. Imagine you had a relatively small collection of about 10 million of these documents. If we do the same thing for every single one of those fields, we just saved 60 percent of our storage on that document by using shorter field names, and we haven't even brought compression into this yet.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/860.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=860)

Something to keep in mind: if you go back and look at your documents, think about this. Look at your field names. They don't have to be verbose. It's good that they're human readable, but you and I are not reading at the speed and frequency that most of our workloads are running through. So the dictionary compression goes through and creates a mapping of those common field names and replaces them.  The data stored in DocumentDB when you use this is storing the references, not the full field names, just the references to them. Essentially, it's building a very sophisticated dictionary by analyzing a large sample set. You have to have at least 1,000 documents before it kicks in. So as soon as you turn it on until you get to 1,000 documents, you won't have that dictionary, and then it will create it. When you have millions of documents, it will see that customer_identification_number occurs in predictable positions and predictable contexts, and it will create a more efficient coding standard because of that.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/900.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=900)

We did some tests.  We'll start off with red because red is negative, red means bad. What we see in red is the CPU usage.

CPU utilization when we turn on standard compression went up by about 17%, from 59% to 69%. Clearly, compression isn't free. That CPU represents the computational cost of compressing and decompressing those documents. The obvious impact here is the blue line. Even versus LZ4, the amount of storage that we're keeping on disk with this compression is significantly lower. We dropped from about 340 gigabytes down to 125 gigabytes.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/950.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=950)

Then what we see in the same color as your headphones, we saw that throughput went down from 142 megabits to  85 megabits. But what's cool here is it went down while our operations increased, so we were able to increase from 92,000 inserts per second to 143,000 inserts per second while lowering this throughput. You're getting improvements across the board.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/970.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=970)

We also wanted to test how this worked with different schemas. LZ4 is great with big documents, while Z standard is great with small documents. So we started off with the GitHub user database, which is a publicly available dataset with documents about 1 kilobyte in size. When we use Z standard over LZ4, we saw a greater than 3x improvement in compression ratio. 

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/990.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=990)

We also created our own retail schema with about 2 kilobyte documents, and we saw about a 2x increase in that compression ratio.  At the very end, we have this large metadata with 13 megabyte documents, and we saw the compression ratio go from about 18 to 1 with LZ4 to 147 to 1. Imagine that your 13 megabyte document just went down to 90 kilobytes. That's a very substantial decrease.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1030.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1030)

What's cool about that last example is that it's not a test. That is a production customer that did that. When we released this, they applied it to their workload, and they were able to see an 8x increase, not from uncompressed versus LZ4. 

At enterprise scale, I want to focus on the CPU again, the negative part. But at enterprise scale, storage costs often exceed the CPU costs of slightly slower compression. For example, a 40% compression ratio can translate into pretty substantial cost reductions for what you're paying for storage. Beyond direct storage cost, compression reduces your IO operations. It lowers your backup footprint, and it even decreases your network bandwidth usage.

Compressed documents require fewer IO operations because more logical data fits within each physical IO operation. Compressed documents give you better cache utilization because you can fit more of these documents in cache. We keep documents compressed on storage, over the wire, and in your cache on those compute instances. So that might decrease the need for your compute instance size, giving you additional cost savings through cost optimizations.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1110.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1110)

Here we're just seeing what the cost difference is because that's the physical thing. Everything else we're talking aboutâ€”your cache, your compute sizeâ€”that depends on your workload. I want to stretch your minds a little bit and think about this as a multi-dimensional improvement.  A couple of years ago we released IO optimized storage for our customers that have very busy, very heavy IO workloads, and this was really marketed as a cost improvement. It allowed you to stop paying for IO and basically get it for free, but the trade-off is you're paying more for storage.

Behind the scenes, there's actually a lot of performance increases, throughput increases, and latency decreases for that. We won't go into that too much here, but you get cost benefits and performance benefits. Now the trade-off is you don't pay for IO anymore, but you do pay 3 times more for storage. As of right now in US East One, you're paying 10 cents per gigabyte per month with standard. With IO optimized, you're paying 30 cents, so 3 times more.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1180.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1180)

In this example, with LZ4 we were at 1.7 terabytes compressed, but with Z standard we went down to 500 gigabytes. What that means is that our inflection point for where IO optimize can make sense goes down significantly. Our break-even point for Z standard was about 1.7 billion operations per month, about 660 operations per second.  Once we use Z standard, that inflection point lowered to about 500 million operations per month or about 193 operations per second, so you don't have to have this massive enterprise scale to start recognizing these cost savings when you start combining these.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1220.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1220)

Z standard allows you to combine with other DocumentDB features and really multiply your savings and possibly even multiply your performance. Where do you use it? Where does this work best? Dictionary compression is really good when you have repetitive field names and repetitive structured  elements, collections that have deeply nested fields with consistent schemas, standardized schemas. They're going to get the best out of this because they have field names that are in a predictable spot and the dictionary can help with that.

However, on the other side, if you have a lot of documents that have variable schema names, very sparse schemas, or highly variable structures, you're only going to see modest improvements. You're still going to see improvement, but not nearly as much as if you had standardized schema. Compression is usually better with larger documents. Compression works great on big objects, and that's where LZ4 and Z standard work with those big documents. We saw it with the 13 megabyte documents, but also in the smaller documents.

Read heavy workloads generally are going to benefit more from Z standard than write heavy, just because there's less IO bandwidth and we can process those a lot more efficiently in cache. Things like high frequency point queries are going to have improved latency. But on the other side, if you're doing large range queries that are returning multiple documents, you might see a performance decrease because of that overhead, the compression and decompression overhead we saw earlier.

For write operations, simple inserts benefit from this again because of those lower IO operations. However, if you are doing any kind of complex operations where you're doing multiple writes at once, there's a lot more compression that has to happen and higher CPU usage.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1320.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1320)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1340.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1340)

### Graviton 4 Instances: Achieving 30% Performance Gains Over Graviton 3

Next, Graviton for instances.  Is anybody using Graviton for instances with DocumentDB yet? You started to raise your hand. You want to. You're right there. Not with DocumentDB. Got you. Graviton, stepping away from DocumentDB, is unique to AWS. It was really designed to give you the best price performance for cloud workloads.  Graviton instances are really good at CPU intensive workloads and memory intensive workloads like databases, so it really makes sense.

When we released Graviton 2 instances in DocumentDB, we saw a 30% price performance increase. Each Graviton instance that we released from 2 to 3 and now to 4 brings pretty significant performance gains. This 4th generation is seeing 30% performance gains over Graviton 3, not Graviton 2, but Graviton 3. Similar to when we released Graviton 2, we're seeing pretty significant performance impact to workloads and cost benefits from using these.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1390.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1390)

What does that look like? We looked at it from an in-memory versus out of memory workload to see what kind of performance impact we could have. Clearly, in-memory workloads are going to benefit significantly  from this. We're seeing over 100% price performance increase in read workloads and 136% in update workloads. Even for those workloads that are out of memory where things don't fit in cache, you're still seeing a significant increase in your performance there.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1410.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1410)

Now, there is a price difference.  We started off with R5 instances. Actually, we started off with R4, but we'll ignore those. We started off with R5, we released Graviton 2 R6G instances, and we lowered the cost by 5%. What was cool is that even if I did nothing except change my R5 instances to R6G instances, I saved money and my performance increased. It was dead simple. It made sense. With R8G, you're seeing a 5% increase. So essentially if you're going from R5 to R8G, there's no difference. It's staying the same. It went down, it went up. Despite the cost increase though, we can see a performance increase that far exceeds that price premium that you're paying for. These performance gains are consistent across all instance sizes, whether it's extra large up to 16X large, but what you're seeing here is this performance benefit by switching over to these.

### DocumentDB Serverless: Dynamic Scaling with Buffer Cache Resizing

Next, serverless. So no R8G users. Anybody using DocumentDB serverless currently or tried it out? You all have so much to do when you get back. You need to try out serverless. But to talk about this, I'm going to hand things over to Vin and he's going to walk us through that. All right, thanks, Cody. Really awesome, actually some of those things that Cody shared just earlier about the performance benefits of R8G. We actually, when we launched it, already had a few customers come to me. I actually met some of them earlier today about how all they had to do was switch over from an R6G to R8G. There were no functional changes, and they just got better price performance. In fact, some of them were able to actually scale down their instance, so there's a cost benefit.

An indirect cost savings there as well. For those who have missed the first few minutes, I just saw a few folks coming in later. My name is Vin. I'm one of the product managers with DocumentDB. We're going to dive right into Serverless where I'm going to provide a short overview and we're going to dive into some of the inner workings of how we made Serverless work for DocumentDB.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1530.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1530)

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1540.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1550.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1550)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1570.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1570)

The core concept of Serverless is  that the system scales with your workload requirements. As compared to a traditional system, you could either underprovision, which causes some challenges if you have a peak.  This might cause some application challenges if it goes over your system requirements, or you could overprovision.  Again, this is going to cause some other types of challenges in terms of being an inefficient use of your resources. In between this, you could manually alter and change your resources as you go, however, that is an extra step for you. The core idea is that Serverless really scales with your workload needs, and  this is exactly what Serverless in DocumentDB does.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1580.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1580)

In August we launched DocumentDB Serverless.  If you haven't tried it, I don't blame you. It was just right before the holidays and it was at the end of summer, maybe you're coming back from your vacation. The idea is that if you have larger workloads, the instance will scale up to meet the needs of your workload, and it could scale anywhere from less than 1 CPU with 2 gigabytes of memory all the way to 64 CPUs with 512 gigabytes of memory. You're not making those jumps like you do with, say, an R6G large which has 2 CPUs to an XL which has 4 CPUs, which is a more discrete jump.

You can easily switch between Serverless and provisioned instances. I have a Serverless instance in the middle there, sitting beside two other instances, two other replicas which are different provisioned instances. What this really enables you to do is adopt Serverless without having to do any data movement because of that benefit of separation of compute and storage. Whether you have 100 terabytes of data or 10 gigabytes of data, the amount of time to switch between provisioned and Serverless is pretty much constant.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1660.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1660)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1680.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1680)

Last but not least, we introduced a new unit to measure database Serverless resources and it's going to be called DocumentDB Capacity Units. Let's talk a little bit about that as well because there are  some cost implications here. DocumentDB Capacity Units, or DCU for short, has one DCU with approximately 2 gigabytes of memory, some CPU, and network resources, and this comes in a package.  We don't scale your CPU and your memory independently. This comes as a package. On the right side you can see that there's a max and minimum DCU. When you're provisioning your cluster, what you could do is specify the number of max DCUs or minimum DCUs to help you control your cost or provide performance guarantees, and our system does scale in increments as small as half a DCU.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1710.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1710)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1730.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1730)

You're not scaling in discrete steps of, say, 2 vCPUs all the way to 4 CPUs. This does more incremental scaling.  We scale on many dimensions. There's an internal algorithm where we scale on CPU utilization, memory pressure, network throughputâ€”all these aspects will contribute to scaling and adding more DCUs to your system. The last point is actually a very important point,  and I'll tell you why later, so just keep a note of this last point here. Later in the slide I'll be referencing this. An interesting aspect is that we don't scale linearly. If you have a larger database, it will scale faster than that of a small one. Let's say you have a 20 DCU instance, it's going to be able to add 10 DCUs faster than that of an instance that has only 1 DCU as a starting point.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1760.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1760)

Let's dive a little bit deeper into some of the implementation details. One of the key aspects  of how we made Serverless work for DocumentDB is something I specifically want to talk about: buffer cache resizing. For a while now you could already dynamically change your resources applied to a virtual system. If you're working with cgroups and Linux or containers, which is built on the concept of cgroups, you could actually change the setting of how many CPUs and how much memory is being applied to your container or in Kubernetes. However, the challenge with the database is if you just decide to reduce the amount of memory, this could cause a lot of thrashing. You may be evicting the wrong pages, and that's going to cause a lot of performance issues and it's going to drive up I/O as well.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1830.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1830)

Let's talk about this diagram. The bars you see from pink to red represent pages inside our buffer pool. The darker the red, the more frequently it is accessed. We follow an internal algorithm for how we sort the pages that combines both least recently used and least frequently used. 

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1840.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1840)

Let's say I do a lot of frequent reads from my buffer pool. My application accesses those pages in deep red. Now let's say I need to go to disk to get some data.  I'm going to fetch some data from disk, and notice how the way we sort it is not in the very front. Just because we accessed it recently doesn't mean we access it very frequently. So later on when we do any resizing activities, we may truncate those pages before we do more frequent ones.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1870.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1870)

Let's take a look at how we shrink, and it's pretty simple. For pages that haven't been used for some time based on a combination of LRU and LFU, we're just going to truncate and drop those pages from our buffer pool.  This enables us to shrink our buffer pool and resize it. This is what you see when a service scales back down when you're not using that many resources. I really want to stress that this is a bit of magic here because if we evict the wrong pages very frequently, it's going to drive up I/O. This strategy really ensures that we're not doing things that cause performance and cost issues.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1910.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1910)

### Serverless in Action: Comparing Provisioned and Serverless Instance Performance

Let's go to an example. I want to show you two workloads: one against a provisioned instance and the other against a serverless instance.  The view you are seeing here is from a tool called Performance Insights. It measures how much load is on your instance, and the dotted line you see in the middle represents how many resources, measured in vCPUs, are on your instance. We choose vCPU because it represents approximately how many processes run at any given time.

You can see in this diagram that most of the time we have about 3 or 4 average active sessions. That means there are 3 or 4 queries that want to run at any given time on average. However, we only have 2 vCPUs. Even if we add more load to our database, it's static. It's not adding any more resources and not changing the amount of resources available. This means I may be experiencing additional query latency even if my indexes are in place and my queries are optimized. My database is underprovisioned.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/1980.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=1980)

Let's take a look at it from another view. We could slice this by wait, and what this tells me is what my process is spending a lot of its time doing.  Notice how there's a lot of red, or perhaps brown from the back. This is I/O, and what this indicates to me is that not only am I trying to do 3 or 4 things at any given time with only 2 vCPUs, I'm actually spending a lot of time going to disk, spending a lot of time on I/O. To me this indicates that maybe I don't have enough memory. I don't have enough memory for the working set of my query, so I'm being very inefficient and it's going to increase the query latency of my workload.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2020.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2020)

Now let's take a look at the same workload on a serverless instance.  On the bottom there, the bar graphs show about 3, which is again 3 average active sessions, but notice that the bar is already at 16. This tells me that the system needed more resources, probably for memory. Even though I'm only running 3 processes at any given time, I needed 16 vCPU or DCU worth of resources to run this query. Later on to the right, I added more artificial load just for demonstration purposes to show you that the vCPU line, which represents the number of resources on my instance, is not static. It does scale as I'm adding more load to our database.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2070.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2070)

Now I want to show you from another view. This is the view based on wait, what are my CPUs doing?  Notice here that it's mostly green and it's actually not spending a lot of time doing I/O. This signals to me that this is actually quite efficient. I'm not spending a lot of time going to disk, getting data, bringing it back and processing it. I'm actually spending a lot of time in CPU, which means I'm just processing this data, so quite efficient there.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2100.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2100)

### DocumentDB 8.0 Launch: Performance, Compatibility, and the New Query Planner

So if you haven't tried Aurora Serverless, I highly recommend it. It's really easy to switch back and forth from provisioned as well. All right, switching gears to DocumentDB 8.0 and the new query planner, which is the core of DocumentDB 8.0. 

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2120.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2120)

Who here heard that we launched DocumentDB 8.0? Very few folks. Well, I wouldn't blame you because we actually just launched it three weeks ago, pre-re:Invent, probably during Thanksgiving, for this thing.  Before this event, so if you missed it, this is a very high level recap of what's included. A big focus this year in terms of our latest launch is really around performance, and I think Cody already covered some of those aspects with compression. But the new query planner is also a very key part of DocumentDB 8.0, which I'm going to go into shortly. The last performance piece is around vector indexes. To future proof you a little bit about some of those apps that you are going to build on DocumentDB, we introduced parallel index build for vector indexes, which provides up to 30 times faster index builds for vector indexes.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2190.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2190)

The second piece is MongoDB API compatibility. We heard from a lot of our customers that they want to bring their MongoDB 7.6 and 8.0 applications. They have the drivers that work with those versions. You can now bring those workloads to DocumentDB as well. We've also introduced a handful of new MongoDB APIs and functions such as views and collations that are included as well. So if you are waiting on some of these features to adopt DocumentDB, we have them now. 

### The New Query Planner: Improved Index Selection and Aggregation Pipeline Optimization

Let's go into the new query planner, which we're going to spend the last bit of this session on. Starting with 8.0, we launched a new query planner, and this is going to be a large collection of improvements. However, for the remainder of the time, I'm only going to show two examples that highlight some of the things that the new query planner can do. First is improvements around choosing an index. The new query planner has expanded index support, delivering performance improvements for certain operations, and this includes, for example, if you have negation operations, we will now choose an index over a collection scan.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2240.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2250.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2260.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2260)

Let's walk through a very simple example. I have a sample document here, a very simple one. I'm going to create an index on it.  And then I want to do a find, but notice how that find has a $nin operation, or "not in" operation. Now with query planner version one, you could actually rewrite your query to take advantage of the index, but the default behavior was to use a collection scan.  You can see here it says COLLSCAN.  However, with the new query planner, we default to using an index scan. As noted there, the second thing I'd like to call out and point out to you is that when you do explain plan, we also included the planner version as part of the output. So if you are doing a migration and you want to use the first, the old planner version as you're switching over, we allow you to switch between planner versions, but it's highly recommended that you use the latest one.

That was a very simple example where we changed a single stage and we swapped it from collection scan to index scan. But let's take a look at a more complex example with aggregation pipelines. Not only do we change certain stages, we actually have the ability to take a look at your query as a whole and find optimizations. So here there are 22 stages in my aggregation pipeline. One of them is a $lookup, which if you're familiar with SQL, is equivalent to a join. And $unwind is an operation that flattens out any document that has an array in it. So by doing first what this query is going to do is first do a join effectively and then flatten out all the arrays. But with the new query planner, we actually simplify this and do a single step, and I'll show you that through an example in a bit.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2350.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2350)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2360.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2370.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2370)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2380.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2380)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2390.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2390)

Now for those who are unfamiliar with $unwind, I want to show just a very simple example of what it looks like. If your document looks something like this,  and you do an $unwind on the array field, you get three documents.  So if you do an $unwind on the array field, you get three instead of one. Alright, so let's go through an example. Let's say I have two collections. One of them is my orders collection here.  I have the customer name and the product IDs of which they ordered. And then I have a second collection which has all the product details.  So laptop with the price, phone with the price, and so on. Now let's say I want a set of documents that has all the orders for each product detail.  Right, so you can see here effectively what I'm expanding on is the orders collection. I want to blow that up and I want to get a detail of every single product that is being bought in the orders. The way I would do this

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2410.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2410)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2430.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2430)

is use the exact same query I've shown before. First, I'm going to join these two collections,  and then I'm going to do an unwind on it, which flattens out each one of those arrays. With planner version one, there's a lot going on here. What I'm showing you on the left is the output of the explained plan for reference purposes, and I have the tree view equivalent on the very right.  One thing to note is that this nested loop lookup stage is actually joining these two collections, and then I'm going to store it in memory so they can unwind it later.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2450.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2450)

Now this ends up looking something like this, where I have order number one with two products on the product details, and order number two  with multiple product details. You can imagine that if an array had hundreds of entries and I have lots of different products to look at, this object could be very, very big. The fact that we're just storing it in memory means that this intermediate step could be very, very expensive. Whereas with planner version three, one thing that we do is that we do not create this temporary object and store it into memory.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2490.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2490)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2510.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2510)

Instead, if we take a look at both of these collections, one thing that you'll note is that the final stages produce the documents. The way the query planner does that is it notices that I'm trying to do an unwind  after a lookup stage. So instead of building that temporary structure and then unwinding it later, as I'm doing the lookup for each individual product ID, why don't I just do the lookup against the products collection and stream those results as the final set? That's exactly what we do. If  you compare the tree structure of both these query plans, one thing I'd like to point out is this nested loop lookup returned eight documents. I know I'm working with a very small collection here, but if you have a lot of documents, you could think of this as almost like O(n) squared notation of runtime. This stage can use up a lot of memory.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2570.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2570)

### Behind-the-Scenes Improvements and Session Wrap-Up

These are just two examples of the new query planner. There are actually tens and hundreds of optimizations that we're planning to do, which you could check on our developer docs. This is just to showcase what we're doing with our new query planner. All right, one more thing. I know this wasn't on the agenda slide, but I really want to show this slide, and I just really want to highlight that not everything we do in DocumentDB is an explicit feature. There are a lot of things that we do behind the scenes just to make things better, and I want to show you something that's actually what our engineers  see. It's a very chaotic graph, but this actually represents all the time somewhat that our customers do when they do a switchover.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2610.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2610)

So if you're not familiar with switchover, this is when you're running a cluster across two different regions and you want to promote one over the other. Between August 4th and August 11th on this graph, before we actually introduced an optimization, the P90 time to do a complete switchover was 300 seconds. We actually rolled out a change starting August 11th, and we finished our deployment where we simplified the promotion workflow behind the scenes.  We actually found that our P99 can go as low as 30 seconds and our P90 as low as 30 seconds. So this is a 90 percent improvement over switchover. This is just one of the many things that we do as a part of our service that just makes things better. This is something I really want to show you all.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2630.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2630)

OK, for the summary recap, there's a lot of things that we launched this year. This recording will be available on YouTube later, which is why it's being recorded in the back. So if you miss anything, you can always come back and check out our what's new or our release notes pages, which has every single thing that we've launched this year as well. One thing I like to call out is that you may have seen some folks wearing this hoodie.  I think the expo is just about to open tonight or tomorrow. If you go to the database booth, remember that thing I talked about that was really important about serverless databases.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0b82e81248e87729/2660.jpg)](https://www.youtube.com/watch?v=M75JFvwhjtQ&t=2660)

One thing that you can do is go to the booth and tell them one interesting thing that you learned. As a reminder, if you forgot about it, with serverless instances, large instances scale faster than smaller ones. So remember that. I think the booth is just opening up right now, and we are one of the sessions that just follows right before the booth opening.  So hopefully you all are getting the secret there before others do, given that the session is on a Monday. And with that said, thank you so much for attending the session. I know it's Monday at 5:30, but the last ask I have of all of you, and this enables Cody and I to come back year after year, is to really complete the session survey in the mobile app. So thank you so much and have a great re:Invent.


----

; This article is entirely auto-generated using Amazon Bedrock.
