---
title: 'AWS re:Invent 2025 -Fidelity Investments: Text-to-SQL for data analytics at enterprise scale-IND3323'
published: true
description: 'In this video, Fidelity Investments presents their enterprise-scale Text-to-SQL solution that converts natural language into SQL queries. Manish Worlikar and Ray Zhang explain their three-pillar architecture: AI-ready data with consistent naming and explicit relationships, semantic layers that translate business language to database terminology, and dynamic workflows managing query processing. Key techniques include context engineering to manage large schemas, query rewrite for handling ambiguous questions, entity search using vector stores, and surgical teaching examples. They achieved 93-95% execution accuracy and 28-second average response time using Anthropic Claude, emphasizing that quality over quantity in examples and deterministic workflows outperform complex systems.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 -Fidelity Investments: Text-to-SQL for data analytics at enterprise scale-IND3323**

> In this video, Fidelity Investments presents their enterprise-scale Text-to-SQL solution that converts natural language into SQL queries. Manish Worlikar and Ray Zhang explain their three-pillar architecture: AI-ready data with consistent naming and explicit relationships, semantic layers that translate business language to database terminology, and dynamic workflows managing query processing. Key techniques include context engineering to manage large schemas, query rewrite for handling ambiguous questions, entity search using vector stores, and surgical teaching examples. They achieved 93-95% execution accuracy and 28-second average response time using Anthropic Claude, emphasizing that quality over quantity in examples and deterministic workflows outperform complex systems.

{% youtube https://www.youtube.com/watch?v=YA7Svfb0ttU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/0.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=0)

### Identifying the Business Problem: From Data Access Challenges to Natural Language Solutions

 Welcome everyone to day 4 of AWS re:Invent 2025. We are super excited to present to you in this lightning talk on how Fidelity Investments implemented an enterprise scale Text-to-SQL solution that can convert natural language into precise queries to derive actionable insights. I'm excited to be here and happy to introduce the Fidelity Investments team.

Thank you, Vikram. My name is Manish Worlikar, and I head the AI Center of Excellence for one of the business units in Fidelity called Fidelity Institutional. Ray, you want to introduce yourself? Yeah, good morning, everyone. My name is Ray Zhang. I lead the Text-to-SQL implementations.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/70.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=70)

So today what we will do is we'll cover our journey from identifying and confirming the business problem to the actual technical details that Ray Zhang will cover later.  We'll start with the journey. Our approach is more user-centric. What we did is we observed our users and how they are interacting with the data. We saw that they're using multiple applications, think about CRM, dashboards, Excel, and piecing together the data. Also many times they're calling a friend or other team members trying to get an understanding of the data.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/100.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=100)

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/130.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=130)

 What we found out is that it is not the problem or the challenge with the data. What we discovered was it is the access to the data that is the bigger challenge. We also understand that this problem that we face is not unique or isolated. It is more applicable to multiple enterprises where data is scattered and complex. SQL is a powerful language, but the thing is there's a technical dependency with the technical team. On the other hand, the business needs a very intuitive data access,  like quick, faster data-driven insights.

So this picture at the center actually will help you understand why the challenge persists. As you see data at the bottom, as the data travels towards the user, think about the data traveling through multiple layers. Think about layers as data preparation, data transformation, and then eventually it gets served via a UI channel. Think about dashboards, think about application UI to the user. As the data is moving along the layers towards the user, it is getting more fixed, more rigid and templated. But on the other hand, the user questions are smart and flexible.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/200.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=200)

So I just wanted to pause here and ask you all, does it sound familiar to you? Do you see this in your enterprise? Raise your hand if you see it. Yeah, I do see it here. So basically what we thought is, instead of doing this, what if we do one thing, bring the user question directly to the data.  And that led us to natural language to SQL query capabilities. We said, okay, we need to build this capability.

What this helps is it totally changes the game, the way we approached it. It democratizes the access to the data. It also reduces dependencies on the SQL experts. Imagine multiple personnel within your organization actually get enabled with this data access. It raises the floor of your organization when it comes to insights from data. Also, business is able to now quickly get data-driven insights to make informed decisions and create business impact. With this, what I will do is I'll pass on to Ray Zhang to talk about technical details.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/250.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=250)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/260.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=260)

### Building the Foundation: AI-Ready Data and Semantic Layer Architecture

 I'm going to talk about  our technical implementation of Text-to-SQL systems. I will detail the architecture we built. Our implementation is based on three pillars of intelligence. The first one is AI-ready data. AI-ready data is the foundation. It's how you structure the data to make it understandable both by humans and by the large language model. The second one is the semantic layer. The semantic layer is the translator that translates the business language into the database terminologies. And the last one is the dynamic workflow, which is managing the entire process from the user input in natural language to executable queries. So these three components need to work seamlessly so the user can get the data they need without writing a single line of code.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/310.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/320.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=320)

So now we're going to  start by looking at the foundation about data and the semantic layers. To make data AI-ready, we follow these four principles. The first one is to  use consistent naming conventions and use a stronger relationship foundation. Just to make sure, what does that mean? Every table should have an identity. You need to explicitly define the primary keys, and the foreign keys need to be explicit instead of implicit. This will allow the LLM to navigate around the data using the relationships.

The second principle is a single source of truth. The last one is making use of natural hierarchies. For example, if you have the investor accounts and holdings, and then if you mirror this design, this data will be instantly understood by the LLM. If a user asks a question like, what is the total portfolio value for this investor, the LLM should direct the queries to the investor holding table and investor profile table using the pre-aggregated data instead of generating complex aggregation SQL.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/390.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=390)

 These AI-ready data principles can feed directly into the semantic files. In our semantic layer, data is represented, and this information is represented in semantic files. As you can see, using the investment manager example, we have three tables. There is funding information, investor profiles, and investor holdings. Every table has a list of columns, and each column has a list of attributes. Every piece of information adds additional context for the LLM to understand. You can see there are business descriptions, which bridge the language gap. The example values allow the model to see the shape of the real data. The synonyms bridge the language gap as well.

For these dimensional columns, we also have the distinct values because you want to preserve the values as-is so that when they perform the filter, they can perform the filter logic correctly. Finally, we also encode the relationships. Here, these two tables, investor profile and investor holding, have a one-to-many relationship. This will help the LLM write the join clause correctly.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/480.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=480)

Now,  with this, to create those semantic files manually is a tedious task and error-prone. So we actually built tools to automate and generate the semantic files. This is the block diagram that shows the whole workflow. One thing I want to point out is the last step, which is very crucial, which requires a human review of the business descriptions and the synonyms. This is an iterative process. Sometimes if you find a generated SQL is incorrect, you want to go back and fine-tune the semantic file. Sometimes you might even have to go back and update the schemas.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/540.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=540)

### Dynamic Workflow Implementation: Context Engineering, Performance Results, and Lessons Learned

So now we have the foundations. We're ready to talk about how we process the natural language queries. The first step is to  look at the query rewrite. Users do not always ask perfect questions. The query rewrite bridges the gap between the vague user intent and executable, precise queries. When a user question comes in, the query rewrite determines if the question is clear. Do I have the data to answer this question? Which part of the data do I need to answer this question? Based on these results, it makes a three-way decision: either to accept, reject, or to clarify.

For example, based on the investment profile example, if the user asks a question like, what fund is going to outperform next quarter, these are predictive questions. We only have historical data, and the system should respond and stop early. But if the user asks the question, what is the best fund, this is an ambiguity in the question. The system responds, do you mean by AUM or by a particular asset class?

With a query rewrite, the user doesn't have to understand the underlying column name. He can speak naturally and the system does the translations. It's basically teaching the user and the system to communicate better. So now the question is clear. We're going to talk about the most complex, important part of the workflow: context engineering.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/630.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=630)

 Context engineering is the difference between the AI that delivers and AI that guesses. In our implementation, the user question is combined with six semantic components. That's including the system prompt, question-specific schema, teaching examples, entity values, the output format, which is the response in how you want the LLM to respond, and the tools used. The context assembly is just to assemble these components together to create a precise prompt and use it for the SQL generations.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/680.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=680)

 One important challenge for context engineering is about managing the size of context windows. In the enterprise database, you can have hundreds of tables, thousands of columns. The resulting semantic file could easily be over 100K. It's impossible to inject all this information in the context window. So we do an actual step, just call it sending those in a condensed format to the LLM and ask the LLM to tell us what tables and columns are needed to answer these questions. And then we filter out everything else. This will result in the following payload, the result of the payload will be 90% smaller but 100% relevant.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/730.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=730)

 So other things we know, LLMs learn by teaching examples through in-context learning. So we do have memory stores, in-memory stores, of validated SQL and the question-SQL pairs. And whenever a user question comes, it's going to evaluate what are the three most similar questions based on the user question, and we'll insert it into the context. So when the LLM receives this as examples, they should be able to generate a SQL that aligns with the best practices. And then we also put a monitoring system in place just to capture all the interactions between the users and the systems. And we use the LLM to judge, to evaluate those queries to see if there's anything that looks suspicious.

And then we have the human expert periodically review those questions that might be problematic. If indeed there are errors, we're going to fix the examples and insert it into the store. As you can see, this is a continuous learning loop. In the bottom line, it's kind of like we have SQL experts that remember all the queries they write, learn all the mistakes they made. Over time, they just expand their knowledge. So we are building an institutional memory that ensures the future query is always going to be smarter than the past.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/820.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=820)

 So another important technique we use is entity search. Because the users prefer to use shortcuts, use abbreviations. So how do we leverage it? We talked about earlier in the dimensional column, we're trying to store all the values, database values in the distinct value. But in the enterprise database, you could have many entities, and each one has thousands of values. So what we are doing is that we take an extra step, put all these entity values into vector store and perform the semantic match. Only those top matches get inserted in the context windows. So this has come to the final step.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/860.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=860)

 Now we have the context ready. We're going to send it ready to send it to the LLM for SQL generations. So we do have some security guardrails in place to make sure it only generates select-only syntax. There's no delete, insert, or update. And also we have a 16-point instruction telling the LLM how to perform the join, how to create the where clause, and other rules. So when the SQL comes back from the LLM, we also perform the syntax check. Even if the syntax is in error, we're going to send the error message along with the original question to the LLM for retry, with up to three times the retry.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/910.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=910)

So only, as you can see, the validated and syntax-correct SQL will reach the database.  Our process is sound, but one thing that we found is counterintuitive, and we would like to share with everyone. We found out that more examples actually hurt performance. We used to start with having this example set that has over 100 examples, and the accuracy was not that good. What we found out is that many of these examples weren't necessary. The language model is perfectly fine generating correct SQL without them.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/970.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=970)

So we trimmed those examples to only 30, but all the remaining examples have to exist for a reason. Either they address specific edge cases or they teach business rules. This is kind of like having high-signal guidance with low noise. These examples are basically teaching the model how to think. We also spent  quite a significant amount of time building a test suite. This is as important as it provides quality gates. Every time when we upgrade the model versions or add new data sources, we need to repeat this test just to ensure there are no regressions. These are example tests to prove that the model works.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/1000.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=1000)

Now we'll look at the performance, how the current performance looks.  We're using the Anthropic Claude model. We measure the performance in three dimensions. The first one is execution accuracy, which measures the row count between the targeted SQL and the generated SQL. Currently, we stand at 93 to 95 percent. The result set comparison uses an LLM judge to compare the first 500 rows between the target and the executed SQL, and we're at about 89 to 92 percent. The average response time is 28 seconds. The response time is not only a measurement of the user experience, but also measures how efficiently we use token space and token usage.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b0309fa3f143feb9/1070.jpg)](https://www.youtube.com/watch?v=YA7Svfb0ttU&t=1070)

This is how the performance evolved over time. We didn't get this number on day one. This is through continuous improvement and making sure the model continues learning and expanding its knowledge to get to where we are now. This brings us to the lessons we learned. We  tried different things so you don't have to. What we found works is using a deterministic workflow. This is a predictable workflow that is more reliable and production-ready on day one. Also, use a clear schema, just make sure the schema is structured and well-documented. This will prevent interpretation errors.

Use surgical teaching examples, making sure every example exists for a reason. Quality is more important than quantity. Also, optimize context to make sure you send just enough information to the language model and nothing extra. The bottom line is that simple, well-engineered systems outperform shiny and complex ones. That brings us toward the end. Thank you everyone for your time and attention. We'll be outside, so feel free to come over if you have any questions. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
