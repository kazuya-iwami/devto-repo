---
title: 'AWS re:Invent 2025 - Modernize Apple platform development with AWS and EC2 Mac (CMP334)'
published: true
description: 'In this video, Vishal from AWS and engineers from Riot Games and Supercell discuss EC2 Mac implementation for game development. The session covers how EC2 Mac solved infrastructure challenges like physical hardware maintenance, inability to scale, and operational overhead. Miranda from Riot explains their migration from 60 on-premises Macs in Vegas to EC2 Mac, achieving 3X build time improvements and using Terraform for automation. Toni from Supercell describes their unique architecture using TART virtualization on EC2 Mac instances, scaling from 150 to 1,800 daily builds with reduced IT staff. Both companies leverage M4 instances with 2TB NVMe storage, Auto Scaling Groups, and AWS services like Systems Manager and CloudWatch for fleet management.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/0.jpg'
series: ''
canonical_url: null
id: 3085412
date: '2025-12-05T05:42:09Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Modernize Apple platform development with AWS and EC2 Mac (CMP334)**

> In this video, Vishal from AWS and engineers from Riot Games and Supercell discuss EC2 Mac implementation for game development. The session covers how EC2 Mac solved infrastructure challenges like physical hardware maintenance, inability to scale, and operational overhead. Miranda from Riot explains their migration from 60 on-premises Macs in Vegas to EC2 Mac, achieving 3X build time improvements and using Terraform for automation. Toni from Supercell describes their unique architecture using TART virtualization on EC2 Mac instances, scaling from 150 to 1,800 daily builds with reduced IT staff. Both companies leverage M4 instances with 2TB NVMe storage, Auto Scaling Groups, and AWS services like Systems Manager and CloudWatch for fleet management.

{% youtube https://www.youtube.com/watch?v=uFIRieQbPvg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/0.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=0)

### Introduction to EC2 Mac: Addressing Critical Pain Points in Apple Development

 Thank you all for coming today. My name is Vishal, and I'm the Senior Product Manager for EC2 Mac at AWS. I'm excited to be joined by two of my customers that I work closely with. We have Miranda from Riot Games and Toni from Supercell. I'm looking forward to hearing from them about how they use EC2 Mac to build and test applications for their organizations.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/40.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=40)

I'm going to start by covering the history of EC2 Mac and how we came to develop it at AWS. I'll discuss the major pain points and key struggles that customers told us they faced, and how Amazon uses EC2 Mac internally.  Then we'll pass it over to Miranda and Toni to share insights into their own organizations about how they've implemented EC2 Mac and the benefits they've seen.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/70.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=70)

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/90.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=90)

Let me make sure everyone can still hear me. All right, awesome. Let's take a step back and understand the application development lifecycle for Apple platforms. In the earlier stages, we have the design, code, and develop stage. These stages aren't necessarily macOS dependent,  but as we move into the build stage and beyond, you need Xcode, which everyone knows requires Mac hardware to run. This means you absolutely need to buy Mac hardware and scale it as your applications grow.  It's this dependency between Xcode and macOS that made Apple development on any hyperscale impossible until we introduced EC2 Mac back in 2020.

Today, the total market for this opportunity is well over 34 million developers, many of whom are in this room or will be watching on YouTube when this gets posted. The last time I made this presentation, there were about one million apps on the App Store. I believe we've now scaled to two million apps and more than two billion active devices. Apple has also introduced many new platforms, including Vision OS that came about a year or two ago. Today with EC2 Mac, we have customers across different industries and functions building, testing, and developing applications for each of these platforms.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/100.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=100)

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/140.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=140)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/150.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=150)

 I want to share with you some of the key struggles that our customers told us about when they were giving us feedback around EC2 Mac and the business case.  The first challenge was infrastructure overhead. Miranda is going to talk a bit about this in the future, but I'm excited to discuss how the ability to deal with physical hardware in their data centers was something customers absolutely hated. They had to drive multiple hours, fly to different data centers, unplug machines, deal with data center technicians, and manage all that headache, which cost time, money, and other resources.  Pinterest gave us a really good quote about how the physical challenge of going to data centers was a real big struggle for them, and that's one of the big reasons why they use EC2 Mac.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/180.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=180)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/200.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=200)

 The next challenge was the inability to have automatic fleet management. Groupon told us about this, and it definitely increased their time to market. Every time something went wrong, they had to get IT involved, and this was a real big struggle that they've solved by moving to EC2 Mac.  The last challenge was the inability to scale. Sometimes your workloads require fewer resources, and sometimes they need more. We see a lot of customers who have fewer builds or tests on weekends, so they scale down and then scale back up on weekdays. This was one of the big benefits that IQVIA saw in EC2 Mac, and they're optimizing by using various resources like auto scaling groups with EC2 Mac. All in all, this reduces time to market and accelerates innovation for your organization.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/230.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=230)

### Engineering EC2 Mac: From 2018 Mac Minis to M4 Pro Hardware

 This is where we brought in EC2 Mac back in 2020 here at re:Invent. This is the picture we always use for EC2 Mac. This is a 2018 Intel Mac mini, which was our first ever Mac hardware that we retrofitted in our AWS data centers. These are connected with different PCIe to Thunderbolt cables, and this is an example of one droplet, as we call them on AWS, but essentially it's one server. When you as a customer use EC2 Mac, you have full access to the underlying hardware offered by Apple. The PCIe to Thunderbolt connections allow us to implement AWS Nitro, which gives you all the networking and EBS bandwidth that you get with the rest of AWS.

Engineering teams had to do a lot of manual work to make sure that Apple hardware works with EC2 hardware. We've even gone so far as to have some of these older platforms equipped with robotic fingers that actually press and hold the power button to turn the machines on and off. It's a lot of work, and I give huge credit to my engineering teams for building the patents and everything needed to operate these machines. But this photo is too outdated, so I decided to bring in some new photos.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/310.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=310)

Today here we see  some of these are our M2 Pro, M2, and the most recently announced M4 and M4 Pro Mac minis as well. This is exactly how it looks in our data center. On the upper left hand side you can see that the form factor remains about the same. We still have PCIe to Thunderbolt connections, but the Mac has changed. As you know, the M4 and M4 Pro Mac minis have gotten almost the size of an Apple TV essentially, and as many of you know, the power button has gone to the bottom. So there was a lot of engineering needed to make sure that the same hardware functionality and the reboot mechanisms all work.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/370.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=370)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/380.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=380)

Some of these slides you might not be able to see on the screen, but the M4 and M4 Pro, which we announced on September 12th, come with new internal SSD drives that a lot of our customers are using. I know Miranda and I talk a lot about this as well. This is something that we've added, and while it's not really visible in the pictures here, it definitely gives customers a lot more performance for local caching mechanisms as well.  I want to quickly cover why customers continue to use EC2 Mac and continue to stay with EC2 over the course of now what's going to be about six years. 

### Automation and Operational Efficiency: SIP, ImageBuilder, and Security Integration

Number one of the pain points that we talked about earlier with IQVIA, Groupon, and Pinterest was that they really wanted to improve their automation story. Without going into too much detail around every single spec that we have, we've now built automation for some of the highest requested features that customers have. This includes the ability to disable and enable SIP, which many developers need to test their custom code. For those that don't know, SIP is System Integrity Protection. It's one of the most painful things about working with Mac hardware and macOS.

Our engineering teams have done a really cool job on this. Just to give you a look behind the scenes of what SIP actually involves, we have custom software that goes in and mimics what a mouse movement and GUI interaction looks like when you actually click on a user. Every time we change a particular macOS version, we have to make sure that the microservice we have is able to recognize the newest GUI elements. For example, with the latest release of Tahoe that we just announced a few weeks ago, Liquid Glass made everything very tough for us with different pixels, different placement, and the way that the user icon actually shows up on the screen. Our engineering teams had to spend a lot of time to rework everything to make sure that it works and meets our security standards.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/470.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=470)

SIP is just one example of features that we've had to do a lot of effort on to make sure that it works as effectively as it did on Sonoma or Sequoia before. We talked a little bit about operational efficiency struggles that customers had before. One of the big things that we introduced a year or two ago was the ability to integrate with ImageBuilder on AWS.  Customers today have both Intel machines that run on x86 AMIs and Apple Silicon AMIs, and they've been able to use that for various different services like ImageBuilder, but you can also use Packer to make your custom images.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/500.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=500)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/510.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=510)

A few years ago we introduced Replace Root Volume, which allows you to provision fresh new macOS environments very quickly.  A lot of customers also have a very important monitoring and logging story, and so obviously CloudWatch is natively integrated, but then we also have other integrations with other monitoring services.  Lastly, everything is secured through your Amazon VPC, and so with IAM controls and access, it works just like any other EC2 instance that we have.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/530.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/540.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=540)

### 2024 Innovations: M4 Launch, 2TB Storage, and Customer Success Stories

I'm excited to share what we've delivered this year. Firstly, we talked a little bit about this, but M4 and M4 Pro are now GA and have been GA for about two and a half months now.  We have a really cool video on our website if you guys want to go check that out afterwards.  We also improved our resiliency story by introducing support for dedicated host auto recovery and reboot-based migration for instances. If hardware fails, as our CTO always says in our keynotes, but essentially any time it does, we've now introduced better resiliency measures so that your instances can reboot onto other hardware, and you don't need to worry about maintaining that transition when an instance or hardware goes down.

Going back to number one here, I want to point out the two terabyte instant store volume. Historically, Apple gives us an internal SSD, but unfortunately due to the way that we do our terminations and reboots, if something goes wrong, we won't be able to capture that data and move it to another instance. With the two terabyte local store volume, we're essentially giving you more space for your caching mechanisms all at no additional charge to what we have organically with our pricing mechanisms with EC2 Mac. This has been one of our biggest customer asks.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/610.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=610)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/640.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/650.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=650)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/680.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=680)

You can still use EBS volumes, but this is essentially free storage for you to optimize using your AMIs.  Just two weeks ago, we introduced Amazon DCV support for EC2 Mac. DCV is a high display resolution protocol that can give you up to 4K resolution when you access Mac instances via GUI. This is currently free to use and is generally available as well. Many customers have been using DCV for better simulator-based testing or unit tests for any of their applications that need a GUI interface. We delivered System Integrity Protection configurations earlier this year, and most recently we delivered macOS Sequoia as Amazon Machine Images as well.  Here is the current landscape of where our platforms are today.  We continue to support Intel Macs as well. As we all know, Apple is transitioning away from Intel, but we still support those with Sonoma and Sequoia images. Along with the latest M4 and M4 Pro, which now come with 2 terabyte discs, I highly recommend you tune into Matt Garman's keynote tomorrow for some additional future announcements coming on EC2 Mac. 

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/710.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=710)

I'm super proud and happy to share that these are all just a handful of customers using EC2 Mac today. As you can see, they're customers across different platforms and different industry verticals, and we have a good selection of partners as well that can help you ensure you get the right and best usage out of EC2 Mac. At a high level, these are some of the benefits that customers have shared with us, and this is also available on our public pages.  The one that really speaks to me is the reduction in build time as well as the reduction in build failures. This is one of the biggest reasons why customers continue to come to EC2 Mac. They view the entire AWS ecosystem with the networking and the resiliency measures to be among the best. Many of our customers have had their own data centers go down, and they come to AWS saying they never want to face that challenge again. They have been very happy with the resiliency aspect of it, especially.

All in all, we continue to invest in this platform very heavily. Looking forward, we're continuing to support the latest Mac hardware, increase our regional availability, and increase our global availability of Mac hardware across different types. As a product manager, we always work backwards from the customer. If any of you have feedback or interest in trying out EC2 Mac, there's no room for Q&A on stage, but I'm happy to chat with anyone afterwards to discuss your interest and how we can get you started with EC2 Mac. With that being said, I'm super excited to pass it off to Miranda, who's going to talk to you about Riot's experience with EC2 Mac.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/800.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=800)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/810.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=810)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/830.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=830)

### Riot Games' Journey: Escaping the Vegas Data Center

Hi, I'm Miranda Pearson. I'm an IT Systems Engineer on the IT Infrastructure Team at Riot, and we support the Build Farm by providing infrastructure as a service to those teams so they don't have to think about build infrastructure and can just focus on what they do best: building games.   This journey will start from our starting point, work through the journey we took to get there, how we rebuilt, the choices we made, what we got out of that, and what's next. At Riot, just to give context, we use Apple development workflows for three different things.  We use them for signing workflows, game builds, and audio asset processing. These three workflows have different shapes, different requirements, and different needs, and that ultimately shapes how we meet those needs with EC2 Mac.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/850.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/860.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=860)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/880.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=880)

Our story starts in Vegas actually at Switch Data Center. We had a pool of about 60 hardware Macs on-premises. We were losing about 2 percent a year and we were plagued with issues.   We hit the limits of what we were able to patch out performance-wise when we started coming up against the constraints of the data center design and the custom mounts that we had to use in that space. That was a picture of our on-premises fleet, the cheese graters in the rack on the previous slide. This next one is also the trash can from our original on-premises fleet. We had drives literally melting, not an exaggeration at all, literally melting.  VMs kept running if storage was gone and we didn't know until someone else told us they were gone and the builds weren't working.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/920.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/930.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=930)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/950.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=950)

Doing one set of repairs took away about 8% of the fleet every time we had to repair.  Next, we'll get into the forcing functions and the pain that pushed us into a point where we had to do something about it. As we talked about earlier, the heat,  heating, cooling, IO storage, and compute limits were being hit, and that was trickling down into developers, which impacted build times, which impacted build velocity, which impacted the way we were able to deliver games and changes to players. 

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/970.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=970)

The biggest pain for our team was being on-premises and physically here to maintain our stack, to do a firmware update, to do a patch, to do a quick fix. We had to get on a plane and actually come to Vegas to do that. All of that just became unsustainable.  The first trigger that started our journey was that Riot made the call to get out of data centers as a business. We were no longer going to be maintaining on-premises hardware, so we had to start the journey of moving out. The second was the forcing function of a security incident that required us to rebuild completely from the ground up. We couldn't reuse any of our original infrastructure and had to rebuild in a safe space, so AWS was a great landing spot for us for that.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1010.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1010)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1030.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1030)

Another issue was future incompatibility. We were on VMware at the time and ESXi 8 was dropping support for Mac, so we also had no choice. Pretty much speaks for itself, but we hit the breaking point where it was not going to scale for us to get on a plane and physically come in person to maintain a fleet that wasn't going to scale with the workflows and workloads of developers that are building games and experiences for players.  Next, we'll get into how we rebuilt. 

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1060.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1060)

### Riot's EC2 Mac Implementation: Terraform, Lambda, and 3X Performance Gains

EC2 Mac was our savior. AWS had our back with the capacity that we couldn't get easily on-premises. We were able to stretch and expand that as needed, and we no longer had to get on a plane to come here and physically maintain our infrastructure. The turnaround time for lifecycle was a lot more smooth, and we were able to automate things. Terraform became our lever to get out of our trash can trauma cycle and  turn our physical fragility into something that we could maintain as code.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1070.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1080.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1080)

For our customers, the journey  starts when a customer submits a pull request to a GitHub repository that triggers Jenkins to run a build and do a Terraform plan to provision those resources.  Once that plan goes into place, it starts to pull in an AMI, which is maintained by our product security team and then shared in a centralized account and shared out with customer accounts. Once that Terraform hits the customer account, it uses that AMI which has baked-in certificates, security features, and basic tooling that product security mandates for our production workloads.

In that account, Jenkins uses STS to assume a centralized role that we share and maintain so that teams don't have to do that locally in their accounts. There's a codified representation of that access. With that temporary credential, they're able to go into the account with that secure base AMI, launch the instance in the account, and then from there we have event-driven automation which will pick up that there's a new instance and bootstrap it or do whatever it needs to do in response to that new instance spinning up. If there are any failures there, that will go down to CloudWatch and we'll get notified.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1170.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1170)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1190.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1190)

The bootstrap is done via Lambda and we're able to do deterministic and dynamic bootstrapping with Systems Manager. The Systems Manager documents are maintained and shared from a centralized account,  which is a tooling account, and those are shared down from our customer accounts so that we can maintain a single source of truth for those bootstrap documents and patching documents. So what changed after we made the lift and shift? 

Our first shift from on-premises Intel to EC2 Mac Intel showed a 3X improvement in our main build time. Signing increased by 2X and Code Build improved by 2.5X.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1220.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1220)

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1250.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1250)

With CodeBuild 2.5, the mere lift and shift allowed us to get rid of a lot of the noise we were seeing with on-premises hardware failures and really see the signal.  Our first shift after migration was to move to Silicon. We chose to do a hardware and software upgrade at the same time, which was able to amplify our gains and get us faster, cheaper, better scalable infrastructure for our build teams. 

For us, fleet diversity isn't just an exercise for fun. We have a lot of different shapes of workloads, and these are how we map the shapes of those workloads to the different EC2 Mac instance types. The greatest thing about EC2 Mac that we get now is that teams don't have to lock into a specific instance type. They're able to evolve as their workloads, workflows, and pipelines do to meet the needs of what those builds are. Something like Unreal might rely on M4 or M4 Pro, and we're continuing to see how to evolve our fleet for that.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1300.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1300)

Where we're going and what's next, as we continue to evolve and operationalize and optimize our fleet, our next steps are to explore shared capacity to see how we can make that capacity feel more elastic for teams rather than being tied into a specific node.  We're switching more of our ARM workloads to M4 and M4 Pro to get better performance and increased build throughput. Asset caching is another priority because we have huge Perforce depots of about two terabytes for a build, and those can get really size heavy. Anything that we can do to cache those assets ahead of time makes it easier for build engineers and developers.

Finally, we want to explore unlocking better storage bottlenecks by using local NVMe across different workloads. Now that I've shared our journey about what we've done with EC2 Mac, I'll hand it to Tony to talk about how Supercell did the same thing in a slightly different way.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1400.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1400)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1410.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1410)

### Supercell's Approach: Escaping the Basement with Virtualization and UTM

Thank you, Miranda. Let's take a look at what Supercell did in terms of the journey to using EC2 Mac.  Supercell is a Finnish-based gaming company well known for Clash Royale and Clash of Clans.  We had a similar situation. We didn't escape from a Nevada desert. We escaped from our own basement, where we had a lot of Macs running the same way with those cheese graters and all the other stuff. My name is Tony SyvÃ¤nen. I'm a senior cloud governance engineer working for Supercell.

I want to acknowledge that my colleague Jarno Morimai actually led the creation of this solution and also created this presentation. My part in this was working on the architecture and troubleshooting problems from the setup itself. As a background, we had a lot of those Intel-based Macs downstairs as well. A lot of these teams, what we call cells, hence the name Supercell, are really independent. Each of those teams had their own hardware that our central IT team and the build team were maintaining for these teams. Because those cells are super independent, they of course had quite a different looking build infrastructure as well, which was part of the challenge here.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1490.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1490)

We have a similar story. We had a lot of zombies in the basement.  They were growing in the dark, moist environment. I don't know how they got there. We're going to look at how we went through getting them quarantined, basically what the solution was. We'll look a little bit deeper into the architecture just like Miranda did.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1520.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1520)

We do have a different kind of architecture, so bear with me. We'll look at the results and then learnings and what we're actually going to build next. But let's start with the on-premises infrastructure. The on-premises setup was all Intel Mac  back in 2023. Things were going fine in there in the darkness, but there were a lot of problems with the hardware itself. We didn't have to fly somewhereâ€”we just had to go downstairsâ€”but it was still taking quite a lot of time for our IT engineers to reinstall the Macs, troubleshoot them, and deal with hardware issues.

Also, with Riot Games, VMware was basically saying that they're not going to be supporting ESXi on Mac hardware, especially on Apple Silicon. They're not going to support it at all, so we had to find some kind of solution for how to keep running our build infrastructure. In the worst days, it could be that one-third of the whole Mac fleetâ€”we had multiple racks of this hardwareâ€”was actually not operable at all. Our game teams were suffering because build times were getting longer and longer. They had a lot of builds in the queue, so it was causing quite a lot of problems.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1630.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1630)

At the same time, while we had one-third of the fleet broken, they wanted more. New game teams were popping up and wanted to be able to develop, sign, and build their games. The live games, something like Brawl Stars and Clash Royale, were growing and wanted to do more builds. So we had a whole conflictâ€”this friction between broken hardware and at the same time high demand for new things. We had two options. One option was to start building a data center somewhere to actually get out from the basement. But then we remembered back in 2023 that a wise man had told us, "Friends don't let friends build data centers."  This is a quote from 2013. It's still valid in 2023 and it's still valid today as well.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1660.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1660)

Instead, we went with looking at EC2 Mac instances, which had already launched at that pointâ€”multiple years earlierâ€”but we at Supercell hadn't looked at them until the Mac instances actually became available on EC2 Mac, or the Apple Silicon instances. Our solution is based on these four pillars,  basically the weapons that we use to start slaying the zombies in the basement. First off, of course, the EC2 Mac instances, getting especially the Apple Silicon ones running right out of the gate. That would actually improve the performance like we saw from the numbers, and also we don't have to go to the basement anymore. We can actually look into the sky and the clouds and just maintain it that way.

Unlike Riot Games, we actually ended up using virtualization on top of the EC2 Mac instances. I'll go into details why, but we use UTM, which is based on the official Apple virtualization framework. So it's actually using Apple's own macOS native virtualization to spawn officially supported virtual macOS on top of the Mac hardware as well. The reason for that is that the EC2 Mac instances still have a scrubbing time that takes quite a while when you switch the AMI. There is, of course, now the Replace Root Volume mechanism as well, but back when we started developing this solution, that was not available yet, so we went with virtualization.

The benefit of that is that we can actually really quickly switch between different UTM VM images, which more or less behave like containers if you're familiar with them. You can quickly start the machine as well. On top of that, of course, we use Terraform to manage this whole infrastructure, and we use Packer to create those UTM VM images as well. The fourth pillar is Nix, which is a nice tool that you can use to define what kind of tools you want to have installed in the operating system itself. You can list the version of Xcode you want, the version of whatever other tools you want to use like the AWS SDK. You can list those and then just run Nix to customize the image on the fly.

This happens on every build actually. So when the build starts running, Nix customizes the machine on the fly just before the build itself starts.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/1850.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=1850)

### Supercell's Architecture Deep Dive: Auto Scaling, TART VMs, and 10X Build Growth

Supercell was born in the cloud. When we were founded in 2010, everything was already built on AWS, so we had years of AWS experience, especially with EC2 and all the surrounding services. Let me walk you through how we implemented these tools and pillars. We started with the AWS Region, VPC, and multiple Availability Zones , and on top of that we have the Auto Scaling Group.

What is special about EC2 Mac concerning Auto Scaling Groups is that if you only put an Auto Scaling Group in place, it won't work. The reason is that EC2 Macs are actually dedicated hosts, so you have to provision a dedicated host first, and then on top of that you can provision an EC2 instance. There is an extra lifecycle step here, but AWS has another service that can help you manage the relationship between the Auto Scaling Group instances and the physical on-demand machines: AWS License Manager.

The License Manager has a feature called host resource group, which can automatically provision on-demand machines based on the Auto Scaling Group's request for new hardware. What is really neat about it is that if you don't have any instance running on that on-demand machine for a while, it will try to remove it for you. With EC2 Mac, after the 24-hour Apple-mandated minimum time to have the on-demand machine allocated to you, the License Manager will make sure that it gets removed. The cost for that dedicated host will stop at that point.

The way you attach the Auto Scaling Group to the License Manager is through the EC2 Launch Template. You specify that you want to use this host resource group as the source of hardware for this Auto Scaling Group. We also licensed the TART virtualization engine, which is open source but offers official support through licensing. The License Manager is a nice way to ensure we never go above the number of licenses we have purchased.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2000.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2000)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2020.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2020)

Now we can get those physical machines, those EC2 Macs. We launch an EC2 instance and use the default AMIs that AWS provides . We don't do any customization of the AMI itself, but we do use user data. When we launch the latest AMI from AWS , the user data makes sure that Cirrus Lab TART gets installed and then quickly kicks in the image we want to run on top of that EC2 instance. In the user data we launch a virtual machine, and in that virtual machine, the TART VM image that we custom-built with Terraform and Packer has several things.

First, it has GitHub Actions and the GitHub Actions runner, and it has the Jenkins agent. We are in transition from Jenkins to GitHub Actions more and more per team, so each image has support for both. To make the decision about which one is being used, we coded our own agent called the Coco agent, which is our own scripting code that looks at the metadata and knows which team it belongs to. If that team uses Jenkins, it connects to the Jenkins controller at the appropriate IP address. If the team is using GitHub Actions runner, it connects to GitHub.com to get the jobs and put the machine in the queue.

Into the image we have also built Xcode and all the other command line tools.

Along with any other default tooling needed for signing and so on, we also have the ability for teams to customize whatever they want at runtime as well. So that's what's in here, and how does this all get built?

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2130.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2130)

The Supercell Developer Services team, which takes care of running this environment, uses GitHub Actions with Packer to build the TART images. Those VM images are then saved on EFS, the elastic file storage, which is mounted to the EC2 Mac instance on the base layer macOS. This allows TART to quickly switch between any Xcode versions, macOS versions, and so on. We have all of those different variations and different combinations built on the EFS storage, allowing us to quickly take them into use. 

Some scripts are stored in S3 as well. What's unique for us is that we don't have any EBS volume other than the AMI locked into this. Instead, we use the local NVMe storage as the disc for the TART VMs. We could use EBS for it, but we found that the EC2 Mac NVMe is really high performing, and as Vishal said, it comes for free. So why not use it? We're taking full advantage of that local NVMe to make the builds really snappy.

We also use the parameter store to store secrets like how to sign the applications, because all of our games are mobile games, so we need to sign them officially all the time. None of the people have direct access into the EC2 Mac or even the TART VM. Since we are running the TART VM on top of macOS and we do need to sometimes go with VNC inside the TART VM, we use the Systems Manager to tunnel ourselves into the macOS itself so nothing is exposed to the outside world. You have to have AWS access and you have to have the permissions given to you through the SSM and IAM to be able to even connect to the machine itself.

This is how we run the overall infrastructure. We use CloudWatch Logs and CloudWatch Metrics to get basic monitoring, but on top of that, our guys in the Developer Services are also using Telegraf and OpenTelemetry to get really detailed tracing of how long each stage of the build takes and what happens there as well. This is something we developed in-house. We're using both CloudWatch and Grafana Prometheus to have our own metrics as well. This is how we run the majority of the build farm for building our games for iPhone, iPad, and so on.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2330.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2330)

It's run by an auto scaling group. There are some standalone machines as well for really corner cases where they actually need to get the hardware directly. For example, if they need the GPU, we cannot use the TART VM because GPUs are not exposed directly into the virtual machine itself. So we have some cases of standalone machines as well. 

After we got the solution running, how's our life after the zombie apocalypse was dealt with? We did manage to escape the basement. I think we only have a few of those Intel Macs still running in there for some really old build jobs because we still have some developers who still believe in the Intel MacBook Pros as well. So we kind of have to still have a couple of Intel-based builders just for those guys until we can convince them to move towards Apple Silicon as well.

We do use auto scaling groups, so we have automation that every weekend we shed the load and scale down to the minimum number of machines. Then on Monday, or actually nowadays also on the Sunday side, we start scaling up the fleet of builders so we can get some cost optimization in the play as well. At least our IT is really happy because they don't have to go to the basement anymore and give some daylight into the Mac and just switch the cable around and now it works type of thing. So we actually reduced the operational overhead significantly.

There used to be multiple IT staff going downstairs to do physical work on fixing the Macs. Now we have basically one person taking care of thousands of machines on AWS, which is a really good improvement. In terms of numbers, even for us we now have 10 times more build shops than before using the solution, but we only needed 3.3 times the number of Macs. That is a really good result, especially considering that one person is maintaining this. We used to have around 150 build shops per day on Macs in the basement, and nowadays in the cloud we have over 1,800 builds happening daily. There is quite a lot of growth that we were able to achieve with this solution.

Additionally, we can now take new versions of macOS into use quite quickly because everything is virtualized. We can switch to the latest version of macOS in the TART VM even before the official AMI is released, without updating the underlying Mac host itself. This flexibility is a significant advantage of our cloud-based approach.

### Lessons Learned and Future Roadmap: M4 Expansion and Upcoming re:Invent Sessions

What did we learn in this journey over the last two to three years? In the beginning, back in 2023, a lot of this was completely undocumented. There was some experience with EC2 Mac instances, but because we wanted to do virtualization, there was essentially no documentation. We had to do a lot of trial and error figuring out how the TART VMs worked in production. macOS and Apple can give us a lot of curveballs in terms of macOS updates. When we tried new versions, we encountered new security features that required manual enabling on every machine.

It has helped us tremendously to work closely with our AWS account team to plan out our capacity. We are mostly European-based with some studios in Shanghai and North America. Clear communication about our machine needs has been essential to moving forward. Overall, it is not a cheap solution. EC2 Mac instances are not the cheapest types available, being dedicated hosts, but neither was the manual work we were doing downstairs. We went from having five to seven people maintaining the environment in the basement to just 1.5 people maintaining this larger environment in the cloud. For us overall, it was still a win-win situation.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2500.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2500)

It had a huge impact and unlocked a lot of things for the game teams themselves to develop faster and try new things more quickly.  Looking at what the future brings us beyond the zombie apocalypse, we are really excited for the new M4 instances and the two terabyte local NVMe storage because it allows us to make the images bigger. The old machines had 256 gigabytes of disks. If you look at any kind of modern game building, we are lucky as a mobile game company that we can still fit into that, but I would imagine a lot of other companies like Riot Games would not fit into 256 gigabytes to build anything at all.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2630.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2630)

We are still looking forward to the two terabytes because that would unlock us for the first time to run two VMs per physical EC2 Mac. We would basically be able to double our capacity without any extra cost using the VM approach. What we do not have right now is fully dynamic auto-scaling. It is basically on a calendar, so it knows that every Monday it must scale up to a certain number and every Friday it has to scale down. In the future, we will have dynamic scaling through the Cocoa agent that was shown earlier. 

This will enable dynamic following of the size of the queues, so we'll be able to automatically, with our own little coordinator tool, deploy new machines and retire them quickly if the queue size of the builds is at zero, for example. In general, this has also now unlocked the ability for people to do more work on the actual improvement of the pipelines themselves instead of trying to do troubleshooting day to day on the fleet. So there will be better pipelines coming, and we're going to now be able to extend with all the automation in placeâ€”the Terraform, the Packers, and all this stuffâ€”we can now easily expand it to new regions as well.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2780.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2780)

For example, we are now deploying it in the US as well because we have a couple of game teams now based in the US in Seattle and Los Angeles. We can now quickly, with the already built Terraform, just bring up the whole environment for them in the US as well so that they can have quick and snappy builds in the US. Lastly, on the zombie team, they're coming for you, Barbara, so if you're not careful,  the zombie apocalypse can also happen to you if you're still stuck on your EC2 Mac, on your Intel Mac somewhere in the basement or closet, hiding in the dark corners of the world.

Excellent, I'm going to actually hand it back to Vishal and he's going to tell you more about what else is coming in terms of the EC2 Mac presentations. But other than that, you survived the zombie apocalypse with me and thank you. Before we wrap up here, guys, thank you so much for spending time with us here today. Thank you to Tony and Miranda for working with me through the multiple months that we had to put this together.

Before I let you guys go, I did want to tell you what else to expect at re:Invent for EC2 Mac and macOS in general. First, as I've hinted, please make sure to not miss our AWS CEO's keynote tomorrow. Matt Garman will be speaking, I believe, at around 8 a.m. until 10:30. Please tune in for some exciting news on EC2 Mac during his presentation.

Secondly, we do have a few sessions that I wanted to call out and make sure that they're on your radar. The first one is going to be on Wednesday the 3rd. The ID is CMP 306, and that one is actually going to be a hands-on workshop where you guys can actually play around with a lot of the features that we talked about today such as System Integrity Protection and Replace Root Volume. I'm super excited to actually get your laptops out and actually play around with that, and two of my colleagues will be presenting that as well.

On the same day we have another session that's called CMP 346, and this one is actually really cool because it's the first time we're going to be talking about how we can do ML and inference using EC2 Mac and the amazing GPU power that comes with Apple silicon. Some of my colleagues are going to be presenting on that as well. It's the first time that we're going to be talking about running actual ML and inference workloads using Apple silicon chips, so make sure not to miss that.

Then if you want to see me again, I'll be talking at CMP 344, which is again on Wednesday, and that is actually me and my colleague going through and doing a live demo on using EC2 Mac and GitHub Actions, or GitLab, Circle CI, whatever automation tool that you guys use. He'll actually be going through and running an actual build change using a live demo in Xcode to see how it actually runs on EC2 Mac and you'll be able to see the live change on the TestFlight app that we will be screen sharing as well.

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/be1422083d172311/2830.jpg)](https://www.youtube.com/watch?v=uFIRieQbPvg&t=2830)

These are three other sessions that I highly encourage you guys to join, as well as Matt Garman's keynote tomorrow and any of the other keynotes as well. With that being said, I think a few of us will hang around if you have any questions, but thank you so much for coming.  Please make sure to complete the session survey in the app as well. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
