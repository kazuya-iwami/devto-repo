---
title: 'AWS re:Invent 2025 - Next-Gen Scale for Next-Gen Playâ€”EA''s Apex Legends & Amazon GameLift (IND204)'
published: true
description: 'In this video, Trevor Moore from AWS, Tom Penrose from Code Wizards, and Jared Cugno from EA Apex Legends discuss the successful migration of Apex Legends to Amazon GameLift servers. They detail the 10-day, zero-downtime migration that improved player experience with reduced server degradations while maintaining developer velocity. Key topics include GameLift''s fully managed infrastructure supporting 55+ regions, the custom Portal and Wrapper architecture enabling gradual traffic shifting, comprehensive testing strategies including 100 million CCU validation, and post-migration benefits like enhanced observability and regional flexibility. The presentation emphasizes player-first approach, metrics-driven decisions, and the partnership model that delivered measurable performance improvements without disrupting live service operations.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/0.jpg'
series: ''
canonical_url: null
id: 3093103
date: '2025-12-08T20:13:38Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Next-Gen Scale for Next-Gen Playâ€”EA's Apex Legends & Amazon GameLift (IND204)**

> In this video, Trevor Moore from AWS, Tom Penrose from Code Wizards, and Jared Cugno from EA Apex Legends discuss the successful migration of Apex Legends to Amazon GameLift servers. They detail the 10-day, zero-downtime migration that improved player experience with reduced server degradations while maintaining developer velocity. Key topics include GameLift's fully managed infrastructure supporting 55+ regions, the custom Portal and Wrapper architecture enabling gradual traffic shifting, comprehensive testing strategies including 100 million CCU validation, and post-migration benefits like enhanced observability and regional flexibility. The presentation emphasizes player-first approach, metrics-driven decisions, and the partnership model that delivered measurable performance improvements without disrupting live service operations.

{% youtube https://www.youtube.com/watch?v=UClWz0cB4lA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/0.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=0)

### Introduction: EA's Strategic Partnership with AWS and the Apex Legends Challenge

 Let's go ahead and get started. Thanks everybody for being here. Hope everyone's having a great re:Invent. Today we're going to be talking about one of the most exciting evolutions in online gaming. It's the migration of EA's Apex Legends to Amazon GameLift servers. To be honest with you, this is really more than a migration story. It's a story about the motivations, the challenges, the learnings, and the solutions that really drove the transformation.

The hope is that you'll learn a little bit more about GameLift servers and what we really see here at AWS as a shift in how multiplayer games are built and scale. Quick introduction, my name's Trevor Moore. I lead the relationship between Electronic Arts and AWS. The real stars of the show are the folks on stage with me, Tom Penrose, Chief Online Services Officer for Code Wizards, and Jared Cugno, Head of Technology for EA Apex Legends.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/60.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=60)

 Quick review of the agenda. So we're going to start off today by giving just some context to the relationship between AWS and EA. We'll jump into the evolution of Apex Legends. We'll give a little bit of a deep dive into multiplayer games and the challenges involved with operating them at scale. We'll chat more about GameLift servers and some key benefits there, and then we'll highlight the partnership with Code Wizards and what good looks like when migrating a live service game from one platform to another.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/90.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=90)

 So let's start off with a little bit of background with EA and AWS. It's a 10+ year relationship. At its core, our relationship with EA really revolves around three key pillars. The first is it's a strategic partnership. So EA is one of a small handful of customers here at AWS that we call strategic. Obviously there's a little bit of a financial aspect involved there, but more importantly, EA plays a significant role when it comes to how AWS builds products and services, whether it's a new region or an offering like GameLift. EA's voice is definitely heard on the AWS roadmap.

The second is innovation. Something that we often talk about with EA is regardless of your role or title, AWS is the platform for innovation. Moving to AWS, it's not just about saving money or scaling up for a big launch, which we do those things very well, but it's about having the building blocks to stand up a net new business or concept from scratch in days or weeks as opposed to years.

Lastly is the player experience. This is a non-negotiable. At Amazon we talk a lot about being customer obsessed. At EA it's all about being player obsessed. So whether it's a POC or a production ready workload, any conversation, any decision, any idea always starts with the player in mind.

So with that said, I think just to kind of tie this all together here a little bit, I think it's really important to understand how EA operates as a business. They have all of these studios all over the world that make amazing IP like FC and Madden, Apex, Battlefield. These studios, they all have the ability to pick and choose whatever technology they think is best for their team. Oftentimes that is AWS, but not always.

So generally speaking, EA operates like a federation. They're associated, but they're not always unified when it comes to technology. On top of that, as we all know, technology becomes outdated fairly quickly, which was one of the challenges I think that Respawn was starting to face as they're operating this super successful live service game at scale. But before we dive too deep into the technologies and kind of the solutions here, let's talk a little bit more about Apex themselves.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/240.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=240)

### Apex Legends' Explosive Growth and the Need for Modernization

 If you're in this room, you're probably likely familiar with Respawn. It's the studio behind Titanfall, Apex Legends. They were acquired by EA back in 2017. The team was mostly made up of ex-Call of Duty, Activision guys, a lot of AAA legends, super deep understanding, and really a proven ability to build first-person shooter games.

Fast forward to 2019. Apex Legends launches. It was a really interesting event. The game dropped out of nowhere. There was no marketing. There was no nothing, but it just absolutely took off. I think it went from 0 to 2 million players within the first 24 hours. Within the first month it reached 50 million players. It's done well over $2 billion in revenue for EA.

Apex Legends itself is a battle royale style game. It's really movement-based, high twitch, latency sensitive. It's a live service game as we all know, so a lot of new content, new seasons dropping every 3 months.

If you know anything about Apex Legends, you know they have a very passionate fan base and a community that's not afraid to voice their opinion, which we all love to see. I think what we observed a couple of years back was that the expectations of the players were starting to evolve, and cloud technology around this time was changing. Game server hosting was starting to change around that time as well.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/330.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=330)

 I recall sitting down, I think it was late 2023, with Jared in LA. It was very clear that Respawn was starting to look at different hosting options. Yes, they wanted to save on costs, but there were other key areas that they wanted to explore as well. First, the player experience needed to be better, meaning scale, but there were also some ongoing issues with latency, lag, and packet loss, which was something that we heard a lot about. There was definitely some room for improvement in those areas.

Second, the Respawn team wanted to modernize their hosting. Respawn wanted better access to things like logs and observability. They wanted to iterate faster and have access to modern hardware, which meant moving off of bare metal. One thing that became super clear to me and the rest of the AWS folks was that if Respawn and Apex Legends were to make a move, parity was not good enough. Jared made it very clear that they needed to have some real data-driven impact to justify any changes they were going to make to their service.

Lastly, and I think this is a really big one, it's super critical for a studio in general, but especially a studio like Respawn, to maintain developer velocity. Apex Legends is constantly pushing out new versions and new updates to the game into production. Those launch dates and new seasons don't shift, so speed and agility is really essential for a studio like Respawn. They were really looking for not only a change, but they were looking for a technology that was going to adapt with them and not vice versa.

With all that said, Respawn was really looking for a new partner, a new platform, and an opportunity to innovate and improve. The reality is, I think Respawn is probably not alone. Probably a lot of folks in this room are in the same boat. These multiplayer games are hard, really hard. They have unique challenges, and before we talk about how we solve those challenges and the solutions that we have to help you out, I'm going to hand the mic over to Tom, who's going to talk more about the nature of multiplayer games, what makes them so difficult, and how we help to solve that. So Tom, over to you.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/480.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=480)

### The Complexity of Operating Multiplayer Games at Scale

Yeah, so as Trevor said,  multiplayer games are hard. At that high level, there's so many things going on with multiplayer games to actually make the components that allow players to play. If you look at here, this might be a typical diagram of what a live service game looks like, and this doesn't include all the components. At the top there, we've got the game backend, so you've got services like entitlement, which determines what players have access to, what weapons and skins, and what they're actually able to access in the game.

We've got things like economy that drives the live service model, what players have bought, what they're spending credits on, and what their bank balance or their wallet looks like. These things are so important for the game. We've got services that make the game fun, things like achievements, leaderboards, and ranked status. All these things actually engage players and make the game more exciting. Then you have the backend services like analytics, which track what maps and what characters players are playing the most of. All of these things drive the decisions in the game, where players are having fun, and what they're enjoying.

Then finally, you have services like remote config, which allow you to make those changes quickly to a live service game. What damage does that weapon do? What maps are on rotation right now? What's currently being played? These things can be tweaked without having to push a whole new version of the game, which is again critical for responding to those player impacts that we're seeing. That's just one blob of that live service infrastructure.

You've then got the multiplayer backend, which we're going to talk loads about today: the matchmaker, the lobby service, and the scaling. How do we have enough compute online for people to actually be able to play at any one time? Then you've got all the stuff that happens behind the scenes, the development environment, staging, QA, and CI/CD pipelines for being able to deploy all of this really quickly. Four, five, six builds a day so that we can test new features really quickly. All of this stuff needs to get built into a whole framework, which then allows us to get content out to players as quickly as possible.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/600.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=600)

With all of these services, any single one having a slight disruption or issue or problem means player pain. It means players can't play, or we can't get stuff out to players quick enough. So it's a hard problem to solve. 

When we look at the number of players playing or capacity available, we see massive swings day to day. Each of these spikes represents a 24-hour period in a live service game. This is normal to see things like time zones rotating, different types of players coming on at different types of days, and certain maps rotating. All these things are just our player base and how many players are playing at once.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/650.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=650)

It can also be external things like sports tournaments or public holidays, or other games going down and suddenly loads more people are showing up in your game. These are all things we need to consider as part of the capacity planning and how we adapt on a day-to-day basis. When we look at a more yearly basis or month over month, we then see bigger shifts in player dynamics.  Here are three season launches, and you can see when new content gets put out in a game, loads of players show up because they want to play the cool new stuff.

We've got to think about that and all the different capacity planning and deployment that comes with these new season launches, keeping the game fresh and keeping the game fun. On the other side of that, if we're not seeing players and we're seeing a decline for whatever reason or slight shifts in that, we need to think about bringing that capacity down, protecting the revenue of the studio and making sure that they have all the funds available to be able to put it right back into the game and build the content. Those pennies need to get spent on how the player is playing and enjoying the game and not the back-end things.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/690.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=690)

 Of course, whatever happens with all of these capacity planning or different parts of the stack, the player experience has to come first. Our goal is for the game to be up 24/7 in all time zones, working efficiently. That's what our job is. For game servers, it's not just the capacity we need to think about. A game like Apex has a high proportion of ranked matches, so we need to be very considerate of things like latency, location, and how we're matching together.

Apex itself has an even bigger challenge, which is 60-player matches. So how do we find 60 players in different regions that are trying to play in the right game type, game mode? All these different things go into matchmaking decisions, which are just hard problems to solve. There's lots to think about when we are thinking about this migration. So Trevor, do you want to tell us a little bit about the GameLift service product and why this might be a good fit?

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/750.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=750)

### Amazon GameLift Servers: A Purpose-Built Solution with Proven Reliability

Absolutely. So how do we help? What can we do? Let's talk about GameLift servers. It's a service that's been around for a while. You've likely looked at GameLift in the past.  I would encourage you to take a look at it today. It's a completely different product that's changed a lot over the years. We have an incredible product and leadership team who's really driving the GameLift roadmap. Again, take a look at it today. I'm really curious to get your feedback on it.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/770.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=770)

A little bit more about GameLift and what it is specifically. It's a purpose-built game server hosting solution for multiplayer games, meaning  it's fully managed. It's a fully managed service. It takes the operational burden of having to manage game servers off of the end user. It's a service that's built on the back of AWS. It has global coverage. It knows how to scale. We recently ran a 100 million CCU player test to validate that level of scale, and Tom's going to talk more about that here in a minute. It also offers flexible hybrid options.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/820.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=820)

Some studios only want to operate in a hybrid model, so the service allows you to bring your own compute if that's a requirement for you. There's also a feature called GameLift Anywhere, which is a component where you can test locally so you don't have to spin up a bunch of compute. Essentially, you just send a game session to your local desktop. A few other things that I think differentiate GameLift here are  there's a service or a feature called FlexMatch, which is our matchmaking service built inside of GameLift.

Containers, this is another big one. GameLift now supports containers for building and running game server packages. Back-end plugins, there's a lot of new integrations with popular game backends, whether it's Pragma or AccelByte. Those integrations are baked into GameLift as well. Telemetry metrics, this is also a really big one that we've heard from our customers. There's a lot of really good data capture around fleet performance, game session activities, server process health. Things of that nature are all what you can find inside GameLift.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/860.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=860)

 This slide here really illustrates the global presence of GameLift. Anywhere players are playing, we likely have a presence in that region or that location. We're continuing to improve that, and likely this slide that you're looking at here is probably outdated as of today because we're constantly adding new regions and new zones for GameLift. I think by the end of next year we'll have about 55 local zones and regions that GameLift supports. Outside of these regions, outside of these local zones, there's also many different instance types and sizes that you can find with GameLift. If you're looking for C class or M class instances, those are all available with GameLift as well.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/900.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=900)

 That's GameLift in general and just a brief overview of the service.

I think now that you have an understanding of what GameLift is, how it operates, and how it scales, I'm going to throw it back over to Tom to talk a little bit more about some of the data, some of the graphs, and some of the things that we've seen in terms of implementation and what that looks like.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/930.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=930)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/950.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=950)

Yeah, cool, thanks Trevor. So I appreciate for some folks this looks like a very boring graph, but for me as an engineer, it's possibly one of the greatest graphs ever.  The blue line shows the service availability for GameLift over quite a long period of time against the green line, which is their target service level objective or SLO. So what you can see here is a rock solid platform with no real problems at all, really built for that scale and high availability. Same graph in terms of time, so looking at the same time period, but you can see here incoming  request volume to the GameLift service. And what we talked about earlier with the player populations and things spiking day to day, you can see that on the left. You can see those constantly changing amounts of players and therefore incoming API request volumes. But the thing that's really interesting here is these larger spikes. These are seasonal launches, game launches, and big events. And even with this massively changing call volume, still 99.99% availability in terms of actually delivered by the team, which I think is absolutely phenomenal.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/990.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=990)

And not only that, one of my favorite parts about working with the GameLift team is they're really good at shipping stuff. I personally feel, having worked in live service  for a long time, it's very hard to ship new features quickly whilst keeping a really rock solid platform. What we typically see is when new features get shipped, they're a bit rough for a little while with other platforms, but we just don't see that here. So some examples of some awesome new features in the last four months include SSM in console. So if you're debugging a problem as a developer and you've got an issue with one of your game servers, literally being able to click a button in the UI and get straight on the console of your Linux box is just awesome. It really speeds up the developer debugging workflow. We've got GameLift servers telemetry metrics, so GameLift already has a pretty rich monitoring stack, but this is directly getting stats out of your game server itself. So if you want to do custom metrics and things and then visualizing them in places like Grafana, which is fantastic, we'll talk a lot about observability in a minute. And then we've got some other features like local zone expansion, so getting servers closer to players, new OS support, and updated plugins for newer versions of things like Unreal, which is fantastic to see.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1050.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1050)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1070.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1070)

### Building a Better Together Partnership: AWS, Code Wizards, and Player-First Philosophy

 So it was quick to see, or we quickly understood as a team that GameLift was going to be the right tech choice for Apex. But Respawn were focused on shipping awesome game content, and quite frankly, they should be focused on the players and getting stuff out. They didn't want to do a migration, right? So how are we going to actually get that done? Well, we formed a bit of a better together partnership  between AWS and Code Wizards, picking that rock solid high scale, high performance service for actually doing the workload itself, but also game centric humans who know games, know how to ship games, and can actually do a migration. But not only that, support the game and help improve the game post launch. We talked about parity is not good enough, so this was one of the things that we really wanted to do, to continue that improvement once we'd done the migration. So together we make a really strong package for helping game studios run in the cloud.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1100.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1100)

 So a few examples of that. GameLift already has a really great monitoring stack. You can see that in the UI and have a look at your fleet overview and things like that. We wanted to go a little bit deeper. So when you've got a title that's already running, there's lots of custom things that have been built over the years which need supporting and adapting. So we built on top of this to give some of that rich monitoring, like things like per game session data, overall game dashboards, and custom game metrics that are specific to Apex, things like that. And we built them already into workflows that they were used to using, so that meant that the development teams didn't need to adjust their day to day working and they could focus on shipping game content, which was really important to us. So this was a key part of the project.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1150.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1150)

And then we also needed to think about doing the migration itself, right, and building some tooling to allow that.  This is things like custom CI/CD pipelines already built into their existing deployment flows. It's the zero downtime patching process to make sure that we could ship content out to players without taking the game down. And some special migration tooling to actually do the migration itself, which we're going to talk quite a bit about. I want to give a shout out to the GameLift team here actually. They built in as part of this migration some new APIs into the product for everyone, which made this migration a lot easier, just specific ways of working with different types of servers. One of the nice things about the product is it's not that you have to use it this way. There's lots of different ways to use the product depending on your game and your workflow and how you want to use it.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1200.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1200)

So one of the key points we formed early on in this project was everything we did had to be about player experience.  So we had to take all of those different points that we were working through and tie them back to how we're going to improve this for the player, and not just keep what we had running right now.

So this meant that we spent a lot of time thinking a bit differently about things like metrics, deployment processes, and how we were actually going to do things to see if we could build a better system really.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1220.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1220)

 And then we also needed to think about post-migration, so we'd got the migration done, yay, great. But afterwards we had to think about how we're going to continue that constant improvement, that continued support. So we actually, in addition to the migration, helped out with providing a 24/7 service with games operations experts to kind of keep the monitoring, keep improving, finding those proactive gains and improving things over time. The service is a mix of monitoring, alerting, and ultimately super smart humans that catch issues before players see them, and that's been really great, I think long term for us improving this service.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1260.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1260)

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1270.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1270)

### Migration Planning: Proof of Concept and Project Timeline

So let's have a look at the fun stuff, the actual migration  itself, 10 days, zero downtime. Like what does good look like? So we started with a bit of a very high level plan.  We wanted initially to build out a proof of concept. What was that going to look like and could we actually make the game work? Then we went into a build phase, building all the migration tooling, building all these things that were going to happen. We went into a test phase and then we did the migrations themselves, starting with dev and ending up in production.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1290.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1290)

So the proof of concept, we started first with  three kind of core pillars that we were going to achieve to prove to us that this was actually going to work, that this was going to be something viable. We started with the main flow. Can players smoothly and quickly connect to GameLift servers and play a match? It was as simple as that, right? Is this thing actually going to work? Then one of the key points we needed to look at was zero downtime, the ability to patch the game without disrupting players at all. So being able to iterate constantly on game patches, get content out to players, and not having to worry about taking the game down or changing any player behavior to allow us to do that.

And then finally, and we talked a lot about this, metrics and observability. The reason why this was so important was because we found it difficult to know what this parity thing means, how can we prove that we've done better, and we're all engineers, right, we wanted to prove that with numbers. So we wanted to start with what's our baseline, what does good look like, and then we knew when we'd make a change, what and how has that improved things, and that really helped actually with picking some of the paths and decision points that we made along the way. It also means that we could give the operations teams and the developer teams access to all the information, including some new things, so they can react really quickly to things happening in the game. Again, live service is all about being able to adapt to the players.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1370.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1370)

So we moved past the POC and then we got into a full project plan.  So we mapped out all of the major milestones that needed to happen. You can see along the top here, all of the different parts that made up the migration. The eagle-eyed amongst you might notice that the planning for this project was indeed a little bit more than 10 days. It takes a lot of work to be able to pull off a migration like this. We say 10 days, but I think we could have actually done the migration in one day or maybe even two. We purposefully picked 10 days because we wanted to do it slow and steady and be able to do it in a really kind of measured way, and we'll talk about why we did that in a bit.

We had to get things right on the timeline around December holidays, but also around season releases. We could not get in the way of the team shipping content, so we had to think about the migration and how that would map into all of the content plans they had, including doing season launches whilst we were doing migrations and things like that. So really critical as part of the planning and really important.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1430.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1430)

### Architectural Design: Portal and Wrapper Components for Zero-Downtime Migration

So we made a bit of a plan and then we got into the build phase. So we started  by thinking about what the right architecture should look like and how we should do things. We talked at the very beginning about one of the key parts of this project was we could not impact Respawn game development. They had to be focused on shipping core content, which meant no changes to the game server or the game client. So that was one of the architectural choices that we were making, no changes to the game itself. So we had to think about how we were going to design around that.

We worked heavily with the GameLift team also to build something that wasn't just going to get them through a migration and then they'd be landed with a bunch of tech debt after the migration. We wanted to think about what tooling could we build to allow the migration to happen, but then give points that we could then build on in the future to make it even better. So we had to really think about what, instead of just get them off their current provider and get them onto GameLift as quickly as possible, what did the future look like? How are we going to improve this?

So first thing to note on the left hand side of this diagram you can see obviously there's the players. So a player loads up an Apex game client and they connect to a matchmaking backend, so that's Respawn's game backend. That then decides who's going to play together, what matches are going to happen, all those different points of things, and then it fires off an API request to a provider to say, hey, I'd like a game server.

The game server information for that IP address and all those things come back, and the players can connect to the game server. So that's the kind of rough end-to-end flow.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1520.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1520)

What we decided to do was a design that had two components. The first component we called Portal. So you can think of this as an API proxy or a kind of game-based load balancer. It captured those requests from the Respawn game backend, which was ultimately talking to the API to the previous provider, and we mimicked those APIs so that we could capture incoming requests. We had the ability to then just transparently proxy those requests through to the old provider, which meant that we could essentially be in the loop but keeping things as they were.  We then implemented the SDK for Amazon GameLift servers, and that allowed us to set those things up. We then had this ability to be able to take in an API request from Respawn's game backend and either send it to the old provider or send it to Amazon GameLift Services, so we had a bit of control there.

On top of that, we then built a bit of software that we called Switcher, and its job was to be able to very carefully and fine-grainedly control where we were pushing requests. Broken down by things like region or even game type, we could set a percentage of how many requests we wanted to go and where we wanted them to go, and that actually ended up being one of the key parts of why this migration was so smooth. An example of what this might look like: we set in a config file, hey, in Australia we want 1% of requests to go to Amazon GameLift, and we want 99% of requests to go to the old provider. And then we could adapt that on the fly constantly to adjust when we were doing the migration itself. That ended up being one of the reasons we could do it in such a controlled and slow manner.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1610.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1620.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1620)

 And the second part of the architecture was what we called the Wrapper. So once that request made its way down to an actual virtual machine  itself, we then need to start the game server binary to actually start the game. We couldn't implement the Amazon GameLift Services SDK in the game server itself because again, we didn't want the Respawn team having to worry about that sort of infrastructure. We wanted them shipping game content. So we built a little Go binary that implements that SDK. When the request comes in, we start a game server, and that allows the whole flow to happen and players to be able to connect.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1680.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1680)

That ended up, we initially were thinking that this might not be a wise design choice because we're essentially adding another loop in the chain, right, but it actually turned out to be an excellent decision because we then had a lot of things that we could do externally of the game server binary. Things like telemetry, logging, monitoring, a bunch of which already exist in the SDKs in GameLift servers, and then we built on top of those to add some even more rich metrics and things like that for the Respawn team. So it actually turned out to be a really wise architectural choice. For anyone who wants to give this a go, the Wrapper is actually open source. You can get it  on that QR code and have a look. It just allows you to get up and running with GameLift servers really quickly, so I'd urge you to give it a try and try it out.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1690.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1690)

### Comprehensive Testing Strategy: Component, Functional, and Large-Scale Load Tests

So we built a thing. We had this architecture in place. We'd built  the code, and we were ready to go. Well, we didn't want to go live without doing some testing, of course. So once we'd built all those tools, the serious testing kind of started to kick off. And as I talked about before, it was really important to have a baseline of what good looks like before we even started. So how would we know that the load test is successful? Well, we'd only really do that with data.

So early on before we started testing, we set some SLOs based on all the metrics we had from the game that was currently running. Some examples: things like queue times, match placement, success rates, you know, how quickly players can get into a match. All these kind of things were things that we measured quite early on and had a rough idea of what we needed to hit as part of these load tests. This would turn out to be really critical for us because we could then measure what we were improving. We made this change, oh, it made it better. Oh, we made this change, oh, not quite so good, right, we need to try something else. So that was a really critical kind of design choice early on.

We took the overall testing plan and we broke it down into three categories. The first one being what we call component testing. So going right back to the beginning of this talk when we talked about the multiplayer game overall system and all those different components, we take a component at a time and we look at how they're going to be run in production. So what requests are coming in, what requests are going out. We use a tool called K6 from Grafana, and we use that to build realistic production patterns to kind of throw at services. So ultimately we're just flooding a service with API requests and seeing how it responds. But again, having that measurement, understanding what good looks like, and then being able to adjust those tests on the fly worked really well.

So we took those components, that's Portal, the Wrapper, the Respawn matchmaker, all the different parts of the stack, GameLift itself, and then we threw these requests at them and see, are we above our thresholds or below our thresholds, are we looking good?

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1810.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1810)

Once we knew each component looked good, we then moved on to what we call functional testing. So this is where we connect all the components together.  And we run a kind of low and slow test, so for 24, 48, 72 hours we run a smaller number of game servers, but we run them continuously. The reason this is important is you test that you can reuse matches, you test it for memory leaks, you test for port binding issues, server reuse problems, all these things that might happen when you're constantly having players cycle in and out of matches.

This testing is quite critical because a lot of people do that kind of big load test, throw a load of API requests and go, "Cool, everything was green," and then you miss a critical memory leak or you miss the second time the server boots up, it doesn't actually work because it hasn't released its port properly. So these things are really critical to test, and this actually ended up finding a few problems that we then had to tweak and figure things out, so it was really worthwhile doing. The metrics we use to do functional testing are usually player experience metrics, so things like time to match, latency, how quickly I can get into a match, those are really critical things for functional testing.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1870.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1870)

 Final test, this is the fun one, the large scale test, right? Throw that lever to a million, see how much compute we can get. Spinning up global live service scale machines costs some serious investment monetarily, that is. So you really only want to do this a few times in your game's life cycle, and you only really want to do it when you're kind of 90% certain that both your component and your functional tests are working correctly.

So don't throw a load of money at a big load test when you're not quite sure that you're going to pass. This really needs to be your kind of last line of defense. Measuring for this is all about the outcomes that we already talked about, right? Time to match latency is the same as those other tests, but does that continue to work at higher levels of scale?

We did this test in super close conjunction with the Amazon GameLift team who were watching all of the stats on their side, making sure everything looked great and also with the Respawn folks too, so it was a really collaborative thing and quite a fun test actually. Side note from us, one of the things that we really wanted to do as part of that large scale test was testing it as if we were in production. Well, what does that mean? Well, if I've got hundreds of thousands of players playing a game and I suddenly need to roll out a security patch, I should be able to patch that without disrupting 100,000 players.

So let's do that as part of the load test. Let's do an AB patch and see how it actually works. Again, making sure that all those ancillary things that you need to do once you've shipped a game actually work at that level of scale. Some other side advice from a few years of doing this is remember to test all of the ancillary services that come around the game, so things like crash handling, monitoring and observability, all those different parts of the stack.

Quite often we see that with game launches, the focus is on, "Hey, is the compute working? Is everything online?" and not about all the additional services. And we've often seen game developers launch a game, ship brilliantly, the game servers are working, but they're completely blind because their monitoring stack fell over. So a really important thing to test as part of those large, large scale load tests as well.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/1990.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=1990)

### Migration Strategy: Gradual Rollout with Fine-Grained Control

So we've built a thing, we've tested the thing and we're happy. Now we need to plan for an actual migration.  Our portal tool, as we talked about, allowed us to move a percentage of requests bit by bit, and that actually ended up forming a large part of the strategy here. So we would pick a region at a time and we would look at what percentage looked reasonable. So 10% migrated, 20% migrated, 30% migrated, and we could do it in really slow stages whilst watching the metrics, seeing what was going on and really quickly rolling back if we needed to.

Rollback for this service took around 30 seconds, so we could really quickly adapt if we were seeing metrics that weren't quite right. Roll back, have a look at it, analyze it, what went wrong, and go again. And it just meant that we weren't panicking because we were halfway through a migration and something wasn't working correctly. It's fine, roll it back. Let's have a look at what's going on. And that actually worked out again to be really, really smooth as part of the final migration.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2040.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2040)

The really interesting learning from this was not actually technical at all.  In doing this and being really confident and doing really small steps, we built confidence both as a project team, but also with Respawn, right? We were moving their game, it's a scary thing. We built confidence really early on that this was going to work correctly. We knew and trusted the process and everything was working well, so it wasn't actually a technical reason for this at all. It was more the human element of being able to go, "No, we totally trust the process and we totally trust the metrics and we're ready to go."

Some very boring but very important things. Carefully scheduling all these different migrations around patching, security releases, season releases, all those things was pretty critical and ended up with a lot of kind of calendar Tetris, but again worked really well. Only having one change go out to production at once, whether you're doing a migration or a patch or whatever was really good because again we made this change and this thing happened so we could roll it back quickly or you know we just knew exactly what was going on.

We weren't like, "Wait, was it your change or my change or was it the season launch or this?" So once we planned that carefully, that worked really well and again allowed us to move really quickly. And again, very boring, but lots and lots of maintenance notifications. Who's doing what, when are they doing it, what's just been done, what's not been done, when are we rolling back, when are we not?

Really good communication between Respawn and AWS. Everyone knows what's going on, and again, everyone can react to the same thing then.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2120.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2120)

Some final other things, especially when you're thinking about a large migration like this: big capacity planning.  If we're about to move 50% of a location over, let's make sure we've got the right level of servers pre-warmed. Do we know exactly what we need? Are we going to go over the top, or are we going to come in slightly under? So again, we had to think quite a lot about that. How much capacity do we want to pre-warm without just wasting money? So that was cool.

And also being really considerate of time zones. Doing these kinds of migrations out of peak time zones is great because there's less players, therefore potentially less disruption, but then you're not actually testing when the region does come into peak. So if you're going to do it at the low time zone, you need to think about how you're monitoring when it does come into peak and you're at your highest level of capacity. So there's actually swings and roundabouts to be considered. Do I do it in the peak time zone and do I trust that migration, but I know that this is going to be the kind of highest player point for the next few days, or do I do it in the low and then watch later on? So there's some considerations to that. We actually picked, depending on the region, a combination of both.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2190.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2190)

### Executing the Migration: From Development to Production with 24/7 Monitoring

So finally we started with the actual migration. We started with development. Interesting learning here: it was actually harder than  production on reflection. Well, why is that? Lots and lots of change in a development environment for a game studio. Two, three, four, ten, fifteen builds a day sometimes get pushed into a development environment when people are testing. That means you've got QA teams sat around wanting to test new features, hit schedules, and it actually means that change and that churn makes it quite difficult when you're testing new things. Wait, was the change the thing that we did to roll out the migration, or was it the new game build that they're trying some crazy new feature and it didn't quite work properly? So there was a lot to think about there.

But in doing dev first we did have a lot of confidence in the tooling because we kind of knew how it worked, we knew the quirks and how things were working, so it was really worthwhile doing. Again, we picked a key set of metrics that were important to players just to make sure that we knew exactly what number meant they were having a good experience and what number didn't, and we heavily indexed on those as part of all of the migration planning. When there was an issue, roll back, give us time to investigate, and then go again.

We prescheduled all of our incidents, which might sound weird. We call them war rooms, but it just meant that for major changes and major rollouts, all of the people that were there, everything was centralized in one place. Everyone knew what was going on, and it meant that we could react again really quickly if there were any challenges. And through all of this we use that for super regular communications with both AWS and Respawn. Again, everyone's on the same page. Everyone knows what's happening. Everyone knows what's going out to players, especially important for things like community management. If we're seeing player issues in a certain region or the metrics are not looking quite right, everyone knows what everyone is seeing.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2300.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2300)

And once we'd actually completed the maintenance for the day and we'd done that particular region or we'd done a percentage of the region, we then needed to think about what happens.  You've done your thing for the day, awesome, everything went really well. Cool, we're all going home now. Well, no, that doesn't work. We had to make sure that there was 24/7 monitoring and alerting available, so our team took over in the evening to look after those stats and metrics. Is everything still looking okay? That's when we talked about regions coming into peak so we could check that. Oh actually no, it was totally fine once we had enough capacity online and things like that. So really important to think about the monitoring after you've done the migration.

Proactive support is super key, so finding those problems before players see them. You don't want to see that post on Reddit going, "Oh, Australia's broken again, what's going on?" You want to know that beforehand. So that kind of breaks down into two things. One, having really fine-tuned alerts on the metrics you care about, those player experience metrics, and also having amazing engineers who kind of have a bit of sixth sense for game problems, watching, being proactive. Or that doesn't look quite right, or this trend doesn't look quite right against yesterday's trend. I'm going to have a quick look into this. So really important.

And again, trying to do as much as possible before the actual migration or a problem happens with the migration, so things like runbooks. What are we going to do in key scenarios if things happen? What happens if this region fails? Well, we're going to do this, this and this, and everyone's thought about that plan in advance. Again, it gives you much quicker time to respond. Escalation paths for each part of that stack and that service. Who's responsible for what, who are we going to get hold of if it's broken?

One of the key things we worked on early on was having great relationships between partners. If we need to get hold of the Amazon GameLift service team, how are we going to do that? If we need to get hold of someone in Jared's team or someone on the cables side, how are we going to talk to each person and how are we going to get hold of each other? It might seem a bit silly, but it's not always the thing you think about when you're kind of working up to a big migration. Those little things make huge differences in terms of time for response, which means player pain or lack of it, hopefully.

One weird little learning we did find was around severities and priorities. P0 does not mean P0 to everyone. Sometimes people call it P1, sometimes people call it P0, so early on aligning on what was important and what wasn't was critical so that when someone calls out a P0, everyone's like, right, we're all on it, there's a big problem here, rather than a misunderstanding. Knowing when the problems are going to happen is really, really critical.

One of the great things we built as part of all of this was really strong links into the GameLift engineering team. We share updates, issues we're seeing, problems they're having, and we get really good feedback from them as well, which ultimately leads to better support for everyone, including having their engineers on call and being able to jump on a call at 3 o'clock in the morning to fix a problem. That's really, really critical. So Jared, a little bit about you and what we actually saw as part of the migration then.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2470.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2470)

### Respawn's Perspective: Zero Player Impact and Dramatic Performance Improvements

 Alright, thank you. I'm Jared. I'm the Head of Technology for Apex Legends. I've been at Respawn for just over two years now. Before this, I was at Riot Games for about seven years. I was running tech for Team Fight Tactics when I left Riot, and before that I also ran tech for Wild Rift, which is their League of Legends mobile product. So I've been in live service games for a bit now.

Coming in to Respawn and taking a look at what we had, one of the things I really wanted to make sure we had was this hyper focus on, hey, if we're going to be moving our game servers off to something new, player experience is the most important thing. Obviously cost and some other factors weigh into that, but I needed to make sure that our players were not going to be impacted in a way that was negative. Ideally they wouldn't even notice, at least on the day of migration, and it couldn't impact our ability to deliver our seasonal content.

We're a live service game. We're constantly making new changes. It is a struggle endlessly to get content out the door, get that patch out the door. We couldn't have a migration impact something that was going to be connected to the features and marketing that we were going to be putting out for our players, so we were looking for someone that could actually help us with that. That partnership between Respawn, AWS, and Code Wizards was key.

Very early on I could tell that they got it. They understood what was important to us. They were willing to put the time and effort into proof of concept, making sure that they were going through all the pieces that we thought were really, really important. Ultimately it was a very long due diligence with a lot of different components and tests to make sure that we knew what we were getting into and we knew that if there were issues that we were going to be able to face, we had mitigations for them. We can jump into that, and again, the partnership was paramount to that.

I had trust throughout the entire process, especially seeing deliverable after deliverable, and then always that focus on player experience. It just matters that our players have the experience that we need for them. The most impressive thing was that migration. There was zero player impact. Players moved on to the next infrastructure. It just kind of happened behind the scenes for them, and we were able to just do it region by region in a very careful and methodical way, which again players didn't notice, but they did notice the other side of that, which I'll talk to you about.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2610.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2610)

 So not only did they not notice that the migration happened, what they did start to feel was the performance increase that we were seeing here. Pre-migration we're tracking a metric here that you can see on the screen. This is the percent of matches that have a game server degradation, so something that a player feels that just feels a little off. It could be hit registration, it could be rubber banding, slow motion. There's a lot of different factors that kind of play into that, but it's a metric we care a lot about because ultimately when players say fix your servers, they're really saying, hey, I had a bad experience and it was something to do with any type of those KPIs.

What we saw post-migration was a dramatic drop off in those degradations, which is phenomenal. So not only did we move through the migration and see zero player impact, we started to see the positive benefits that we were hoping for. In addition to that, not only do we have the game servers themselves, our players are globally distributed. We're always looking at where our populations are and we're always looking at ways that we can optimize what a player experience in any given region could be.

With GameLift and Code Wizards we're able to target regions. We could look at different spots. We can see, oh, it might make sense to move some players from this region, co-populate them over here. We didn't have that flexibility before and that's really, really important for us when we think about where we are today, but where we're going, because the game is evolving. We're always getting new players and we're always looking to see how can we make the experience globally consistent for all of our players.

The other side of this is we're constantly updating the game. And in doing so we're constantly introducing new features, new capabilities.

And that's great for players from having cool new things to do. But it also means that we have a strain that we're putting back onto the game servers. Every new thing we do could have a performance impact. With the partnership with the telemetry we have and the ability to see what's happening in production, we're able to get in front of those things, put a feature out, and have confidence that it's going to conform to the experience that we're looking for. And we can really lean into the partnership in terms of responding to those things, so we can go back and upgrade a feature if we need to or optimize it.

But it's been really, really important for us to be able to not only take what we had before and move to this new system, but now we look to the future and we're talking about really, really cool new player experiences. And I have the confidence that we're going to have the game server infrastructure behind it to do it. With that said, I'll give it back to Tom.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2770.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2770)

### Post-Migration Success: Continuous Improvement and Key Learnings

Yeah, thanks, Jared. And to be honest, as an engineer, it's so awesome to see that kind of feedback when players actually start having a better experience. You're like, yes, we did it. It worked. So we all know that the real work begins post-migration, right? We've done a cool thing, awesome, it worked, and we're seeing what we need  to. But in the world of games, nothing ever stands still.

So we've fallen into a really cool workflow with GameLift and Respawn post-migration, which usually is we've identified a problem or issue for a customer or even a developer, right? I don't know how to fix the game because I don't have the stat I need or I don't have this metric or I can't see visibility. What usually happens is we build a quick tool to solve the problem or a thing in the infrastructure. What's great is we then feed that back into the GameLift service team. They look at how they can build that directly into the product. They look at how they can make it work for all customers and make it a bit more hardened and rock solid, and then they ship it out as a feature.

So that's been a really lovely workflow for us because it means that the product improves, everyone wins, and we can still respond really, really quickly to players as well, which is so critical. Iteration is awesome. So a cool few things we've managed to deliver post-launch already. New regions including local zones, as Jared said, kind of really adapting where players are playing and what's going to be best for them. Adding custom tooling to find those slow-mo servers, hit registration, all those things are so critical with a game like Apex, so being able to find and pinpoint and understand why that happened just means more visibility, more monitoring, so we worked a lot on that.

Adding extra tooling for Respawn engineering to kind of get more debugging information when we have that slow-mo or that hit reg issue, what happened in the code, what was going on? Was it infrastructure? Was it the code itself? Was it the new feature? Really, really difficult to pinpoint that. So being able to dive in there really quickly is really important. And then finally we did also do some cost reductions, so being able to kind of make the infrastructure more cost effective, which just means that that money can get plowed back into players and on the experience, again, so, so critical.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3668d20ee09810a6/2880.jpg)](https://www.youtube.com/watch?v=UClWz0cB4lA&t=2880)

So, we told you at the beginning that we were going to do  a migration. What did we learn as part of that? Well, the big things, load test, test and test some more, right? Think about those three different types of testing, and each of them tells you a different thing, but be really confident in your infrastructure. Invest heavily in observability and metrics, especially anyone that tells you about player experience. So what are those metrics that are really key for your players? Make sure you have those and you really understand what they look like.

No big bangs, and what I mean by that is have really fine control over what you roll out and roll back in production. So don't have this big, hey, we did a migration, we moved every single region at the same time, oh look, it all fell over. Be really, really careful and considerate with how we're moving traffic so that you can measure, respond, move back if you need to. And then finally, think about the teams that are working on this stuff. Build a tight team that really cares. If that's a third-party service provider or if it's in your own team, make sure everyone really understands the challenge you're solving and why you want to solve it, and that they're passionate about solving it, because if you've got that, you're going to do things much better and much quicker.

So thank you all for attending. We were so proud of this migration and the things that we built here. If you can please fill out the session survey in the mobile app, that would be brilliant for us. And if anyone wants to talk about game servers, migrations, or anything, we'll be hanging around for about half an hour afterwards. Just come and chat to us, we're more than happy to. Thanks for your time.


----

; This article is entirely auto-generated using Amazon Bedrock.
