---
title: 'AWS re:Invent 2025 - Agentic Workflows: How Salesforce Manages 1000+ Clusters (OPN310)'
published: true
description: 'In this video, Vikram Venkataraman from AWS and Srikanth Rajan from Salesforce discuss implementing AI ops and agentic workflows for managing Kubernetes at scale. Salesforce''s Hyperforce platform manages 1,400+ clusters with millions of pods, facing significant operational challenges. They built a multi-agent framework using Amazon Bedrock that integrates with Prometheus, K8sGPT, and Argo CD to reduce mean time to identify and remediate issues. The solution includes safe operations with human-in-the-loop approval, achieving 30% improvement in troubleshooting time and saving 150 hours monthly. Key learnings emphasize runbook quality, progressive autonomy, and Slack-based user experience. Future exploration includes knowledge graphs for complex root cause analysis and AI-driven anomaly detection beyond traditional runbook execution.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Agentic Workflows: How Salesforce Manages 1000+ Clusters (OPN310)**

> In this video, Vikram Venkataraman from AWS and Srikanth Rajan from Salesforce discuss implementing AI ops and agentic workflows for managing Kubernetes at scale. Salesforce's Hyperforce platform manages 1,400+ clusters with millions of pods, facing significant operational challenges. They built a multi-agent framework using Amazon Bedrock that integrates with Prometheus, K8sGPT, and Argo CD to reduce mean time to identify and remediate issues. The solution includes safe operations with human-in-the-loop approval, achieving 30% improvement in troubleshooting time and saving 150 hours monthly. Key learnings emphasize runbook quality, progressive autonomy, and Slack-based user experience. Future exploration includes knowledge graphs for complex root cause analysis and AI-driven anomaly detection beyond traditional runbook execution.

{% youtube https://www.youtube.com/watch?v=Ew3nqJQ4uOs %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/0.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=0)

### The Challenge of Operating Kubernetes at Scale: When a 5-Minute Fix Takes 5 Hours

 Welcome to OPN 310, Agentic Workflows featuring Salesforce. My name is Vikram Venkataraman. I'm a Principal Solutions Architect at AWS, and I work with some of our strategic customers that operate Kubernetes clusters at large scale. Along with me, I have Srikanth Rajan here, Senior Director at Salesforce.

All right folks, I have some good news and bad news. The bad news is that getting paged at 2 a.m. troubleshooting for a 5-minute fix is not going to go away. But if we can build an intelligent system that can correlate telemetry signals and help us make decisions faster, that's going to place us in a good spot. That's where we feel agentic workflows and AI ops can help us, and that's going to be the focus of today's session.

As far as the agenda, we'll talk about Kubernetes challenge landscape. We'll spend some time understanding the challenges in operating Kubernetes at large scale. Then we'll shift gears and talk about AI ops. We'll introduce you to AI ops and how it helps us move from reactive to predictive operations. Then I'm going to hand it over to Srikanth, who's going to talk about Salesforce's journey into AI ops. We can always answer any questions after the session.

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/30.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=30)

 Quick show of hands, how many of you here have 50 or more Kubernetes clusters in your environment? Quite a lot of you. How about keeping your hands raised if you have more than 100? Any 500 or more clusters? People who own 500 or more clusters? I still see one person there. That's awesome. I mean, that's exactly the problem statement here. If you have that many clusters operating Kubernetes clusters, taking care of the operations is not an easy job.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/130.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=130)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/150.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=150)

I'm a big fan of analogy, folks. Like Las Vegas, our clusters never sleep, isn't it?  There's always constant movement of vehicles in and out of the city. There is weather, there are crashes, you name it. Now put yourself in the shoes of a person who sits at the traffic control center ensuring smooth operations. It's not an easy task. Just multiply this chaos by 10 more Las Vegas. That's exactly what we talk about when we say managing 1,000 clusters or how it feels  to manage 1,000 or more Kubernetes clusters, folks.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/160.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=160)

Now putting yourself in the shoes of an on-call engineer who wakes up at 2 a.m. getting paged to see thousands of alerts waiting to be attended. In order to get to the bottom of the problem, he has to skim through 50,000 time series of metrics and 2 petabytes of logs.  Folks, he's already racing against the time when a 5-minute fix takes 5 hours to troubleshoot. That clearly tells us there is something wrong in the way we operate. This is not a monitoring challenge. This is an intelligent crisis where we are unable to correlate the telemetry signals.

Just to summarize the list of challenges that he might have, he has to first isolate the noise away from the actual information. Then he has to make sure that he collects all the information and telemetry signals as your request flows through your complex microservices architecture. Tracing can help, but again, you have to connect the dots. Thirdly, we have a variety of monitoring tools at our disposal. We have Prometheus for metrics, we have OpenSearch for logs and traces. He or she should log into these systems in order to get insights from the telemetry backend, and that's not an easy job.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/230.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=230)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/240.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=240)

Finally, when it comes to applying the fix, it's not the application owner who wakes up at 3 a.m. to apply these fixes. It's the on-call engineers, and  lack of runbooks is going to further increase your mean time to identify and resolve operational issues.  That's where AI ops or agentic workflows comes into the picture. We don't Google anymore, do we? No, we just prompt.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/250.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=250)

### Understanding AI Ops and Agentic Workflows: From Anomaly Detection to Multi-Agent Collaboration

 AI ops or agentic workflows is not something new, folks. We've been doing AI ops in the name of anomaly detection. Who here has enabled anomaly detection in their environment? We did what we call a little bit of AI ops even before this term became a buzzword. Then we were using predictive auto-scaling or even intelligent automation where a single or simple batch script goes and fixes some kind of issue. But there is a big difference between the traditional tools that I was talking about and AI ops. AI ops is powered by the so-called LLMs that gives you the intelligence to correlate telemetry signals, and that's exactly what we're going to talk about, purely from an operations perspective.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/300.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=300)

So what is the fundamental block of your AI ops framework? Agents. It's not a new word.  So what are agents? Put in simple terms, agents have specific goals, folks. This is exactly where you feed the context to the agents.

You tell the agents, "You are a Kubernetes operations assistant. You'll have to go through metrics, logs, traces, correlate them, and identify the root cause of the problem, or go one step ahead and provide a possible remediation to the issues."

Agents also have tools. Tools are nothing but how a human being would interact with different tools. For example, you run kubectl commands to get some insights or run a PromQL query to get insights from your Prometheus backend. Agents also have memory, both short-term and long-term. Short-term memory allows the agent to understand the previous responses and provide results based on the previous conversation. Long-term memory is important because the same agentic framework or workflow is going to be accessed by different users. For example, someone from a business intelligence team need not be worried about all the telemetry signals inside. So the agent has to preserve user preferences and provide responses in a meaningful way.

Agents also invoke actions. If you're dealing with Kubernetes things like restarting a pod, increasing the resource of the pod, or rolling back a particular version of a deployment, these are different actions that we as humans do, and the same actions are what our agents perform on our compute platform, or simply put, Kubernetes clusters. All of these components have a tight observation loop where you constantly monitor the performance of the LLM and decide whether it's responding in a way that you would anticipate.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/410.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=410)

Just to expand on the different set of agents that we've seen so far, there is a simple assistant where you prompt the agent,  ask a question, and get the response back. This is like the kids' play. Then you have the deterministic agent. This is where you define the evaluation logic for the agents. Especially when it comes to troubleshooting, you tell the agent that you have to strictly follow a list of actions to get to the root cause of the issue.

Things like as soon as you get prompted, check your audit log to see if something has changed, isolate whether it's an application issue or an infrastructure issue, and see if there are any dependent issues that are failing and have that checked as well. When I say deterministic agents, this is where it provides non-hallucinated output. It doesn't look into anything else. It just purely follows your script and provides a response based on that.

Thirdly, you have the autonomous agent where you don't have to feed quite a lot of context. It can figure it out itself. It has its own evaluation logic based on the model training, and it's going to give you the response based on that. Finally, you have the multi-agent collaboration, which is going to be the focus of today's talk. There are different individual agents, and there is an agent of the agents that communicates with these individual agents, gets the task done, and provides the response back to you.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/480.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=480)

### Building the Prototype: A Multi-Agent System for Amazon EKS Operations

As soon as we build one successful agent, are we going to stop? Definitely not. The success for having a good and robust agentic use case is having a good agentic framework.  This is where you define the security guardrails. You provide the access controls so that the agents don't behave in a weird way. You also configure the runtime. You integrate the framework with observability environment so that you monitor the agent's responses, latencies, and so on.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/530.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=530)

This is exactly the place where you also integrate it with a knowledge base so that it has some kind of context when it comes to troubleshooting issues specific to your environment. Let's get to the real problem statement.  I've been working with Srikanth so closely. When his team had challenges, they managed 1,000+ Kubernetes clusters, and you can imagine the operational challenges that they may have when dealing with those 1,000+ clusters. Most of the time their engineers were spending their time on identifying the issues while the fix just took a few seconds. So we wanted to come up with a prototype that gets insights from these telemetry signals and helps them reach the root cause of the problem faster.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/570.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=570)

They use Amazon EKS, so we started with a single EKS cluster.  This is built on top of Bedrock's multi-agent collaboration feature, which means that there is an agent of the agents which can communicate with individual agents. We had three agents with the prototype. There's one agent that communicates with Prometheus and gets insights from your time series backend. Now you can ask literally questions like, "What's the CPU utilization of a Kubernetes system namespace?" or "Do you see any container restarts?" or "What's the memory utilization across the cluster or across the node?" You can ask any kind of questions, and all of these questions get transformed as PromQL queries by the agent.

The agent transforms natural language into PromQL, executes the query against the backend, and then retrieves the response. We also deployed the KHGPT operator to gain insights about your Kubernetes events and real-time pod locks. A second agent integrates with the KHGPT operator and provides information about what went wrong with a specific pod around a specific point in time, giving you those kinds of insights.

These two agents comprise your MTTI side of the equation, where you understand the mean time to identify your operational issues. The third agent is the Argo CD agent. We wanted to go one step further and remediate issues like some of the well-known ones that Argo CD operations might help with, such as increasing resources or restarting pods. We built a third agent that integrates with your Argo CD controller and can perform all those basic actions. This particular agent is part of your MTTR side of the equation.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/700.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=700)

Within a single prototype, we were able to address both MTTI and MTTR. In simple terms, this is how an interface would look. There is a collaboratory agent that acts as your central UI for your ops team. They can ask any kind of questions, such as "Are you seeing any errors in my Kubernetes clusters?" It reaches out to KHGPT and gets the response back, telling you "Yes, I do see 6 errors." When it comes to understanding the utilization of your deployments, you ask questions about what your utilization looks like,  and it reaches out to Amazon Managed Prometheus or the agent that integrates with your Prometheus backend to give you the response.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/710.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=710)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/730.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=730)

Finally, if you want to make any corrective actions,  there is a human-in-the-loop process. You tell the agent to go ahead and increase the CPU of the deployment. It gets confirmation from you, invokes the Argo CD controller, and takes care of the action. It is very simple. It is a one-stop shop where you can  literally interact with the agentic workflows and get the job done.

### Salesforce's Hyperforce Platform: Managing 1,400+ Kubernetes Clusters at Enterprise Scale

Now it is time for me to hand it over to Srikanth to talk about how they evolved from this simple prototype and built a robust AI ops framework that helps them manage 1,000+ clusters and reduce mean time to identify and remediate Kubernetes operational issues. Thank you, Vikram. I have been building software systems to manage infrastructure for the last 20 years. We have all built a lot of automation over time to keep our production systems stable and highly available.

Even though we built a lot of automation, there remained a gap where we are still reliant on human intuition to solve some of the complex production issues, whether it is a hard performance problem, cascading failures, or even the most subtle things like figuring out why something that is working in production is not working anymore. I have always dreamed about the time when this whole infrastructure space can become completely autonomous and machines can really manage themselves. That is why I am excited about the emergence of generative AI and the possibilities it provides to build towards an autonomous future.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/820.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/830.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=830)

I am Srikanth Rajan, Senior Director of Software Engineering at Salesforce. I am excited to talk  about our journey to implement an AI-powered self-remediation loop to solve problems at real, large-scale production community deployment.  A quick introduction to who we are at Salesforce. We are the Hyperforce Kubernetes platform team. Hyperforce is the next-generation infrastructure platform for Salesforce.

Hyperforce powers all of Salesforce clouds, including Sales Cloud, Service Cloud, Marketing Cloud, Agent Force, MuleSoft, and more. We are the compute layer within Hyperforce and provide Kubernetes platform as a service for all internal application teams within Salesforce. We abstract all of the Kubernetes lifecycle management complexity away from the product development teams so they can focus on building their application logic without worrying about the infrastructure.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/900.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=900)

We run a wide variety of applications on top of our platform, from huge Java monoliths to high-latency, highly transactional databases to low-latency caches, big data and streaming applications, networking applications, and more. Here is a simplified view of our tech stack.  I have not listed all the components that we manage as a platform, otherwise the font size would need a microscope.

Essentially, we are a multi-substrate platform that spans multiple cloud vendors. As an enterprise platform, we provide out-of-the-box support for all standard Kubernetes capabilities like storage, networking, auto scaling, load balancing, mesh, and ingress. We also provide easy integrations to Salesforce and custom infrastructure services for managing identity, authentication, secrets, certificates, and more.

Our secret sauce is providing clean and easy-to-use abstractions to development teams that allow them to build and ship containerized applications quickly and safely. In essence, we exist to bridge the gaps with native Kubernetes, enforce best practices, and manage the cost and complexity of running a Kubernetes fleet so that our product teams do not have to do it all on their own.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/970.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=970)

 Here is a quick peek at the operational scale with which we operate today. We manage more than 1,400 clusters across multiple substrates. We run hundreds of thousands of compute nodes, and our platform scales millions of pods today. We also provide support for 40+ operators, controllers, and integrations, and there is a ton of monitoring that we have built for observing this massive infrastructure.

More importantly, we spend a lot of time today supporting this infrastructure. More than 1,000 hours are spent on support. We are at an interesting point as an infrastructure platform. We are looking at 5X growth for our platform in the next couple of years. Solving for operational scale and scaling our operations to 5X the size of this platform is one of the important business goals we have today, and we are looking at AI as one of the ways we can scale our operational support for our platform.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1040.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1040)

### Evolution of Tooling and Early Agent Experiments: From Sloop to Kubectl Automation

 I also wanted to talk quickly about the evolution of our tooling in general. We have built a lot of tools to manage the complex infrastructure we have developed over time. Many of these are in-house, but some we have open-sourced, like Sloop. We have tons of dashboards for visualizing our monitoring and metrics. We built Sloop for visualizing the historical state of Kubernetes resources like pods, deployments, and daemon sets.

As our fleet grew, we wanted tooling to identify configuration drifts across clusters and figure out anomalies. We built something called Periscope that does cross-cluster fleet-wide analysis. We built Cube Magic Mirror to automate our Kubernetes troubleshooting workflows and get auto-generated root cause analysis. We built Cube Magic Timeline, which can help correlate events across various layers of infrastructure to find anomalies.

More interestingly, we built something called a pseudo API that streams Kubernetes data, events, resources, and other data from live production clusters onto a secure database in an internal environment. We built a pseudo API server that mimics the Kubernetes API but talks to this database in the backend. This allowed us to run kubectl on a read-only replica of the data from Kubernetes without even logging into production. We had built a lot of tools over time, but it has not solved our operational problems fully. There are still gaps.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1140.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1140)

 What are those gaps? These tools are pretty much siloed. They do not really talk to each other. We have to manually pass context between them to troubleshoot issues. The learning curve is still high. Every engineer on the team needs to learn all of these tools and know how to effectively use them and when to use them to figure out problems.

The feedback loop is somewhat limited for this tooling. Though we tried as much as possible to implement periodic review processes, we have not been successful. Most of these tools remain static over their lifetime and do not get updated frequently. All of this put together means the operational toil is still high. We needed a solution with which we can scale our production support. That is where we saw the opportunity with AI agents. Using AI agents, we thought we could solve some of these problems.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1220.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1220)

To improve our MTTR and also eliminate human toil in the process, generative AI is making waves and fundamentally transforming the way we operate in the platform engineering space.  We are trying to leverage it across the various areas of our software development life cycle. We're already seeing the impact of this from white coding to spec-driven development for developing features like new Kubernetes operators, and also using agents for automating the QA functionality and the analysis of complex release and build failures. However, this talk is specifically focused on how we are applying this transformation into the operational space, especially using agents for self-healing and infrastructure resiliency.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1260.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1260)

 When we started looking at the agents, we didn't go with a big bang from the beginning. We chose to start small and wanted to first understand what these agents are capable of and how best to leverage them. So our first agent was the on-call report agent. Every week our on-call engineers prepare a report which summarizes the happenings of the past week, the incidents that happened, what the alert trends were, and what open investigations need to be handed off. All of this is nicely captured in the report, but it was all done manually.

Now we automated this process using AI agents. These agents connect to Slack, connect to our alerting systems, and connect to our observability systems to get all the data and synthesize them into a nice report so our engineers don't have to spend time creating this report on a weekly basis. It automated about 90 percent of the work that the engineer would do, and the engineer can just spend 5 to 10 minutes to summarize it, and then the report is ready. This was an instant hit. As soon as we built this agent, our on-call engineers adopted it immediately and it saved them toil.

Then we had an idea to automate kubectl commands using natural language queries. We already had the Pseudo API which can query the status of production clusters in near real time. So we built an agent that looks at Slack questions, translates them into kubectl commands, and executes them using the Pseudo API. This was very handy for our troubleshooting. Our on-call engineers can directly ask in Slack, "What is the status of this pod on this cluster?" and they would get the results directly on Slack. This also got a lot of adoption from our application teams who started using it for troubleshooting their applications as well.

After the success with kubectl agents, we expanded our agentic capabilities with the life site analysis agent, which is slightly more complicated than the other two. We have a rigorous weekly process where our engineers look at the availability dips and other golden signals for all the components that we manage across the 1,400 cluster fleet. Engineers spend a lot of time looking at the dashboards trying to figure out which clusters had availability dips and then spend time triaging them to understand why the availability dip happened and figure out the mitigation steps for how we can avoid this from happening again in the future. This was pretty laborious, with engineers spending at least a couple of days every week.

So we automated this work with AI agents. The agents perform anomaly detection across all the 1,400 clusters, automatically find the availability dips, and also do the first level RCA to understand what caused these availability dips. This greatly eliminated the human toil that we had to spend on the repetitive work on a weekly basis, and this was again a huge hit. Our on-call engineers really love it. So as we built more and more agents like this, we learned a lot and clearly saw the potential that we could apply agents to implement a self-healing loop in production, which can greatly improve our production support problem.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1510.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1510)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1530.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1530)

### Implementing the Self-Healing Loop: A Multi-Agent Framework with RAG and Human Oversight

Here's a quick screenshot of our on-call report agent from Slack.  Our engineer is asking the AI ops agent to generate the on-call report for a specific time frame. The agent is able to generate that, create the document, and share the document link directly in Slack. Similarly, here is another screenshot from the Kubernetes cuddle agent.  Here the engineer is asking to get the events from a specific namespace for a particular deployment. The agent is then able to translate that question into a Kubernetes command, execute it using a pseudo API, and retrieve the events to show them directly in Slack.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1560.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1560)

After a lot of experiments, we landed on this multi-agent framework for implementing self-healing.  We heavily rely on the managed internal agentic framework and the embedded RAG database to power this. The agentic framework is responsible for data governance, especially ensuring data privacy and data security. Given that we are dealing with sensitive logs, metrics, and other telemetry data, data governance becomes really critical. Fortunately for us, this LangChain LangGraph-based agentic framework was managed by another internal team, so we could directly leverage it for our architecture.

Let's talk about the self-healing loop and how it works. It all starts with alerts. When an alert comes in, it lands in Slack, which is the space where we collaborate and track the progress of investigations. As soon as the alerts land in Slack, the manager AI agent kicks in and plays the role of chief orchestrator. The manager agent leverages the power of the runbook knowledge that we ingested into the RAG vector databases and the infrastructure and topology knowledge that we provided as context to the LLM.

The manager agent then augments the alert data with the context retrieved from the RAG runbooks and performs reasoning with the LLM to figure out the right troubleshooting steps for debugging the problem. Once the manager agent generates the troubleshooting plan, it needs the telemetry data to correlate the problem. The telemetry data like logs, metrics, events, and traces are spread across various systems. The manager agent then delegates the task of gathering the required troubleshooting data to a set of worker agents.

The worker agents specialize in talking to various data systems across the infrastructure, getting the right data, and transferring it back to the manager. The worker agents use MCP wherever possible. Our adoption of MCP is somewhat limited because not every infrastructure system has MCP, so wherever it's available, we use it. If not, it's a direct integration with that specific system. We also made a very conscious decision that we want to reuse all of the existing automation tools that we have built over the years, so we built specific worker agents that talk to KMM, loop, pseudo API, Periscope, and all of the other tools that we built.

These tools can enhance the troubleshooting for us, and we are reusing them as much as we can. Once all of the troubleshooting data is retrieved, the manager then synthesizes the data and again uses the power of the runbook knowledge and the LLM-based reasoning to summarize the root cause. Once the root cause is ready, it is then passed on to the AI remediation agent. The remediation agent is responsible for figuring out the right set of actions to take to remediate the problem.

These actions could be restarting parts, restarting nodes, doing a rollout restart on a deployment, or even changing some configurations. The remediation agents use something called safe operations to execute these operations in production. When we let AI take actions directly in production, we wanted to have critical human oversight to make sure that it is doing the right thing. So we implemented a Slack-based human-in-the-loop approval process. Unless explicitly allowed, the AI remediation agents cannot take any actions in production without human approval.

At least in the very beginning, we also implemented a Slack-based feedback loop so that when AI makes mistakes in either troubleshooting steps or remediation steps, the on-call engineers can quickly click a button in Slack and provide that feedback. That feedback is then captured to improve our agents and runbooks. This architecture allowed us to reuse all of our existing tools and runbooks and augment them with the intelligence of AI agents to meaningfully connect them and implement a full self-healing loop.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/1850.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=1850)

### Ensuring Chain Safety: Safe Operations, Guardrails, and Progressive Autonomy in Production



When we talk about letting agents take actions directly in production, the biggest elephant in the room is chain safety. How are we going to make this safe? How can we make sure that the AI does not hallucinate and makes the correct decisions all the time? What are the risks and how can we mitigate them? There are four things that we identified, starting with unbounded access. If we let AI make changes at will in production, it can be really catastrophic. AI could choose to delete your Kubernetes control plane, delete your application, or your node pool, and all of these can result in outages. We wanted to limit the access of AI to a very limited and curated set of operations, and AI cannot do anything that we do not allow it to. This greatly mitigates the risk of AI making catastrophic changes in production.

The second thing is the lack of guardrails. There are tools like kubectl or cloud SDKs that lack the necessary operational guardrails to ensure safety. Implementing safeguards or guardrails for every single operation makes these actions safer in production. Wherever possible, we also need to make sure that every change we make can be quickly rolled back in case it causes additional issues. The third concern is poor visibility. If we do not track AI operations closely, it can increase the risk we see in production. Establishing strict change management processes and auditing controls and ensuring that we have a periodic review process to track all AI-driven operations would greatly help reduce that risk.

Lastly, even with all the guardrails and visibility that we put in place, we still want to be sure and we need to have a way to guarantee that AI is making all the right decisions in production. That is where adopting progressive autonomy can help. To begin with, every node operation can happen in production only without human approval. As we gain more and more confidence with AI, we can relax the constraints and give more autonomy to AI over time, allowing AI agents to make more autonomous actions one by one as we gain more confidence.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/2020.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=2020)



We implemented safe operations using Argo Workflow. Every single operation that we built is an Argo Workflow that has the necessary guardrails. AI agents can access these Argo Workflows through our in-house compute API with necessary human approvals and oversight, and all of these operations that the AI agents are doing are closely tracked via change management. We have built a lot of visibility with dashboards. What are these guardrails in general and how can they make these operations safe? For example, if you are restarting a pod, are we respecting the pod disruption budget? If we directly do kubectl delete on a deployment, it can cause a partial or full outage. Similarly, if you are restarting a set of nodes, how many of them are we restarting at a given time? If we frequently restart these nodes, it can again result in outages.

If you are scaling up or scaling down a cluster, are we looking at the utilization of the cluster? This has happened to us a couple of times where we scaled down a busy cluster and it caused incidents. These kinds of issues can easily happen if we do not have the right guardrails in place and let AI take action directly.

We looked at what our seasoned engineers do to make our operations safe in production and coded them as guardrails within every single operation that we allowed AI to take. To sum it up, the combination of safe operations with guardrails and the necessary human oversight at the beginning helped us gain confidence with AI agents, and we were able to implement them in production.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/2140.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=2140)

Here is a screenshot of  our remediation approval process, again in Slack. There was an alert that a node had some problem or an application had some problem. So the AI agents investigated and figured out that this specific node has some disk-related issues and draining this node is the right fix. It asks for approval from the on-call engineer in Slack. The on-call engineer can look at the RCA, make sure that the AI investigation is correct and this is the right remediation step, and they can click yes to basically drain the node in production.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1976b9c0e6e5244/2200.jpg)](https://www.youtube.com/watch?v=Ew3nqJQ4uOs&t=2200)

### Impact and Key Learnings: 30% Improvement in Troubleshooting Time and 150 Hours Saved Monthly

In fact, we added one more layer of approval after this. After the on-call engineer clicks yes, it goes to another engineer or a manager who has to approve before the action gets executed in production. So what is the impact so far with the implementation of the agentic loop?  As we deployed agents into production, we implemented a set of success metrics. We saw a considerable improvement on our troubleshooting time. It improved by 30%. The agentic architecture saved almost 150 hours a month, which roughly equates to one engineer's worth of bandwidth that we could apply to other areas.

As we implemented this AI-powered self-remediation loop, we learned a lot. There are many takeaways. The most important thing with this architecture is the runbooks. The structure of the runbook and the accuracy of the runbook determines the success of this overall architecture. AI is only as good as the data that we provided. If you have duplicate runbooks with conflicting information, we cannot expect AI to make right decisions. Runbook structure also matters. We figured that the structure of the runbook has a direct impact on the efficiency of the RAG chunking and the retrieval process.

In summary, having a good runbook strategy which clearly defines when and how the runbooks are created and modified and kept up to date is critical for the AI agents to work correctly in production. We obviously spent a lot of additional work to build safe operations, but we felt it is non-negotiable given that safety and reliability are the most important factors, especially when we are letting the automation happen in production.

Using strict LLM prompts can help reduce hallucination. We asked AI that for every decision that it takes, it has to be backed with real data. If some data is missing, it is better not to make any decisions and ask humans for input. Ensuring continuous feedback loops also helps. The continuous feedback loops help us continuously train and improve our systems. Our runbooks improve, our agents improve, and the overall success rate of the self-healing process improves with the feedback loops.

Progressive autonomy helps. Starting with full human oversight to review everything that AI is doing and slowly relaxing it over a period of time and giving it more autonomy and letting AI do independent actions is a viable way to scale your agentic actions in production. Also, leveraging existing tools helped a lot. It was the quickest and the biggest wins that we had. AI agents were able to connect all of these tools meaningfully and we realized the value of using it pretty quickly.

Last but not the least, the Slack-based user experience is also very powerful.

First of all, it made it very easy for us to implement the approval and multi-layer approval processes and the feedback loops. Plus, if AI makes any mistakes either with the troubleshooting steps or the remediation steps, the engineers can then directly collaborate on Slack and continue the troubleshooting, fix it, and then the whole thread can be summarized and fed back as knowledge for improving future decisions. This makes the integration with a tool like Slack very powerful. The added benefit is that we could then automatically trigger these agents on alerts as well as invoke them on demand from Slack using Slack queries.

### Future Directions: Knowledge Graphs, Historical Learning, and Beyond Runbook Execution

We have implemented the AI-powered self-remediation loop. What next? We feel that we have only scratched the surface of what the AI agents can do. This is probably the first step in a long journey ahead. Obviously, we wanted to scale our AI agents to eliminate 80% of the production support toil that we have today. That is our ultimate business goal. To get there, there are two or three things that we are exploring right now to expand our agentic capabilities.

The biggest problem that we had with the current architecture today is connecting the dots. If there is a complex production problem that goes beyond a simple pod restart or a simple node restart, our runbook-based solution struggles to really connect the dots. Let me give you a good example. Assume that there is an application that is experiencing high latency because its internal DNS calls are timing out intermittently. CoreDNS itself, which serves the DNS records, is running on another node that is experiencing network bandwidth exhaustion. There is another part that is running on the same node that is doing high volume network transfer, which is causing the network bandwidth exhaustion.

How do we meaningfully connect from the pod that is experiencing the application latencies to the CoreDNS that is running on a different node altogether, to the node that is running the CoreDNS, to the NIC card, and then to the application that is really causing the problem? It is very difficult to write a runbook that solves this problem. Even if we manage to write one, it is very specific to a single use case. How can we do this meaningfully across the entire scale of the infrastructure for all kinds of problems that we can face? That is a very humongous challenge.

We are attempting to solve this using knowledge graphs. Essentially, we wanted to teach AI the same way we would teach humans. We want to say, "This is how the infrastructure is defined, these are the components, this is how they are related to each other, and these are the failures that can happen at each component. If it fails, this is how it would impact other components." If we are able to capture this knowledge in a structured way in knowledge graphs, we can then let AI traverse that knowledge graph and figure out the root causes for some of the complex problems which require human oversight today. This is one of the things that we are exploring.

The second thing that we are exploring today is how best we can use the feedback and the knowledge that we are getting from our feedback. If we can record our successes and failures in a way where we know that for a given problem, the most probable root cause and the most probable remediation step is this, we can then let AI use that information to speed up the diagnosis process and also improve the accuracy of the diagnosis. How can we record the history of successes and failures in a very meaningful way that can be used by AI is another thing that we are exploring.

Lastly, we wanted to explore the possibilities of what AI can do beyond just the runbook executions and beyond what we tell it to do. There are so many hard performance problems that we face today that even humans do not know what is causing them, and we struggle over weeks, days, or months to find the root cause. There are millions of metrics and data points and terabytes of logs. Can we throw it all to AI? Can AI help fish out the anomalies from this vast amount of data and tell us where the problem is? This is one of the areas that we want to explore. We do not have a good answer yet, but we are hoping we find something useful from this exploration.

With that, we are at the end of the presentation. Thank you everyone for listening to it. I hope you all have a very good rest of your days.


----

; This article is entirely auto-generated using Amazon Bedrock.
