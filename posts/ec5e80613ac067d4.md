---
title: 'AWS re:Invent 2025 - Better, faster, cheaper: How Valkey is revolutionizing caching (DAT458)'
published: true
description: 'In this video, Joseph Idziorek and Madelyn Olson from AWS discuss Valkey, an open source BSD-licensed data store forked from Redis 7.2 after Redis changed its licensing in March 2024. They detail major performance improvements including multi-threaded IO architecture achieving 230% throughput gains while maintaining single-threaded command execution, enabling ElastiCache Serverless with 20-33% price reductions. Madelyn covers reliability enhancements like dual channel replication, forward compatibility for safe downgrades, and improved cluster failover times from minutes to 10-15 seconds. Memory efficiency improvements reduced overhead from 52 to 29 bytes per key-value pair, with customers seeing up to 41% memory savings. New features include bloom filters for probabilistic data structures. The roadmap includes full text search, improved durability, multi-threaded snapshots, and time series data types, all developed openly through the Linux Foundation with 50+ contributing organizations.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/0.jpg'
series: ''
canonical_url: null
id: 3085392
date: '2025-12-05T05:29:51Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Better, faster, cheaper: How Valkey is revolutionizing caching (DAT458)**

> In this video, Joseph Idziorek and Madelyn Olson from AWS discuss Valkey, an open source BSD-licensed data store forked from Redis 7.2 after Redis changed its licensing in March 2024. They detail major performance improvements including multi-threaded IO architecture achieving 230% throughput gains while maintaining single-threaded command execution, enabling ElastiCache Serverless with 20-33% price reductions. Madelyn covers reliability enhancements like dual channel replication, forward compatibility for safe downgrades, and improved cluster failover times from minutes to 10-15 seconds. Memory efficiency improvements reduced overhead from 52 to 29 bytes per key-value pair, with customers seeing up to 41% memory savings. New features include bloom filters for probabilistic data structures. The roadmap includes full text search, improved durability, multi-threaded snapshots, and time series data types, all developed openly through the Linux Foundation with 50+ contributing organizations.

{% youtube https://www.youtube.com/watch?v=iUw1d4bUNJk %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/0.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=0)

### Introduction: Building Better, Faster, and Cheaper with Valkey and ElastiCache

 Welcome. My name is Joseph Idziorek. I am a Director of Product Management at AWS. I have been with AWS for about nine years building database services. One of the services that I have the pleasure of working on every day is Amazon ElastiCache. During the creation of this presentation, I misspelled the word "principal" for probably the seventeenth time in my career. You always have to remember it is your pal. Your principal engineers are your pals. They are your friends. Joining me is my friend Madelyn Olson. We have been working together at AWS for over nine years now. Besides our passion for databases, we both hail from Minnesota, so we are kind of the only two people on the team that can talk about winters, ice fishing, snowmobiling to school, casserole, and hot dish. So I welcome you today. I appreciate you taking the time to join us.

In my role as a product manager, I talk to a bunch of customers across different databases, and the best conversations I have are ones where I get to ask you what you are building, what is on your roadmap, and how we can help you build. Sometimes I have to ask the question, what is not going so well for you? How can we make it better? What are the things that you are struggling with? When I ask those questions, I rarely ever hear people say that they do not need any features anymore, that everything is full featured, or that it is fast enough, or that it is pretty well priced. Even if we made the price lower, they would not know what to do with it. They would be happy to give us more money. None of those things ever come up. Customers are always asking us with every iteration to make it better, make it faster, and make it cheaper.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/110.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=110)

That is really the premise for our talk today. With Valkey and ElastiCache, with every release we are making, we are measuring against ourselves how we make it better, faster, and cheaper.  We will talk about what Valkey is and how we got here today. I am going to talk about the multi-threaded architecture that we just came out with and how that benefits a lot of the workloads that you have and what that means to you. Then Madelyn is going to come up and talk about some of the reliability improvements that we have around dual channel replication, forward compatibility, and cluster resilience, some reductions in memory overhead, and also talk about one of my favorite features, bloom filters. We will talk about what is next after that.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/140.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=140)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/170.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=170)

### The Origin Story of Valkey: From Redis Fork to Multi-Vendor Open Source Project

 The origin of Valkey is a really fascinating story. I have been in the database world quite a while now. I would say rarely do you get these inflection points or opportunities in your career to work on something like Valkey. Maybe it happens once a decade, maybe even once in a career. It has been a really cool experience. For something to have that level of gravitas or excitement, it kind of has to start from some interesting origins. Valkey is kind of one of those projects  that was really born out of necessity.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/200.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=200)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/210.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=210)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/230.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=230)

So what is Valkey? Valkey is an open source BSD licensed data store for high performance key value workloads such as caching, session stores, leaderboards, and semantic caching, among a number of other workloads. Valkey is an open source project backed by the Linux Foundation. It will be BSD licensed and open source forever. The way that we got here today is that when Redis came out in 2009, it was BSD licensed and open source  up until March of 2024.  Redis changed the license to a dual stack license, which really caused some changes in the community. In response to that licensing change, the community actually forked the last open source version of Redis and created Valkey 7.2.  We were off to the races.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/240.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=240)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/250.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=250)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/260.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=260)

Within the last year and a half, we have launched the first major version of Valkey 8.0 in September 2024,  Valkey 8.1 in March 2025, and then another major version release for Valkey 9.0 on October 21, 2025.  Within a year and a half of the creation of the Valkey project, we have had two major releases and quite a bit of momentum.  What comes next? Probably 10.0. But maybe the moon. Maybe 100. We will see. Well, one of those two things is going to happen. But I think that really the beauty of it is as an open source project, you will be able to see it coming a long, long way away and be able to see what the roadmap is and the stuff that we are working on for the Valkey project.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/280.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=280)

 For Valkey, I think one of the really beautiful things about Valkey and one of the reasons for its success and the tremendous growth we have seen over the last year and a half is really the multi-vendor governance model. It is not any one vendor that controls the license or the roadmap, but rather a collaboration between multiple different vendors. It is a drop-in replacement for Redis 7.2. It will be stewarded by the Linux Foundation forever and it will be BSD licensed, permissible open source forever.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/330.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=330)

There are 50+ organizations contributing to the Valkey project with 150+ code contributors. We now have 10+ managed service providers for Valkey. After a year and a half, which is tremendous, we have over 1000 commits and a tremendous amount of container pulls. We are seeing tremendous momentum on the project  and growing. Having been in the database industry for a while, it is a small group of people. I might work with somebody six years later and they are working at another company, so I actually get to work with some of my former colleagues who now work at Google, collaborating on what the roadmap for Valkey is and how that is taking form. That has been really fun. Lots of friends are contributing across Verizon, Heroku, and Snap. It is nice to have all these minds working towards a common purpose.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/370.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=370)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/390.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=390)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/410.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=410)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/430.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=430)

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/440.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=440)

Part of that is the technical steering  committee, which is not a single company but six individuals who have a tremendous amount of experience working on Redis and are now leading the Valkey project. One of those steering committee members is Madelyn, who will be joining us today. Really an expert among experts in this space.  With the creation of Valkey, we also offer it in Amazon ElastiCache. We have added a serverless version of it, and I will talk a little bit about the pricing in a while. With each new version of Valkey, we will be offering it in ElastiCache as well. From Redis 7.1, which is  the last version of Redis that we offer on ElastiCache, we now offer Valkey 7.2, which is a drop-in replacement for Redis 7.2 minor version upgrade. It is seamless with zero downtime. We have 7.8, 8.1, 8.2, and then coming will be 9.0 and all the subsequent releases after that. We will track  the open source project in ElastiCache and continue to shrink that time from open source release to when it is available in ElastiCache as well. 

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/460.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=460)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/470.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=470)

### Solving Spiky Workload Challenges Through Multi-Threaded Architecture

From the ElastiCache team perspective, and since we are in Vegas and we have to use all those analogies, we are really all in on Valkey. It is really our future. We are very excited about it and about the Valkey project and all the contributions and broad momentum there.  Let us talk a little bit about some of the substance of Valkey.  Like most of the physical world, caching workloads that are steady state are super simple. You just pick an instance, plus or minus a little bit of throughput, and you are good to go. The most interesting and most challenging customer workloads for caching are the ones that are spiky. These workloads include ride hailing services when an event ends, selling tickets for a concert, a game that becomes popular, food delivery services dealing with morning, noon, and lunch rushes, or selling shoes where you get these big spikes.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/540.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=540)

We typically see from customers that they provision a certain amount of capacity. In this case, I am using an m7g.2xlarge for illustrative purposes. You feel pretty good about it, but then you see that one spike at the end and you start to feel a little bit nervous and want to provision more capacity. You want to be able to deal with any successes in your business. The last thing you want is your cache to not be able to deliver the throughput your business needs it the most.  So you go through this dance of whether to scale up or scale out. You start to decompose your application and ask whether this is throughput bound or memory bound, and you start to think about the complexity of how you are trying to do that.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/550.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/560.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=560)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/600.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=600)

What if I postured you a different outfit? What if we better utilize what we already have? This is kind of like the glass is half empty, glass is half full, it is twice as big as it needs to be.  This is interesting: how many of you know that Valkey 7.2, the original version of Valkey, has a single threaded architecture? Good raise of hands, I would say about 30 percent. One of the motivations for why this is a single threaded architecture in Valkey is we want to keep the execution of commands all in a single thread to be able to serialize the commands to prevent against complicated communication between multiple different threads, at least on the execution side, and sometimes some challenging race conditions. Architecturally we want to keep the execution on the main thread.  But that does not mean we cannot do other parts on other threads. So we look at bottlenecks for a system like  Valkey and this is for the event loop we have right here.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/630.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=630)

This event loop we have here is something we use in the database world all the time. These are flame graphs, and they're not only beautiful art pieces but also functional. The way to read a flame graph for this call stack is that your horizontal axis represents how much CPU time each call stack actually creates, and the vertical depth shows how many function calls are within that particular call stack. 

Looking at where we're spending time, Valkey has a very simple loop for how we process commands. We get a command from the kernel, read the client command, parse the command, execute the command, and then send the response back to the client. A lot of time gets spent writing that response back to the client, a little bit gets spent reading from the client, and we spend a bit more time getting and parsing that command as well, plus our epoll time.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/680.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=680)

If we see the decomposition, we want to keep the execute command on a single thread, but there's a lot of work that doesn't necessarily have to be on that single thread.  Looking at CPU clock rates versus cores, clock rates aren't improving as much as they used to. We're getting a lot more core density on individual machines, so expecting to scale up for higher throughput workloads with faster CPUs typically isn't going to be the answer, at least for the workloads we're seeing. Trying to have hardware outpace our customer demand really isn't the answer either.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/700.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=700)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/710.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=710)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/740.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=740)

So we asked ourselves how we could take a single-threaded architecture  and evolve it into a multi-threaded architecture.  We started to take those CPU-heavy aspects of the execution loop and evolve the architecture to spin those off into multiple IO threads. On the main thread, we look at the CPU usage, and if it starts to spike, we can spin another IO thread and start to offload parts of those commands while still keeping the execution on the same main thread. 

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/750.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=750)

With that approach, we can start to add more work and get more throughput and more parallelism in the commands that come in, still keeping the execution on the main thread. Even if we see the second IO thread  starting to become more utilized, we can spin that off and create another IO thread and then saturate that one as well. We get to use a lot more of the cores that are actually on the machines you're already using to do a lot more throughput while still preserving the same architectural primitives that we want in Valkey by keeping the execution on a single primary thread.

### 230% Performance Improvement: Real-World Impact of Multi-Threading

When the team came to me and they were working on this, they said, "Joe, we improved the performance by 230 percent." I said, "No you didn't." In the database world, if you get double-digit improvements in performance, that's pretty rare these days. We've been after this stuff for quite a while. They said, "Valkey 7.2 is single-threaded. We made it multi-threaded." I said, "That's pretty cool." That even surprised me. That's a significant gain.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/780.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=780)

We had a customer article published recently from Nextdoor, and one of the things they said really spoke to me about the Valkey project is how the folks like Madelyn Olson who have been maintaining the project have really gone back to core engineering work on just performance, scale, and reliability and what that means to them and their business. They've appreciated how creating multi-threaded IO is something we might have taken for granted or thought might not have been prioritized, but the tremendous gains that we're seeing are fantastic. 

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/850.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=850)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/870.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=870)

I know anybody sitting in the audience who's a database person is thinking, "Yeah, but how did you benchmark it? Can you tell me more?"  Here's some more information about a couple of blogs on how we benchmarked it. These will be available in the slides afterwards or up on YouTube, where the presentation is also posted. 

So what's the result? If you have a workload that looks like this and we add in this fancy new multi-threaded architecture, these are the things we're seeing from customers. They're able to say, "Hey, we don't need that 2X large anymore. We're getting significantly more throughput. We can go down to an X large and cut your costs in half for that particular workload." Not only that, I didn't even make my axis big enough on this, right? You have significantly more headroom to be able to scale those workloads.

Again, this is in the spirit of making it better but also cheaper as well to run more workloads on the stuff that you're already buying.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/920.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=920)

### ElastiCache Serverless: Achieving 100% Utilization with Dynamic Scaling

I hope the engineer in you is unsatisfied with this graph. The unsatisfying part to me is that you take the area under the curve of the purple line, subtract it from the area under the curve of the blue line, and you see that we're wasting a lot of resources and not heavily utilizing our capacity. 

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/950.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=950)

Wouldn't it look better if the graph looked something like this? This is really what we've built with Serverless. Serverless allows us to offer you 100% utilized service. You don't have to go through the undifferentiated heavy lifting of trying to figure out how much capacity you need, account for spikes, and all the other complexities. We can offer it to you on a pay-per-request and pay-per-memory usage model. This was really made possible by the multi-threading work that we did in Valkey. 

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/990.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=990)

Because of that, the way we built the serverless architecture in ElastiCache is kind of read left to right from your client application. Your application goes down toward the caching nodes. As client application requests come in, they go through an NLB, which resolves similar to a proxy fleet. That proxy fleet knows where your cache cluster and cache nodes are located, and then it either fetches or writes to those particular caching nodes. What happens on those caching nodes is that data is really distributed in Valkey across a number of different slots, and any one of those slots at any given time can spike up. 

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1020.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1020)

The burden on us as a serverless provider is to manage those spikes. This is where the multi-threaded IO work really comes in. It allows us to much more effectively handle these spikes on a given machine. We can spike multiple different slots at any given time, and we can spike it pretty quickly. For example, we can go from 30K to 240K requests per second in 5 seconds. That allows us to be very efficient within an individual machine. 

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1040.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1040)

Then we have to go through the same decision: should we scale up or should we scale out? As we measure the heat on any individual machine, we can also move slots to other machines. We can double the throughput for any given slot every 2 to 3 minutes. This is our pleasure to do this work for you so that you don't have to think about it anymore, and we get pretty efficient at doing this. 

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1050.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1050)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1090.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1090)

At the end, you get something that looks like this, and this line is ours to manage.  A lot of times I get the question about how this is possible. When we introduced Valkey, we introduced it at a 20% price reduction for node-based and 33% reduction for serverless. The more efficient we can manage our resources through multi-threaded IO, the more work we can do on one machine, and the lower our costs become. These are the types of projects we do all the time, and we're able to pass those savings back to you as customers. 

Upgrading from 7.1 to 7.2 costs 20 to 33% less with no work done on your end. It just happens. Going from 7.2 to 8.0 brings 230% improvements for throughput-bound workloads. Just upgrading from 7.2 to 8.0 is a significant amount of price-performance improvement without having to change anything in your application. The pricing or the core system just gets better. With that, I'd like to welcome Madeleine up on stage to talk about some of the reliability improvements we've been working on.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1150.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1150)

### Reliability Improvements: Managed Upgrades and the N+K Strategy

Thank you, Joe. So just to reintroduce myself, my name is Madeleine. I've been working on Amazon ElastiCache for over a decade now. I started contributing originally to Redis open source back in 2018. I was added to the Redis core team, which is effectively the open source maintainer group, in 2020. So I've been working on this for a long time. When Valkey came around in 2024, that wasn't anything new. We have a lot of history in building in open source and building a lot of really cool, exciting engineering work. 

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1160.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1160)

I'm going to continue and talk about some of the reliability improvements that we built into the Valkey engine that improve both open source and our managed services.  Traditionally in caching, you want most of your workload to be sent to the cache. The cache is very efficient and highly performant. When the cache goes down, more requests are sent to the backend, which usually have higher latency and costs. The backend might not be properly scaled to handle that increased load. Increasingly we're seeing things like semantic caching come into architectures, which also has a lot of the same properties that we saw with traditional databases. So we want the cache to be as available as possible, significantly more available than the backend service that it's caching data from. We focus on two main areas. The first is managed upgrades.

When you have a version of Valkey, you want to constantly patch it to get all the improvements we discussed: performance and security. You also want to handle unmanaged changes, such as when a node fails. You need to detect that quickly and perform a failover to standby replicas that are available. Let's discuss these two flows in more detail.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1230.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1230)

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1250.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1250)

I'll start with the steady state. Valkey has a primary and replica architecture  that is sharded. This is a single shard of a Valkey cluster, and these can scale out horizontally to up to 1000 different shards. The clients are aware of the topology and will route requests to the specific shard that owns that set of data, and then it gets replicated logically to the replicas.  Unlike some systems like Postgres, which do physical replication, Valkey actually sends the commands to the replicas.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1270.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1270)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1290.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1290)

Let's look at a managed upgrade and see how that works. Within Last Cache and the open source community, we recommend what we call an N+K upgrade strategy, which means you add  two additional nodes, one for each node currently in a shard. In this case, we're upgrading from Valkey 7.2 to Valkey 8.0. We're going to add a replica and designate it as the node that will become the primary leader. Then we'll have a secondary replica that will eventually take over the other replica's  responsibilities.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1300.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1300)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1320.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1320)

To bring these nodes up to date, we use a two-part process. First, we take a full snapshot of  the load on the primary, basically all the data, all the keys and values. Then we send that to the two new replicas together. This adds extra load on the primary, so we want to limit the amount of work done during this process and limit the duration to minimize the impact on end users. 

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1350.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1360.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1360)

Once that snapshot is complete, we stream all the incremental writes that came into the primary during this period of time. Once that's done, the replicas are now in sync and able to serve data. We can then begin transferring the ownership from the existing primary to the new primary. We do this through a process called failover, which is a fundamental building block within our service and the open source community.  This is the safe transfer of slot or shard ownership from the previous primary to the new primary. 

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1380.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1380)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1390.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1390)

During this process, we start blocking writes so end users don't observe this because it tends to be a very short period of time. What they actually see is a slight write latency increase as we wait to process those writes until the new primary takes over. This allows us to ensure that the primary and the new primary are caught up with each other.  During this time, read availability is unimpacted. Once everything is in sync, we're able to complete this failover process  and the new primary is able to take over. The write that we paused earlier is now able to get processed.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1400.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1400)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1410.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1410)

We then use the exact same mechanism  we discussed earlier to restart the full sync to get these new nodes all in sync. However, since we already have a point in time snapshot, we're able to skip that step and just stream the incremental writes. 

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1450.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1450)

### Forward Compatibility and Dual Channel Replication for Faster Syncing

Let's discuss some of the problems with this approach and some of the things we fixed. One issue with this process is that the full snapshot I mentioned is not forward compatible, though it is backwards compatible. If you're on a Valkey 8.0 cluster, you're able to receive a snapshot from a 7.2, but not the other way around. A 7.2 cluster cannot receive a snapshot from an 8.0. In this case, the replica sends its version and the primary will reject that version. 

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1480.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1480)

However, one thing we wanted to improve was based on customer feedback. Customers asked us if they could safely downgrade, meaning they wanted to go to 8.0 and then go back to 7.2. We actually worked with the community to implement that functionality so that when a replica requests a snapshot, the newer versions can actually send a downgraded version of that information to the older versions.  This is great if you want to have a blue-green deployment in production, but it also helps us solve another problem. If a replica for any reason is unable to connect to the primary or the new primary in a timely fashion, it will no longer have a valid point in time snapshot. It won't be able to get that snapshot because of the forward compatibility issue I mentioned, which results in these two extra nodes being unable to sync from the primary, so they just sit there and are unable to serve traffic.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1520.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1530.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1530)

This can result in an availability outage if you want to fail back for any reason. However, starting in Valkey 9.1, which is still not even released but we like to think ahead,  Valkey version 7.2 will be able to successfully stay in steady state with these 9.1 clusters. 

Another improvement we worked on is ensuring the full sync happens as fast as possible because the snapshot puts some load on the primary. One of the things we built in Valkey 8.0 is what we call dual channel replication, which allows us to send both the snapshot and the incremental writes at the same time. This allows us to offload some of the extra memory pressure from the primary onto the replica. We are able to do this in part because over time we have seen that network cards have become much faster and we are able to transfer significantly more data at any given point in time.

On older versions of hardware, we were typically bottlenecked on network throughput. Since we no longer have that problem with more modern hardware, we are able to stream the data together. These are the planned maintenance work improvements that I wanted to discuss. Next, I want to talk about unplanned failures and how Valkey detects when a node fails and triggers a failover.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1600.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1600)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1610.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1610)

### Unplanned Failure Detection: Improving Cluster Failover with Lexicographic Ordering

In our recommended high availability mode Valkey cluster, there is something called the cluster bus.  All the nodes are periodically sending ping and pong messages to each other. These messages include some information.  They basically include what data I own and what shards I own, something called the node epoch, which is a unique identifier for the given node. It is basically a version number indicating what version of this information I have. Each primary in the cluster has its own unique node epoch. We are also gossiping this information so that other nodes can learn about it more quickly.

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1640.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1640)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1660.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1660)

There is also another response which is no response, which means the node is dead and we should do a failover for it.  Let me look at this in more detail with a concrete example. We have three shards, and they each have their own unique epoch. We have shard one with epoch one, shard two with epoch two, and so on. Each of these shards has one primary and one replica. We just walked through how planned operations work, so in that case there is a very safe transfer and we bump the epoch as part of that transfer. 

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1670.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1670)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1680.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1680)

What happens when a primary fails? In this case, we have shard two where the primary failed.  The replica will eventually detect this through the gossiping through the cluster bus and will basically request from the other primaries that it would like to take over this shard.  Within distributed systems we have something called a quorum, which is the minimum number of nodes you need to basically commit a decision. In this case it is the other primaries in the cluster. This request will start with an epoch, which is basically saying this shard wants to take over this information with this epoch.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1710.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1720.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1720)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1730.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1730)

Every other primary is able to vote once per epoch. Since every node needs to have a unique epoch, we can only vote for one epoch.  Once it is granted, that replica will take over. This is how the failover mechanism works today in Valkey 8.0 and lower.  Let me talk about a failure mode and how we fix that in future versions. If we have multiple concurrent failures,  we sometimes see this when there is a zonal issue with networking, so there is a network split and multiple failovers trying to happen concurrently.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1760.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1760)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1770.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1770)

In this case, we now have a fourth shard, and both shard two and shard four have failed, and we are trying to get a decision to do a failover. We will assume that this is two is quorum for simplicity.  Replicas are often able to reach different primaries more quickly, so if they both started this election at the same time, they would both get their granted response from the closer primaries and they would then get a rejected response from the further away primaries.  This is perfectly fine in distributed systems. What we typically do is then retry with some amount of jitter or backoff and allow the system to eventually converge, but this takes more time and as we discussed, we want the system to be as available as possible.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1800.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1800)

What we did instead is each replica will basically take its lexicographic order  and will wait a short amount of time based on how far down it is. The first node with the highest lexicographic order node ID will try to do the election first. Once that one succeeds, the next replica will start its election and then get elected.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1820.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1820)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1850.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1850)

This is effectively the same thing that was happening anyway with the backoff and jitter, but since we codified it, we were able to dramatically improve the failover times for large clusters. What we observed is that an election that would often take several minutes was reduced down to about 10 to 15 seconds to get all of the nodes failed over for large clusters like 2 to 300 nodes.  

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1860.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1870.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1870)

So let me go over what we talked about within the replication group. We made replication improvements in Valkey 8.0 by introducing dual channel replication.  In Valkey 8.1, we built the functionality to do faster failovers.  Coming soon will be the forward replication compatibility, which will be an improvement that we will see in future versions, most likely Valkey 9.1, and will come to ElastiCache relatively shortly.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/1900.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=1900)

### Memory Efficiency Revolution: Reducing Overhead from 52 to 29 Bytes with Swiss Tables

Next, I am going to move on to another section and talk about how we made Valkey more memory efficient. This is a very exciting area for me because this is typically what saves end customers the most amount of money.  Most caching workloads and most high performance workloads are bound by memory, and obviously memory is very expensive. We do not want to be spending very much extra resources on something that is quite precious.

Back in Valkey 7.2, we did some analysis to figure out how much overhead there is for us to store various pieces of information. A simple key value pair in Valkey uses 52 bytes of overhead. Now that might not sound like a lot, but when we did some analysis on the metadata within Amazon ElastiCache, we found that the P50 values for the key and value sizes was 16 bytes for the key sizes and about 80 bytes for the values, which adds up to be about close to 100 bytes. So you are spending almost 33 percent of your overall memory allocations basically for overhead, and that is the best case.

If you are adding TTLs, which is extremely common for caching workloads, you typically like to add time to lives so the data gets kicked out. You are adding another 32 bytes of overhead. In cluster mode, which is basically what we think is the de facto best configuration, you are spending another 16 bytes as well. So we really wanted to figure out how to improve this.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2000.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2000)

We actually started our memory journey back before the fork. We were working on this back in Valkey 7.2, and our goal is to basically rebuild some of the core data structures in Valkey to save memory without losing performance. Let me walk you through what we do. Let us talk a little bit about computer science fundamentals. 

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2030.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2030)

The main data structure backing Valkey is what we call a hash table. The way a hash table works is it has an array of buckets, and we are going to put those key value pairs into those buckets. How do we decide what bucket to put them in? We use a hash function. For simplicity, we will say we have a key value pair called foo. We will employ a hash function on it and it will tell us that the bucket is 00. 

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2040.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2040)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2060.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2060)

We had a separate memory allocation for this, which is a 24 byte allocation. A lot of the 52 bytes we just talked about has its sole function basically to hold all of the information and hold pointers to all the information.  It has a pointer to the key. When we are traversing trying to find if this is our key value pair, we will follow this key pointer and we will see that it is pointing to a string called foo. We will do that comparison from our original key value and we will see yes, this is actually the key value pair we are looking for. So then we will follow the value pointer. 

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2090.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2090)

It will point to this intermediary object that we have called the Valkey object. Within Valkey there are a lot of different types the value can be. Traditionally it is usually strings, but it can also be stuff like sorted sets, hashes, and so on. So we need to include some metadata to determine which type it is. We also have some low level information like reference counting if we have multiple different pointers to this object. And then finally we have the pointer to the actual value, in this case foo. 

Of course if there is a collision, so two keys hashed to the same bucket, we have to resolve that. We do this through linked list collision resolution, which is basically as long as there is a next pointer, you have to keep checking the next pointers to find your value.

Every one of these light blue lines represents a memory lookup, which is slow. In our scheme, it's about 100 nanoseconds. A typical Valkey command takes about 1 microsecond, so 100 nanoseconds is significant because we have to go to main memory. We'd prefer data to be in CPU cache. Additionally, it uses 8 bytes of overhead, which we want to avoid, especially with small keys and values.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2140.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2140)

The first thing we did was move away from this simple static structure. If you're a C developer, it's annoying to have dynamically allocated structs and figure out where information is inside the structure, but we decided that complexity is worth it so we don't have to pay this 8 bytes for the extra pointer. We also implemented variable allocation sizes. 

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2180.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2180)

Next, we asked why we have this generic container object at all. We decided to embed everything directly into the container, and that container becomes the Valkey object. This required us to rebuild a lot of the internals and the hash table so that when we want to look up a key, every type has to declare how to find a key in the object it's looking for. 

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2200.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2200)

Let's look more closely at the problem I mentioned about finding where data is inside a structure. This was the Valkey object we had before. All of this is measured in bits. The value pointer, for a 64-bit system, is 64 bits. We basically have to keep track of information to say where something is inside the structure. 

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2220.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2220)

I mentioned that we started to embed the key, so we now need to keep some extra overhead so we know where it is. We can then offset into that to find the actual location of the key inside the structure. 

One thing in that previous slide that wasn't there was the next pointer. We also want to get rid of that. A traditional mechanism for solving this is to use linear probing instead of linked lists for collision resolution. We keep a lot of extra empty buckets inside our table array and keep it relatively empty. Instead of sticking an object exactly where it's supposed to be in the right hash bucket, we stick it in the next available hash bucket. This was popular in the early 2000s, but it doesn't account for modern hardware architecture.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2260.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2260)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2290.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2290)

Most modern hardware uses 64-byte cache lines. When the CPU tries to fetch memory from main memory, it actually pulls the adjacent 64 bytes with it.  An alternative strategy we use is based on learnings from a paper that Google wrote about what's called a Swiss table. These tables have large 64-byte buckets with 7 entries and some extra metadata. When we have a collision, we stick it in the next available slot inside the bucket. 

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2320.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2320)

When we want to check if something is in this bucket, we go and check every single one of these entries to see if something matches our key. Earlier we talked about "foo." If there were 6 items inside bucket 00, we check every single one of those entries to look for our item. That's not very efficient. The people from Google were aware of this, and they took advantage of SIMD operations, which are single instruction, multiple data, to accelerate ruling out if some of those entries were incorrect. 

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2370.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2370)

I want to differentiate the lower bits from the hash function. If a hash function mapped out to a long 24-bit string, the bits to the right are what's actually used to compute the buckets, and the top bits are often discarded because we don't need them to determine which bucket an object is in. Instead, we store this in the metadata I talked about. We keep one byte of information, which is 8 bits, at the top in this metadata block, and we're able to very efficiently check through them with SIMD operations. 

This is a very simple set of SIMD operations in C, which basically lets us say, "Let's get all the hashes out." We have an array of 8 byte hashes for what's currently in the buckets. We create a vector of the hash value we're looking for. Then we have the CPU go and check all of those in parallel. This allows us to check which of these buckets potentially has the entry we're actually looking for. Since we're using one full byte, there's about a 1 in 256 chance of a false positive.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2420.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2420)

There's one last piece of unresolved problems that we had to solve with this implementation, which is checking if a bucket is full and what to do in that case. We still need to have a next pointer, which we basically do around the entire bucket.  Instead of paying 8 bytes for every object for the next pointer, we're only paying effectively 7/8 bytes, so about 1 byte.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2450.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2450)

Let's look at some of the results we had from this. Between Valkey 7.2 and Valkey 8.1, which is the version where we actually released this new hash table, we were able to remove those three main pointers we talked about: the key pointer, the server object pointer, and the next pointer, which saves about 24 bytes. We had one extra byte of overhead that we added in, so that means we're saving about 23 bytes for a simple key-value pair.  That's the reduction from 52 to 29 bytes. There are some other benefits of this implementation as well. We actually removed 16 bytes from the TTL and we were able to completely remove the 16-byte overhead from the cluster mode implementation. In the best case, you're saving about 55 bytes of memory total.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2500.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2500)

Let's put that into some actual concrete examples.  We had a customer that moved from Valkey 7.2 to Valkey 8.1, which is sort of like the best case. This customer had very small keys and values. We re-ran their migration over a much longer time period. They had about 150 million keys with values that were relatively small, only about 24 bytes, and keys were also about 16 bytes. They saw about a 20 percent memory savings moving to 8.0 and then a further 27 percent memory savings when moving to 8.1. You'll notice we say 41 percent total, and we need to multiply those together. We can't just simply add them.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2540.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2540)

We wanted to improve memory primarily, but we also didn't want to regress on performance. I also want to call out the significant performance improvements we saw with this architecture. We did a bunch of micro benchmarking and macro benchmarking, and basically the actual time we spend fetching items out of this hash table was cut about in half.  There are two main performance workloads you like to test with hash tables when doing caching. The first is the cache hit case, where ideally you'd want to be getting data out of the cache. But also if you have a lot of cache misses, you want to make sure that the full miss rate when the item is not in the hash table is also being properly accounted for. We saw the performance most improved on that missed case, and that's because of the SIMD operations we're doing. We're able to rule out basically all of the entries inside those buckets, so we didn't need to check anything, which is a great improvement.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2610.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2610)

### Bloom Filters: Achieving 98% Memory Reduction for Probabilistic Data Structures

We also saw a great improvement for cases where we actually had to look up items because we're doing fewer memory references overall.  The last thing I want to talk about for memory improvements is probabilistic data structures and specifically a bloom filter. A bloom filter is a probabilistic data structure on top of a set. A set is an unordered collection of items, and you typically ask one of two questions about it: is an item in a set, and how big is a set? Valkey actually has two types of probabilistic data structures. The hyperloglog solves the answer to the question of how big a set is, and a bloom filter answers the other question of whether an item is in a set. They both use probabilistic mechanisms to reduce the amount of memory being stored while keeping the same benefits.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2690.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2690)

With a hyperloglog, it's an approximate of the cardinality. For bloom filters, it doesn't exactly answer the question of whether an item is in a set. It answers the question of whether the item is definitely not in the set or maybe in the set. An item might not have ever been added to the set, but the bloom filter might report it, which is what we call a false positive.  Let me try to enumerate that with an example from a customer we talked with. They had a workload where they basically wanted to have a microservice that could detect and block malicious IPs. They had a database of all the IPs they believed to be malicious, and every time a request came in, they were checking this file.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2710.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2710)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2730.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2730)

And if it was, they would say no.  At one point in time, they moved this to a distributed system and wanted to add a cache to include all this information. They were hydrating the cache with a set of all the malicious IPs, and instead of checking the database, they were checking the cache. If an item was in the set, they would block it, and if it was not in the set, they would not block it. 

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2750.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2750)

What they observed with all of this was that it was consuming a very large amount of memory. It was very fast, it worked, and it was pretty distributed, but they wanted to use less memory and save costs. So what they did was replace that with a bloom filter.  The same mechanism still existed. From the database, they populate the bloom filter within the cache, and if the bloom filter says an item is not in it, we can trust that we can say the item is definitely allowed and it can go through. We expect most of our requests to be valid requests, and it's really only in extraneous cases that something is being malicious.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2780.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2780)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2790.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2800.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2800)

But in the malicious case, we have to do a little bit of extra work. An item might falsely report if it's actually in the bloom filter, which is the false positive case we talked about. The item is actually maybe in the set, so we do need to go back and check the database in that case.   This particular customer observed no material increase on the actual database.  They said it was about less than one percent increase in CPU utilization, but they were able to reduce ninety-eight percent of the memory they were using within the cache itself.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2850.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2850)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2860.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2860)

On top of what Joe set up at the beginning, we care a lot about improving the fundamentals of Valkey, and that includes improving reliability, performance, and memory efficiency. We also care a lot about adding new functionality that fits into the ecosystem of caching and high performance key-value databases. Just to recap this section about what we talked about and when it all came out, we had a twenty percent less overhead reduction for Valkey 8.0 with the first part of the hash table stuff we talked about.  There was an additional twenty-seven percent memory reduction in Valkey 8.1, and we also launched Bloom filters as part of 8.1.  All this stuff is currently available inside ElastiCache for Valkey.

### What's Next: Full Text Search, Durability Improvements, and Community Contributions

We covered basically most of the stuff we talked about, so I want to do a quick recap of everything before we get to what's next. Valkey is a vendor neutral open source project, and we're really excited and think it's developing quite quickly. We talked about stuff like multi-threaded IO, which dramatically improves performance. We talked about some of the reliability stuff, which is improving unplanned failures with faster failovers and faster replication with dual channel replication. We talked about how we've been trying to make Valkey cheaper overall by doing memory reduction as well as adding new data types like bloom filters to more efficiently utilize memory so that users can choose data types that better fit their use cases.

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/2920.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=2920)

Let's talk about what's next. One thing I'm hopeful that some of you might come out of this conversation with is that Valkey is, unlike a lot of other AWS services, our roadmap is fully in the open.  I'll give you a slide in a second that says you can go find the code that we're working on. We're actively working on full text search capabilities, which we see a lot in caching workloads for e-commerce websites, where you want to return some results very quickly. It's also going to help support hybrid search for things like vector similarity search, which I haven't mentioned Gen AI yet. This is the obligatory Gen AI reference. If you're doing stuff like memory for agents, this can really help improve that performance.

We're also actively working on trying to improve durability. We talked a lot about ElastiCache yesterday. There's also a sister service called MemoryDB, which is the durable version of Valkey. We're working with the community to add that built in to the community so that everyone can self-manage and get the same high durability guarantees that they can get from MemoryDB, which I personally am very excited about. I talked a lot in the reliability section about why it's important to do full snapshots quickly. So we're actually doing two more improvements there. We're trying to fully multi-thread that implementation so that it can happen significantly more quickly.

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/3040.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=3040)

ElastiCache Serverless will be able to take advantage of this because we'll be able to dynamically add cores to our caching nodes, but you can also do this in self-managed stuff like Kubernetes, which allows you to dynamically add cores as well. We're really excited about all that. We're also still working on trying to improve the data types available to end users so they can better utilize the hardware that they have. The next big thing we're working on is time series data types. Any workloads where there's a lot of data being ingested and you want to get real-time dashboarding on top of it, that's another thing we're excited to build and work on. 

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/3060.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=3060)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ec5e80613ac067d4/3090.jpg)](https://www.youtube.com/watch?v=iUw1d4bUNJk&t=3090)

Here is my ask: if you are curious, if you are excited, if you would contribute and talk about the workloads that you want to solve in an open fashion, we want people to do that. You don't even have to know C. I know Valkey is all written in C, but even if you write in higher level languages, we do want people to contribute.  If you're interested in learning more, both Joe and I will be out in the main area. If you have a question about my section, feel free to ask me about it. If you have any questions about Joe's section, also feel free to ask me. We also have a bunch more AWS specific sessions during this re:Invent, so feel free to go check those out. A lot of these breakout sessions typically have space before the actual talk, even if they're full now. Thank you everyone, and enjoy your re:Invent. 


----

; This article is entirely auto-generated using Amazon Bedrock.
