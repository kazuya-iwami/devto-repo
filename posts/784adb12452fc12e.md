---
title: 'AWS re:Invent 2025 - Building multi-tenant SaaS agents with Amazon Bedrock AgentCore (SAS407)'
published: true
description: 'In this video, Bill Tarr and Ujwal Bukka demonstrate building multi-tenant SaaS agents using Amazon Bedrock AgentCore, addressing five key architectural challenges: tenant onboarding, SaaS identity, data partitioning, tenant isolation, and observability. They explain AgentCore primitives including Runtime, Gateway, Identity, and Memory, showing how to implement silo, pool, and hybrid deployment models. The presentation includes detailed code examples for JWT token management with Amazon Cognito, ABAC roles for tenant isolation, metadata filtering for shared knowledge bases, and custom CloudWatch metrics for tracking per-tenant inference costs. A hands-on workshop (SAS403-R) with working code on GitHub accompanies the session, featuring a supervisor pattern architecture with orchestrator, knowledge-base, log analysis, and coder agents.'
tags: ''
series: ''
canonical_url: null
id: 3085114
date: '2025-12-05T03:08:19Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Building multi-tenant SaaS agents with Amazon Bedrock AgentCore (SAS407)**

> In this video, Bill Tarr and Ujwal Bukka demonstrate building multi-tenant SaaS agents using Amazon Bedrock AgentCore, addressing five key architectural challenges: tenant onboarding, SaaS identity, data partitioning, tenant isolation, and observability. They explain AgentCore primitives including Runtime, Gateway, Identity, and Memory, showing how to implement silo, pool, and hybrid deployment models. The presentation includes detailed code examples for JWT token management with Amazon Cognito, ABAC roles for tenant isolation, metadata filtering for shared knowledge bases, and custom CloudWatch metrics for tracking per-tenant inference costs. A hands-on workshop (SAS403-R) with working code on GitHub accompanies the session, featuring a supervisor pattern architecture with orchestrator, knowledge-base, log analysis, and coder agents.

{% youtube https://www.youtube.com/watch?v=uwXrtyXXuy8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction to Multi-Tenant SaaS Agents with Amazon Bedrock AgentCore

Thank you so much for coming. I really appreciate your support in attending re:Invent and coming to see our talk. If you've been paying attention over the last year, you've probably heard about generative AI and the agentic revolution that's been happening. Hopefully, you've also heard about Amazon Bedrock AgentCore, and that's why you're here because that is the topic of our discussion today. We're going to talk about what it means to build multi-tenant SaaS agents in Amazon Bedrock AgentCore.

[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/0.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=0)

 I want you to think about any SaaS applications you've built or used and how they've evolved over the years, along with the best practices we know today. We'll explore what it means to take those same practices and apply them to agentic solutions. My name is Bill Tarr, and I'm a Principal Partner Solutions Architect. I'm fortunate to be joined by Ujwal Bukka, a Senior Partner Solutions Architect with AWS. Before we get started, I want to mention that this is a 400-level session, so we'll spend time digging into code and showing actual examples. We will set the table a bit, but we do expect you to have some background in coding and perhaps some familiarity with AgentCore already.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/80.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=80)

 I want to let you know that there's also a workshop that reflects almost all of the practices we'll discuss in this session. If you like getting your hands dirty and working with code, this workshop is happening today and tomorrow. It's listed as SAS403-R in the catalog. Don't hesitate to go wait in line. This is a great workshop where all the things you see today and even more are reflected in the hands-on experience. The workshop is really the basis of this entire talk. If you aren't someone who likes workshops or you don't want to attend, you can go out to GitHub and grab the code. There's an actual codebase with working code behind this. This isn't vaporware or us waving our hands around. This is a real solution with a workshop you can attend.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/120.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=120)

 If you can't get to the workshops here, reach out to your AWS SA, and they can help you get access to this workshop. So what is this workshop that we built? Here's the sample architecture. I don't want to spend too much time on it, but I want you to see this at the beginning because you're going to see a number of components that will be reflected throughout this entire talk.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/140.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=140)

 You'll see that there's an AgentCore runtime. This is where our code lives and where we're going to execute our agents. We have several agents. There's going to be an orchestrator agent that's going to call a knowledge-based agent, a log analysis agent, and a coder agent. Why? Because it's going to help us solve code problems as they occur. It's actually going to be able to go to a knowledge base, look at our documentation, and see how the code should operate. It's going to be able to go to our logs through Athena and S3 and examine our logs to see if we can find a reason that the problem is happening. It has a coder agent that perhaps can actually create code or review code as we want to roll fixes out.

Around all of these, we have the tools that live in the AgentCore Gateway. We're going to get into what it means to manage tools in AgentCore. We're going to talk about AgentCore Identity. What does it mean to manage authorization and authentication in a SaaS solution, in a multi-tenant solution, and in AgentCore itself? And then finally, observability. I wouldn't want to do any talk around multi-tenancy without talking about observability and how we implement the ability to get visibility into the operations of our solution.

### Five Core SaaS Architectural Challenges: From Onboarding to Observability

With that, I said I was going to talk a bit about SaaS. We're going to focus on five main SaaS architectural challenges that we see with customers we talk to who are building SaaS today, before you even think about agents. One of them is challenges with tenant onboarding.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/240.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=240)

 When we say onboarding in SaaS, what we mean is how quickly you can get your customers from learning about your product to getting value out of your product. This is one of the core components of SaaS. If your customers have a bad onboarding experience, if it takes them a week to start using the software, there's a pretty good chance they've wandered off. SaaS identity is another challenge. What does it mean to authorize and authenticate a user and how does tenant identity and tenant context inject itself into this process? How do we make sure that propagates through our whole solution?

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/260.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=260)

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/270.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=270)

  Data partitioning is another key challenge. What does it mean to properly manage our tenant data? How do we segregate our tenant data into either logical or physical buckets that keeps the data safe from other tenants' data?

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/280.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=280)

 Tenant isolation is another critical challenge. What are the policies that we use that define what a tenant has access to and what they don't? This tenant can access this service, this tenant can access this database, and they shouldn't ever cross those boundaries. Tenant resources should be specific and explicitly defined for the tenant.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/300.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=300)

 And finally, SaaS observability. I've said that I would never have a SaaS talk without talking about observability. The ability to monitor your tenant's health applies across the board to every SaaS solution.

In fact, it applies to every solution. If you're building a solution and deploying it on AWS without any source of observability, I want to have a conversation with you. But if you're building an agentic solution where there can be many moving parts, I really want to have a conversation with you.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/350.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=350)

### Understanding Agents and Amazon Bedrock AgentCore Architecture

With that, Ujwal, could you kick us off and tell us how we get started with AgentCore and agents in general? Sure, so the topic of discussion today is building multi-tenant agents using Amazon Bedrock AgentCore. The fundamental component, as you can see, is the agent. Let me start by defining an agent and then try to add more layers on top of it. 

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/370.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=370)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/410.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=410)

I'll try to define what an agent is, but by now most of you might already know what an agent is. What I'll do is highlight some of the architecture challenges you might encounter when building an agent.  To start off, you write a piece of code and deploy that code to run in some kind of compute. When you see an agent, assume that it's just a piece of code running in some compute. Then you configure that agent with a large language model and expose that agent to your customers so they can interact with it. To extend the functionality of the agent, you need to write some tools code. A tool is a piece of code that helps you reach out to some external system or external resource. The tool code has to run in some kind of compute through which the agent tries to reach out to an AWS resource or some other external resource. 

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/440.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=440)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/460.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=460)

Now that you have this agent, users might interact with it, so there is an inbound call that you need to authorize. The agent might also reach out to an external resource, so there is an outbound call that you need to authorize as well. You need to have some kind of identity that helps you with the authorization of both the inbound call and the outbound call.  When we talk about agents, there should be some kind of memory management component so you can store the agent's conversation memory and the agent can resolve a task assigned to it within that particular context. Last but not least, as Bill was mentioning, we need some kind of observability component that helps you have eyes and ears into the agent to understand what the agent is doing and why it took a particular decision. 

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/480.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=480)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/500.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=500)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/510.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=510)

Hopefully you can see that when I define the agent, I also touch upon some of the architecture challenges you might encounter. That brings us to the next slide where I would like to introduce you to Amazon Bedrock AgentCore, which addresses the challenges I've established in the previous slide.  To start off, you can build your agent code and deploy it in the AgentCore Runtime primitive to securely scale and deploy your agent code.  You can deploy your tools code in an AgentCore Gateway primitive. Think of this primitive as MCP as a service. Then you can configure the agent running in the AgentCore Runtime with the AgentCore Gateway through MCP so that it can invoke those tools. 

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/530.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/540.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=540)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/550.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/560.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=560)

Again, the user invoking the agent creates an inbound call, and the agent reaching out to external resources creates an outbound call. To help with those inbound and outbound authorizations,  you can use another primitive called AgentCore Identity, which helps you with both calls. For memory management, you can use another primitive called AgentCore Memory,  which has short-term memory and long-term memory, which we'll discuss in a bit. Then you can leverage AgentCore Observability, which primarily helps  you understand why a particular agent is doing a particular task in a certain way and helps you capture any kind of custom metrics related to that particular agent.  Last but not least, AgentCore also gives you some custom tools that you could leverage if it makes sense for your architecture.

### Multi-Tenancy Deployment Models: Silo, Pooled, and Hybrid Approaches

Bill, now that we have established some of these fundamental components, can you let us know how we could bring in a multi-tenant flavor here? Yeah, absolutely. So when we think about SaaS in general, one of the concepts we want to be thinking about is how we want to deploy our solution to reach different types of customers. It would be very rare with a SaaS solution to find ourselves with a single archetype of customers. We might want to go upmarket to bigger customers or downmarket to smaller customers. We might want to have more configurability, but there are different concerns we have to think about. The first of those is how we apply multi-tenancy in different models.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/610.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=610)

When we talk about these, a couple of primary  models really start to emerge. The first of which you'll hear us call either silo or dedicated. In a silo or dedicated model, it's pretty straightforward. Everybody gets their own stack. You get your own web server, your own compute. Everybody has their own independent architecture, usually top to bottom.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/650.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=650)

On the other extreme is pooled or shared infrastructure. In this case, it's an entirely different challenge. Most of the architecture will be shared, and at runtime we're going to have to make decisions about who you are, what you're trying to do, and whether you have permission to do the operations you're currently trying to do.  There are trade-offs, and those usually boil down to a couple of primary facts.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/660.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=660)

Silo is a simpler architecture.  Any of us could take a complete AWS stack, run it once, and then turn around and run it in another AWS account. It's not very hard. On the other hand, it's also not terribly efficient. When you have two accounts, it doesn't seem so bad. What about when you have 1,000 accounts? It's going to get a lot harder to operate and maintain that solution.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/680.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=680)

On the other side,  we have more complexity. To really properly do pooled or shared solutions, we have to think deeply about what the multi-tenant concerns are and how we've handled them. How do we apply governance? How do we think about tenant isolation? How have we done data partitioning? All of those questions become very important, and these trade-offs come in different flavors because you won't necessarily make a clean choice between these two.

You might not have a completely siloed or pooled architecture. You might have a hybrid or bridge architecture where perhaps one piece is completely shared and other pieces are completely dedicated, even all the way down to a workload-by-workload decision. You might be deciding whether this is a pooled solution or a completely siloed solution. With that, we'll hand it back to you to talk a little bit about multi-tenant agents and how those fit into these different deployment models.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/760.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=760)

### Building Multi-Tenant Agents and Implementing Tenant Onboarding

So far we can define what an agent is, and we've also introduced you to the AgentCore and then Bill has introduced us to some of the multi-tenant concepts. Let's try to bring all of them together and try to build our agents or turn our agents into multi-tenant agents. Let's start with the silo agent. When I say the silo agent, it's an agent dedicated for a particular tenant. The way you could create that is you could have  your dedicated agent deployed in a dedicated AgentCore runtime.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/770.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=770)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/780.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=780)

It can be configured with a dedicated AgentCore memory, and then you could extend that agent with the  tools core, which are dedicated for that particular tenant. These are deployed in a dedicated AgentCore gateway, which would reach out to your dedicated tenant-specific  resources. In this case, I'm showcasing some of the AWS resources, and it can also be a third-party API. This would be the silo agent.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/800.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=800)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/810.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=810)

Moving on to the pool agent, we use a similar pattern where you have your pool agents deployed in a pool AgentCore runtime configured with pool AgentCore memory.  It is also configured with a pool AgentCore gateway where you have pool tools which reach out to the pooled AWS resources.  When we introduced the silo and pool models, we showcased two extreme cases, but in reality, some of the components would be shared and some would be dedicated, so you would build a bridge or hybrid model, which Bill was alluding to earlier.

I would like to point your attention to the AgentCore memory. This is the first time I'm introducing short-term memory and long-term memory. Short-term memory is where you could store raw events on a per-session basis, like user inputs, agent replies, or any kind of system-level changes. Long-term memory is where you could leverage to store conversation summaries across sessions, user preferences, or any kind of system-level changes or system preferences.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/890.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=890)

Now that we have established some of the fundamental components of multi-tenant agents, let's try to take these things and start resolving the SaaS architecture challenges which Bill explained earlier. To start off with, we'll start with tenant onboarding. For tenant onboarding as a SaaS best practice, we have been advocating that as a SaaS provider, you need to build some kind of a control plane experience where it has a bunch of services like onboarding service, tenant provisioning service, tenant management service, and tenant registration service. 

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/910.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=910)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/930.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=930)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/950.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=950)

All these services will help you manage, operate, and deploy or onboard your tenants. When you're trying to onboard your tenant, that's when  your tenant-specific architecture will be deployed in a SaaS application plane. That's where your multi-tenant architecture or your secret sauce lies in. So now, leveraging the SaaS control plane, you could onboard a tenant which would map to a silo model, and then you would provision this silo architecture which you saw in the previous slide.  And since it is a silo model, you could provision these resources only when you're onboarding a particular tenant, because they are dedicated resources. Similarly, you would use the same control plane experience and onboard another tenant, maybe mapping to a pool model where you would provision this infrastructure for that particular tenant.  Again, since this is a pool model which is shared across multiple tenants, you could upfront provision or pre-provision this particular infrastructure. Similarly, you could extend this experience where if you onboard a tenant mapping to the pool model, you would just map it to that particular model. The way you would expose these models to your customers is through tiers. You could have a basic tier which maps to a pool model, or you could have a premium tier which maps to a silo model.

### SaaS Identity Management: JWT Tokens and Custom Claims in Cognito

So Bill, now that we have resolved one of the SaaS architecture challenges, can you set the stage here for resolving the next SaaS architecture challenge? Yeah, absolutely. We're clicking through architectural challenges one at a time. We talked about onboarding. Let's get into identity a little bit. Now SaaS identity has always been true across all SaaS applications. Some of the concepts still remain the same. I want to go over some of the primary tenants and just a little bit of a twist on why we think about this slightly differently when we're building AgentCore.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1020.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1020)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1060.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1060)

It's always been true that we have tenant users coming in and there's going to be some form of an identity provider.  We need something to manage all of these users. In our particular example, we decided to use Amazon Cognito. It is an identity provider that does provide tenant boundaries. The tenant boundaries we have choices in Cognito, but the one we chose to use is a user pool per tenant. So a user pool per tenant simply means we have a nice tenant boundary. All of our users for each tenant are bucketed up nicely. This particular user right here belongs to tenant one. And as you can see, they have some custom claims or what we're calling custom claims here on this user. The reason I say calling custom claims is that in fact, if you look in Cognito, we call these attributes in Cognito, but downstream when we instantiate and vend this JWT token,  you're going to find them called custom claims inside the JWT token. So we can have tenant status, tier, any number of other metadata about this particular user that we want to proliferate through the rest of our application.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1080.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1080)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1110.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1120.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1120)

Now the JWT token is a vehicle for how we pass that data downstream, and I want you to keep in mind that if you haven't done JWT tokens before, it's okay. We don't have to understand this representation, but in fact they're just basically JSON in there and there are a couple of different parts, and this is kind of relevant when we talk about AgentCore.  There's the identity token which automatically inherits all of the custom claims I was just talking about that can be useful for some applications. Sometimes you can simply rely on that identity token alone. On the other hand, there's another part to the token which is called the access token, and the access token actually doesn't inherently  inherit all of those custom claims. The access token is going to be passed around inside AgentCore. One strategy we can use is to use a pre-token generation Lambda trigger.  That can take all of those custom claims and copy them into the access token, which can be convenient for when you're passing it around and you're going to see us talk about how that access token is passed around and reused at different layers of AgentCore.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1180.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1180)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1200.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1200)

So could you walk us through what that looks like in AgentCore with the identity? Sure. So now that Bill has established how we need to create a SaaS identity, let me walk you through here how you could leverage AgentCore identity and let this SaaS identity flow through your architecture. We'll also walk you through how you could leverage tenant context from that SaaS identity and use the AgentCore identity primitive and enforce authorization. So earlier when I was talking about the agent, I mentioned that there are two types of authorizations you have to do: one for the inbound call and the other for the outbound call.  Inbound call comes into play when a particular user is invoking a particular tenant which is running in some kind of an agent or runtime environment or something along those lines. The outbound authorization comes into play when your agent is trying to reach out to an external system. So in here, we'll first talk about how you could leverage AgentCore identity and do the inbound authorization. So my assumption here is that you have some  kind of an identity provider configured in such a way that it provides you with a SaaS identity, which Bill has explained earlier.

When a user hits your website, you redirect them to the identity provider, where they authenticate and generate a JWT token. This can be an access token or an ID token, and it contains the user information as well as tenant-specific information. Essentially, it serves as an identity.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1230.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1230)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1260.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1260)

### AgentCore Identity: Inbound and Outbound Authorization with Workload Identities

Using the JWT token, tenants can invoke the agents running in the  agent core runtime. I am deploying the agent code and tools code here. The agent core runtime must authorize the JWT token coming from a particular user. To do this, you can leverage AgentCore Identity, where you configure your agent core runtime with AgentCore Identity and also configure AgentCore Identity with your identity provider. 

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1280.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1280)

Once you complete this configuration, AgentCore Identity retrieves the inbound JWT token, communicates with your identity provider, and authorizes that particular call. After authorization, the JWT token becomes available within the agent core runtime. This is how inbound authorization can be implemented using AgentCore Identity.  It is quite straightforward. Now let's move on to outbound authorization.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1290.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1300.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1300)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1320.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1320)

For outbound authorization, consider the architecture we saw previously.  Outbound authorization comes into play when your agents attempt to reach external resources.  Let's assume this external resource is an AWS resource. If it is an AWS resource, you can leverage IAM. You attach an execution role to your agent core runtime, which authorizes that particular call. However, things become interesting when your tools attempt to reach an external resource  or an agent that expects an OAuth token or an API key. In those cases, you can leverage AgentCore Identity.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1340.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1340)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1360.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1360)

For those external resources, they might have some kind of identity provider or credential provider. You can configure your AgentCore Identity with those credential providers or identity providers.  Since you deployed your agent in the agent core runtime, AgentCore Identity inherently creates an identity for each agent, called workload identities. Keep that thought in mind because you will see how this workload identity is leveraged by AgentCore Identity. 

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1370.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1370)

Using AgentCore Identity, you can bring a human into the loop.  You can surface a form to the user saying that your tool is trying to reach an external resource and asking if they authorize this particular call. Assuming the user authorizes that call, the agent core runtime and AgentCore Identity must communicate and generate an access token, which gives the tools access to interact with those external resources.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1430.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1430)

The process works as follows: the agent core runtime makes a call to AgentCore Identity, asking to generate an access token for a particular agent. AgentCore Identity takes that request, maps it to a workload identity, which is the identity of the agent, and then maps that to the configured identity provider and generates a workload access token. Now the agent tools, using this workload access token, can interact with the external resources. 

Since you deployed these agents in the agent core runtime, all the steps I mentioned happen automatically for you. All you need to do in the code is use one annotation: @RequiresAccessToken. Then you get hold of that particular access token. The similar process happens for API keys as well. You can also bring a multi-tenant angle to this outbound authorization.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1500.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1500)

Let's assume your downstream external resource, which could be a third-party API or another service within your organization, expects some additional information within that access token or workload access token. That additional information can be metadata or tenant-specific information. You can build that into it by leveraging AgentCore Identity. Since AgentCore Identity is already configured with the identity providers, those external identity providers can customize the workflow that generates the access token  and add custom claims. This is what Bill explained when discussing the token generation process.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1540.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1550.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1550)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1560.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1560)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1570.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1580.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1580)

When AgentCore identity generates a workload access token, it can also include additional metadata that you can pass into external resources. So far, we've discussed all this in the context of AgentCore Runtime, but the best practices I've talked about will still apply for AgentCore Gateway.  The way it looks is the same architecture you saw in the previous slide. The only difference is that the tools code is now deployed in AgentCore Gateway and reaches out to external  resources. The agent code running in AgentCore Runtime can use the inbound JWT token to reach out and make calls to AgentCore Gateway.  AgentCore Gateway can be configured with AgentCore Identity to authorize that particular request. Additionally,  AgentCore Gateway has a new feature called Gateway Interceptor, which can intercept requests coming into AgentCore Gateway and access all the headers.  You can also access the JWT token and make it available for your tools, or you can get the tenant context from it and leverage those things.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1630.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1630)

### Data Partitioning Strategies Across AgentCore Memory and AWS Resources

Moving on to data partitioning, Bill, can you recap and set the stage for us? I told you the identity part was hard, and that's probably the hardest part. I promise you none of them are going to be quite as challenging. Data partitioning is actually straightforward to think about. It's really how we bucket up our tenant data. What are the challenges we face? There are cost challenges and operational challenges, but there are only so many ways to do this. You have to have some sort of separation, logical or physical, between the data.  In terms of data partitioning, we've seen some hints already. We've heard the term memory, so memory probably makes us think there's going to be some partitioning somewhere, and we've seen some downstream AWS services that might need to be partitioned.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1660.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1660)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1680.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1680)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1690.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1690)

When we talk about data partitioning, you need to look at the areas in this architecture where you're dealing with data. One area is with AgentCore Memory, where you're storing tenant-specific data. The other area  is the AWS resources. We're showcasing a knowledge base and a DynamoDB table. Let's pick these two areas and understand how you could implement data partitioning in either a silo model or a pool model. To start with the silo AgentCore Memory model,  you have a dedicated AgentCore Runtime. In here, you have a dedicated agent. The white box I'm showing is the agent implementation code.  You would have a dedicated AgentCore Memory specific to that particular tenant, and if you have multiple tenants, you see something like this.

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1700.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1700)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1710.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1720.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1720)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1730.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1730)

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1750.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1750)

Now, if the agent is trying to create an event in short-term memory,  it needs a memory ID. Memory ID is a unique ID created when you're creating AgentCore Memory. Then it needs a session ID.  In short-term memory, you store events by session ID. Each session has a unique session ID.  Then you need some kind of actor ID. Think of this actor ID as a unique key for your agent or for the user.  Another way to look at this is like a primary key in a relational database. The convention you could use is to partition the data by tenant ID and subject. These values come from your input JWT token.  By making this actor ID equal to tenant ID and subject, you're making it unique for that particular tenant user, partitioning the data by that ID.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1770.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1770)

For long-term memory, the way it stores information is by namespaces.  You can see the convention you have to use for naming the namespaces. I'd like to point your attention to the actor ID there. You see some variables with curly braces. Those are variables where you could inject values in real time. Leverage that same actor ID in this namespace so you have a unique namespace for that particular tenant user, partitioning the data that way. Again, I'd like to point your attention back to actor ID equals tenant ID and subject, where my assumption here is that you're using that convention to partition the data by tenant users.

But if you want to partition by tenant only, then in those cases the actor ID can be just a tenant ID. However, if that is the case, then this convention might not be a big help for you because you're already creating a dedicated memory for that particular tenant, so it might not be of much help.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1840.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1850.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1860.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1860)

This convention comes in handy when you move to the pool model where you're creating a pool  agent in a pooling runtime environment which is interacting with a pooled memory which is shared across multiple tenants. When this agent tries to reach  out to the short-term memory, it uses this convention where you're partitioning the data by tenant user. Similarly, when you're  interacting with the long-term memory, you could use this namespace with that actor ID so that you are partitioned for each of the tenant users.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1880.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1880)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1900.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1900)

Now I'll walk you through some of the AWS resources which you have seen in this architecture and talk about how you could partition the data  there. To start with, we'll look at the knowledge base silo model where you create a dedicated knowledge base for each of your tenants, which is configured with a dedicated vector database. In the pool model, you could have a pooled knowledge base which is configured with a pooled vector store which is shared across multiple tenants. 

The way the data is partitioned or tagged here is that when you're talking about a pooled knowledge base, you would ingest all the tenant-specific data. Amazon Bedrock Knowledge Base provides you a feature where you could attach additional metadata to it. So when you're ingesting that data into the knowledge base, you could also attach the tenant ID to it so that the data, even though it's living in the knowledge base, is tagged with that particular tenant ID. You will see how this will be useful for you later when you're trying to pull the tenant-specific data from this shared or pooled knowledge base.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1940.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1950.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1950)

Applying the same principle for another AWS resource, Amazon DynamoDB table, the silo  model would be table per tenant, or the pool model would be a single table shared across multiple tenants, but you would have a tenantId as a partition key, so that is unique for that particular tenant.  Applying the same principle for S3 bucket, the silo model would be a bucket for each of your tenants, whereas the pool model would be a prefix for each of your particular tenant within the same bucket.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/1970.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=1970)

### Enforcing Tenant Isolation with IAM Policies and Metadata Filtering

Moving on to the next architecture challenge,  partitioning the data is not enough. We need to make sure that a particular tenant is able to only get tenant-specific resources. That's the definition of tenant isolation. So now what we'll do is go back to all the areas where we partition the data and see how we could implement tenant isolation for those areas.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2000.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2000)

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2010.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2010)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2020.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2020)

To start with, we'll go back to the silo agent core memory. You have this dedicated agent  which is interacting with your dedicated agent core memory and then with the short-term memory and the long-term memory, it is using this namespace prefix which you have defined.  Since it's a dedicated resource, you could piggyback on IAM where you could attach an execution role to this agent core runtime which gives access only to the tenant  specific resources. That's how you could enforce tenant isolation here.

However, the assumption I'm making here is that if you look at the actor ID, which is equal to tenant ID, where I'm partitioning the data only by tenant, if you want to do partition by tenant ID and the tenant user level even in the silo model, then this might not be handy for you. You need to do a little bit more fine-grained authorization to enforce tenant isolation, which you will learn when we are talking about the pool model.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2080.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2080)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2100.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2100)

For the pool model, you have the pool agent running in a pool runtime, and then it is interacting with the pool memory, short-term memory, using this actor ID and the namespace.  Since it is pooled and shared across multiple tenants, what you could do is the agent can assume this particular role. Basically, you have to create some kind of an attribute-based access control role. In that role, if you look at the highlighted area, what I'm trying to do there is put a condition saying that you give access to this content in this memory only for this actor ID. And then similarly for the long-term memory, you could have another IAM policy which  has this condition which is giving access only to that particular namespace.

So now, trying to sum up everything together here, the pool agent from the JWT token gets the tenant context. It can create the actor ID, the tenant ID, and the subject, inject that actor ID into the IAM policy, and then it can interact with a security token service. It's another service through which it assumes this particular role and gets scoped credentials which are specific to this particular tenant. Using those credentials, it can only access or interact with tenant-specific data in the shared or pooled AgentCore memory. That's how you would enforce tenant isolation in this model.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2170.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2170)

Now, moving on to the AWS resources silo model, you have siloed agents running in the silo tenant agent called runtime, which is interacting with siloed tools which interact with the siloed resources. Again, since everything is siloed, you could piggyback on IAM resources where you could create an execution role or IAM  role which gives access only to retaining specific resources. You can attach those execution roles to the pool tools or to your agent quarantine which will give you only that particular access. Pretty straightforward.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2190.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2190)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2220.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2220)

Whereas the pool AWS resources, you have the pool agent running in and interacting with the pool tools which goes to the pool resources.  When you're implementing the isolation, for some AWS services like what you saw in the AgentCore memory, and for other services like Amazon DynamoDB, you have good integration with IAM where you could create an attribute-based access control role and then enforce isolation. But for some reasons it won't be the case. Let's talk about how we would implement this with DynamoDB table. Similar to what you have seen earlier, you could create an IAM role, a backend role,  which gives only permission to that particular DynamoDB table item in that particular DynamoDB table. If you look at the highlighted piece there, you're creating a condition there, saying that for this particular tenant, only give this particular tenant items. So in this case, the pool tools from the JWT token get the tenant ID, inject that into the backend role, and get the scoped credentials. Using the scoped credentials, it can interact with the tenant people.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2270.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2270)

But if you move towards the pooled knowledge base, the pooled knowledge base as of today doesn't provide you a way to implement this ABAC approach. But in turn, you can piggyback on another feature provided by the pooled knowledge base called metadata filtering. So if you remember earlier when I talked about the pooled knowledge base, I said as a best practice when you're ingesting this tenant-specific data,  attach tenant-specific metadata, like tenant ID to it, and the data would be tagged to it. Now, using this metadata filtering, what you could do is you could pull only tenant-specific data by using your tenant ID. Think of this as a SQL query you are making, and you need to attach a where clause like tenant ID is equal to the tenant ID value. So to put it in the context, the pool tools from the JWT token get the tenant ID, construct that metadata filtering query with the tenant ID, and get only tenant-specific resources from this pooled knowledge base. That's how you could enforce tenant isolation for this case.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2340.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2340)

### SaaS Observability: Measuring Unit Economics and Cost Per Tenant

Now, let me hand it over to Bill to help us run through our next SaaS architecture challenge. Right on. So you probably gathered already that I feel pretty seriously about observability in a SaaS application. So let's get into some of the details of what we mean when we're talking about applying observability in a multi-tenant solution. So in general, the challenges around SaaS observability haven't exactly changed in the agentic world. There are some specifics that do change, but overall we still have tenants and they're calling  specific resources. In this case they happen to be agents. We could apply the same practice if we were using microservices or features.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2350.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2350)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2370.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2370)

All of these calls need to be instrumented.  Now typically you'll have logs, you might emit activity events, you might pick consumption events, whatever you're emitting out of your application, you need to inject tenant context into that, which includes tenancy. What does that look like? It might look something like this, right? It might be a simple JSON that has agents and tiers, tenant  IDs and regions. It could be any number of other items. Really this is an exercise in reaching out to our business users who will operate their business intelligence tools and finding out how they want to slice and dice this data.

Because especially with agents with the expense that can be added onto your solution, they're going to have to be rethinking how their prices in the solution. They're going to be thinking about how their customers are consuming the solution. All of those are measured by sending out metrics which will allow their analytics to operate the business more efficiently. Now, as we mentioned, AgentCore has an observability component, so we've talked about runtime, we've talked about the gateways, identity, memory, and all of these things we've covered.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2420.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2420)

I think a particularly telling slide shows how each of the pieces we've discussed so far are directly tied into observability already.  This is one of the really nice things that Agent Core has done, because this is a product built from scratch. Instead of observability being a bolt-on that somebody thinks about after the factâ€”like, "oh yeah, I guess we should probably do some observability"â€”it's there from the start. All of the different components of Agent Core can actually send observability data to it, and in fact it has some nice built-in tools already.

Like an OpenTelemetry implementation, the A dot SDK that we can use to send the tenant ID off of custom metrics. It has a nice integration with CloudWatch already. If you haven't used it yet, write an agent, have it do some logs, and you're going to find a pretty nice agentic dashboard that already exists in CloudWatch that you can use to troubleshoot your agents and understand in general what it means to see the operation of your agents.

Now this is a bit of a rant. Tenant ID is not a first class concept in observability. It isn't for CloudWatch. It isn't for our third party solutions. It rarely is anywhere, and this kind of makes sense. Not everything is a SaaS solution, right? So there are solutions out there that don't need tenant ID. So when we're slicing and dicing our data and thinking about how we do this, it usually comes down to custom metrics and custom dashboards when we're building multi-tenant solutions, because tenant ID has to be a first class concept when we're building multi-tenant solutions. This is just as true for agents as anywhere else.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2520.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2520)

While the Agent Core built-in dashboards can be very helpful, you're very likely still going to have to complement those with custom dashboards that you create that revolve around your tenant ID.  Now why is this so important? Why is observability so important in multi-tenancy? One example that we almost always end up talking to our customers about is understanding the unit economics of SaaS. What is the cost of operating an individual tenant? How do we measure that? And the answer is it's actually not that easy to do. We have to build some custom observability and we have to emit custom metrics.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2550.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2550)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2590.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2590)

Now, when we talk about this,  one of the things we say is because this is hard to do, we want to prioritize what we want to measure. What's the expensive part of an agentic solution? Usually it's our models. All of those invocations actually add up. The inference costs are probably the primary thing that your business is going to raise their hand and say, "hey, why does this thing cost so much?" And the answer to that is to actually measure the tokens input and output out of the LLM. With your observability you can see this is actually pretty simple code. This is just a custom library recording metrics that do the model invocations. It counts the number of input and output tokens. 

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2610.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2610)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2640.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2640)

We're going to collect those and we're going to pass them downstream so our agents can send those off to CloudWatch and CloudWatch can collect those. You can start to see that the tenant specific metrics are flowing through. If we're using CloudWatch, we can certainly get to the point where we can filter those with CloudWatch Logs Insights, right?  CloudWatch Logs Insights is a simple query language. If you're using DataDog, if you're using New Relic, if you're using Honeycomb, they all have their own query languages. It's not like if you're using third parties it'll be any different, but this is ours. If you look there, you don't even have to look at the whole thing, just look toward the bottom and you'll see "group by tenant ID," and that's really all this query is doing for us. It's taking all those invocations, breaking them down by tenant, so that in turn we can pass that off to our observability tools or in the case of what we're going to try to do here, use those to calculate the actual cost per tenant. 

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2650.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2660.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2670.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2670)

So here what we're going to do on the next step is we're going to correlate our agent usage with the actual costs. We've already done the first part.  We've already actually created the CloudWatch Insights, so maybe we use a Lambda to occasionally invoke that, take those costs and get the actual underlying costs, right? So we've got usage.  What does the Cost and Usage Report say? How much did it cost for each one of those invocations? Combine those two concepts and perhaps we could store them in DynamoDB and they might  look something like this: tenant one having so many invocations.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2680.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2680)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2700.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2700)

Now that we've broken down our individual inference cost per tenant,  we can visualize those, perhaps with QuickSight or another BI tool, and create nice dashboards that our business users can drive to understand a deeper understanding of what it means to operate our tenants. Cost per tenant might look something like this, and cost per tier might look something like this, right, where we can actually say, "OK, this is about the cost per tenant," and tools like  QuickSight with their own integrations with models now can also let those business users drill into those individual tiers and see how much it costs for the tenants inside those tiers as long as we provide them with that data.

It's our responsibility as the builders of the agent to emit that data that unlocks all of those cases where business users can actually drill down into this data and understand what's happening under the hood.

### Complete Architecture Walkthrough: Integrating All Multi-Tenant Concepts

With that, we started off with the sample architecture. Could you take us back to the sample architecture and walk through it with all of the different things we've talked about today and talk about how they apply to that architecture? Sure. So now I'll present you the same sample architecture, but let's look from the lens of what we have learned or touch upon the things here to start off with.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2750.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2750)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2760.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2760)

The architecture which I'm presenting is  a pool model which is all shared across multiple tenants. So what you see is we've created a bunch of agents which are deployed in a pool agent called runtime, and these  agents are shared across multiple tenants and they're configured with a large language model. Hopefully you could see that we are using the SANs framework to build these agents and then we are using a supervisor pattern where you have an orchestrated agent which is configured with a bunch of subagents.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2790.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2790)

Now, when you assign a task to this particular orchestrator agent, it will invoke the knowledge-based agent, which will in turn try to invoke a tool, which is a knowledge-based tool that is shared across multiple tenants,  which is deployed in a pooled agent core gateway. This gateway is interacting with a pooled knowledge base, which is plugged in with a pooled vector database. So everything is shared across multiple tenants.

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2820.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2820)

Assuming that the orchestrator agent did not get a meaningful answer for this particular request from the knowledge-based agent, then it would invoke the log analysis agent, which is configured to pull the data from the S3 bucket that has the application logs, where we have all the tenant-specific application logs.  In this whole architecture, you will see in this workshop that we have used a SaaS control plane and tenant onboarding experience to provision this pool model into the SaaS application plane.

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2860.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2860)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2880.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2880)

From the SaaS identity concepts perspective, when you're invoking this agent, what we do is we ask for your credentials where you authenticate against an AD ADP and then generate a JWT token.  This JWT token is nothing but an access token which has the tenant information in it. Then, using the tenant information in the JWT token, you would invoke this agent which is running in agent core runtime. We use agent core identity to authorize that particular request and make the JWT token available for the  agents within the agent core runtime.

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2890.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2890)

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/2900.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=2900)

Then, when those agents are making a call to the tools in the agent core gateway, they use the JWT token again to authorize that particular call. We use agent core identity for this authorization.  Here we are only trying to interact with AWS resources, but as I mentioned earlier, you could exchange the agent core identity concepts and apply them for outbound authorization calls also when you're trying to reach out to an external resource or external agent. 

From the data partitioning point of view, the areas where you need to do data partitioning are, as we discussed earlier, the knowledge base. The knowledge base is shared across multiple tenants. We use the metadata filtering concept. We ingest the data with tenant-specific data in it and partition the data in that way. Whereas for the log analysis tool agent, we have a shared S3 bucket. We have partitioned the data with a prefix for each of the tenants, with the prefix name as a tenant ID.

For enforcing tenant isolation, going back to the knowledge base, we use metadata filtering where the knowledge-base tool gets the tenant context from the JWT token. Then it uses the tenant context with metadata filtering to get only tenant-specific resources or tenant-specific data from the Amazon Bedrock knowledge base. For the log analysis agent, the way we enforce tenant isolation is since the S3 bucket has good integration with attribute-based access control roles, it has good integration with IAM where you could create an ABAC role. So the log analysis tool gets the tenant context from the JWT token, injects that into the ABAC role, and gets access only to the tenant-specific application logs within that prefix of that particular tenant in the S3 bucket. That's how it enforces tenant isolation.

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/3000.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=3000)

And last but not least, as mentioned,  we used agent core observability specifically to capture tenant-specific metrics related to the number of input tokens and output tokens generated for that particular tenant. When the agents are executing a particular task, we take those things and log them using agent core observability to CloudWatch logs. So hopefully you can see where we started off, where we introduced you to all the different concepts, and we bring them all together to build an architecture which checks all the boxes of what we talked about.

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/3040.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=3040)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/784adb12452fc12e/3050.jpg)](https://www.youtube.com/watch?v=uwXrtyXXuy8&t=3050)

That's all we had for you today. Hopefully this is helpful for you.  Don't forget about the workshop that is today and tomorrow.  So today and tomorrow, definitely it's worth waiting in line for if you aren't signed up already. Thank you everybody.


----

; This article is entirely auto-generated using Amazon Bedrock.
