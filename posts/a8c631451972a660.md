---
title: 'AWS re:Invent 2025 - Designing mission critical applications with serverless services (CNS362)'
published: true
description: 'In this video, Luca Mezzalira, Sara Gerion, and Zeeshan Pervaiz from AWS and Booking.com discuss modernizing a 20-year-old mission-critical reservation system using serverless architecture. Booking.com faced challenges with their Perl monolith including 90+ dependencies, tight coupling, and four-month developer onboarding. They migrated to AWS Lambda and Step Functions, reducing onboarding to one month and achieving 100% traffic on the new system. The session covers seven Rs of migration, decomposition patterns like strangler fig and branch by abstraction, and mixing synchronous/asynchronous communication using services like DynamoDB, EventBridge, and API Gateway. Key insight: distributed systems are living systems requiring continuous evolution across people, processes, and technology.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/0.jpg'
series: ''
canonical_url: null
id: 3087087
date: '2025-12-05T18:03:14Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Designing mission critical applications with serverless services (CNS362)**

> In this video, Luca Mezzalira, Sara Gerion, and Zeeshan Pervaiz from AWS and Booking.com discuss modernizing a 20-year-old mission-critical reservation system using serverless architecture. Booking.com faced challenges with their Perl monolith including 90+ dependencies, tight coupling, and four-month developer onboarding. They migrated to AWS Lambda and Step Functions, reducing onboarding to one month and achieving 100% traffic on the new system. The session covers seven Rs of migration, decomposition patterns like strangler fig and branch by abstraction, and mixing synchronous/asynchronous communication using services like DynamoDB, EventBridge, and API Gateway. Key insight: distributed systems are living systems requiring continuous evolution across people, processes, and technology.

{% youtube https://www.youtube.com/watch?v=DF8sK2xkhgQ %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/0.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=0)

### The Challenge of Scaling Mission-Critical Systems and the Promise of Distributed Architecture

 Imagine this: you are part of a team responsible for maintaining a mission-critical system inside your organization. This system is so important to the company that it enables the company to scale to hundreds of developers working together on this massive application. Obviously, there are benefits to doing that and having all this great work done by developers, but there are also challenges. Instead of deploying multiple times per week like you were doing in the past, now you deploy once a month. The testing lifecycle takes tens of minutes to be fulfilled, and sometimes when you are deploying, you need to roll back because there are many issues. Does that sound familiar to some of you?

It does for me. I work a lot in this kind of application, and I understood the benefit of leveraging distributed systems where needed because an architecture is not forever. During this session, we are going to see how Booking.com went through this journey of modernization by leveraging serverless in order to overcome the challenges I described. My name is Luca Mezzalira. I'm a Principal Serverless Specialist at Amazon Web Services.

Today with me is Sara Gerion. Hi everyone, my name is Sara. I'm a Principal Solutions Architect based in Amsterdam, the Netherlands. I am passionate about observability, resilience, and surveillance. And I'm Zeeshan Pervaiz. I'm an Engineering Manager at Booking.com. My team was responsible for the modernization of a legacy, twenty-year-old reservation system at Booking.com.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/110.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=110)

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/120.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=120)

Let's start, but before we do, we need to understand what distributed systems are. Very often you see a lot of great definitions,  but for me, the best one is this:  Distributed systems are living systems. If we enter into the mindset that distributed systems will require attention and care, and we start to iterate over and over again, challenging sometimes our decisions, we are halfway through to success.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/140.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=140)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/150.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=150)

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/160.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=160)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/170.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=170)

Usually, when you embrace distributed systems, you want to reach specific goals.  The first one is organization scalability. You want to have the possibility to plug a new team or a partner inside your system to accelerate the delivery of specific functions.  You want to have business agility, so if something happens internally or externally or inside your industry that requires a shift towards a different direction, you will be able to make it because you just need to change a part of the system.  You also want to have a faster feedback loop. Many of you have heard about DORA metrics, and these are metrics that enable you to succeed in this very complex world.  More importantly, you want to reduce external dependencies for teams because the reality is when you work in a monolith with hundreds of engineers working together, you understand that at some point you have to slow down because there is a lot of coordination.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/190.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=190)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/200.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=200)

You want to reduce external dependencies, decrease the cognitive load, and finally reduce the blast radius.  When you deploy your application or service, if that service goes down, it doesn't impact the entire system like it happens in monolithic systems.  I think in order to achieve that, you need to leverage modularity. Let's try to understand what modularity means. It's the quality of consisting of separate parts that, when combined, form a complete whole. Most importantly, a system lacks modularity when a tweak to one of its components affects the functioning of others. This is not something you can find only on the tech side. This is from Cambridge Dictionaryâ€”it's the definition of modularity that applies to everything.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/210.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=210)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/230.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=230)

### Modularity Through Infrastructure: Why Serverless is a Strategic Choice

 Modularity in our case can be expressed in different ways. Maybe using code, so you leverage strong encapsulation and use design patterns that we have seen for decades working pretty well. But then at some point, when you have a decade of software that is up and running, these kinds of things can start to be diluted.  You decouple the business logic from the environment, maybe using hexagonal architecture, but you need to be disciplined in the development teams. Teams sometimes are changing, people are leaving the company, moving to other teams, and so on. Therefore, modularity using code can be done, but it requires a lot of discipline.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/260.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=260)

On the other side, you can apply modularity on the infrastructure level where you have more options to express your intent as an architect.  I want to use synchronous and asynchronous programming when it's needed because that enables me to have a more resilient and scalable system. You configure more than you code because there are a lot of built-in behaviors available inside serverless systems. More importantly, you have more control on what you develop because you develop only a chunk of the system, and there are certain behaviors that are already built in inside your service. Because what we think at AWS is that serverless is not more associated with Lambda, but it is a strategyâ€”a strategy that enables you to remove the undifferentiating heavy lifting that we take the burden on and enables you to maximize your business value, which is not writing code but generating value.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/310.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=310)

When you embrace serverless, you focus on defining the API and defining your business logic.  All the rest will be taken care of by us. You select the right service for the job and you will be good to go.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/350.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=350)

What we are witnessing with customersâ€”and I usually touch between 50 and 100 customers every yearâ€”is a paradigm shift. In order to win new customers, we need to build better products, which means we need to innovate more often and be advanced compared to our competitors. We need to release features faster because that is the only way to test our assumptions.  The DORA metrics comes very handy in this case, and therefore you need to focus more on the business logic and decouple software systems. Serverless seems like the perfect fit for what you need to do.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/360.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=360)

Serverless is very often thought of as enabling only event-driven architectures, but that is not true. With serverless, it is just a tool and you can express whatever architecture you need.  You want to be point-to-point using microservices and HTTP responses? You can do that. You want to use event-driven architecture because a part of the system, as we are going to see later with Sara, benefits from this approach? You will do that. You can also use it for data architecture. Very often I have customers that are using serverless services for ETL or for drawing data towards the data lakes, and you can also use it at the edge of your architecture.

Integration patterns are one of my favorite ways to use serverless because SaaS products very often have quotas that we need to be aware of. You cannot just hammer it every second with the same throughput that you have inside your system. These kinds of patterns and architectures are what you can use in serverless. But now, without further ado, let me introduce you to Zeeshan, who is going to start the discussion around how Booking.com embraces all this goodness.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/440.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=440)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/460.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=460)

### Booking.com's Mission-Critical Reservation System: From Cohesive Monolith to Big Ball of Mud

Thank you, Luca, for explaining the foundational concepts of serverless and the associated architecture patterns.  My name is Zeeshan, and for the last three years I have been working at Booking.com. The target of my team was only one thing: to modernize our reservation system, which was 20+ years old. It was a mission-critical system. But before we start, let me ask one question. How many of you have booked a trip through Booking.com? I see some hands. Thank you.  Thank you for choosing our platform.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/470.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=470)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/480.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=480)

At Booking, we believe life is made of experiences, experiences we share with each other.  Experiences both big and small, and that is why millions of customers each day choose Booking.com as a platform to experience the world.  Therefore, our mission is to make it easier for everyone to experience the world. At the core, it is powered by Booking.com's mission-critical reservation system, which handles millions of reservations each day. At scale, to stay competitive, we need to deliver features faster, and the features should bring value to the customers, but also quickly and reliably, so that they can enjoy their experiences.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/510.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=510)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/520.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=520)

However, engineering teams are under constant pressure to improve and reduce the time to market.  This puts them under a lot of pressure, and apparently it is not so easy for the engineering teams.  Before I go into the why part of why we modernized, I would like to talk about some of the things that were really stopping us from delivering features faster at the pace that the business demanded.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/540.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=540)

Let us start with the first one. Booking.com is a monolith as it began.  It was written in Perl. When it started, it was a small, cohesive, and easy repository. It was easy to build, easy to test, and easy to deploy. But as features and teams multiplied, cross-dependencies grew, builds slowed down, tests ballooned, and the boundaries were blurred. Tight coupling made changes brittle. Deployments were becoming painful, and debugging became a nightmare. Overall, it culminated into a big ball of mud. Teams then faced a choice: keep patching a fragile monolith system or modernize by splitting it into services.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/580.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=580)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/590.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=590)

### The Technical and Human Cost: 90+ Dependencies, Domain Divergence, and Team Burnout

Before we look into how our team solved the challenges they faced, let us speak a bit about what specific problems they were facing.  Our legacy system had 90+ dependencies,  which included services as well as database tables. More than 80% of those dependencies were hard dependencies. That means if there was any failure, there was a cascading effect on other systems.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/630.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=630)

This oversized web of dependencies resulted in weak modularity that Luca was also referring to. Additionally, there was a leaky boundary in the data layer where the internal data model was exposed to other services, causing services to erode encapsulation. Since it was a tightly coupled Perl monolith,  developers added code wherever they wanted and wherever the data was available, which introduced coupling creep. There were also shared data models used across different services, and as a result of this excessive coupling and wide blast radius, a single outage would cascade and the modules could not degrade gracefully.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/660.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=660)

Another challenge we faced was domain divergence in reservation workflows and data structures.  Logic from different domains accumulated, including pricing, commission, customer service, messaging, and fraud, among others. The domain boundaries were blurred. For example, in our reservation workflow, commission was being calculated, which could have been done independently. In another example, the reservation system was calculating charges and then writing them into a database that was not even owned by us, so it was outside of the reservation domain.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/690.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=690)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/720.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=720)

Many of these problems gave rise to either shared ownership or no ownership at all.  The team frequently encountered problems in finding the right owners, and we found a lot of dead code that was left uncleaned. This made it difficult for the team to perform root cause analysis during outages and find the owners who knew what the code was doing. Our system was running on bare metal servers, which came with its own maintenance challenges.  It was bundled logic in one giant service, changes in rollouts were risky, and there was a lot of time spent on server maintenance, such as applying security patches, replacing faulty servers, or performing updates.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/760.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=760)

Regular manual capacity checks were required to ensure that servers were configured correctly and could handle the load. Though somewhat automated, this was taking a lot of time that we could easily use in building new features for our customers. These were not just technical problems; they had a profound impact on our team as well.  Teams spent more time dealing with issues related to ownership, which was not a pleasant topic. This led to burnout, low motivation in the team, and resulting disengagement. On the other hand, it took us four months to onboard a new team member, and even before that, it took us about fifty percent of the time to give them fifty percent knowledge of the domain.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/810.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=810)

There was not enough documentation, and the system was highly complex. This high complexity, unclear ownership, and mix of domains resulted in bugs and outages, which increased the KTLO of the team. All of this combined took significant energy from the team that could have been spent building new features. The team reached a point where they decided this could not continue anymore.  Around this time, we also received a big list of features that we needed to implement, and the team decided that the only way to sustain and be future-proof was to modernize our system.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/830.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=830)

### Designing the North Star Architecture: From Skepticism to Serverless Proof of Concept

We started looking into our future needs and capabilities required to ensure that we remain fit for the purpose.  Based on the team's learning from hands-on experience in the reservation domain, we listed down some problems that the team was experiencing, and then we prioritized from those lists a couple of things that we thought we really needed to work on. Some of those things were isolating unknown code, defining boundaries, removing coupling, and then with this prioritized list of things that we wanted to fix, we came up with the North Star architecture.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/860.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=860)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/900.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=900)

The North Star architecture, or our target architecture, was aimed at solving problems related to unclear boundaries, coupling of code, and reducing unnecessary dependencies.  We came up with a high-level architecture which was platform independent, so we did not think about moving it to the cloud at that time, but we designed an architecture that was modularizing our systems, and we were looking at different approaches for that. Once we had this architecture, it was reviewed by a lot of our internal architects, including cloud architects, as well as some principal developers.  During our discussions, there was an inclination that the future architecture should go through transformation phases instead of going from the first one, then stepping to the second one, and then stepping to the third one.

We should really move from the first one directly to AWS managed services and reap the benefits of the cloud. One of our requirements was to have isolated flows and modular code. When we looked at AWS, the Step Functions and Lambda offerings provided what we needed. Lambdas can break logic into smaller reusable functions, and Step Functions can orchestrate these reusable functions into clear isolated workflows. All of this while providing out-of-the-box functionality such as observability and monitoring.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/960.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=960)

Our overall inclination was to pioneer the use of serverless with a mission-critical workload. However, the problem was that our team had no prior experience with the cloud, let alone serverless. So it was not clear for the team how to proceed. Despite promises from leadership that support would be provided, the team was skeptical about serverless. 

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/980.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/990.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=990)

Some team members said let's not take the risk, it's too risky. Some didn't have an opinion about it, and some said let's go do it. Some of the assumptions they had were that it would take too long to learn AWS, Lambda functions would create performance bottlenecks, the cost would be too high, it would be challenging to connect to our internal Booking.com infrastructure, and why choose a mission-critical system for such an experiment.  

However, as an engineering manager, I made sure that I saw a big opportunity in this for learning for the team. I asked the team to do something different and make sure we get hands-on experience with AWS. It was also a great opportunity for our internal cloud teams to learn from a highly mission-critical workload and to see how this workload works on serverless. After a lot of team discussions, one-on-ones, and inspirational speeches, I had to write the first Lambda to inspire the team.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1030.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1030)

Following that, the team started working on a proof of concept. For the proof of concept, we chose a very simple use case to test a request coming from AWS Cloud to one of our services on-premises. The Lambda calls one of our internal dependencies and then gets some data. What we wanted to test was how it performs in terms of performance and what the cost would be with the volume of requests we were trying to create. 

When this worked, it boosted the team's confidence. One of the things that became clear was that it would be challenging to connect to the internal infrastructure of Booking.com. But we also saw very good output in terms of managed services. There were some things lacking in this proof of concept that we came to know later. For example, we were not able to test multi-region deployments or how it connects to other internal tooling like incident management and service-to-service integration. But at least we got the confidence that if we move forward, it's going to work.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1090.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1090)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1110.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1110)

With the learnings from the proof of concept, the choice was made. We are going to modernize using AWS serverless, and the proof of concept gave us the confidence that the selected solution is going to work for us.  The team decided to go ahead and create a serverless architecture. But before we look at the architecture, what were the key takeaways from the proof of concept? 

We saw that the team was able to focus more on the business logic because a lot of the operational stuff was handled by AWS. We noticed that our team can save five to ten percent of their time spending on managing servers, updates, and things like that. Integrating with other AWS services was seamless. For example, we connected with Step Functions later on. The team spent some time and formulated a final solution on serverless, which was reviewed and approved by our cloud architects.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1140.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1150.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1150)

### Building with Step Functions and Lambda: Architecture, Optimization, and Operational Challenges

Now I'm going to explain what the solution looks like.  On the left side you see Booking infrastructure. On the right side you also see Booking services, and in the middle is the new solution and the new reservation system that we built.  One of these Lambda functions triggers a Step Function you can see in the middle, and the Step Function internally had fifteen other Lambdas which were doing different things.

We also made use of DynamoDB to store the request temporarily. We put a time-to-live on that so that the database size is not increasing, but this made it easier for us. Through the Step Functions we were able to access data from DynamoDB from the request whenever it was needed. We would only select the part of the data that was required for us. Overall it created a very flexible, decoupled architecture. For example, we also see at the end that we have used SNS to fan out to different notifications to different SQS queues, and then there is another Lambda on the other side which is asynchronously reading from this queue. Basically, we were decoupling the system.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1210.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1210)

Overall, this was the architecture that we created.  I would like to highlight the Step Functions execution here. If you focus on the green blocks, the large block at the top represents a parallel execution of all these Lambda functions. After that execution completes, we call another Lambda on the left side, which further calls another parallel block. What is also important to see here are all these lines, which are catch blocks. This comes out of the box with Step Functions, so error handling is built in. This saves a lot of time because we don't have to maintain this code anymore.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1250.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1250)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1260.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1260)

It was not a smooth journey. We encountered many problems, but we wanted to solve them, and we showed resilience. They were systematically resolved.   One of the biggest problems we had was Lambda latency and cold starts. I am going to explain the chart that you see on top. This is a latency chart from an experiment we conducted with different configurations to find the most suitable one for our use case. On the left side, you see provisioned concurrency with 0.5 GB of memory. In the middle, you see provisioned concurrency with 1 GB of memory. On the right side, we used SnapStart because we were using Java. AWS offers SnapStart as a solution to warm up the Lambdas, and provisioned concurrency also offers the same opportunity. However, we found that the configuration in the middle was something we could rely on, so we chose provisioned concurrency with 1 GB of memory.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1340.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1340)

If you look at the graph below, you will see that the results were still not good enough for us. We started at 3,200 milliseconds, which was the latency we were getting in our Lambdas. However, you can see in the bottom section that it mentions SDK optimizations, Jakarta optimizations, and other optimizations. This is where we optimized the code to warm up the objects in Java so that the Java initialization time is reduced, achieving the same effect as if the Lambda was already warmed up. This reduced the latency to 140 milliseconds, which was very good. 

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1390.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1390)

When the team started working on the modernization project, we encountered another problem. We had only one environment to work on, which was our development environment, and we had six people in the team. We had to work with our cloud team to come up with a solution for creating personal environments within AWS. They worked with AWS and delivered us a solution. Before that, the team was time slicing, and there was a lot of communication overhead. Because we adopted this solution early on, there were some problems in it which created further challenges for the team to pursue, but over time the solution matured. We expanded from one team to three teams, and eventually 25 people were using this environment. Some of the internal booking tooling was not ready to set up the handling with AWS. 

There were some key features missing. For example, there was no multi-region deployment. We did not know how to do that, and there was no clear strategy on how to do canary deployments, rollbacks, hotfixes, and feature flags. All of these are essential for good operations. Our compliance team was also a bit resistant in approving our requests for the production environment because this was the first time we were doing this. They wanted to make sure that controls were in place. That took us a couple of months of back and forth explanations of how it works and what our use case is. We had to address all their concerns.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1430.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1430)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1460.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1460)

### Shadow Traffic Testing and Experimentation-Driven Go-Live: Achieving 100% Migration Success

At the end, we wanted to test what we built. We took a little over a year to build the architecture that we mentioned.  Most of the time was spent on understanding the dependencies. We had some capacity constraints, and there were some internal alignments, so it was not the development that took the whole time. Once we had the basic use case developed, we started thinking about how to test this and how we could make sure that this solution would work in production. We decided to go with the shadow traffic approach. 

Shadow traffic is an approach in which you can find gaps in functionality when you modernize a system. As you can see in the diagram, essentially we create a copy of the request coming to production and forward it to the new service and compare the outcomes. We followed a similar approach. There was a duplicate request created which went to our modernized backend, and at the end, the output from the legacy and modernized backend was compared by our data quality checker. We created a script for that, and the script generated an output report which mentioned the mismatches. Based on that information, we were able to find where there was a gap between the two systems.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1500.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1500)

There were so many upsides to this approach. 

The shadow traffic approach allowed us to do iterative development. It helped us to test new features with confidence, giving us the confidence to move forward. During this execution, we identified data quality gaps, and we were also able to performance tune our system because we ran 100% shadow traffic on our new system. We scaled from 1 to 100%, and we were able to see the whole load, how it was performing, and how much cost it would take. We kept it running for a month to understand all the problems that could occur with 100% shadow traffic. We also got a good idea about the cost and were able to forecast future costs.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1550.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1550)

As we went live, we used an experimentation approach as our go-live strategy. This is something very common in Booking.  Essentially, we passed a percentage of traffic to the new system and observed how it differed between base and variant. We wanted to make sure that company metrics were not impacted and the business was not affected, so we took an experimentation approach because it helps us de-risk the high-stake migration, explore real-world problems earlier, and build confidence with stakeholders.

Since it was a mission-critical system, we used experimentation and scaled the traffic in stages. You can see here 1, 10, 30, 45, 90, 100. These were some of the stages that we created. The question was how to divide this traffic in the first place. We worked with our data science team and reviewed past data to see how we could divide the traffic into different payment timings, and this is what we used as a criteria. Of course, in your use case, you will have to find something else based on your own use case.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1610.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1610)

Did we get what we wanted to achieve at the end? Was it all worth the effort?  First of all, we unlocked a lot of tech capabilities for the whole organization. For example, we were able to communicate from AWS to on-premises services. Lambdas were able to write to Kafka topics. We also integrated with our internal monitoring, observability tooling, and incident management tooling, which was not available before. We also had Vault integration for service-to-service authentication, especially when we were connecting with on-premises services.

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1640.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1640)

Some of the benefits that we saw were reduced onboarding time.  New developers were getting productive very fast. We saw that new developers were now producing their first merge request within their first month, which used to take four months in the past. There were plenty of resources available and a lot of documentation, so it was very standard how AWS works, which helped us onboard people faster. It improved our developer experience. There was faster debugging due to all the tools that are available out of the box, for example, CloudWatch, metrics, X-Ray tracing, and Step Function execution workflows.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1670.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1670)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1690.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1690)

It was easier for us to perform root cause analysis very fast within a few hours and produce first fixed merge requests for any bugs that we found.  All of this improved the developer experience and helped us respond to incidents faster. It also improved the time to market.  This is the main metric that we wanted to hit. The agility in the architecture provides the ability to move things around quickly. In one particular example, we had to move the whole parallel block of Lambdas from asynchronous to synchronous, and that took us just two weeks. It was not a big deal for us because the system was very flexible and it was easier to move things around.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1730.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1730)

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1740.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1740)

We see amazing results when it comes to improvement in terms of reduced time to market.  I would like to ask, how many of you have booked a trip to Las Vegas using Booking.com?  Your reservation was 100% through Booking's modernized system, which is based on AWS Serverless. The team recently achieved this milestone of 100% traffic on the new system and thereby decommissioned the old reservation creation endpoints. This means all new reservations are going through this new system. This milestone paved the way for other teams to also use our platform and deliver features faster. We also have modification and cancellation use cases, and they are delivering their use cases very fast on the same platform.

To wrap it up, I have shared our modernized journey and what we set to achieve and what our targets were. I hope it gave you some insights into how AWS services can be used for transformation, and that you will explore what fits your needs and what your use cases are. I will leave you with a quote at the end and invite Sarah to share more about service patterns based on broader industry experience.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1820.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1820)

### Modernization Patterns and Service Decomposition: From Strangler Fig to Team-Based Boundaries

Thank you, Jason. We just heard this inspiring story about Booking.com's Accommodation Reservation Backend, or ARB, and how they went through a transformative journey to modernize their legacy on-premises system into a fully serverless architecture. They took the bold choice to make this leap, but in fact, this is not the only way AWS customers can migrate and modernize. 

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1840.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1850.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1860.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1860)

At AWS, we have identified what we call the seven Rs of migration pathways.  We have retain, retire, relocate, and re-host.  We also have re-platform, repurchase, and refactor. Refactor is the main topic of today's discussion, as you can imagine.  Refactoring essentially means taking the leap from migrating from on-premises to AWS and immediately adopting serverless technologies to unlock the benefits of cloud-native capabilities. What is important to understand is that there is no one-size-fits-all approach. Companies around the world of different sizes need to choose what fits their use case and what makes the most sense for their company's requirements.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/1900.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=1900)

The slide I just showed you speaks about technology, but when we think about software modernization and modernization of our architecture, it is not only about technology. It is about technology, processes, and people. Think about it as a triangle. If any of the sides of this triangle are weak, it has an impact on the other aspects and pillars. It might be that in your company's case, not all three pillars are perfect from the beginning. 

When I say people, processes, and technology, what I mean is that you need multidisciplinary teams that have a deep understanding of the technical and business context in which your company operated on-premises and will operate in the cloud. You need processes that empower people to make fast decisions and enable iterative progress. Lastly, you need the technology that best fits your specific use cases. There is no one-size-fits-all solution, and the successes or issues you have in one pillar may have a ripple effect on all the others.

Let me walk you through a practical example. Customers sometimes face the choice between adopting a monorepo for their system or codebase versus multiple repositories. On the surface, this sounds like a technical decision, but let us look at it more deeply. When you decide to adopt a monorepo, this comes with technical consequences. You may have less availability of tooling out of the box to manage this large codebase. Practically, this means you have to build your own specific tooling to manage the codebase, maintain it, and build it from scratch rather than using something readily available.

From a processes perspective, because you have a monorepo, it might mean that you can adopt trunk-based development, which informs how your processes work. From the people perspective, because you have a monorepo, this can come with the consequence that you have frequent or higher touchpoints of merge requests across teams. This is the monorepo choice, but if we go with multiple repositories, from a technical standpoint, we have more tooling out of the box to manage these different codebases more independently. From a processes standpoint, this means you may need to enforce strict API contracts to communicate between the systems. From a people perspective, because these teams operate more independently in different codebases, this comes with the consequence that they do not have very frequent opportunities to collaborate.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2130.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2130)

As you can see, there is not a one-size-fits-all approach, and it is about choosing what fits for your use case. Keeping this triangle as our guiding principle to modernize our  application, how do we transform a highly coupled monolith into a loosely coupled collection of several services? There are different ways to achieve this.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2150.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2150)

Let's start with a very famous one: the strangler fig pattern . This pattern takes its name from the use case of this plant that slowly replaces its host tree. As the name suggests, this is about incremental modernization. Let's take an example of an e-commerce organization that wants to move away from their monolith and migrate to AWS. How would they do this? Let's look at the first phase. In the monolith, they have different core functionalities: product management, order management, and invoicing. At the beginning, they are all on-premises.

In the next phase of the strangler fig pattern, the company identifies a use case for modernizationâ€”in this case, invoicing. In this coexistence phase, what happens is that you have a routing layer that enables the legacy system and the new modernized system to coexist. Once the invoicing components are modernized and moved to the cloud, the traffic is routed to these two coexisting systems. Once the first phase is successful, you incrementally move all the others until you have all of them in the new modernized applications.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2290.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2290)

The next pattern we typically identify is branch by abstraction. This is particularly valuable when modernizing shared capabilities. This can sound similar to the pattern we just discussed, but the first one was more about routing and coexisting on two different systems. This is about abstracting the complexity of having two implementations at the same time. In this case, we have the e-commerce company with product management, order management, and invoicing management functionality. We identified that there is a common functionality  that all of them rely on: the notification functionality.

Think about when a customer buys somethingâ€”there is an order email summary that tells the customer about the order. It can be about different things: it can be about invoicing, and it can also tell the customer about new products that have been added to the catalog. This is a shared functionality across all of these services. What this pattern does is hide the complexity of having two implementations at the same time.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2330.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2330)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2360.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2360)

Then we have the decomposed by business capability pattern . In this case, the capability could be the order service or order management functionality, and you decide to modernize with your architectural boundaries informed by your business functionalities and capabilities. Your business capabilities become the map of your modernization. Then we have decomposed by subdomain , which could sound similar to the other one, but it is slightly different.

Think about the order notification. We have an order service that publishes an event, and this event triggers notifications in your internal banking systems related to different things. When you decide to modernize your architecture, you realize that the business functionality actually varies depending on where your order is being processed. In the case of orders that happen in the US, you have orders that need to have different business logic depending on the state. California may have different regulations.

And there are things that need to be done differently based on privacy notices, customer data, and so on. In Europe, for instance, you have different business logic because you have GDPR compliance and other compliances related to Europe. During your modernization process, you notice that this is quite different. What this means is that you decide as a company that your subdomain will inform the architectural boundaries of your system essentially.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2480.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2480)

In this case, you have an AWS Lambda order service that publishes an event, and then you have Amazon SNS that propagates this event to Lambda based in the US, which has different business logic. On the other side, you have a similar architecture for customers based in Europe. Then you can encapsulate this logic and easily change this logic without impacting the one from US customers or the other way around. 

Then we have decompose by transactions. Think about your order functionality again. As you try to modernize, you realize that the user experience when a customer tries to create an order is slightly different from the user experience of a customer trying to cancel an order. As an architect or builder, you realize that when customers are trying to create an order, they want and need immediate feedback. It should be almost instant. However, when a customer decides to cancel an order, that triggers a chain of actions that take more time, and the customer does not need to know right away what the outcome of that orchestration is. They just need to know that the order has been submitted.

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2590.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2590)

This different user experience informs your architectural boundaries. By decoupling and implementing different architecture based on the transactionâ€”create or cancelâ€”you can now handle the two things differently. In the first one on top, create order, you can have Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The user creates the order, you store the data in DynamoDB, and you get the response very quickly. When you want to cancel the order, you have to process a lot of data and talk with different banking systems as well as potentially other third-party systems. That will take some time. So in this case, we need an API Gateway as well as AWS Step Functions to manage the orchestration of this. 

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2630.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2630)

Then we have team-based service decomposition. Remember when I talked about the three pillarsâ€”people, processes, and technology. This is an interesting pattern because in this case, the people inform your architectural boundaries. You have one team that is familiar with and owns the order codebase and order systems, then you have a product team that manages the product capability as well as the codebase, and an invoicing team and payment team. This is an interesting way in which you can also decompose your monolith. 

### Rethinking Communication Patterns: Mixing Synchronous and Asynchronous for Better User Experience

We have explored so far the seminars. We have seen how the strangler pattern or the branch by extraction pattern can help you decompose the monolith, but there is also another crucial aspect of modernization that deserves our attention, which is how our services communicate with each other. Think about as we break our monolithâ€”how can we rethink the way that our components talk to each other in a way that improves our customer experience as well as how our teams communicate with each other. Remember, the triangle is about processes, people, and technology.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2670.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2670)

One of the key aspects about adopting serverless is that you can challenge your own assumptions about user experience and how your system should communicate and how teams could communicate to improve your overall customer satisfaction and time to market and so on.  Let's go back to the example of having an e-commerce organization. Let's say that your product team comes up with an idea that you want to implement a new gift code functionality.

Your teams need to implement a gift code service. This gift code needs to communicate with other third-party gift code systems within other SaaS vendors. The gift code needs to be attached to internal systems, for instance, your user account needs to be attached to be redeemed. To validate this gift code in real time, you need to have a CRM system, an external system that validates this gift code.

In a synchronous world, consider what could happen during the holiday season or high season. From the customer perspective, a customer goes to your e-commerce website and tries to redeem a gift code. The process of validating and redeeming the gift code is usually synchronous, but this is a problem during high season or peak periods because your CRM might be an external CRM that you have no control over. It might be experiencing difficulty, so the response time is longer, which has an impact in a synchronous world on your own system as well as on the user experience of your customer. That is not great. How can we rethink this? How can we leverage AWS serverless services to rethink how this could be architected?

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2810.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2810)

This is where serverless shines with asynchronous patterns.  When a gift code is entered, we immediately save it to Amazon DynamoDB. DynamoDB streams then trigger EventBridge Pipes, which handles the event propagation. The system needs to validate the code across multiple external systems, our loyalty program provider to check point balances, fraud detection, and so on. We need to verify legitimacy. After that, we can rely on EventBridge Pipes to transform the payload to make it compliant to the CRM format that the CRM needs.

If the code is invalid, we can notify the customer through the existing notification service. The customer, in the meantime, which is very important, can enjoy the instant response and continue shopping because at the beginning we have the synchronous feedback loop. In this case, and this is crucial, we are not saying that everything should be asynchronous. This is very important. The beauty of serverless is that we can mix these patterns. We use EventBridge Pipes in this case for asynchronous gift code validation and enrichment during the shopping part, but then we switch to synchronous API calls during the checkout part. We only use synchronous calls when we absolutely need that immediate conversation.

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2930.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2930)

This is not only about implementation. This is about rethinking our assumptions about user experience and system integration. Sometimes the best solution is letting go of immediate consistency in favor of eventual consistency when the business context allows it. Lastly, pipes enrich the event with user data from our internal user management service.  Going through the flow again, we have Amazon API Gateway and AWS Lambda with DynamoDB that handle the synchronous part, improving the customer experience. Then we have event propagation to EventBridge that handles the asynchronous experience. This allows us to rethink how our system works, how the customer experience should be, how our teams should cooperate with each other, what they own, and how we can modularize these architectures so we enable faster development and better experience.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/2970.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=2970)

As we wrap up today's discussion about modernization,  let's recap what we have discussed so far. Distributed systems are living systems, which means they are not static or immutable. They evolve and grow because your company and organization evolve and grow. Your processes change, and your people evolve as well. Through the seminars and through the inspiring story that was shared about accommodation reservation, we can think beyond lift and shift. We can think about serverless not only as a technology but also as a key strategic decision.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/3070.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=3070)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a8c631451972a660/3080.jpg)](https://www.youtube.com/watch?v=DF8sK2xkhgQ&t=3080)

While decomposing the monolith and the patterns, we saw what the patterns are, for instance the strangler fig pattern, and what we can decompose and modernize. Think about decomposing by business capabilities. At the end, we have understood how we can unlock asynchronous communication in our systems and challenge our own assumptions by using AWS services like DynamoDB, EventBridge, Lambda, API Gateway, and so on. Lastly, I want to conclude this session with something really important, and I want you to bring it back home with you.  Grow, not build software. Software is non-immutable. Software evolves. 


----

; This article is entirely auto-generated using Amazon Bedrock.
