---
title: 'AWS re:Invent 2025 - Keynote with CEO Matt Garman'
published: true
description: 'In this video, AWS CEO Matt Garman announces AWS has grown to a $132 billion business with 20% year-over-year growth. He unveils major AI infrastructure advances including P6e-GB300 instances with NVIDIA GB300, AWS AI Factories for dedicated on-premises AI infrastructure, and Trainium3 UltraServers delivering 4.4x more compute. Key launches include Amazon Nova 2 model family (Lite, Pro, Sonic, Omni), Nova Forge for open training models creating custom "Novellas," and Amazon Bedrock AgentCore enhancements with Policy and Evaluations. Three frontier agents debut: Kiro autonomous agent for software development, AWS Security Agent for proactive security testing, and AWS DevOps Agent for incident resolution. Additional announcements include AWS Transform Custom for code modernization, Amazon QuickSight enterprise AI assistant, and 25 core service updates spanning compute (new X, C8a, M8azn instances), storage (S3 50TB object limit, S3 Vectors GA, S3 Tables Intelligent-Tiering), databases (RDS 256TB storage, Database Savings Plans), and security (enhanced Security Hub, unified CloudWatch data store). Sony and Adobe executives share their AWS transformation stories, demonstrating real-world AI agent implementations.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/30.jpg'
series: ''
canonical_url: null
id: 3088175
date: '2025-12-06T06:06:18Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Keynote with CEO Matt Garman**

> In this video, AWS CEO Matt Garman announces AWS has grown to a $132 billion business with 20% year-over-year growth. He unveils major AI infrastructure advances including P6e-GB300 instances with NVIDIA GB300, AWS AI Factories for dedicated on-premises AI infrastructure, and Trainium3 UltraServers delivering 4.4x more compute. Key launches include Amazon Nova 2 model family (Lite, Pro, Sonic, Omni), Nova Forge for open training models creating custom "Novellas," and Amazon Bedrock AgentCore enhancements with Policy and Evaluations. Three frontier agents debut: Kiro autonomous agent for software development, AWS Security Agent for proactive security testing, and AWS DevOps Agent for incident resolution. Additional announcements include AWS Transform Custom for code modernization, Amazon QuickSight enterprise AI assistant, and 25 core service updates spanning compute (new X, C8a, M8azn instances), storage (S3 50TB object limit, S3 Vectors GA, S3 Tables Intelligent-Tiering), databases (RDS 256TB storage, Database Savings Plans), and security (enhanced Security Hub, unified CloudWatch data store). Sony and Adobe executives share their AWS transformation stories, demonstrating real-world AI agent implementations.

{% youtube https://www.youtube.com/watch?v=q3Sb9PemsSo %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part












### Welcome to re:Invent 2024: AWS Reaches $132 Billion with Accelerating Growth

            Please welcome the CEO of AWS, Matt Garman.



[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/190.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=190)

 Welcome, everyone, to the 14th Annual re:Invent. It's so awesome to be here.  We have over 60,000 people here with us in person and almost 2 million watching online, including a bunch of you that are joining us from Fortnite out there.  It's where we're streaming the keynote for the first time. Welcome to everybody, and thank you all for joining us.


It is incredible to feel the energy as you walk through the halls here in Las Vegas, and it matches a lot of what I've been seeing recently as I've been talking to you in recent months. It has been an unbelievable year. AWS has grown to be a $132 billion business, accelerating 20% year over year. I want to put this a little bit in perspective. The amount we grew in the last year alone,  it's about $22 billion. That absolute growth over the last 12 months is larger than the annual revenue of more than half of the Fortune 500, and that growth is coming from across the business.


S3 continues to grow, with customers storing more than 500 trillion objects, hundreds of exabytes of data, and every day averaging over 200 million requests a second. For the third year in a row, more than half of the CPU capacity that we've added to the AWS Cloud comes from Graviton.  We have millions of customers using our database services, and Amazon Bedrock is now powering AI inference for more than 100,000 companies around the world.



### Major Milestones: From Bedrock AgentCore to Ocelot Quantum Chip and Expanding Global Infrastructure

 This year, we gave you the first building blocks for deploying and operating highly capable agents securely at enterprise scale with Bedrock AgentCore, and we're seeing incredibly strong momentum from AgentCore. In fact, just a few months since launch, the AgentCore SDK has already been downloaded more than 2 million times. And we announced Ocelot,  our first quantum computing chip prototype. Ocelot is a breakthrough in making quantum a reality, reducing the costs of implementing quantum error correction by more than 90%.


Now, all of this starts with a foundation of secure, available, and resilient planet-scale infrastructure  that is frankly unmatched anywhere. AWS has by far the largest and most broadly deployed AI cloud infrastructure anywhere in the world.


[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/330.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=330)

 Our global network of data centers spans 38 regions, 120 availability zones, and we've already announced plans for three more regions. In fact, in the last year alone, we've added 3.8 gigawatts of data center capacity, more than anyone in the world. And we have the world's largest private network, which has increased 50% over the last 12 months  to now be more than 9 million kilometers of terrestrial and subsea cable. That's enough optical cabling to reach from the Earth to the moon and back over 11 times.


### Customer Success and Startup Innovation: Building the Future on AWS

 But at Amazon, everything starts with the customer. So I want to start off by thanking all of you. Today, we have millions of customers running every imaginable use case, the largest enterprises in the world across every single industry and vertical running their businesses on us. You've transformed industries like financial services, healthcare, media entertainment, telecommunication, even government agencies all around the world.


And as you all know, at AWS, security is priority one.  For us, everything is built on that foundation. This is the reason why the U.S. Intelligence Community has chosen AWS as its cloud of choice for more than a decade. It's why companies like Nasdaq have moved their trading markets to AWS and why Pfizer chose AWS as their core for their digital transformation.


[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/400.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=400)

And since the beginning, we've known  how important partners would be to our customer success. That's why we're so proud to have such a massive network of partners. Many of them are here with us this week.  Thank you to all of our partners, SaaS providers, system integrators, and solution providers, serving our massive set of customers all around the world. We couldn't do it without you.



[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/430.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=430)

 And while I personally appreciate all of our customers, I will tell you that I have a special affinity for our startup customers. More unicorn startups have been built on AWS  than anywhere else, and it isn't even close. Thank you to all of you, the innovators out there. More than ever, every startup, and AI startups in particular,  are flocking to AWS. 85% of the Forbes 2025 AI 50, 85% of the CNBC Disruptor 50 are all running on AWS. It's incredibly amazing, and I'm personally amazed at what these founders are inventing, and I thought you all might like to hear from some of them today.



### AudioShake: Transforming Sound Separation with AWS Infrastructure

 Let's hear from the first, AudioShake, who was the winner of last year's re:Invent Unicorn Tank pitch competition. Let's take a rainforest. Or a playground. Or no, better yet, three musicians on a street corner. Okay, what if we could just isolate the music?  Now, the car driving by. Or just the conversation going on between the people in the background.


[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/490.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=490)

At AudioShake, we separate sound  so that humans and machines can access it, make sense of it, and understand it in all kinds of new ways. Our multi-speaker separator,  which is the world's first high-resolution separator of speakers into different streams. So it could be isolating individual voices in environments like call centers.


 We're also used across media and entertainment. But if we think about hearing and speaking impairments, there's a lot that sound separation can do to help. We work with some nonprofits that work in the ALS space where they're using old recordings of their patients, separating the voices so that then it can be cloned and the patient can speak with their original voice before the voice started to degrade.

When we first started, we were a three-person team. Having the infrastructure to actually get our models in the hands of real customers is something that we couldn't have done without AWS. We run our entire production pipeline on AWS, so everything from inference and storage through to job orchestration and all of production.


We are moving into a world where sound should be a lot more customizable than it is today. Eventually, sound separation should be able to help people who have hearing challenges to hear the way they want to hear, while also simultaneously going deeper into helping machines make sense of the real world.  Very cool. And thank you to the AudioShake team for sharing.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/580.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=580)

### Empowering Developers: The Freedom to Invent Since Day One

Now, none of what we do at AWS happens without builders, and specifically developers. AWS has always been passionate about developers, and this conference is,  and frankly it always has been, a learning conference. It's a little bit different, and it's dedicated to all of you out there. Thank you to every developer out there and the millions of additional AWS developers all around the world.

A special call out to our AWS Heroes over here. I see you. Thank you so much. Awesome. And thank you also to the million-plus members of our user group community in 129 countries all around the world.




 So why do we do this? What motivates us? Why are we just as passionate today as we were 20 years ago when we first launched AWS? What drives us every day is giving you all the freedom to invent. This has been our motivation since the very beginning when we started AWS. We wanted to make it possible for every developer or inventor in her dorm room or garage to access the technology infrastructure and capabilities so that they could build whatever they could imagine.


 20 years ago, it just wasn't possible for developers or builders to get the servers or compute capacity that they needed without investing significant capital and time. Developers were spending way too much of their time procuring servers, managing infrastructure, and not enough of that time building. We actually felt this ourselves inside of Amazon. We had a company full of builders who had these incredible ideas of how they could make our customers' lives better, but they couldn't move as fast as they wanted.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/670.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=670)


[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/690.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=690)

 So we asked ourselves, why not? Why couldn't developers focus on building instead of on infrastructure?  Why couldn't we bring the time and the cost of experimentation down to zero?  Why not make every idea possible? And we've spent the last two decades innovating towards those goals.


### The AI Inflection Point: From Technical Wonder to Real Business Value Through Agents

Giving all of you the freedom to keep inventing is why we're here today. And right now, we're witnessing an explosion of invention with AI. Every single customer experience, every single company, frankly every single industry is in the process right now of being reinvented, and we're still in the early days  of what AI is going to deliver, and the technology is iterating faster than anything any of us have ever witnessed before. It wasn't that long ago that we were all testing and experimenting with chatbots, and now it seems like there's something new every day.


But when I speak to customers and many of you out there, you haven't yet seen the returns that match up to the promise of AI. The true value of AI has not yet been unlocked, but a lot of that is changing fast too.  AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf. This is where we're starting to see material business returns from your AI investments.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/760.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=760)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/770.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=770)

I believe that the advent of AI agents has brought us to an inflection point in AI's trajectory. It's turning from a technical wonder  into something that delivers us real value. This change is going to have as much impact on your business as the internet or the cloud. I believe that in the future, there's going to be  billions of agents inside of every company and across every imaginable field.


Already, we see agents accelerating healthcare discoveries, improving customer service,  making payroll processing more efficient, and agents are also starting to scale people's impact up by 10x in some cases so they have more time to invent more. Wouldn't it be awesome if everyone could see that level of impact? We think so, and that's why we ask the question, why not?


Getting to a future of billions of agents  where every organization is getting real-world value and results from AI is going to require us to push the limits of what's possible with the infrastructure. We're going to have to invent new building blocks for agentic systems and applications. We want to reimagine every single process and the way that all of us work. At AWS, we've been innovating at all of the layers of the stack to give you all the freedom to invent what's next. We have a lot to share. Let's get started.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/830.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=830)

### Best-in-Class GPU Infrastructure: AWS and NVIDIA's 15-Year Partnership

First off, what are the components that you need  to deliver agents that are going to truly deliver value for you? It starts with having the most scalable and powerful AI infrastructure to power everything. You have to have a highly scalable and secure cloud that delivers the absolute best performance for your AI workloads, and you're going to want it at the lowest possible cost across your model training and customization and inference.

Now, that's quite easy to say, but to deliver that requires optimizing across every single layer of hardware and software, and that is something that only AWS does. It turns out there are no shortcuts. Now, when you think about AI infrastructure, one of the first things that comes to mind is GPUs, and AWS is by far the best place to run NVIDIA GPUs.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/880.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=880)

We were actually the first to offer NVIDIA GPUs in the cloud  and we've been collaborating together with NVIDIA for over 15 years. And what that means is that we've learned to operate GPUs at scale. In fact, if you talk to anyone who's run large GPU clusters at any other provider, they'll tell you that AWS is by far the most stable at running a GPU cluster. We're much better at avoiding node failures and definitely deliver the best reliability. And the reason for that is because we sweat the details, minor things like debugging BIOS to prevent GPU reboots.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/920.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=920)

If you go to other places, they just kind of accept that as how it works, and you go on about your business. Not us. We'll investigate and root-cause every single one of them. Then we collaborate with our partners at NVIDIA  to make sure that we're making constant improvements. Nothing is too small for us to be focused on. Those details really matter, and it's why we lead the industry in GPU reliability. It takes hard work and real engineering to make that happen.


[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/960.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=960)

### Introducing P6e-GB300 and AWS AI Factories for Dedicated AI Infrastructure

We improve on those new dimensions with every generation. This year, we launched our P6 generation of EC2 instances  featuring the NVIDIA Blackwell processor, the P6e GB200 UltraServer, which provides over 20x the compute compared to our previous P5en generation. These are ideal for customers that are working with really large AI models out there. We're doing that again today. I'm excited to announce the new P6e-GB300.  These are powered by NVIDIA's latest GB300 NVL72 systems, and we continue to bring you the best-in-class compute for the most demanding AI workloads.


Our full stack approach to hardware and software plus operational rigor delivers you the absolute best performance and reliability for the biggest organizations in the world. This includes NVIDIA themselves, by the way, who run their large-scale generative AI cluster project, Ceiba, on AWS, and many others like OpenAI who are actively running on AWS today.  They're using clusters of EC2 UltraServers with hundreds of thousands of chips, today GB200s and soon GB300s, and they have the ability to scale to more than tens of millions of CPUs to manage their agentic workflows. All of this is to support their ChatGPT application, which I'm sure many of you use, as well as the training of their next-generation models.



Or take Humain.  Humain is Saudi Arabia's newly created company that's responsible for driving AI innovation in the region. We recently announced a partnership together on this groundbreaking AI zone for the Kingdom of Saudi Arabia. This partnership will bring customers high-performing infrastructure, models, AI services like SageMaker and Bedrock, all while helping meet the kingdom's standards for security, privacy, and responsible AI standards. Now, this type of work has sparked some interest in others, large government organizations in the public sector who are interested in a similar concept. We sat back and asked ourselves, "Could we deliver this type of AI zone to a broader set of customers,  maybe even something that could leverage customers' existing data centers?"

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1090.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1090)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1120.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1120)

That's why today, we're excited to announce AWS AI Factories. With this launch, we're enabling customers to deploy dedicated AI infrastructure from AWS in their own data centers for exclusive use for them. Effectively, AWS AI Factories operate like a private AWS region, letting customers leverage their own data center space  and power capacity that they've already acquired. We also give them access to leading AWS AI infrastructure and services, including the very latest in training of UltraServers or NVIDIA GPUs and access to services like SageMaker and Bedrock. The AI Factories operate exclusively for each customer, and it helps with that separation, maintaining the security and reliability that you get from AWS while also meeting stringent compliance and sovereignty requirements. We're super excited to see what these AI Factories unlock for customers. 




### AWS Trainium: Purpose-Built AI Chips Powering Over 1 Million Deployments

Now, at AWS, we have always been about choice, and if you want to have the absolute best in AI infrastructure, you need to have the best compute  for AI training and inference. AWS is by far leading the way with the broadest set of options, including our groundbreaking purpose-built AI processors.  AWS Trainium is our custom AI chip designed to offer the best price performance for AI workloads. Now, customers love Trainium for what it achieves for the training workloads,  but I'm going to have to pause and be a little bit vocally self-critical here. People often give us a little bit of a hard time about product naming in AWS. It's true.



Well, it turns out Trainium is no exception. We named it Trainium because it's designed to be an awesome chip for AI training.  And it is, but as it turns out, Trainium2 is actually the best system in the world currently for inference. Customers often ask me, "How can I best take advantage of the benefits of Trainium?" And what I tell them is, "You probably are already using it and you just didn't know it." In fact, if you look at all the inference that's running in Amazon Bedrock today,  the majority is actually powered by Trainium already, and the performance advantage of Trainium is really noticeable. If you're using any of Claude's latest generation models in Bedrock, all of that traffic is running on Trainium, which is delivering the best end-to-end response times compared to any other major provider.


And that's part of the reason  why we've deployed over 1 million Trainium chips already to date. Now, we've gotten to a million chips in record speed, and that's because we control the whole stack. We can optimize end to end how we roll it out, and it allows us to move even faster.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1230.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1230)


In fact, we've been able to ramp the volumes of Trainium2 in our data center  four times faster than the next fastest AI chip we've ever ramped, and we're selling those as fast as we can make them. Trainium already represents a multi-billion dollar business today  and continues to grow really rapidly. Now, what does it look like when all of this comes together in a system purpose-built around Trainium?

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1250.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1250)

 It wasn't that long ago that people had this saying where they said that the data center was the new computer. Well, when you're training this next generation of models, it turns out that the data center campus is the new computer. One of the best models in the world today is Anthropic's Claude. We wanted to give you a little bit of a look behind the scenes at how a model like this is born, made possible by Trainium and Project Rainier. Let's take a look.








[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1350.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1350)

### Trainium3 and Trainium4: Pushing the Boundaries of AI Training and Inference Performance

      Really cool to see the massive scale  that we've gotten to with Trainium so quickly. So what's next? Last year, we announced that we were already hard at work on our next chip, Trainium3, designed to make AI workloads  better, faster, and more cost effective.


[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1370.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1370)

Today, I'm excited to announce that Trainium3 UltraServers are now generally available.  Now, these UltraServers are our most advanced, containing the very first three-nanometer AI chip in the AWS Cloud.  Trainium3 offers the industry's best price performance for large-scale AI training and inference. Now, we've talked about the incredible results that we've seen with Trainium2 this past year, but Trainium3 UltraServers bring another huge leap forward: 4.4 times more compute, 3.9 times the memory bandwidth, and this one is super important, five times more AI tokens per megawatt of power.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1410.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1410)



And as a special surprise, I have a rack of our UltraServers on stage with me today. Our largest Trn3 UltraServers  combine 144 total Trainium3 chips acting together in a single scale-up domain connected by custom neuron switches.  This delivers a massive 362 FP8 petaflops of compute, over 700 terabytes per second of aggregate bandwidth,  and all in a single compute instance.



And our custom-built EFA networks support scaling these out to clusters of hundreds of thousands of chips. No one else can deliver this for you.  It requires all of these system-level pieces to be co-designed together. It requires multiple types of custom silicon. It requires scale-up and scale-out networking.  It requires a detailed and integrated software stack, and of course, the industry's leading data centers.


In a real-world example of how performance improves,  we ran through a number of open-source models, open-weights models, that we've been optimized to run on Trainium2, and we wanted to see how they'd run on Trainium3. As one example, here's an inference benchmark for a popular open-source GPT-OSS-120B model from OpenAI, and we ran it on both Trainium2 and Trainium3.


 As you see here, with Trn3, we get remarkable efficiency gains over Trainium2. You see over five times higher output tokens per megawatt, all while maintaining the same latency per user, what we call interactivity in this chart here. And this is just one example. We see similar results as we run this across a number of different models, which is fantastic. We're excited to see what Trn3 is going to unlock for customers, but we're also not stopping there.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1510.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1510)

I wanted to give you a bit of a sneak peek as to what's coming around the corner.  And that's why I'm excited to announce that we're already hard at work on Trainium4. We're well into designing it, and we're excited about what we're seeing already.


Trainium4 is going to bring massive leaps across every single dimension. Compared to Trainium3, Trainium4 will deliver  six times the FP64 compute performance, four times more memory bandwidth, and two times more high memory bandwidth capacity to support the very largest models that you have in the world. Trainium continues to push the bounds of what's possible with AI infrastructure, so you all can be freed to push the bounds of your industry. Let's hear from a startup that's using AWS's massive AI infrastructure to transform computational biology.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1570.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1570)



### LILA: Building Scientific Superintelligence with AWS Computing Power

We're trying to create a kind of beautiful mind for science that can be a polymath across fields, material science, chemistry, and life, but the internet and prior data only take you so far.  You have to be capable of testing things in the real world. LILA is building the first of what we call AI science factories, which are an infrastructure through which AI can autonomously propose hypotheses,  design experiments, and then run those experiments in the real world with all of the results of the successes and failures flowing into our models,  and become super intelligent within them by running the scientific method itself.


It won't surprise anybody that building scientific superintelligence requires a lot of computing. LILA is at trillions of tokens of scientific reasoning today. We expect that to go up by no less than 100x over the next few years. AWS is an incredible partner because as the scale and speed and intelligence of science goes up, the scale and speed of computing and the security of that process  is going to be more important than ever. AWS is the best in the world at that combination.




What this means for humankind  is that building a very broad, new kind of scientific mind and infrastructure that can scalably set that mind in motion  to find cures, new energy technologies, new materials, and more, that we can collectively pull a better future into the present. 


[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1670.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1670)

### Amazon Bedrock: The Comprehensive Platform for Production-Ready Generative AI

Really amazing to see what this scale of compute is enabling for customers like LILA to be able to accomplish. It's just incredible invention  that's happening at the infrastructure layer. But we also know that infrastructure is just a part of the story. We're seeing nearly every single application in the world being reinvented by AI,  and we're moving to a future where inference is such an integral part of every single application that everyone builds.


Now, to be successful in that future, you need a secure, scalable, feature-rich inference platform that you all can build on, and that's why we developed Amazon Bedrock. Bedrock is a comprehensive platform that helps you fast-track your generative AI applications as you move from prototype into production. With Bedrock, you get a broad choice of all the latest models. You have the ability to customize these models  for your individual use case and your performance needs. You get the tools to integrate them into your data and the capabilities to add guardrails as you need them. All of this with the security and integrations that make it easy to build on applications and data that you already have in AWS.


[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1730.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1730)

And companies of every size and in every industry all around the world are using Bedrock.  Customers like BMW, GoDaddy, Strava, those are just a few, with more than twice as many customers building on Bedrock compared to just this time last year.  Bedrock is seeing unprecedented momentum, but it's not just the number of customers that are using it, it's actually the volume of the usage that's quite astounding.


[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1750.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1750)

In fact, some customers are processing  a huge number of requests through Bedrock. Today, some of the largest-scale AI applications in the world all run on this platform. I actually asked the team to check for me,  and we now have over 50 customers that have processed more than 1 trillion tokens each through Bedrock. Incredible scale and momentum.


### Expanding Model Choice: New Open-Weights Models from Mistral AI and Beyond

Now, when you all start building a generative AI application, the very first thing you likely decide on is which model are you going to use. Which is the one that gives you the best cost,  the lowest latency? What's going to give you the best answers? And a lot of times, actually, the right answer is a mix of different models for your applications or your agents, which is why we think model choice is so critical.



We've never believed that there was going to be one model to rule them all, but rather that there would be a ton of great models out there, and it's why we've continued  to rapidly build upon an already wide selection of models. We have open-weights models and proprietary models, general purpose or specialized ones, we have really large ones and small models,  and we've nearly doubled the number of models that we offer in Bedrock over the last year. Today, I'm pleased to announce



[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1830.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1830)

that we're introducing a whole host of new open-weights models.  These models include ones like Google's Gemma,  MiniMax M2, and Nvidia's Nemotron, and today, we have a couple of new models that are debuting to the world for the very first time. 

I'm excited to announce that today from Mistral AI and available immediately on Bedrock are two new sets of open-weights models. The first is Mistral Large, which is a big leap forward from their Large 2 model, doubling the context window size and vastly increasing the number of model parameters by more than five times. We're also launching today Ministral 3, which is a cool set of three models that offer really great deployment flexibility for ultra-efficient edge devices or single GPU deployments or advanced local operations. It's going to be super fun to see how you all use all of these open-weights models.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1870.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1870)


### Amazon Nova 2: Next-Generation Models with Frontier-Level Intelligence

 Now, in addition to providing a huge selection of third-party models on Bedrock, last year we announced Amazon Nova, which is Amazon's family of foundation models, delivering the industry's best price performance for many workloads out there. Over the last year, we've actually extended the Nova family to support more use cases and deliver more possibilities for you that deliver real value.  We've unlocked speech-to-speech use cases as an example with Amazon Nova Sonic, and just a few weeks ago, we launched the industry's best-performing model for creating embeddings across multiple modalities with Nova's Multimodal Embeddings.


And the momentum has been really fantastic. Nova has grown to be used  by tens of thousands of customers today, everyone from marketing giants like dentsu to tech leaders like Infosys or Blue Origin or Robinhood to innovative startups like Ninja Tech AI. And today, we're making Nova even better, announcing a new generation of Nova with Nova 2.


Nova 2 delivers cost-optimized, low-latency models with frontier-level intelligence. The Nova 2 family includes Nova 2 Lite,  which is our fast and cost-effective reasoning model suitable for a broad set of workloads, and Nova 2 Pro, which is our most intelligent reasoning model for highly complex workloads. We're also introducing Nova 2 Sonic, which is our next generation speech-to-speech model that enables real-time, human-like conversational AI for all of your applications.

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/1960.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=1960)

 Now, Nova 2 Lite delivers incredible price performance for many workloads that we actually see our customers wanting to deliver in production. Nova 2 Lite compares really favorably in industry benchmarks to models like Claude Haiku 3.5, GPT-4o mini, and Gemini Flash 2.0. In particular, Nova 2 Lite excels at things like instruction following, tool calling, generating code, and extracting information from documents, often matching or exceeding the performance that we see from these comparable models at an industry-leading cost performance. We think that Nova 2 Lite is going to be a real workhorse and is going to be really popular for a wide variety of your use cases out there.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2000.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2000)

 Nova 2 Pro is our most intelligent reasoning model, and it's going to be great for when you have those really complex workloads. In particular, we look at really important areas where you need your agents to be great, and that's where Nova 2 Pro really shines, where skills like instruction following and agentic tool use are critical. In fact, for those, Nova 2 is frequently coming out on top. And if you look at artificial analysis benchmarks in those areas, Nova 2 Pro delivers better absolute results compared to leading models like GPT-4o, Gemini 2.0 Pro, and Claude 3.5 Sonnet.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2050.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2050)

And for applications that need voice capabilities, Nova 2 Sonic offers industry-leading conversational quality at awesome price performance. With improved latency and significantly expanded language support, we think you're going to love it.  Now, I also have one more Nova model that I want to talk about that has a unique set of capabilities.


### Nova 2 Omni: The Industry's First Unified Multimodal Reasoning Model

Now, today's models out there are actually quite good at reasoning across one type of modality.  Say they're looking at an image or listening to audio or outputting text and then maybe outputting in a different modality, say they may be reading texts and then creating an image. But in the real world, you have to understand multiple modalities at the same time. Take for example this keynote. If you wanted a model to try to understand all that was going on in the keynote today, you'd want to get the nuance and understanding of everything that we're saying. That means you'd have to listen to what I'm saying, you'd have to understand the contents of all these slides, you'd have to be able to watch the videos and understand what's going on and what we're showing.

Now, let's say you want to take that same model and produce a summary output for your sales team where it had a summary of all the launches we announced today, along with some images and marketing material. Now, if you wanted to do this, you could, right?


This is possible today, but it means testing a wide variety of different models, stitching them all together, and trying to accomplish this outcome, which is totally doable but is quite hard. It'd be easier if you had a single model that could do all of that, and that's why I'm excited to announce Amazon Nova 2 Omni. It's a unified model for multimodal reasoning  and image generation. Amazon Nova 2 Omni is the industry's first reasoning model that supports text, image, video, and audio input and then supports text and image generation output.



So that's four new industry-leading models from Amazon Nova, and we're just getting started. Up next, let's hear from Gradial who's building some pretty cool capabilities  with Nova and Bedrock. The biggest slowdown in marketing isn't creativity. It's everything that happens after, and Gradial helps achieve that. Today, the content operations world is fairly manual. To get from a creative brief onto a website  takes four to six weeks and involves 20 different steps that require designers, engineers, copywriters, and web strategists.



[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2210.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2210)


And so what Gradial's done  is we've connected all of those different systems in place so that we can take it from idea to action. Our orchestration agent decides which subagents  that it actually uses, if it's going to use the authoring agent, the Figma agent, the Sitecore agent, and it will combine each of those agents to actually get a task done and actually make recommendations of how your content can convert audiences better and faster. We will forever live in a multi-model world.  There isn't one model that fits all, and so AWS Bedrock and Nova gives us the freedom to use efficiency where we need it, use power when we need it,  and use reasoning when we need it.




That was crucial for us. AWS is extremely invested in startup success,  and that is very clear. I don't think that anybody got into marketing to do a link swap. I think that folks got into marketing to be creative.  If you free up that time, just imagine all of the things that will be created years from now.  Really amazing to see what Gradial has been able to do with Bedrock.



[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2300.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2300)

### The Power of Your Data: Why Deep Integration Drives Real AI Value

Now, the models that are available today  are really incredible, and it continues to impress me what everyone out there is able to accomplish with them. But as these models are used to power  more and more mission-critical line-of-business applications and your agentic workloads, it turns out that the AI's ability to understand your company's data is what really starts to deliver huge value for your company and for your customers. And I'll just pause here. I can't stress this strongly enough: your data is unique. It's what differentiates you from the competition. And I see this over and over again. If your models have more specific knowledge about you and your data and your processes, you can do a lot more. 



Now, the wizardry here comes when you can deeply integrate a model with your unique data and IP. But in order to do this well,  it's critical that you have your data in the cloud. So what are the best ways to get these models to access your data? Clearly, third-party models don't start with access to your data. They don't natively know about your business, and frankly, you wouldn't want them to, since you don't want your proprietary data embedded in those models so that everyone else could use it. It's why actually the isolation that we provide  inside of Bedrock is so important to prevent your data from leaking back to the core models.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2360.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2360)

Now, the most common techniques that we see people successfully use today to combine your data with the models are things like leveraging RAG or vector data to provide your chosen model with context at inference time. And these are quite effective to help your models more effectively navigate your massive set of data and return relevant results. Usually what we see though is this only goes so far.  Almost every customer I talk to wishes that they could somehow teach the model to really understand their data, really understand their deep domain knowledge, the expertise. They want the model to know their expertise when it's making its decisions.



Let's take for a second, you work at a hardware company  that's looking to accelerate R&D for new products. You'd optimally want a model that understands your past products, your manufacturing preferences, your historical success and failure rates, whatever process constraints you might have, and then you want something that could combine all of these to provide intelligent guidance for your design engineers. And it turns out whatever your company is,  you have this incredibly vast corpus of IP and data that would be super valuable if it was integrated into the model you use.

Now, the natural question is, why not just train a custom model? It turns out there's only really two ways to do this today. You could build your own model from scratch, include your own data in that. But of course, this is super expensive and frankly, you probably don't have all of the data that you would need to build the general intelligence in the model. And then even if you did, you may not have the in-house expertise to pre-train a frontier model anyway. So that's probably not very practical for most companies.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2420.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2420)


 What most people do instead is they start with an open-weights model and modify them. And there's lots of ability to customize there. You can tune weights with techniques like fine-tuning, reinforcement learning, and you can try to build something that really focuses on your use case. However, it turns out there's actually limits to how effective this is as well. It's really hard to teach a model a completely new domain  that it wasn't already pre-trained on, and it turns out the more you customize models, the more you add a bunch of data in post-training, these models tend to forget some of that interesting stuff that it learned earlier, the core reasoning.

It's a little bit like humans trying to learn a new language. When you start, when you're really young, it's actually relatively easy to pick up, but when you try to learn a new language later in life, it's actually much, much harder. Model training is kind of like this too. Now, there's been some pretty cool things done with the limited ability that you have to tune these open-weights models, but you can only go so far. Today, you just don't have a great way to get a frontier model that deeply understands your data and your domain.



### Amazon Nova Forge: Open Training Models for Proprietary Domain Expertise

But what if it was possible? What if you could integrate your data  at the right time during the training of a frontier model and then create a proprietary model that was just for you? I think this is actually what customers really want, and so we asked ourselves, why not? And today, I'm excited to announce Amazon Nova Forge.  Nova Forge is a new service that introduces the concept of open training models. With Nova Forge, you get exclusive access to a variety of Nova training checkpoints, and then you get the ability for you to blend in your own proprietary data together with an Amazon-curated training dataset at every stage of the model training.


[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2570.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2570)

This allows you to produce a model that deeply understands your information, all without forgetting the core information that the thing has been trained on.  We call these resulting models Novellas, and then we allow you to easily upload your Novella and run it in Amazon Bedrock. Let me show you how this works. Let's say you're that hardware manufacturer  that we discussed earlier. You have several hundreds of gigabytes of data, billions of tokens related to your past designs, your failure modes, your review notes, et cetera, and you decide that you're going to start from an 80% pre-trained Amazon Nova 2 Lite checkpoint.

Using our provided tool set, you blend all of your data in with that Amazon-curated training dataset, and then you run the provided recipes to finish pre-training that model, but this time with all of your data included. This introduces your domain-specific knowledge, all without losing the important foundational capabilities of the model like reasoning. Nova Forge also provides the ability to use remote reward functions and reinforcement fine-tuning to further improve your model, letting you plug real-world environments into the training loop. And because your baseline model already understands your business, these post-training techniques are actually much more effective.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2650.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2650)

Once you're ready, you import this model, your Novella, into Amazon Bedrock and you run inference on it just like you would any other Bedrock model. Now, your industrial engineers can ask questions like, "What are the pros and cons of design A versus design B?" and get responses that are specific to your company's historical results, manufacturing constraints, and customer preferences. We've already been working with a few customers to test out Nova Forge,  and they're already seeing transformative results from customizing Nova's open training models.

Let's dive a little bit into the example with Reddit. Reddit uses generative AI to moderate content for multiple different safety dimensions across their chats and searches. Fine-tuning existing models didn't get them the performance they needed. They even tried using multiple models of different safety dimensions, but it was super complex and even then, they couldn't get the accuracy they wanted for the specific requirements of their community.

With Forge, however, Reddit was able to integrate their own proprietary domain data during pre-training, enabling the model to develop integrated representations that naturally combined the general language understanding with their own community-specific knowledge. For the first time, they were able to produce a model that met their accuracy and cost-efficiency targets, and at the same time, it was much easier to deploy and operate.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2710.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2710)


We think this idea of open training models is going to completely transform what companies can invent with AI.  Now here to share how Sony is transforming and reinventing their business on AWS, please welcome Chief Digital Officer and Corporate Executive Officer from Sony Group Corporation, John Kodera. 

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2760.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2760)


### Sony's Kando Journey: Reinventing Entertainment Through AWS and AI

Good morning. Today, I'd like to talk to you about a word that holds a very special place for Sony: kando. The direct translation of the word into English is emotion, but kando means more than that in Japanese. It captures feelings of deep emotional connections  and experiences when watching a movie, listening to music, or playing a game. For us at Sony,  kando is what we strive to create and deliver to our customers in all aspects of our work. Kando is at the core of who we are.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2790.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2790)



Our founders created Sony in 1946 with the dream to enrich people's lives  through the power of technology, a world where technology can deliver new experiences, lifestyles, and kando.  Driven by this vision, we have delivered innovative products, creating entirely new industries and customer experiences along the way. With each era of technology, from analog to digital, the internet, the cloud,  Sony has reinvented itself again and again.


[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2860.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2860)


Today, Sony is more than a hardware technology company. We are also a leader in entertainment across games, music, pictures, and anime. There is no other company in the world like Sony with the depths of our business portfolio and touchpoints with fans and creators. One of the most remarkable successes this year  is the anime movie "Demon Slayer: Kimetsu no Yaiba Infinity Castle." As of late November, this film has become  the highest ever grossing Japanese film released worldwide and fifth highest grossing film across all categories in 2025. 



[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2910.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2910)

As we've done with Demon Slayer, we hope to keep delivering new kando by marrying the creator's vision  with a deep understanding of their fans, and our relationship with AWS plays a pivotal role to make this happen.  One example started in the early 2010s when I was president of the network service company for PlayStation and other Sony devices. We chose AWS as our provider for its global footprint, high standards in availability, and scalability. 

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/2930.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=2930)



In 2020, for the launch of PlayStation 5, we utilized AWS building blocks for our network architecture. These services allowed us to scale out at a moment's notice and accelerated our shift to microservices,  increasing deployment by 400% with one-tenth the lead time. Today, our relationship with AWS  supports safe, secure, and high-quality gaming experiences for up to 129 million gamers  to connect and experience kando together.





[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3000.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3000)

Moving forward, we see incredible potential  for growing the fan community, connecting fans with similar tastes and interests across our diverse portfolio of content IPs.  At the same time, we also want to better serve our creator community by providing them with more tools, connections,  and insights to their fan base. We call this the Sony Engagement Platform, creating deeper understanding and connection  between the fans who experience content and the creators who are making it. 


The foundation for the Engagement Platform is the Sony Data Ocean. It utilizes data insights generated from multiple connected data lakes.  Built using AWS services, it enables us to process up to 760 terabytes of data from more than 500 data sources across Sony Group.


[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3050.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3050)

Of course, to make the most effective use of our data  and deliver kando to our customers, we have to effectively harness the power of AI and agents to empower our employees and augment our business capabilities. To maximize our productivity in the enterprise setting,  we are actively promoting the usage of generative AI, our home-grown enterprise LLM built using Amazon Bedrock. We have over 57,000 users today since its introduction two years ago, and we are serving 150,000 inference requests per day.


[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3090.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3090)


Today, we are integrating new agent capabilities into our platform to enable a new level  of advanced operational efficiency across our businesses. By placing Amazon Bedrock AgentCore  at the center of our agentic AI system, we gained the ability to easily govern, deploy, and manage more useful agentic capabilities  to accelerate our enterprise AI transformation.


Today, we are pleased to share that we are adopting Amazon Nova Forge  to apply state-of-the-art customized models to our unique business and operations. We fine-tuned an Amazon Nova 2.0 Lite model that outperforms baseline models for tasks like reference consistency and document grounding. We are now aiming to increase the efficiency of Sony's compliance review and assessment processes by 100x.

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3150.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3150)


In addition to the enterprise setting, Sony is fully committed to the responsible and ethical development  and use of AI in the creative domain. We hold ourselves to the highest standard and respect toward the rights of creators and performers. So where are we going from here? We will continue to create and deliver kando,  fulfilling the aspirations of both fans and creators, building meaningful connections between them.


[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3220.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3220)

In the future, we will continue to expand our fans' engagement with their favorite content IP  across multiple entertainment genres. As we have done with Uncharted and The Last of Us, we hope to connect fans and creators in both virtual and physical environments, including location-based entertainment. Realizing our vision will require even greater and stronger collaboration with AWS, as well as our creative and technology partners in the audience.  We look forward to creating and delivering even greater kando to our fans across the world. Thanks for listening. Thank you.


[![Thumbnail 3250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3250.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3250)

### Building for Change: The Foundation for Agentic Transformation

 Thank you so much, Kodera-san. Such a great story. Just like Sony has seen, a key to their success  over the long run is having the right foundation for that innovation. What really excites me about Sony's story is that because they have their data and their applications in the cloud on AWS, it's that much easier for them to deal with any uncertainty that comes their way.


Not many companies have been able to transition as successfully as Sony, from an electronics device company into a global digital media business, such a major change, and so cool to see. Having the right technology platform in AWS has helped them on that journey.  Now, when you have your data in the cloud, it turns out you can move more rapidly and you can adjust to any of those unexpected changes that come your way.

The world is not slowing down. In fact, if there's one thing that I think we can all count on, it's that more change is coming. Now, one of the biggest opportunities that is going to change everyone's business is agents. Agents are exciting because they can take action.


Agents can get things done. They can reason dynamically and create workflows to solve a job in the best way  without you needing to pre-program them. These agents work in non-deterministic ways, which is part of what makes them so powerful, but it also means that the foundation and tools that got us to where we are building software aren't necessarily the ones that we need for agents.

[![Thumbnail 3340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3340.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3340)


### Amazon Bedrock AgentCore: The Most Advanced Platform for Building Agents at Scale

That's why we launched Amazon Bedrock AgentCore, delivering the most advanced agentic platform so that you all could build, deploy, and operate agents securely at scale. We designed AgentCore at the same time to be comprehensive but also modular.  It has a secure serverless runtime that can deliver to agents so that agents can run in complete session isolation.  AgentCore Memory enables agents to keep context, handling both short-term and long-term memory so that they can learn and get better over time.


We provide an AgentCore Gateway so agents can easily discover  and securely connect to tools, data, and other agents. We have AgentCore Identity that provides you a way to do secure authentication and gives you controls over what tools and data your agents can access. AgentCore Observability gives you real-time visibility in the deployed agent workflows that you have.

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3380.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3380)


We have a variety of foundational tools  that allow your agents to securely execute real-world workflows, things like Code Interpreter that gives you access to a secure code execution environment, or our Managed Browser service, which provides a managed environment that makes it easy for your agents to access the internet. AgentCore is truly unique in what it enables for building of agents, and it's significantly different  than anything else out there.

We built AgentCore to be open and modular, so you can use it with a variety of frameworks, things like CrewAI or LlamaIndex or LangChain or AWS's Strands agent. You can also use it with any model out there, whether it's from the variety of models that we have in Bedrock or from models like OpenAI's GPT or Gemini models. You only have to use the building blocks that you need. We don't force you as builders to go down a single fixed path. We allow you to pick and choose which services you want for your own situation.

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3430.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3430)


 AgentCore also then makes it easy to deploy your agents privately and securely inside of your Amazon VPC and then allows you to scale to thousands of sessions to support high-traffic use cases. It's also super fast and easy to deploy your agents. Agents can be deployed in under a minute with just drag and drop or a few lines of code.  This is part of why we're seeing so much momentum with AgentCore, as our customers rapidly adopt it as the foundation for their agentic applications.

[![Thumbnail 3480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3480.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3480)

We see it across industries, from companies in regulated industries like Visa or National Australia Bank or Rio Tinto. We see it from ISVs like Lummi and ADP or startups like Cohere Health and Snorkel AI. The momentum is really accelerating. I'm going to talk about a few of them. Adena Friedman is the CEO of Nasdaq, and she and her team are moving really fast to build agents  that can do real work in core areas of their business.


Before AgentCore, they were planning on dedicating a whole team to build a foundation infrastructure that they needed to reliably operate and build resilient agents to meet their very high standards. AgentCore, however, now frees them from this heavy lifting so they can just focus on building great agents. Bristol-Myers Squibb built a new agent that's able to evaluate more than 10,000 compounds across multiple hypotheses in less than an hour.  This is a process that used to take their researchers four to six weeks.


[![Thumbnail 3530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3530.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3530)

The company's drug discovery agent uses AgentCore Runtime for its ability to seamlessly and dynamically scale  and to keep their sensitive data secure and isolated. We see ISVs like Workday who are building the software of the future on AgentCore.  AgentCore's Code Interpreter delivered exactly what they needed, the essential features and security requirements and data protection that was needed to power their planning agent. This capability reduces the time spent on routine planning analysis by 30%, saving them nearly a hundred hours of work every month.



### The Challenge of Agent Control: From Teenagers to Autonomous Systems

You don't have to build your own agents either.  Many companies are using AWS Marketplace as the trusted place to publish and procure prebuilt agents, tools, solutions, and professional services, and this is where AWS Partners can help you all move even faster. We're really excited about what customers have been able to do with AgentCore, but we're far from done.  One big challenge that we've seen when you're building agents is how do you get them to behave predictably and in line with your intents?

What makes agents powerful is this ability to reason and act autonomously, but that also makes it hard for you to have complete confidence that your agents aren't going to stray way out of bounds. This is a little bit like raising a teenager. I currently have two awesome teenagers myself at home. As your kids get older, you have to start giving them more autonomy and freedom so that they can learn, or adulting as they like to call it. But you also want to put some ground rules in place to avoid major issues.

[![Thumbnail 3610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3610.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3610)

Think about when your kids start driving.  This is the current situation that I'm in. All of a sudden, the kids have all of this autonomy. There's a ton of things that they can go and do by themselves, but you still want to have those guardrails in place, like you have to be home by a certain time or you don't want to drive more than, say, five miles an hour over the speed limit, things like that.



One way you can actually build trust in agents  is by making sure that they have the right permissions to access your tools and your data. AgentCore Identity provides a great way to do this today, but while permissions on your tools that your agents can access is a good start, what you really want to be able to control is the specific actions that your agents can or cannot take with those tools. You have actions like, what is the agent going to do with those tools?  How can they use them? Who are the tools for?

[![Thumbnail 3670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3670.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3670)

Today, customers struggle with this. You can embed policies directly inside your agent's code, but because agents generate and execute their own code on the fly, these safeguards are really best effort and can only provide you weak guarantees, and they're really difficult to audit.  In practice, this means today you can't with certainty control what your agent does or does not do, while also giving it the agency to go and complete these workflows on its own. As a result, most customers feel that they're blocked from being able to deploy agents to their most valuable, critical use cases.

[![Thumbnail 3710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3710.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3710)

### AgentCore Policy: Deterministic Controls for Enterprise Agent Behavior

That's why we're announcing Policy in AgentCore today. Policy provides you with real-time deterministic controls for how your agents interact with your enterprise tools and your data. Now, you can set up these policies that can define which tools your agents can access,  but also how they access them, whether they're APIs or Lambda functions or MCP servers or popular third-party services like Salesforce or Slack. You can also then define what actions they can perform and under what conditions.


[![Thumbnail 3750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3750.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3750)

What AgentCore does is it then evaluates every single agent action against this policy before ever granting access to your tools or your data. We'll walk through a simple example. Let's say you're in AgentCore Policy, and just using natural language, you define a policy. Say something like,  "I want you to block all refunds from customers when the reimbursement amount is greater than $1,000." Then under the hood, what happens is your prompt is converted to Cedar,  which is a popular open-source language that's powered by our automated reasoning work across authorization and our verifiable systems inside of AWS.


[![Thumbnail 3780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3780.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3780)

 Once established, these policies are then deployed to your AgentCore Gateway and they're evaluated in milliseconds, which ensures that all of your actions are checked instantly and consistently to keep your agent workflows fast and responsive. The design of where this sits is actually super important. Because this policy enforcement is outside of your agent's application code,  the policy evaluation actually sits in between your agent and all of your data and your APIs and tools, so you can predictably control their behavior.

Going back to our example in the refund policy, every agent action is checked against your policies before the agent is able to access the tools. So let's pretend a situation happens where a refund is over the limit that you've defined, the agent is blocked from issuing that refund. Now that you have these clear policies in place, organizations can much more deeply trust the agents that they're building and deploying, knowing that they'll stay inside the boundaries that you've defined.

[![Thumbnail 3840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3840.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3840)

### Trust But Verify: Introducing AgentCore Evaluations for Continuous Quality Monitoring

Now, of course, you need agents to do more than just follow the explicit rules that you define. You have to know that they're behaving in the right way. Trust, but verify is a phrase that we've co-opted at Amazon as a mental model for how you manage at scale. At AWS, we give our teams incredible autonomy. I trust our teams to go and invent for customers and execute on that mission.  But I also have mechanisms that allow me to dive deep and inspect when things are on track. I want to check that our strategic initiatives that we've identified are in fact getting done in the way that we've intended.


If I go back one more time to our teenagers, I generally trust that they're following the rules, but I can still check my Ring camera to ensure that they got home on time, and I can always check the status of my Life360 app to ensure that they're within the bounds of where I expect.  This same thing applies to agents. To gain confidence, you want visibility into how they're acting.



Now, customers love what they're getting with AgentCore Observability. You get real-time visibility  into all your operational metrics, you can see your agent response times, you can see the computational power that's being used and your error rates and which tools and functions are being accessed. That's all great. But in addition to how agents are performing operationally, there's other things that you actually want to know. You want to know things like,  are your agents making the right decisions? Are they using the best tool for the job? Are their answers correct and appropriate? Are they even on brand?

[![Thumbnail 3920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/3920.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=3920)

These are things that are super hard to measure. Today, it usually actually requires you to have a data scientist. The data scientist is going to build some complex data pipeline. They're going to select a model that's going to try to judge the outputs of their agents.  They have to build the infrastructure to serve these evaluations and then manage quotas and throttling. Each time you want to roll out a new agent or you want to upgrade to a new version of a model that you're using, you have to do all of this work all over again.

But unlike traditional software, actually testing in pre-production even then is really hard. You only know how your agents are going to react and respond when you have them out there in the real world. That means you have to continuously monitor and evaluate your agent behavior in real time and then quickly react if you see them doing something that you don't like. We think we can make this a lot better.



Today, I'm excited to announce AgentCore Evaluations.  Evaluations is a new AgentCore service that helps developers continuously inspect the quality of their agents based on real-world behavior. Evaluations can help you analyze agent behavior for specific criteria, like the ones I mentioned: correctness, helpfulness, harmfulness.  They come with 13 pre-built evaluators for common quality dimensions. Of course, you can always create your own custom scoring system with your own preferred prompts and models as well.


You can easily evaluate agents in the testing phase to correct any issues before you end up deploying them broadly. So now, if you're going to upgrade to a newer version of a model, as an example, you run your Evaluations to evaluate your agent and you want to make sure that it maintains the same level of helpfulness, for example, that you have in your current release. You can actually also use Evaluations in production to catch any of those hard-to-find quality degradations really quickly. You'll see your results in CloudWatch right alongside your AgentCore Observability insights.  AgentCore Evaluations automates what used to take specialized expertise and a bunch of infrastructure heavy lifting into something that everyone can access and allows you to continually improve the quality of your agents. We're quite excited about it.



All right, so this is AgentCore, the agentic platform that's powering  the next wave of agents. We're helping you move quickly to get your agents into production without compromising or making any sacrifices, which is what we're all about. We want you to move fast  so you have the broadest set of capabilities to build for your own customers. Today, we're really excited that we've added two new powerful capabilities in Policy and Evaluations, and I'm really excited to see how this unlocks some powerful production use cases.



[![Thumbnail 4110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4110.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4110)



### Adobe's AI Revolution: Transforming Creativity and Customer Experiences with AWS

Now to tell us more about how they're building agents of their own to transform their business and how they're using AWS  as a key part of their agentic transformation, please welcome Shantanu Narayen, CEO and Chair of Adobe.   Thanks, Matt. Good morning.  Hello, everyone. I'm thrilled to join you at this transformative time.  We're clearly witnessing a golden era of creativity where AI is amplifying human ingenuity and enabling people to bring their imagination to life.


Adobe has been at the forefront of this revolution. From the invention of desktop publishing to the origins of digital documents to groundbreaking advances in imaging and video, we're constantly pushing the boundaries of what's possible. It was actually our transformation  to a cloud-based subscription model over a decade ago that marked the beginning of our relationship with AWS, because it was services like Amazon EC2 and S3 that actually provided us with the scalable as well as secure foundation for Adobe's innovation.

[![Thumbnail 4170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4170.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4170)


As we transition into this era of AI,  AWS is actually helping us innovate faster with the core services that we need, as Matt said, to train models as well as deploy agents. This allows us to focus on what Adobe does best: unleashing creativity across every facet of digital experiences for our business, which span business professionals, consumers, creators, creative professionals, as well as marketing and IT professionals. 




When it comes to AI for creativity, we're re-imagining every stage of the process for people of every skill level. We do this with the knowledge that over 90% of creators are actively using creative-focused generative AI today. To support them,  we're infusing AI into Adobe Firefly, our all-in-one destination for creative workflows driven by AI,  in our flagship Creative Cloud applications like Photoshop, and in Adobe Express, the quick and easy app to create on-brand content.  Our Adobe Firefly model powers capabilities like sketch to image, text to video, generative fill, and generative recolor. These models have been trained and are using both P5 and P6 instances with all the data stored in S3 and FSx for Lustre. These models have been used to generate over 29 billion assets and enable creators to create content with unmatched creative control.



 Our AI assistant now in Adobe Express helps users redefine the entire creative process using conversational editing. These agentic experiences are now powered by our AI platform, and our relationship with AWS helps ensure that these agents operate efficiently and, more importantly, securely. When it comes to productivity, PDF remains the way that people consume information. Over 40 billion PDFs  have been opened and shared with Adobe Acrobat, and every year more than 18 billion PDF files are created and edited by our customers around the world.


[![Thumbnail 4360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4360.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4360)

Today, we're integrating productivity for billions of business professionals and consumers through AI capabilities, including an AI assistant. In August, we announced Adobe Acrobat Studio, a first-of-its-kind platform that brings together Acrobat, Adobe Express, and AI agents to enable users like you to work more efficiently with both structured and unstructured information. Our collaboration with AWS is absolutely key here, given Acrobat Studio uses Amazon SageMaker as well as Amazon Bedrock  to access our and third-party models, helping millions of users research, strategize, analyze, and collaborate even faster.  Adobe PDF is also a new offering that helps consumers and business professionals now collaborate with conversational knowledge hubs that are supported by personalized AI assistance.



Finally, in the AI era, we all know that the role of marketers has evolved to the orchestration of engaging customer experiences for their consumers and customers. To support them by unifying the key elements of customer engagement,  the content supply chain, as well as brand visibility, Adobe Experience Platform is the core foundation for driving this customer engagement, bringing together AI-powered apps and agents to drive engagement and loyalty. It operates at the scale of over 35 trillion segment evaluations and more than 70 billion profile activations per day. Experience Platform runs using AWS building blocks as well as an innovative cellular architecture. Our joint customers can now ingest data from sources like Redshift  into the Adobe Experience Platform to create these profiles, hydrate them, and use these audiences in Adobe's real-time customer data platform.


[![Thumbnail 4480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4480.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4480)

 Key to this customer engagement is creating on-brand content that's delivered at the right time in the right channel at exactly the right moment, because marketers expect that the demand for content will grow five times over the next two years and every business needs a content supply chain to manage this. Adobe GenStudio is our solution to address this in an end-to-end fashion. Amazon Ads is a key collaboration here by integrating our creative and customer experiences with how creatives and marketers can now bring all of these ideas to market.  Finally, brand visibility is clearly top of mind for CMOs as we all turn to these LLMs for information, recommendations, as well as purchase decisions. We actually observed an 1100% year-over-year increase in AI traffic to U.S. retail sites as recently as September.


With products like Adobe Experience Manager, as well as the newly available Adobe LLM Optimizer and Adobe Brand Concierge, we're helping brands stay in front of AI search. We're really excited about the promise of AgentCore and Kiro to help us accelerate the deployment of all these new agentic capabilities. We've already had numerous successful AgentCore proof of concepts.  For example, our Adobe Commerce team was able to run a prototype migration assessment using AgentCore to help our customers identify as well as solve compatibility challenges as they move to this SaaS product.



Adobe has incorporated AI into our tools for over 15 years, delivering hundreds of advances that enhance efficiency and collaboration. 99% of Fortune 100 companies have used Adobe AI  in an application. Across all these categories, AWS is helping us to innovate faster, operate more efficiently,  and deploy new technologies at scale. Whether it's in the data layer where we train our category-leading Adobe Firefly Foundation models, or as Matt said, in making sure that we offer choice in AI models so we can continue to innovate in creative categories, through agent orchestration where we're augmenting this ecosystem, and finally, integrating AI into all of our apps, making it easy for customers of all types to adopt and realize value where they do their work today.


[![Thumbnail 4620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4620.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4620)


It's an incredibly exciting time to stand at this intersection of human and computer interaction,  and the AI transformation Adobe and AWS are driving together, I believe, will redefine digital experiences for billions of people around the world,  and we couldn't be more excited to work with all of you. Thank you. That's great, Shantanu. Thanks so much. It's really exciting to see  how Adobe is pioneering across digital experiences all on top of AWS.


### Amazon QuickSight: Empowering Every Employee with AI-Powered Insights

Now, with the tools and services we're providing, we know that our customers and partners out there are going to build a huge number of incredibly impactful agents, but you can also expect that some of the most capable, powerful agentic solutions are going to come direct from AWS. Let's dive into a few of those now.  As we thought about what agents should we build and which experiences could we reimagine, we focused on areas where we thought we could bring some differentiated expertise to our customers. For example, it turns out Amazon has a very large, heterogeneous global workforce, and we understand the importance and frankly the complexity of tying together all of your enterprise data and systems to empower those employees.

[![Thumbnail 4690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4690.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4690)

We set off to build something that would empower Amazon and our customer set of corporate employees,  which is why we built Amazon QuickSight. With QuickSight, our goal is to give every employee a consumer AI experience, that consumer AI experience that they've come to embrace, but with the context and the data and the security that you all need to get your work done. Just earlier today, I talked about how important deep access to your company's data is when you're trying to make critical decisions, and that's one of the things that makes QuickSight unique and powerful.

[![Thumbnail 4750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4750.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4750)

It brings together all your data sources, your structured data like BI data and your databases and your data warehouses, your data from apps like Microsoft 365 or Jira or ServiceNow or HubSpot or Salesforce, as well as all your unstructured data, things like your own documents or your files that you have in SharePoint or Google Drive or Box, all of that data that you need to make great decisions, and we make it accessible to a powerful suite of agents. With QuickSight, you get a rich set of BI capabilities that make it easy for anyone to discover insights across all of those sources  of structured and unstructured data.

You get a capability to do deep research. This is actually one of my personal favorite features, and it allows QuickSight to investigate complex topics, but then it can pull information from your internal data repositories as well as external sources of data on the internet to pull together a thoughtful, detailed research report complete with source citations so you know exactly where the information comes from. You can create QuickSight Flows which give you the ability to create these little mini personal agents that can automate flows for your everyday, repetitive tasks to drive efficiency for you as an individual, and this can help your teams at your companies be much more efficient and productive at work.


[![Thumbnail 4800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4800.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4800)

A few months ago,  we released QuickSight internally at Amazon and today, we already have hundreds of thousands of users inside of the company. The value that our own employees are getting from QuickSight has quite frankly blown us away.  Teams are telling us that they're completing tasks in one-tenth the time that it used to take.

Take, for one example, our internal Amazon tax team, where they built a QuickSight agent that helps them consolidate all of their sources of tax data, whether they're projects from audits or details from the internet. It performs deep research into any tax code changes or policy changes that might be made, and it presents all of this tax information from all those sources of data in a single view for them. They then use QuickSight to visualize that information, and it allows them to track regulatory changes in real time.

These weren't developers, these were tax people, and they were able to do this without writing any code or pulling any manual reports. Now, when a new tax law emerges, everyone can act on it quickly. It eliminated this siloed set of systems and enabled the team to stay compliant and proactive rather than reactive. We're hearing stories across the company like this over and over again.

[![Thumbnail 4860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4860.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4860)


### Amazon Connect: Leading the Cloud Contact Center Transformation

Another place where we use agents  to transform what's possible for you all is in customer service. This turns out to be another area where Amazon knows a lot about.  Amazon Connect is a leading cloud contact center solution, and it transforms customer experiences across organizations of all sizes. Connect was a pioneer in bringing AI to the contact center with AI-powered self-service. It allows you to intelligently, automatically resolve issues, but it also combines it with AI-driven recommendations so that you can guide your human agents. Connect gives you this ability to deliver personalized, exceptional experiences for all of your customers.


 It's impressive to see how quickly Connect has grown to lead the transformation from these legacy on-premises environments into a cloud, AI, and agent-powered contact center. It's done this for global enterprises like Toyota and State Farm and Capital One and National Australia Bank, as well as for hundreds and hundreds of startups. Customers are seeing the impact of this move to the cloud, and we're seeing this momentum really accelerate the business.

[![Thumbnail 4930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/4930.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=4930)

In fact, it's being shown in the business results. Earlier this year, the Connect business passed  the one billion dollar annualized run rate mark while helping tens of thousands of customers grow their business faster. Thank you to all of you who use Connect. QuickSight and Connect are just two examples of AWS delivering impactful agentic solutions for our customers.



### Writer: Building Enterprise-Grade Agentic AI with Full-Stack Platform Integration

Up next, we're going to hear from a fast-growing startup  that's also helping enterprises get more work done. To share their story about how they're transforming what's possible with agents in the enterprise, please welcome May Habib, CEO of Writer. 




What if Mars,  one of the world's largest consumer goods companies in the world, could run every ad image through compliance in seconds, saving thousands of hours as checks are done instantly? What if AstraZeneca, makers of some of the most innovative drugs, could automate the paperwork needed to get treatments approved all over the world,  saving months of painstaking manual work and getting life-saving treatments to people faster? And what if Qualcomm,  the global technology leader, could uncover the most efficient places to put marketing spend in real time, dramatically boosting campaign performance while saving millions in the process?

[![Thumbnail 5040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5040.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5040)

This is not just the promise of AI. This is all happening today, right now, with agentic AI from Writer. I'm May Habib, Writer's co-founder and CEO. Over the last five years, we've worked with the world's largest companies in the most highly regulated industries  to build a platform for agentic work.


Early on, we saw a gap between the amazing things these large language models were capable of  and what would meet the enterprise's bar for reliability, security, and control. We made a bold decision to be a full-stack platform, one that has the precision and compliance that enterprises need. It's powered by our own enterprise-grade Palmyra LLMs and delivers agents that handle the toughest enterprise workflows.


To truly scale our full-stack vision,  we needed an infrastructure provider that was resilient, secure, and engineered for the enterprise. The majority of the Fortune 500 run on AWS, including so many of our customers. So teaming up with AWS was a no-brainer. AWS stands alone as the cloud provider that enables us to both train our frontier models and deploy our entire platform securely to our enterprise customers.

[![Thumbnail 5120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5120.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5120)

Our work with AWS started two years ago with the model layer. We had just launched our latest Palmyra LLM,  and it was posting top scores on leaderboards. But as our models got larger, the computational power that was needed for both training and inference was growing, and that's where the depth of the AWS stack became a strategic advantage.

Our foundation is built on Amazon SageMaker HyperPod, which gives us a powerful service for large-scale model training. We use P5 instances, and soon P6 instances, to handle the heavy GPU workloads, and they're connected with Elastic Fabric Adapter, which is what makes the high-speed communication between nodes possible so our training runs stay fast and synchronized. We've also paired HyperPod with Amazon FSx for Lustre, so we get data at the speed our models need while keeping costs under control.


[![Thumbnail 5200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5200.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5200)

And the results have been enormous.  We've been able to do runs at one-third of the time, going from six weeks down to two weeks, and our training pipelines have become 90% more reliable. All that work gives us the power and stability to build our latest frontier model, Palmyra X5, trained right on HyperPod. X5 gives exceptional adaptive reasoning, a massive one-million token context window, and near-perfect accuracy in extracting business insights  from even the most complex, high-volume data.


And it delivers this with incredible speed. A million-token prompt in just 22 seconds and multi-turn function calls in 300 milliseconds, outpacing other frontier models at one-quarter of the cost. But our relationship with AWS was never just about creating fast, powerful models. It's always been about building a breakthrough AI platform that can transform how businesses operate.  And with Palmyra X5 as the engine, we're delivering on that vision.


[![Thumbnail 5250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5250.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5250)

[![Thumbnail 5260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5260.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5260)

With Writer, enterprise teams at companies  like Mars and AstraZeneca and Qualcomm work smarter by connecting agents to the data, to the context, and to the business know-how that transforms critical processes,  all without business users needing to write a single line of code. Playbooks are central to Writer. They let teams capture a process once,  linking the tools, the data, and the systems they rely on and turning them into repeatable, intelligent agents.


A playbook becomes a living, dynamic blueprint for how great work gets done. And because they're shared across teams, the highest-impact playbooks can be scaled across organizations instantly.  And very soon, with the help of AWS, Writer is going to be including our next generation of self-evolving LLMs that can learn how organizations operate and anticipate requests on the fly. They're going to be the world's first agents that improve the more that you use them.


But there is a question hanging over all of this, and for the leaders in IT, security, and compliance in the room, those who are held accountable for when something goes wrong, how do we empower business teams to innovate but do it safely and securely?  Writer has to be first and foremost an interoperable platform, one that can observe, control, and connect your agents at scale with the tools and safeguards you already trust.



Today, we're bringing that paradigm to Writer by launching a powerful suite of supervision tools built specifically for the enterprise.  We're giving organizations full visibility and control across the agent lifecycle. Every session tracked, every output compliant,  and every data connector governed in real time. True interoperability means connecting to the systems you trust, so our platform works with the observability, guardrails, and systems that you already use.

[![Thumbnail 5370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5370.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5370)

And beginning today, we're very excited to announce Amazon Bedrock Guardrails now integrate directly  with our platform. Thank you. That means if you've already set up your policies and safety rules in Bedrock, you can apply those exact same guardrails to use in Writer. You don't have to rebuild anything, and you get one consistent, compliant layer of control across your entire AI stack.

We also know that model choice is really important to enterprises. So also starting today, models from Amazon Bedrock are available directly inside of the Writer platform. That means AWS and Writer customers



 can now build agents on Writer using a catalog of different models, from our own Palmyra family to the awesome Nova models you just heard about today and many more, all within a single governed environment. It's the ultimate flexibility without compromising on security. For organizations like Vanguard, long-time customers of Writer and AWS, where trust is non-negotiable, the Writer and Bedrock integrations give them the control they need  to innovate responsibly at scale.



And trust is how companies go from a few scattered POCs to a truly governed, enterprise-wide, impactful AI strategy. You can't scale what you don't trust.  At Writer, our vision is to empower people to transform work and we're very proud to do it with AWS. Thank you. Thanks a lot, May. We're very excited to help customers like Writer make AI and agents real for their customers. 


### AWS Transform: Crushing Technical Debt with Custom Code Transformation Agents

All right, one end user that we haven't talked much yet about is developers. And this turns out to be an area where AWS and Amazon have a really deep expertise. We know that by far one of the biggest pain points today for development teams who are trying to rapidly modernize their applications is dealing with their technical debt. Accenture estimates that tech debt cost companies a combined $2.4 trillion a year in the U.S. alone.  And Gartner says 70% of IT budgets today are consumed by maintaining legacy systems.



We knew this is an area where AI could help. And this is why we built AWS Transform,  to help customers move away from their legacy platforms, things like VMware and mainframes and Windows .NET. With mainframe modernization as an example, our customers have already used Transform to analyze over a billion lines of mainframe code as they move those mainframe applications into the cloud. Using Transform, Thomson Reuters  is modernizing over 1.5 million lines of code per month as they move from Windows onto Linux.

[![Thumbnail 5560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5560.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5560)

We knew that helping you modernize faster would be really popular, but man, it turns out you all really dislike your legacy platforms. Yesterday, at the festival grounds here in Las Vegas, some of you might have seen,  many of you tuned in and cheered as we dropped an old decommissioned rack of servers from a crane and blew them up as an ode to crushing tech debt with AWS Transform. Now, this was pretty fun, but there's a lot more legacy platforms that we need to go after.


A lot more. After we launched Transform last year, we actually quickly sat back and we started prioritizing which transformations would we go after next. We had a ton of ideas, Lambda function upgrades, Python upgrades, maybe Postgres version upgrades, or maybe people who wanted to move from C to Rust migrations. But then we thought about, what about updates to proprietary applications and libraries?  The list is almost infinite.

[![Thumbnail 5620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5620.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5620)

So we asked ourselves, why not support all modernizations? Just yesterday, we launched AWS Transform Custom, which gives you the ability to, thank you. Those guys are excited about AWS Custom. It gives you the ability to create custom code transformation agents  to modernize any code or API or framework or runtime or language translation, even programming languages or frameworks that are only used by your company, and customers are already flocking to it.


[![Thumbnail 5650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5650.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5650)

We've already seen customers doing Angular to React migrations, converting VBA scripts that are embedded in their Excel sheets into Python, converting Bash shell scripts into Rust.  One great customer example is with QAD, a provider of cloud-based ERP solutions and supply chain. Their customers struggled with modernizing  from customized old versions of Progress Software's proprietary advanced business language to their QAD adaptive ERP platform. QAD turned to AWS Transform. They had these engagements that were taking a minimum of two weeks to modernize, and all of a sudden, they were completing them in under three days.


### Kiro: The Agentic Development Environment Delivering 10x Developer Productivity

We're really excited to see what legacy code you're all able to transform.  Now, one of the great things about making all these transformations easier is that it leaves a lot more time for developers to invent and that's what we get excited about. And it turns out that developers today are building faster than ever. AI software tools have seen rapid changes over the last year.

We've moved from things that are doing inline tab completion to authoring chunks of code to actually completing simple multi-part tasks. We really see the potential for the entire developer experience, and frankly, the way that software is built to be completely reimagined.

We're taking what's exciting about AI-powered software development, but we thought that there was an opportunity to add structure to it to make it ready for enterprises to adopt and for high-velocity co-development teams to use more effectively. And this is why we launched Kiro, the agentic development environment for structured AI coding.

[![Thumbnail 5750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5750.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5750)

Kiro helps developers take advantage of the speed of AI coding, but with more structure where they're in the driver's seat every step of the way. Kiro has popularized this idea of spec-driven development. From simple to complex projects, Kiro works alongside developers and teams, turning prompts into these detailed specs and then into working code by its advanced agents. So what you get  and what gets built is exactly what you want and expect. Kiro understands the intent behind your prompts and helps you and your team implement very complex features in large code bases and in fewer shots.

[![Thumbnail 5770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5770.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5770)



Now, the reception to Kiro has been quite frankly overwhelming. Hundreds of thousands of developers have already used Kiro  since the preview of the launch just a few months ago. Let's hear directly from them on how transformative Kiro has been to their work. I use Kiro in almost all the development I do.  I ask it questions, I create specs with it. With Kiro, I was able to ship more code in the last five months than in the past ten years. With Kiro, I'm able to work with a partner  so it feels like we're collaborating on the project together.




It operates the way my brain operates when solving a problem. I can just say, "Hey, Kiro, remember that feature we added in?  Can you also write a test as well?" I can be hands off once I break the problem down and just let Kiro deliver for me. I feel like my world has just opened up  to a completely different perspective. Everything feels possible now. You can go from zero to POC ten times faster. Kiro makes me want to build more.  Honestly, Kiro is just awesome.



We think you're all gonna love  how Kiro will transform your development work. And so, I'm excited to announce today that for any qualified startup, we're giving away a year's worth of Kiro, up to a hundred seats, if you apply in the next month. We are so excited about the impact that Kiro is having on making developers' lives  better each and every day.

And I've frankly been amazed at the impact that this development velocity has seen inside of Amazon. In fact, we've been so blown away that last week, all of Amazon decided to standardize on Kiro as our official AI development environment internally. We took a look at all of the tools out there in the market and we recognized that the best way for us to make our developers faster and more productive was to double down on Kiro. And many of you all are rapidly doing the same.

[![Thumbnail 5880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5880.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5880)


Now, I want to take a quick moment and dive deeper  into one of the stories we heard in this video, because I think the details are pretty eye-opening. Now, this was a quote from Anthony, one of our distinguished engineers.  Now, Anthony is working on a significant re-architecture project, and he and the team originally thought that they would need about thirty developers working for eighteen months to complete this work.

Now, Anthony and the team were intrigued by the potential of agentic AI and the potential for it to really supercharge their output. So they decided that they were gonna fully leverage Kiro to deliver the project. It turned out as the team started really digging in and seeing the full potential of agentic tools, it was better than they expected. And they saw that by leaning in on agentic development, a much smaller team could actually deliver incredible results.

[![Thumbnail 5930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5930.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5930)

Instead of taking thirty developers eighteen months to complete the project, they delivered the entire re-architecture  with only six people in seventy-six days, and with Kiro. This is not just the ten to twenty percent efficiency gains that people were seeing with the first generation of AI coding tools. This is orders of magnitude more efficiency.


### Learning to Work with Agents: How One Team Achieved 5x Faster Delivery

Now, I think this is a super powerful story, and I've related it to a couple of customers over the last month or so, and invariably I get the question, "How did they do it?" Well, at first it turns out it took the team a little bit of time to fully understand how to best leverage agentic tools. They started to see of course  some efficiency gains right away. But these were honestly a little bit more incremental than transformative.

[![Thumbnail 5970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/5970.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=5970)

But a few weeks in, they had an aha moment.  They realized that they couldn't keep operating the same way they always operated. They realized to get the most out of the agents meant changing their workflows, and they wanted to lean in to the strengths of what the agents were, and then they had to question some of the assumptions they always had about how they wrote software. The team learned a ton along the way and was able to spot a whole series of new opportunities for how agents could enable teams to ship faster.

[![Thumbnail 6000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6000.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6000)

The first learning they had, which was how they interacted with these Kiro agents, in the beginning, they would feed the tools small tasks  to ensure that they got the reliable results back.


They would go back and forth with all their tools constantly. But as they learned what the agents were good at and what they were not good at, there was this inflection point where they moved from babysitting individual tasks into directing broad, goal-driven outcomes. And this is when they saw their velocity on shipping features rapidly accelerate. 

Second, they thought about moving even faster, and they recognized that they were thinking much too linearly in assigning tasks to the agent. They realized that the team's velocity was tied to how many concurrent agentic tasks they could run. And if they could have the agent do more in parallel, they'd go faster. So they kept looking for ways to scale out their workloads.

[![Thumbnail 6060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6060.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6060)


Finally, the team observed that as they scaled out, they themselves became the bottlenecks. They had to keep unblocking the agents as they came back because they needed human intervention or direction. It turns out that the longer they could get these agents to work independently, the better. One clear example is when they looked at their commit graphs. Not surprisingly, they saw that progress stopped when everyone went to sleep.  They hypothesized that if the agents could use that time to clear the backlog, the team would be able to wake up in the morning with lots more code to review and be able to keep moving faster. 

[![Thumbnail 6090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6090.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6090)

So we sat back and reflected on these learnings, and we asked ourselves, "Why can't we have agents that are able to do all of these things?" And that's why today we're introducing frontier agents. Frontier agents are a new class of agents that are a step-function change, more capable than what we have today. We generally think about three things  that differentiate frontier agents. One, they're autonomous. You direct them towards a goal and they figure out how to achieve it. Two, they have to be massively scalable. Of course, individually they can perform multiple concurrent tasks, but you have to be able to distribute work across multiple instances of each type of agent. And three, these agents need to be long-running. They may be working for hours, maybe even days in pursuit of ambitious, sometimes frankly amorphous goals without requiring human intervention or direction.

[![Thumbnail 6130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6130.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6130)

### Introducing Frontier Agents: Autonomous, Scalable, and Long-Running Development Agents

Let me introduce the first frontier agent we'll be launching today, and that is the Kiro autonomous agent. The Kiro agent is an agent that transforms how developers and teams build software,  vastly increasing your development team's capacity to invent. The Kiro autonomous agent runs alongside your workflow, maintaining context and automating development tasks so that your team never loses momentum. You simply assign a complex task from the backlog and it independently figures out how to get that work done.


Kiro can now autonomously tackle  a full range of things your developer might need, from delivering new features to triaging bugs, even improving code coverage. And all of this takes place in the background so that your engineers can stay in their flow state, focusing on the big ideas. Kiro autonomous agent connects with your tools that you already use like Jira and GitHub and Slack, and it does that to build a shared understanding of your team and your work.

One of the super cool things is that the Kiro agent is just like another member of your team. It actually learns how you like to work and it continues to deepen its understanding of your code and your products and the standards that your team follows over time. It weaves together everything you do, every spec, every discussion, every pull request, and it builds this collective memory that fuels smarter development.

[![Thumbnail 6200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6200.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6200)


Let's take an example. Let's say you need to upgrade a critical library  that's used across 15 different microservices. Now, if you were to do this with Kiro today, you'd have to first open a repo, prompt it to update the library, then you'd review those changes, fix anything it missed, run your tests, and create a pull request. Then you'd move on to repo two and you'd start all over, re-explaining your context, re-prompting for similar changes,  and you'd do that 14 more times. Each time you did it, you'd have to approve the changes, and if you paused or went home for the day or anything like that, you'd have to remind Kiro of all the context when you start back up since it doesn't maintain state in between sessions.

[![Thumbnail 6240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6240.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6240)



Let's take a look and see what this looks like with the new Kiro autonomous agent.  First, you'll get started in kiro.dev and kick off a task associated with your GitHub repo. You'll describe the problem that you're trying to solve, and then the agent uses that,  and it uses all of its reasoning and knowledge from previous implementations to ask clarifying questions that it doesn't understand as it tries to plan tasks. 



With its deep knowledge of your entire code base, it then quickly identifies where it needs to make updates in all the selected repositories where it needs to go update your libraries.  The agent identifies every affected repo that you have, analyzes how every service that you have uses a library, and updates the code following your patterns. It runs full test suites  and then it opens 15 tested, merge-ready pull requests. All of this is in the background while you work on something else.


And to go even faster, it scales out to more parallel tasks,  each with its own context so that while you have Kiro off there implementing your new library, you can also have it fix a bug that you found last night. And this agent isn't session-based.

[![Thumbnail 6310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6310.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6310)

It doesn't forget. When you give it feedback on one of your pull requests about error handling, it applies that learning to the next 14.  When it sees similar architectural decisions in the past, it references the work that you've done before. You're not re-explaining your code base every time. It already knows how you work and it gets better with every single task that it does.

[![Thumbnail 6330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6330.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6330)

We think that this will help you move much more quickly and it's going to completely change the way that you think about writing code. So that's the Kiro autonomous agent, and we're really excited  about how it's going to allow you to ship more code more quickly. But one other thing that our teams quickly discovered as we started writing tons and tons more code is that you can't just accelerate writing code alone. It's only the beginning.

[![Thumbnail 6370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6370.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6370)

### AWS Security Agent: Building Security into Every Stage of Development

You have to make sure that every stage of the software development lifecycle can scale and accelerate at the same rate. Otherwise, you're just going to create new bottlenecks. We realized that the same lessons that we learned, directing outcomes, scaling out, extending agent autonomy, apply to almost every aspect of the development lifecycle. Now, as we've said this once, we'll say it a thousand times, security has always been our number one priority at AWS,  and we've been working with you, all our customers, to help you all secure your products in the cloud for nearly two decades.


So we naturally thought next about what a security frontier agent would look like. We know that every customer wants their products to be secure, but you have trade-offs. Where do you spend your time? Do you prioritize improving the security of existing features or do you prioritize time on shipping new ones? At Amazon and at AWS,  security is so deeply embedded in everything that we do in our development culture, in our practices.


We perform code reviews, we conduct security reviews of systems architecture, we do tons of penetration testing with huge teams consisting of both internal and external experts that look for vulnerabilities all before any code ever reaches production. But it turns out,  most customers can't afford to do this continually. So what happens is either you don't do all of this or you just do it a couple times a year. And now when development is so accelerated with AI, this can mean that there are multiple releases that are going out the door before your code is rigorously assessed for security risks.

[![Thumbnail 6440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6440.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6440)


We have a firm belief that in order to get security right,  you have to build it into everything you do from the ground up. And so I'm very excited to announce the launch of the AWS Security Agent. This agent will help you build applications that are secure from the very beginning.  AWS Security Agent helps you ship with more confidence. It embeds security expertise upstream and enables you to secure your systems more often.

It proactively reviews your design documents, and it also scans your code for vulnerabilities. And since Security Agent integrates directly with your GitHub pull requests, it provides your developers with feedback directly into their workflows. Security Agent also helps with penetration testing. It turns penetration testing from this practice that was slow and an expensive process into something that's an on-demand practice. It allows you to continuously validate your application security posture.

[![Thumbnail 6500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6500.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6500)


I'll quickly show you how it works.  Let's say your company has an approved way of storing and processing credit card information. But let's say you have a developer that inadvertently works with the wrong approach. This can mean a ton of rework,  and late in the development process, it possibly could mean throwing away months of work. However, the AWS Security Agent can catch these issues early.


[![Thumbnail 6540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6540.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6540)

 It can even catch it from your design documents before you write a line of code by always looking to ensure that you're following your team's best practices. Then when the time does come to submit your code, AWS Security Agent can review your pull request against those same requirements and flag any issues, providing you with concise remediation steps  for anything that it finds. When your code's complete, you simply initiate a penetration test and that agent will immediately jump on it, giving you real-time visibility into its progress.


When it's done, you actually get validated findings complete with suggested remediation code to fix any issues that it does find. No more waiting for resources,  no expensive external consultants. And let's say you have multiple apps that are ready to deploy in production. You can just launch multiple Security Agents in parallel so you can test all of your applications and not get bottlenecked. Now, you're writing code faster and you're deploying it just as fast because you know it's secure.


Now, of course, you know what comes next. You have to operate that code. And we all know that as systems grow, the surface area of what you're operating grows as well. And that means growing DevOps work.  This is something that our own teams inside of Amazon have a ton of experience with. At Amazon, we've always believed that the best way to create a great customer experience is to have developers operate their own code. We've been living DevOps for many years, and what we've learned is that frankly, as your service scales, operations can eat up more and more of your time.

### AWS DevOps Agent: Transforming Incident Response and Operational Excellence

We thought this is another area where we could put our expertise in your hands. Introducing the AWS DevOps Agent. This agent is a frontier agent that resolves and proactively prevents incidents, continuously improving your reliability and your performance.

[![Thumbnail 6630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6630.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6630)

The AWS DevOps Agent investigates incidents  and identifies operational improvements, just like your experienced DevOps engineers would. It learns from your resources, their relationships, and including things like your existing observability solutions, runbooks, code repositories, and CI/CD pipelines. It then correlates all that telemetry and code and deployment data across all of those sources and allows them to understand relationships between your application resources, including, by the way, applications in multi-cloud and hybrid environments.

[![Thumbnail 6670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6670.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6670)


Let me show you how this can transform incident response. Let's say an incident happens, an alarm goes off. Before your on-call engineer can even check in,  AWS DevOps Agent instantly responds, diagnosing that it found some elevated authentication error rates from a Lambda function that was trying to connect to your database.  It uses knowledge of your application topology and the relationship between all those different components to independently work back from the alert to find the root cause of the problem.



In this example, let's say you use Dynatrace for your observability solution. The AWS DevOps Agent uses its built-in integration with Dynatrace to provide more context for the incident. It understands all of your dependencies  and knows your deployment stack that created each and every resource. When it's found the problem, let's say in this case it was a change that was made to your Lambda function's IAM policy,  it then tells you what introduced that change. It turns out it was a simple mistake in your CDK code deployment.

[![Thumbnail 6730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6730.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6730)

By the time your on-call engineer logs on, the DevOps frontier agent already has found the issue, suggested a change, and is ready for your on-call to review the change and approve the fix. What's even better is that it lets you prevent such an incident  from happening in the future by recommending some CI/CD guardrails to catch these type of policy changes before they're ever deployed. And that's it. The DevOps Agent is always on call, fast and accurate, making instant response and operations work easy.

[![Thumbnail 6750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6750.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6750)



Together these three frontier agents, Kiro autonomous agent, AWS Security Agent,  and the AWS DevOps Agent, are gonna completely transform the way your teams build, secure, and operate your software. Let's take a quick look at what your future might look like here. Hey, did you run a pen test? Pen test? I'm still working on these unit tests. You're funny. Like we have time for pen tests.  Why does upgrading one package break five others? Weird. Worked for me. Love that for you. Morning. You look nice. Got paged at 4:00 AM. I feel awful. Cheers. Did you use the latest pull request format? Classic. We're green across the board. Yes! Oh my god. Wow! We got it. Look at that. Oh my god. 

Today, I'm excited to announce the next leap forward. We're launching three new frontier agents. These agents can reduce a lot of the time that's spent on these really important but repetitive, time-consuming and frankly, unfulfilling tasks. This takes what used to be months of work into hours. Whoa. They are gonna transform the way you and your teams build, secure, and operate software. Any dumpster fires last night? Nope. Solved for me. Approved it and went back to sleep in a few minutes. Look at you.

Thank you. All right, let's go. I've got some fire ideas for you guys. Oh, cool. Now, with added pen tests.


[![Thumbnail 6950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/6950.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=6950)

We think that we're only at the beginning of frontier agents, and we're super excited to see what you all achieve with them.  System automatically balances the load across the charging stations. That's awesome. Good work. I think we can all agree that's a future we'd be excited about. Today is a big leap forward in the journey unlocking the value of AI. We're bringing you powerful innovations at every single layer of the stack. 

### 25 Core Service Launches in 10 Minutes: Compute, Storage, Security, and Database Innovations

The innovation that's happening across AI and agents today is truly incredible. But it turns out it's not just our AI and agentic services that are developing a ton of new innovation this week here at re:Invent. There's a bunch of launches that AWS is very excited about. And because AWS is so broad, I know many of you were hoping to hear about our fantastic additions to our core non-AI services as well.

[![Thumbnail 7010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7010.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7010)

It turns out it's actually one of the hardest things of planning and doing this re:Invent keynote. What do you cut from the talk? How do you fit it all in? Well, teams asked us, when our teams were delivering, they said, "I'm gonna keep doing the pace of innovation." And I said, "Well, I don't know how to fit it into one keynote." But I said, "You know, why not try?" So I said,  "If our AWS teams can deliver at such a rapid pace, I can up my keynote game too."



I'm gonna try, we'll see. So if everybody can hang with me  for just a few minutes longer, we're not done yet. I have 25 exciting new product launches across our core AWS services to unveil,  and I'm gonna give myself just 10 minutes to do it. To keep me honest here, the team is rolling out a shot clock and you all can keep track. Okay. All right. All right. Buckle up, everybody. Let's get to it.



[![Thumbnail 7080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7080.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7080)

Let's start with our compute offerings.  We know that one of the things that you all love is that AWS continues to offer the broadest selection of instances. So you always have the best possible instance for your application.  Now, lots of you run memory-intensive applications out there like SAP HANA or SQL Server or EDA. So today, I'm excited to announce our next generation of X family of large memory instances. They're powered by custom Intel Xeon 6 processors, and these instances can provide up to 50% more memory. 


And I'm excited to announce the next generation of our AMD EPYC memory processors as well, giving you three terabytes of memory. Now, you've also told us that you have a lot of really demanding CPU-heavy applications out there, like batch processing and gaming.  So today, we're launching our C8a instances, which are based on the latest AMD EPYC processors and give 30% higher performance.

[![Thumbnail 7120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7120.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7120)

Many of you also run EC2 instances that run security or network applications. And those applications need a lot of compute and super fast networking. For those, we're announcing our C8ine instances  powered by custom Intel Xeon 6 processors using the latest Nitro v6 cards. These instances deliver 2.5 times higher packet performance per vCPU.


What about applications that need really ultra-fast single-thread frequency compute? You got that too. Introducing our M8azn instances, with the absolute fastest CPU clock frequency  available anywhere in the cloud. These instances are ideal for applications like multiplayer gaming, high-frequency trading, and real-time data analytics.


Today, AWS is still the only provider that offers Apple Mac-based instances, and they are really popular. So today, I'm happy to announce two new instances  powered by the latest Apple hardware, announcing the EC2 M3 Ultra Mac and the EC2 M4 Max Mac Instances. Developers can now use the latest Apple hardware to build, test and sign Apple apps in AWS.

[![Thumbnail 7180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7180.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7180)


All right, customers love using Lambda to quickly build functions and run code at scale.  Lambda works great when you want to execute code quickly, but sometimes you have a use case where your Lambda function needs to wait for a response, like waiting on an agent that's working in the background for several hours or maybe even days. We wanted to make it easy for you to program wait times directly into your Lambda functions. So today, we're announcing Lambda durable functions. 

Durable functions make it easy for you to manage state and build long-running workloads with built-in error handling and automatic recovery. How are we doing? Eight launches in about three minutes. I better pick it up.


[![Thumbnail 7250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7250.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7250)

Let's move on to storage. We know you love S3. I mentioned earlier that S3 stores more than 500 trillion objects, hundreds of exabytes of data. That is a lot of data. When we launched S3 in 2006, we had a five-gigabyte maximum object size. Then a couple years later, we increased that to be five terabytes, and that has been sufficiently large for the past decade.  But data has gotten a lot bigger in the past couple of years. So we asked ourselves, what would be the object size that would meet all of your needs today? Should we double it? Triple it?  How about 10x it?



I'm pleased today to announce that we're increasing the maximum object size in S3 by 10x to 50 terabytes. But not just bigger, though.  We knew you also wanted to make S3 faster for batch operations. So starting today, we're improving the performance of batch operations where large batch jobs now run 10x faster. 

[![Thumbnail 7280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7280.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7280)


Last year at re:Invent, I announced S3 Tables, which is a new bucket type optimized for Iceberg tables. It's been incredibly popular,  but as the volumes of table data has started to quickly rise, you all have asked for ways that we can help you save money. So today, we're announcing Intelligent-Tiering for S3 Tables.  This can save you up to 80% on storage costs for the data in your S3 table buckets automatically.


[![Thumbnail 7310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7310.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7310)

[![Thumbnail 7340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7340.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7340)

You also asked us to make it easier  to replicate these tables between regions so that you can get consistent query performance from anywhere. So as of today, you can now automatically replicate your S3 tables  across AWS regions and accounts. Earlier this year, we introduced S3 Access Points for FSx for OpenZFS. This allows you to access your ZFS file system information as if it was data inside of S3. And today, we're making it possible for you to access even more of your file data this way by expanding S3 Access Points to FSx to include support for NetApp ONTAP. Now, ONTAP customers can also access their data seamlessly, just as if it was in S3. 


[![Thumbnail 7360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7360.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7360)

Now, one of the fastest growing data types that you all have are vector embeddings, which are used to make it easier for your AI models to search for and make sense of your data.  Earlier this year, we announced a preview of S3 Vectors, which is the first cloud object store with native support to store and query vectors. And today, I'm happy to announce  the general availability of S3 Vectors. You can now store trillions of vectors in a single S3 bucket and reduce the cost of storing and querying them by 90%.



Now, I expect that many of you will use S3 Vectors in concert with a high-performance vector database. Today, the most popular way to do that at low latency search of your vector embeddings  is with an index in Amazon OpenSearch. But many of you have asked us, is there a way that you can speed up the process of creating that index for all of my data? So today, we're excited to announce GPU acceleration for vector indices in Amazon OpenSearch. By using GPUs to index that data,  you can now index data 10x faster at one-fourth the cost.


How are we doing? Four minutes left? Awesome. Fifteen launches and just a few minutes left. Let's keep going. Let's move on to EMR, which is our popular big data processing service. We launched EMR Serverless four years ago and customers love it because it takes a lot of the muck out of running petabyte-scale processing, but it turns out today, it isn't quite muck-free. Customers still have to provision and manage their EMR storage. But not anymore.  As of today, we're eliminating the need for you to have to provision local storage for your EMR serverless clusters.


[![Thumbnail 7480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7480.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7480)

Let's move on to security. Today, tens of thousands of you rely on GuardDuty to monitor and protect your accounts, applications, and data from threats. This past summer,  we added GuardDuty's extended threat detection to Amazon EKS, and we're pleased with the momentum we're seeing. So of course, naturally, we didn't stop there. And today, we're adding this capability to ECS. Now, you can use AWS's most advanced threat detection capabilities for all of your containers and all of your EC2 instances as well. These are both enabled for all GuardDuty customers at no additional cost. 


Every customer wants to find and fix security issues quickly. The faster and easier, the better. That's why we have Security Hub, which aggregates security data from AWS and third-party sources and helps you identify potential security problems. Earlier this year, we previewed an enhanced version of Security Hub, and today, I'm excited to announce that Security Hub is generally available.  Today, we're also announcing several new capabilities, including near real-time risk analytics,

a trends dashboard, and a new streamlined pricing model. Operations teams out there live and die by their log data, but that log data is everywhere. They're in CloudTrail logs and VPC flow logs and WAF logs and logs from third parties like Okta and CrowdStrike. We thought that we could make that better. So today, we're announcing a new unified data store in CloudWatch for all of your operational security and compliance data. It automates log data collection from AWS and third parties and stores it in S3 or in S3 Tables to make it easier and faster to find issues and unlock new insights.

[![Thumbnail 7530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7530.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7530)


 All right, stick with me everyone. We're almost there. We're on the home stretch. Let's move on to databases. Now, I'm not going to make you raise your hand, don't worry. But I know that many of you out there are still supporting  some legacy SQL Server and Oracle databases. Getting off of them is hard, but at least AWS makes it easier to manage them. We see you out there, and don't worry, we're here to help.

[![Thumbnail 7580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7580.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7580)

One thing I hear from many of you is that your legacy databases have grown very large over time. They're actually bigger than what we support in RDS. So I'm excited to announce we're increasing the storage capacity for RDS for SQL Server and Oracle from 64 terabytes to 256 terabytes.  This also delivers a 4x improvement in IOPS and I/O bandwidth. This is going to make it a lot easier to migrate existing workloads from on-premises to scale them on AWS.



We also want to provide you with more controls  to help you optimize your SQL Server licenses and manage your costs. So we're doing a couple of things. Starting today, you can specify the number of vCPUs that are enabled for your SQL Server database instance. This helps you reduce your CPU licensing costs that you get from Microsoft. And today, we're also adding support  for SQL Server Developer Edition, so you can build and test your applications with no licensing fee.


Oh, the music's coming on. All right, that means I only have a few seconds left, but there's one last thing that I think all of you are going to love. Several years ago, we launched Compute Savings Plans as a way to simplify making commitments across our entire set of compute offerings. And since that day, I've regularly been asked, "When can I get a unified savings plan for databases?" And here it is.  Starting today, we're launching Database Savings Plans. These can save you up to 35% across all your usage for database services.

[![Thumbnail 7660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d515ff501cec6884/7660.jpg)](https://www.youtube.com/watch?v=q3Sb9PemsSo&t=7660)



All right, boom. Did it.  A whole second keynote's worth of new capabilities for all of you in under 10 minutes. Now, you have four full days to go out and learn. Dive deep into the details and start inventing.  Thank you all for coming to re:Invent. Enjoy. 


----

; This article is entirely auto-generated using Amazon Bedrock.
