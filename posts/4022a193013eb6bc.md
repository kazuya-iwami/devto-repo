---
title: 'AWS re:Invent 2025 - Build serverless chatbots using Amazon ElastiCache & Aurora PostgreSQL (DAT314)'
published: true
description: 'In this video, Shayon Sanyal demonstrates building serverless chatbots using ElastiCache and Aurora PostgreSQL through a fictitious travel platform called Flightly. He addresses how slow response times (30 seconds) cost businesses $50 million annually due to 7% conversion loss per second of delay. The solution involves adding ElastiCache as a caching layer achieving sub-millisecond responses with 90%+ cache hit rates, while Aurora PostgreSQL with pgvector handles semantic search. Five reusable patterns are presented: context cache, embedding cache, durable semantic caching, tiered memory management, and multi-agent workflows. The architecture evolves from basic Q&A to agentic AI using Strands Agents SDK and Model Context Protocol (MCP), ultimately scaling to production with Bedrock AgentCore handling millions of daily queries.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/0.jpg'
series: ''
canonical_url: null
id: 3093161
date: '2025-12-08T20:54:23Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Build serverless chatbots using Amazon ElastiCache & Aurora PostgreSQL (DAT314)**

> In this video, Shayon Sanyal demonstrates building serverless chatbots using ElastiCache and Aurora PostgreSQL through a fictitious travel platform called Flightly. He addresses how slow response times (30 seconds) cost businesses $50 million annually due to 7% conversion loss per second of delay. The solution involves adding ElastiCache as a caching layer achieving sub-millisecond responses with 90%+ cache hit rates, while Aurora PostgreSQL with pgvector handles semantic search. Five reusable patterns are presented: context cache, embedding cache, durable semantic caching, tiered memory management, and multi-agent workflows. The architecture evolves from basic Q&A to agentic AI using Strands Agents SDK and Model Context Protocol (MCP), ultimately scaling to production with Bedrock AgentCore handling millions of daily queries.

{% youtube https://www.youtube.com/watch?v=eCiR1mDe5Rw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/0.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=0)

### The $50 Million Problem: When Slow Chatbots Cost Flightly Its Customers

 Good afternoon, everyone. Welcome to day four. I can't believe it. How are we doing, surviving? Yes, good. Well, you made it to day four, and I'm super excited that you've decided to join me for the next 45 minutes or so attending this session. This is session 314. My name is Shayon Sanyal. I'm a Principal Database Specialist at AWS. Today, we are going to dive deep into how to build serverless chatbots using ElastiCache and Aurora PostgreSQL.

Who's using Aurora in production today? Just a show of hands. Oh, majority of you, okay. And who's building chatbots using PostgreSQL as the database? No one. Okay. Well, you're in the right room. So let's dive in.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/60.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=60)

 Our case study today is Flightly. It's a fictitious travel platform that allows customers to search flights, book hotels, and plan vacations. Today we'll crack open the architecture. We'll help Flightly build it. And we'll also see how to evolve this basic chatbot that Flightly has into an agentic AI system. Along the way, we are extracting some reusable architecture patterns that hopefully you can apply in your own systems. Let's dive in.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/90.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=90)

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/100.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=100)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/110.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=110)

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/120.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=120)

 Friday afternoon, I'm planning a vacation to Hawaii. I type my query in Flightly, and it starts thinking. Still thinking, still thinking, still going.  So I do what we all do. I go and check my email. And very quickly I'll notice that this Island Air competitor has a flash sale. Hawaii flights, ironically, great price.  Back to Flightly, still thinking. I book with Island Air. Done.  Flightly just lost me as a customer, and probably thousands of others, not because the product was bad, but the chat was just too slow.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/160.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=160)

Let me show you what this costs the business. Thirty seconds average response time for a single booking. About half of their users hit this performance wall. Average booking value is about 300 bucks. They handle a million requests daily. Now, industry data shows that every second of delay costs you 7% in conversions. With super long response times, even a tiny fraction of users abandoning adds up very quickly.  So, if you just do the math, it's about $1,500 lost every day, which compounds to $50 million annually. So how do we fix this?

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/170.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=170)

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/190.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=190)

 Here's the thing. When Flightly started, they didn't see this coming. They hoped they were building for speed and scale like everyone does, but the team did what most of us would do. They built an MVP. They started quick, launched the platform, user adoption accelerated.  But success created new problems. Thousands of users became millions, and suddenly they are hitting this entirely new set of bottlenecks that obviously doesn't appear unless you're at this scale. So the question kind of shifted. How do you maintain sub-second response times when you're processing millions of requests per day?

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/220.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=220)

 Here's what Flightly's working with: customer data and company data. It's similar data to what we all have in our databases. Customer data includes things such as past bookings, search history, preferences, etc. Company data is flight inventory, hotel inventory, promotions, etc. All of this data lives in Aurora PostgreSQL. Years and years of rich data. Perfect foundation for a chatbot.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/240.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=240)

 But here's the problem. Without a cache, every query hits the database. Every semantic search recomputes vector distances. At a million requests per day, you get exactly what we saw: painful response times. Now, your instinct might be, let's migrate to a faster database. But that includes new infrastructure, new ETL pipelines, migration risk. And Aurora's been rock solid for years. The database isn't the problem, it's the access pattern. So we keep Aurora as our source of truth and add a caching layer. Let's keep that in mind.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/280.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=280)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/290.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/300.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=300)

### Anatomy of a Bottleneck: Understanding the Current Architecture and Its Breaking Points

 Let's understand the current architecture. User query comes in, find me a beach resort in Bali.  The system queries Aurora as a knowledge base. It pulls relevant context, things such as availability, seasonal pricing, and the user's preferences.  All of this gets assembled into a context-aware system prompt, which is nothing but the instruction set for the LLM.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/310.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/320.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=320)

The system prompt does two things.  It defines behavior, so answer travel questions, reference the knowledge base, maintain conversation history, and it sets personality. Be warm, enthusiastic, helpful.  This enriched prompt plus the user's question gets sent to a large language model, which then generates a tailored response.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/330.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=330)

Now, for  a single turn conversation, a single interaction with a single user asking this question, this architecture works great. This is perfect. But watch what happens when we scale this to thousands of users concurrently asking the same questions or different questions. So now we have multiple users hitting the system. And in travel, I like to think of it as the potato rule. It's 80% of your queries are just different questions. They are just 20 questions wearing different outfits.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/390.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=390)

Now watch the latencies stack up. The orange lines are your warning zones. The red lines are now we are hitting the limits of the system. Every single query triggers the exact same expensive pipeline. Your query hits your database, there is a distance calculation, fetches context, assembles the system prompt, and sends it to the LLM. Each operation adds about 100 milliseconds, and that stacks up.  You'll notice very quickly you're spending over 0.2 seconds, over 500 milliseconds per request, and this keeps getting worse.

Response times start degrading from 500 milliseconds to 5 seconds to 30 seconds. At a million users daily, this architecture is unsustainable. Ask your application teams what's wrong and I'm sure they'll all point straight to the database. But it's not the database's fault, it's the access pattern.

### The Speed Layer: Why ElastiCache Is Essential for Conversational AI at Scale

All right, we touched on caching a little bit earlier. Let's dig into why it's essential for conversational AI at scale. When a user asks about baggage policies, frequently asked questions, a cache can serve that in under a millisecond. If it was the database, that's disk IO, buffer lookups, query parsing, query execution, and so on. And that's about 50 milliseconds, even with solid state drives, SSDs. And every database connection means a TCP handshake, a TLS negotiation, cross availability zone round trip. That's about 5 milliseconds per each iteration, and that's just the protocol overhead before your query even hits the database.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/490.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=490)

API economics. Show me flights to Hawaii probably gets asked thousands of times every day. Each uncached request burns inference costs. There's embedding generation, there's LLM invocations. You cache once and you amortize the cost across the fleet. Finally, by definition, semantic search and semantic lookups mean cosine distance calculations  across these 1024, 1536, 3072 vector dimensions, and you probably have millions and millions of those vectors. So caching those top K results skips those heavy floating point cosine distance operations entirely for those repeat queries. That's the kind of thing that we are looking for. This is the speed layer that we're going to add to our architecture.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/520.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=520)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/550.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=550)

Now, this is a great segue on why caching is essential. Let's take a  little detour to the service side of things. Amazon ElastiCache is our fully managed caching service. It's Redis OSS compatible, Memcached compatible, and Valkey compatible. Your application connects to a single DNS endpoint using standard Redis or Valkey protocol. Behind it is a managed proxy fleet that handles connection multiplexing across this distributed cluster, and it gives you nice features such as consistent hashing,  async replication across availability zones, sub-millisecond failover, and so on. Of course, you throw in the service benefits, scaling is automatic.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/570.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=570)

Now, this is the speed layer that eliminates Flightly's bottleneck. Let's see how it transforms the architecture. So ElastiCache's shared cluster architecture  serves data directly from memory, regardless of which availability zone the data lives in. So it guarantees consistent microsecond P99 latencies, whether you are at 100 requests per second or 100,000 requests per second.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/600.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=600)

Now, just before re:Invent, I think about a week before re:Invent or so, we launched vector search for ElastiCache. And with it, durable semantic caching. With ElastiCache Valkey, durable  semantic caching, we are actually hitting 90% plus cache hit rates for repeat queries.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/610.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=610)

What that means is 9 out of your 10 requests get served from memory. 

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/630.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=630)

Third is resilience. Every shard maintains read replicas across availability zones. When hardware fails, and during December holiday traffic, there are high chances that hardware might fail, ElastiCache automatically detects and promotes a read replica to become the new writer.  And of course, holiday traffic is inherently unpredictable. Spring breaks might spike 15 times over a normal February baseline. So ElastiCache can monitor that and provision additional shards as needed to accommodate that growth.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/660.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=660)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/680.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=680)

### The Intelligence Layer: Semantic Search with Aurora PostgreSQL and pgvector

Now we've solved speed, we are getting sub-millisecond responses. There's another component to this. Now we need intelligence. Your users aren't asking, select star from destinations where activities like beach.  They're asking, where can I surf and party. And they don't repeat exact queries. It's the intention. They express intent about those queries. So all of this relates to something called semantic search. Let's see how semantic search actually works. 

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/690.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=690)

You start with your domain knowledge. This would be travel guides, hotel descriptions, visa policies, etc. For Flightly, it's all in Aurora PostgreSQL.  You break this corpus into bite-sized chunks. And then you feed this data through an embedding model. Your text in those documents becomes multi-dimensional vectors, and each dimension captures semantic features, so similar concepts like beach and coast clustered together. Dissimilar concepts will stay apart.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/720.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=720)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/750.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=750)

In practice, Miami's embeddings will look something like this, pristine beaches plus world-class clubs  across, let's say, those 1024 dimensions. Bali's encoding represents tropical surf beaches plus wellness spas. Both contain beach components, mind you. But then Miami leans towards this urban and nightlife theme, whereas Bali leans more towards spiritual and wellness themes. These embeddings get stored in a vector database. Aurora PostgreSQL has pgvector, just a show of hands who's heard of pgvector. 

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/770.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=770)

OK, quite a lot of you, good. So pgvector for those uninitiated is an extension in PostgreSQL for storing and querying vector embeddings. And this enables that semantic lookup, semantic search. On the other side, now, when your user comes and asks this question, where can I surf and party?  That query goes through the same embedding model and gets converted into a 1024 dimensional vector, for example.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/790.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=790)

Now we run semantic search against Aurora with pgvector. This is what the query looks like. You never see it, the model handles it behind the scenes.  What you do see are top matches, Miami and Bali, ranked by a similarity score. Now this is the intelligence layer. So we had speed, now we added the intelligence layer. This is how Flightly also moves beyond a simple keyword match to understanding true user intent.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/840.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=840)

Now, while this slide focuses purely on semantic search using vector embeddings, traditional keyword based full text search cannot be overlooked. So PostgreSQL does have a fully featured full text search component as well. And in practice, many of these search systems combine both, a technique which we call hybrid search. Now, here, we are centered on semantic search, but if you are interested, there is also a session  tomorrow on hybrid search, 409.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/850.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=850)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/870.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=870)

Now, a quick detour to the service side. AWS offers several managed  databases with native vector search, and we keep adding more and more databases to this whole environment and ecosystem. RDS for PostgreSQL supports vector search, so does Aurora PostgreSQL, DocumentDB, OpenSearch, etc. Well, Flightly chose Aurora PostgreSQL since their data already lived there. It was an easy choice. 

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/880.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=880)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/900.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=900)

Now in the Aurora serverless architecture, flight search is usually very spiky, it depends on seasonality, flash sales, etc. The way the architecture  works is again, you have this decoupled compute and storage layer. And with Aurora capacity units, you can control the level of provisioning required for scaling up or scaling down. Now, Aurora I/O Optimized, which is a storage configuration, handles this I/O intensive vector search, plus your regular transactional  workloads, which is bookings, payments, etc.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/920.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=920)

There is no performance degradation because of better techniques that we employ with DynamoDB optimized configuration, plus it gives you the added benefit of predictable costs. Data locality is a big win. Vector embeddings live  in the same PostgreSQL tables as your transactional data. This includes your booking and customer profiles. No external database is needed, no ETL pipelines to set up, no migration risks, and so on.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/940.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=940)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/960.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=960)

Finally, there is the global reach perspective. A global database replicates across regions with  subsecond replication lag. A user in Tokyo gets served from the Asia Pacific replica, while a user in London gets the EU replica. Global data, local read performance. All right, now we have the complete architecture. ElastiCache provides this sub-millisecond performance  for cached data. Aurora is our durable semantic intelligence layer through pgvector. Let's see how these two integrate together. We'll start with the foundational architecture and add in layers of sophistication as we go.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/980.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=980)

### Building Sophistication in Four Steps: From Simple Q&A to Contextual Intelligence

Step one is simple Q&A. This is the baseline pattern. I ask, can you book flights from Vegas to Hawaii?  The query routes directly to Bedrock. There's no preprocessing, no context assembly, just raw user input. The LLM comes back with a response using only its pre-trained knowledge, and this is typically very generic. There is no personalization here, nothing particular to me as a user.

Now, here's the architectural challenge. By definition, LLMs are stateless. They don't remember anything. They don't maintain any kind of session state or continuity between interactions. The moment I close this chat, conversation history is gone. We need a way for the chatbot to remember. So let's add conversational memory using ElastiCache.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1030.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1040.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1050.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1060.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1060)

Step two is adding that conversation  history or chat history. Now ElastiCache is layered in to store that conversation history. When this query arrives, which is a follow-up,  I'm not asking my previous question. It's, what about hotels there? It's assumed that ElastiCache now has the previous context, and it can supply that context to my  prompt, which will again give me a tailored response. This is the shift. We have now transformed from stateless API calls into stateful conversations.  ElastiCache provides this persistent session memory. But notice, the responses here are still fairly generic. There's no personalization here just yet.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1080.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1090.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1090)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1120.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1120)

Step three is adding that personalization layer. Now we are evolving  beyond the basic conversation history. I ask another follow-up, recommend activities for my family. Now ElastiCache's role in this case expands.  We are now caching user profile data. This will be hashes containing family composition. I have two kids, ages four and eight, activity preferences like outdoor adventures, budget constraints like a three hundred dollar maximum. Now, when this query arrives, using ElastiCache get all, you can retrieve the complete preference hash and inject it into the prompt. The LLM will now generate a very tailored response based on my personalized information.  Activities appropriate for ages four to eight, aligned with outdoor adventures.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1150.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1150)

At this stage, we are not just remembering, we are enriching context with persistent user attributes. It knows information about me, my family, and so on. ElastiCache in this use case provides both ephemeral session memory and durable preference storage. This is the shift from remembering to understanding.  Now we are on to the final stage. This is contextual intelligence, also called Retrieval-Augmented Generation.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1160.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1160)

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1170.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1170)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1180.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1180)

I ask, cheapest flight options for next month.  That's it. Aurora PostgreSQL enters the picture. The application generates an embedding, executes a semantic search against the flight model,  routes matching cheapest options filtered by whatever date range that I provide, joined with real-time pricing to give me a very tailored  response. This is RAG in action. We are now using ElastiCache for that sub-millisecond session state and preference lookup. But we are also using Aurora for grounding the data in our domain knowledge through semantic search. Together they are providing this fast, personalized, and accurate customer experience.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1230.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1230)

### From Chatbot to Agentic AI: Taking Actions with Strands Agents and Model Context Protocol

So far, in four steps, we've built a pretty sophisticated conversational system. It has memory, it has chat history, it has personalization, and it has contextual intelligence. But remember, we are still at the stage of a chatbot. What if we could go beyond simple Q&A? What if it could go beyond just giving us information about hotel availability, flight availability, and go one step further and start booking those for us?  That's the shift to agentic AI, from answering questions to taking actions.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1270.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1270)

Before we get into the agentic layer and the architecture, let's see how we got there. It's important to keep this perspective in mind. Two years ago, we started with generative AI chatbots. Single-turn, rule-based, requiring constant human oversight. Then came generative AI agents, goal-driven systems that plan, reason, and use tools. A booking agent will go and search flights, compare prices, execute workflows, but still single domain.  Now we are stepping into the era of agentic AI. Multi-agent systems where specialized agents coordinate very nicely. Your booking agent, your customer service agent, your recommendation agent, all communicating and delegating with minimal oversight. This is what we are building for Flightly.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1290.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1300.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1300)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1310.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1310)

Let's look at the architecture.  So, same travel question as before. The agent retrieves cached context in this case from Valkey, all in parallel. Remember,  so ElastiCache for user profile, ElastiCache for conversation history, ElastiCache for session state. Sub-millisecond retrievals, super quickly you assemble your complete user context.  Simultaneously, we are doing a semantic search against Aurora. Query vectorization, similarity search. This will be typically in two-digit milliseconds. User personalization from Valkey, domain knowledge from Aurora.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1330.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1330)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1360.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1360)

Now is where it gets interesting. Tool invocation through another concept called Model Context  Protocol. So MCP provides a standardized interface for LLMs to communicate with various services, things such as APIs, databases, what have you. The agent can also call external APIs, things such as weather forecasts, travel advisories, FAA regulations, and so on. So the result of all of this becomes a super personalized, actionable response. The agent asks  if you would like to book or explore other options.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1370.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1370)

This is promising, but how do we actually implement this? This is where Strands Agents comes  in. It's an open-source SDK for building AI agents. Lightweight, model agnostic, supports Bedrock natively, but it also goes beyond and has integrations with OpenAI, Anthropic, Ollama, and so on. It handles this multi-agent orchestration, provides native MCP tool support with over 85 tools available. And you can also integrate your existing APIs with custom tools. We'll look at a few examples later.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1410.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1410)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1420.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1420)

Let's look at the code. So, building an agent with Strands is relatively straightforward. You have certain primitives and you just have to implement those. First, you import the core primitives  and configure your LLM. In this case, we are using Claude Sonnet 4 on Bedrock. Second, you create the agent. It's as simple as that, one line of code. That's  it. So, now you have a functional AI agent. But it's super limited, right? It can only do simple Q&A. It doesn't have any capabilities just yet.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1450.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1450)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1460.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1460)

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1470.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1470)

So let's add capabilities. Now we start defining the system prompt. Remember, this is the personality of the agent. Flightly's personality, domain expertise, behavioral constraints, all go into the system prompt. You are Flightly, a travel assistant specializing in flight booking and travel itineraries.  Provide clear, concise, and professional responses with helpful options and pricing. So if a query comes in, something like flights from Vegas to Hawaii,  the agent responds in character. It'll have a travel domain focus, it'll use a professional tone, and give you actionable options with the pricing. 

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1500.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1500)

But remember, agents can also take actions. This is still a simple Q&A. So let's add some tools, which is the ability to act. So, the fastest way to give your agent access to the Aurora PostgreSQL database is via the Aurora PostgreSQL MCP server, which becomes a tool for the agent. With the Aurora PostgreSQL MCP server, you connect directly via standard connection parameters, the ones that you're already used to. So your hostname,  your database credentials. There is no custom tool implementation. You are not writing any handwritten SQL wrappers.

The benefit is there is no maintenance overhead. Anytime your schema evolves, you can just use your standard credentials to connect. Now, Strands provides a list tool sync API which you'll see on line number 16, which discovers these tools automatically. This could be your MCP servers, plus your custom tools, APIs, databases, and so on.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1550.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1550)

Now, here, Aurora PostgreSQL as an MCP server exposes a couple of tools: Get Table Schema and Run Query. Get Table Schema is for schema discovery, and Run Query is for running SQL queries. So in this case, when we import our tool and then make the tool invocation, the call to the database is about two-digit  milliseconds, which is typically what you'll have with a direct connection as well. But then, notice this line. There is a persistent MCP connection. There is zero overhead of reconnection.

Anytime in PostgreSQL, you're trying to re-establish a connection, there is a lot of overhead. There is TCP handshake, TLS negotiation, authentication, and so on. So MCP eliminates this whole bottleneck and uses a persistent connection to the database. The other benefit is, of course, you don't have to write your own SQL query. If you don't know SQL, your agent handles that for you.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1610.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1610)

All right. But your MCP servers can actually go beyond these databases too. Remember we talked about weather APIs? So in this example, let's add a weather API. Same pattern, now for weather. There is a special symbol that we use, the MCP tool decorator, which  you'll see. This defines a function calling Open Weather API externally. And this code snippet actually exposes the weather data API as an MCP tool to the agent. Our Strands agent connects in the same way, just like it did to Aurora. Same protocol, different capability.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1630.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1630)

 But here's the production challenge: a million queries daily. Many semantically similar, like weather in Hawaii, weather forecast, Hawaii forecast, temperature in Honolulu, and so on, asked by a number of different users. Each triggers an API call. Expensive, slow, and unnecessary. So we need some sort of intelligence that goes beyond the standard MCP servers. We need intelligence with tool caching at the tool layer.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1670.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1670)

So, standard MCPs don't cut it. So let's go a little bit beyond and add in custom tools. Now, I've mentioned tools a lot. Tools  are nothing but simply Python programs, functions that give business logic or execution logic to your agent. Simply Python functions. The tool decorator here gives you full control over execution logic.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1690.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1690)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1700.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1700)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1710.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1710)

Here's an example. So search flights takes origin, destination, optional date.  But then the docstring, which is the argument, provides schema information the agent uses to understand when and how to invoke it. Here's where it gets powerful.  The logic defines that check Valkey first. If the thing exists in my cache, return the results in under a millisecond. If it doesn't,  query Aurora. There is a first-time penalty. It'll hit the database, yes, but cache the subsequent results to the Valkey layer. Again, predictability is maintained there.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1740.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1740)

The key takeaway you'll notice here is you want to use MCP for things like schema discovery and non-critical paths, but you want to use custom tools when you need capabilities beyond what MCP provides or when performance is critical.  This is the user output. Anytime your query doesn't hit the cache, it's going to hit the database. Again, two-digit milliseconds, about 85 or so. But then the second time is the benefit. We start getting those benefits right away with the Valkey caching layer.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1760.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1760)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1770.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1770)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1780.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1780)

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1790.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1790)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1800.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1800)

### Five Reusable Architecture Patterns: Blueprints for Production-Ready Conversational Systems

All right. So we've  built Flightly's complete architecture. Now, let's extract some reusable patterns that we'll use, which are basically blueprints you can apply right away to your own applications.  Pattern one is context cache. John asks, what about hotels there? The agent goes and queries Valkey,  retrieves John's complete context, remember the whole parallel operation, chat history from the Wi-Fi discussion, session state, user preferences, and so on.  Simultaneously, it runs a semantic search with Aurora database. In the end, all of this generates a contextual, context-aware response,  taking in John's preferences, and so on. And this pattern is universally applicable.

You can apply this to customer service agents, imagine maintaining ticket context, virtual assistants which need to remember preferences across sessions, healthcare chatbots tracking patient history. Any system where conversation continuity matters and you need that quick context retrieval, this pattern will apply.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1840.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1850.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1860.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1870.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1870)

Pattern number two is embedding cache. This is where we accelerate the semantic search by caching vector embeddings. Pretty neat.  Think about users searching for things to do in a new place. These are almost always short keyword-based lookups. And chances are a lot of those users  are asking the same repeat questions all the time. If I go to Japan, iconic temples in Kyoto, best food in Tokyo, and so on. The first encounter, of course, is going  to be to the database. And there's this whole embedding pipeline that you have to go through, typically completes in about less than 200 milliseconds. But then the subsequent  retrieval is through the caching layer, which is where we get the advantage. And given that these are serverless services, both of them can scale in place, up or down, in or out.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1890.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1890)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1900.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1900)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1910.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1920.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1920)

Now pattern three is interesting. This is the durable semantic caching by ElastiCache. So your data sources  come as input to your embedding model like before, this is the whole pipeline. But a lot of the times, remember I said people phrase things differently. They won't be asking the same questions.  What are the best beach destinations in Hawaii? Can you recommend beach resorts in Hawaii? And so on and so forth.  Now, without semantic caching, this goes through this entire pipeline to get to the response. Now, there is, of course, LLM invocations,  there is embedding generation, there's a lot that's going on. And there is latency plus cost.

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/1950.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=1950)

With semantic caching, what happens is you define the similarity threshold, and beyond that threshold, anytime you have reached that threshold, all of the questions which are similar get clubbed into a group, and then the responses get returned from the cache. So you're caching both the similarity plus the LLM responses in your Valkey layer. And what happens is, by doing so,  you skip the entire pipeline generation process. And you're just going to the cache and retrieving results. Zero LLM cost, no latency, and sub-millisecond performance. There was actually a deep dive session on semantic caching. It was on Monday. So I highly recommend watching the recording on YouTube, session 451. This goes much deeper into semantic caching.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2000.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2000)

This is how it looks like in action. Green is good, red is bad. With semantic caching on the left, questions one through four, we get instant responses from Valkey. This is what we want, smooth, great performance, keeps users engaged. More importantly, bookings complete on time. Without,  same question triggers the full LLM inference every single time. What's worse is your users notice these delays, and then they start abandoning conversations mid-booking.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2020.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2020)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2030.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2030)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2040.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2040)

Pattern four is interesting. This is where we get into a little bit of production architectures, tiered memory management.  User says, book family flights there, avoid that airline. Referencing maybe context from earlier, maybe previous sessions, and so on.  The agent passes messages to Valkey for short-term memory in this case. We'll see it go through. And then, Valkey, in this case, you'll notice  it stores things like chat messages, which is your conversation history, session state, checkpoint. These are typically workflow checkpoints if your agent is going through certain workflows, pipelines, jobs, or just a basic conversation. You can define checkpoints so it can pick up where it left off, similar to database checkpoints. All of that can be cached and stored in Valkey.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2070.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2070)

Now, the beauty is, you can actually have this intelligence behind the scenes.  Your Lambda asynchronously extracts the short-term nuggets of memory, so all your conversation history, session state, checkpoints, and so on. And push it out to Aurora as your long-term storage. Within Aurora, with pgvector, the benefit that you get is you can do things like episodic recall. pgvector has every past history stored as long-term memory with a session ID, memory ID, the whole interaction.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2130.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2130)

What you can do is rebuild that entire conversation by looking at that long-term tiered storage. For this use case, things like this user prefers aisle seats or window seats, they have a budget, a strong budget of maybe $300 and less. These are the things which will be persisted in the long-term storage, which can be looked upon across devices, across channels every single time. And of course, you also have the Model Context Protocol, or MCPs, thrown in.  Within the MCPs, you can do things like you can extract these memory modules, you can list these memory events sequentially, serially, you can retrieve these memory records, rebuild it, and so on. There are a lot of things that you can do with the agent. So this is a hot or short-term and long-term memory separation.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2150.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2160.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2160)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2180.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2180)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2190.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2190)

 Where it gets really interesting is this sort of tiered shared memory architecture with multi-agent workflows, which is what we are building for Flightly.  So add travel insurance before you book my flight, use the email from our support conversation. This is a stateful multi-agent workflow where your shared memory state is stored in between ElastiCache and Aurora PostgreSQL for the agent to do seamless handoff.  You have an orchestrator, in this case, Flightly's orchestrator. It has three specialized agents: your support agent, your booking agent, and your payment agent.  All of these share memory in both of these layers, your long-term storage as well as your short-term memory.

Behind the scenes, we are still doing the same thing. We are extracting the short-term memory from ElastiCache, persisting it to Aurora. And in this way, you're basically sharing the state and the workflow will be your first, your support agent queries Aurora semantically. It will find last week's support conversation, extract the email. Booking agent will go and search flights, reserve them, and write this confirmed booking state to Valkey, that's your checkpoint. Payment agent will go and read this confirmed booking state and add the insurance and then process the payment. So all of this is orchestrated through a common memory layer. There is no context drop off, there is no continuity lost between agent handoffs, and states are synchronized in real time between the two of them.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2250.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2250)

### Scaling to Production: Bedrock AgentCore and the Complete Solution

 All right, we've looked at five different patterns. We've looked at different steps in building this conversational AI system. Now the critical question: how do we scale this to production? A million daily queries, high availability, cost optimization, security. So far, our agents are running locally on our laptops. Your MCP server, Aurora PostgreSQL's MCP server, is running as an STDIO local Python subprocess on your laptop. What if your laptop crashes? What happens mid-transaction? You lose the transaction, obviously, that's a no-go. How do we scale this?

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2290.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2290)

 So this is where Bedrock AgentCore comes in. Just to show, who's not heard of AgentCore? Anyone? Everyone's heard, one person hasn't heard of AgentCore, yes. So AgentCore is an agentic platform to build, run, and deploy agents securely at scale. Think about how we run PostgreSQL on self-managed servers versus Aurora PostgreSQL as a fully managed server. AgentCore is your fully managed capability to run agents.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2320.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2320)

 It has a lot of components to it, we won't go too deep into it. There's a lot of sessions around for AgentCore, but at a very high level, runtime provides your local agents and environment to run at scale on the cloud. You're not running agents locally anymore. The beauty about this framework is you're not restricted to just Bedrock models. You can use OpenAI, you can use Anthropic, you can use Gemini, it doesn't matter. Plus, you can bring your own agentic frameworks, such as Crew AI, Autogen, LangChain, LangGraph, what have you. AgentCore Identity gives you that secure identity and access management for your agents, which is super critical in production. AgentCore Gateway gives you a gateway to your MCP servers, your custom tools, your Lambda functions, your OpenAPI schemas. Your existing tooling or business logic gets fronted by AgentCore Gateway, and it gives you that routing layer to make it more seamless for your existing application.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2390.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2390)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2400.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2400)

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2410.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2410)

There are a couple of other tools like Browser and Code Interpreter. Browser is, if you're familiar with computer use for Claude, you give your agent access to your  computer, which you should not, to do things. Browser is used for that. Code Interpreter is more of a sandboxed code execution environment.  But then you also have some other nicer features like AgentCore Memory, which also offers out of the box short-term and long-term memory modules.  And to wrap all this around with good guardrails and tracing of the agent actions, you have the AgentCore Observability layer as well.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2420.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2430.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2430)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2450.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2450)

So remember my Friday frustration?  Same journey, completely rebuilt. In this case, Valkey caches the frequently asked questions, baggage policies, booking templates,  email templates, and it returns these results in microseconds. Aurora with pgvector handles the route information, looking up seat preferences, long-term memory, semantic search, all in two-digit milliseconds. Nine out of ten customers stay, 60% cost savings,  performance that truly scales. And with that, Flightly is ready for Hawaii, and so are you. With that, thank you for your time, Mahalo.

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4022a193013eb6bc/2460.jpg)](https://www.youtube.com/watch?v=eCiR1mDe5Rw&t=2460)

 I do have a couple of resources to share. And if you do have questions, let's take them outside. But please do remember to complete the session survey. And yes, thank you for attending. Enjoy the rest of re:Invent.


----

; This article is entirely auto-generated using Amazon Bedrock.
