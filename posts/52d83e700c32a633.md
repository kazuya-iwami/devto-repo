---
title: 'AWS re:Invent 2025 - Shell''s HPC Evolution: Accelerating Seismic Processing with AWS GPU Innovation'
published: true
description: 'In this video, Michael Gual from Shell and Husain Shell from AWS Energy discuss Shell''s multi-year journey migrating high-performance computing workloads to AWS. They detail how Shell moved from struggling with POCs (2017-2022) to achieving 3-5x performance improvements using P5EN instances with H200 GPUs and EC2 capacity blocks. The presentation covers their "10X Challenge" that shifted focus from replicating on-premise infrastructure to reimagining workflows, processing petabyte-scale seismic data through hybrid architecture using Direct Connect, FSx, and PCS. They emphasize separating technical validation from commercial considerations, achieving 2.5 years of accelerated wall clock time, and enabling researchers to rapidly test new hardware like GB300s and Blackwells. Future plans include multi-region orchestration, deeper integration with EDI, and continued price-performance optimization.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Shell's HPC Evolution: Accelerating Seismic Processing with AWS GPU Innovation**

> In this video, Michael Gual from Shell and Husain Shell from AWS Energy discuss Shell's multi-year journey migrating high-performance computing workloads to AWS. They detail how Shell moved from struggling with POCs (2017-2022) to achieving 3-5x performance improvements using P5EN instances with H200 GPUs and EC2 capacity blocks. The presentation covers their "10X Challenge" that shifted focus from replicating on-premise infrastructure to reimagining workflows, processing petabyte-scale seismic data through hybrid architecture using Direct Connect, FSx, and PCS. They emphasize separating technical validation from commercial considerations, achieving 2.5 years of accelerated wall clock time, and enabling researchers to rapidly test new hardware like GB300s and Blackwells. Future plans include multi-region orchestration, deeper integration with EDI, and continued price-performance optimization.

{% youtube https://www.youtube.com/watch?v=ujO5kQwVHPs %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/0.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=0)

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/10.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=10)

### Introduction: Shell and AWS Partnership in Energy HPC

 Hello, everyone. My name is Husain Shell. I'm the CTO for AWS Energy and Utilities. Thank you so much for joining us  today and for being here when you could be anywhere else in Vegas. I am extremely proud to present my colleague here and my partner in crime in everything we've been doing around HPC and the energy space with many of our customers. Michael Gual, the head of global HPC engineering and operations for Shell, is here with us to talk about what we have been doing for the last four or five year journey around not only high performance computing but also around generative AI co-innovation and some of the work that we're doing around data and analytics. I'm really excited for what's coming. Fortunately for you, he's going to do most of the talking, but I'm here to answer any questions needed. Without further ado, Michael, please.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/90.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=90)

Thanks, Husain. It's great to be here. Thanks to everyone making the trek. I thought it was wise to try and walk here, and maybe that wasn't as wise of a decision after the keynote, but thanks for making the journey over here. I'm really excited to share the journey today. It's actually been longer than five years and had some struggles, and we'll talk about those in the talk today. But first and most importantly,  a long slide about cautionary notes. Yes, you're welcome to take pictures of it, but don't take financial advice. We're in Vegas, you know.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/100.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=100)

 What are we going to talk about today? I'll give you a bit of context about Shell and what we do beyond the gas stations and what you see on the F1 track. I'll give you a bit of context about Shell. We'll talk about our HPC, our upstream challenges, and then how that translates to HPC, our journey with AWS, which started in 2017 on HPC, and we'll go through that. I'll show you what it actually looks like in an architecture. We'll talk about what's next, and we probably will have some time for Q&A at the end as well, and the hard questions can go to Husain.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/140.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=140)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/180.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=180)

 So first, a bit about Shell and just setting what we do besides the gas stations. Over 93,000 people across 70 countries, really touching all places of energy. What we'll talk a lot about today is our upstream. So some of the big metrics there are 66 million tons of LNG in 2022 and over 22,800 barrels of oil a day equivalent production. Clearly, a lot of different areas and fun numbers. And then what does that look like from a customer sector? Again, servicing integrated energy solutions  across all aspects of energy, including things like data centers, which is a huge use of power these days. But again, hopefully all of you use Shell products on your journeys to Vegas.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/200.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=200)

But let's dive into Upstream and talk about  who really uses our HPC. And first, let's talk about what are we doing technology-wise and what are we trying to change. We're a company where for decades we've done a lot of things proprietary, a lot of proprietary kit, a lot of build it ourselves. And what we're really finding is to unlock future ways of working, integration is going to be one of the keys. So that's where we come into strengthening our core workflows. How do we simplify our workflows? How do we take a whole bunch of different siloed applications and integrate them? How do we embed AI into these deeply technical workflows? And then on the right, where do we then choose to invest where there is still differentiation here, but there's also a lot of market standard. So how do we then pick the right targeted areas to invest, and then that's where we invest strategically there and we try and simplify the overall user experience.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/270.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=270)

It becomes a pretty obvious sort of conclusion that doing this in the cloud and driving this integration in the cloud is a natural place to do it. And that's where, you know, if you look at our partnership with  AWS initiated in 2017, Shell started what was once upon a time SDU, which became OSDU for those familiar with it, which is now also a managed offering within AWS called EDI, which I'm sure we can talk about that when we talk about the whole stack. But Accenture, they're a key strategic partner for us in our hyperscaler strategy. And then in HPC, we've been live since 2022, and that's embedded across our friends in the energy vertical, an embedded ProServe team within my HPC engineering team, and of course our own developers and users.

It's been a real journey, and this is what we'll talk about in a lot more detail shortly here.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/320.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=320)

### Understanding Shell's Upstream HPC Challenges and the Need for Variable Capacity

 Now let's talk a bit about HPC. Who uses HPC? Why do we need HPC? Why do we need more HPC? We're an industry that is famous for having machines in the top 20 list. There is a lot of compute, and very simply, why do we need a lot of compute? To do a lot of physics. What are we doing? Seismic. For those not familiar with seismic, we send sound waves miles and miles beneath the typically sea floor. We listen to those waves up to 40 kilometers away. That generates petabyte scale data. That data then needs to be processed with a lot of compute.

Generally speaking, the algorithms we use are decades old, but we've never been able to afford the compute. Because of that, as we get more and more computation power at affordable rates, we just throw more and more physics at it. We take the assumptions out. We put more and more parameters in it. As we get into more complex areas to find energy, as we get into basins that we've been in and we want to image better, we need more and more geophysics. As we scale those algorithms, we need exponentially more compute.

Generally speaking, we give a geophysicist a new piece of hardware, it is usually taxed immediately until the next one comes out, and similar with storage. The only type of storage we usually have is full. That's what's driving what we need and why we need so much compute. Now let's put HPC in the business context as well. All of this imaging that we do is in the critical path for us as a company to ultimately drive revenue and to drive some of those fancy numbers that I got to show at the very start.

You need to have an image of what the subsurface looks like, and so it's in our critical path. The good news is it's an embarrassingly parallel workload, so if you throw more capacity at it, it will generally go faster. Scale is a nuance. There are always nuances and bottlenecks, but generally speaking it's a very parallelized workload, and more capacity equals faster results.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/460.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=460)

 Then what do we want to do, and why do we want variable capacity? We want to be able to make decisions on capacity related to our business decisions. Otherwise, what do we historically do? We guess. You buy a system and you hope it's enough, and it's a long procurement cycle, and it's one that you're then stuck with that decision for some number of years because of the amount of investment. We wanted to be able to enable a value of information and be able to make capacity decisions based on our business decisions with far more immediacy.

The waiting game of HPC. For those that have deployed systems, and we still manage them on premise as well, it is becoming increasingly difficult to get gear in a data center, whether it's the AI boom, whether it's making sure you have enough power, whether it's just general supply chain challenges. It is quite the challenge, and I'm very happy that AWS does a great job of it for us, and that you guys get to worry about a lot of what we've historically had to deal with.

More complex geophysics is why we need a lot of our HPC, and then why we want the cloud really is around giving us that variability, giving us that flexibility, and really being able to say when we need something that we're able to do it. We had an example this year where we took an innovation strategy all the way to the top of the house, and we said, all right, we want to go enable this, we want to go chase one of those targeted differentiation areas. Within weeks we had that capacity online full and the researchers active.

That's something that historically as an HPC manager, when someone comes to me and says I want more and here's some money, it's like great, where's the rest of the money and where's it for the next three years or five years? This is all right, let's go. It's on, and off we go. So that's where we are now. Let's talk about the challenges, let's talk about how we got there.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/590.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=590)

### The Journey from 2017 to 2022: Overcoming Early Struggles and the Pivotal 10X Challenge

 This has been a long journey, and the good news is it started really slow, I'll be very honest, but it has gotten a lot faster. If I use the Amazonian phrase of a flywheel, when we started this, this was very much a staircase journey.

Going up the staircase was very slow and very challenging, and at times we didn't even know if the next stair was there or if we were just looking at a wall. But we've gotten to a flywheel. So how did it start and what did it look like in 2017 to 2022? I think it's a story that's half technical and probably half emotional as well.

In 2017, when you started to talk about HPC in the cloud, people looked at you funny and said, what are you doing? Why are you doing that? That doesn't work. So from 2017 to 2022, we tried a lot of different POCs. We tried a lot of different things to say, oh, that spot looks interesting, that makes it affordable. Oh, I can't take the interruptions. Okay, fine. Well, let's try something else. Let's try and use a whole bunch of heterogeneous SKUs. Oh, that didn't work well, and we just tried item after item that didn't really work. We never really hit the scale, we never hit the mark.

A lot of what we were doing, and this comes after sort of reflecting on it, is we were just trying to do the exact same thing we did on-premise. Okay, I have 1,000 nodes. Well, therefore I need 100 nodes in AWS. Well, your nodes don't look the same, now I need 200 because now they're only half as powerful, so I need 200. And then you just add so much complexity because you're forcing yourself to do the same thing.

The biggest pivotal moment that we had was the 10X Challenge. So in 2021, which is early 2022 before we went into production, we had, and the person at the office in the audience here today is, let's put the 10X Challenge out there, which is, if we could get 10x wall clock improvement time, we will change the way we think about compute, we will change the way we do our business, we will change everything. Because think about life. If something takes you 10 hours and you can do it in an hour, you will do it differently. So when you do something 10x, you fundamentally change your behavior around it.

So the 10X Challenge became, all right, AWS, forget about trying to replicate what we're doing on-premise. 10x wall clock time. And the other thing we're going to do is we're going to separate technical and commercial. First, prove it technically. Can we do it? How fast can the car go? And if the car can go really fast, that's amazing. We will talk about commercials as well, because if it's really expensive or too expensive, we may never do it. If it's really expensive, we might do it once a year. And if it becomes affordable, we'll do it all the time. And so that became the challenge.

A few other stars aligned when we started looking at it in that metric. One is our key algorithm, which is Full-Waveform Inversion, which is very common in our industry, went from CPU to GPU. Nvidia was, this was in 2022, the P4DE and the A100 80 gigs had just come out. And now, because of Nvidia's reference architecture, the nodes looked very similar to what we were putting on-premise as well. So now we had some homogeneity. We didn't have this, well, here's my CPU nodes don't look like your CPU nodes. So that started to work.

The other thing is, we always used to think because of the petabyte scale data that you had to move everything, and that the data was the gravity or the anchor that was keeping us on-premise. What we started to think about is, well, maybe it doesn't have to be. What if compute is a magnet? And what if compute can actually pull your workflows and enable different ways? And I'll show what that looks like on the next slide, but that's one of the other things of how do we deconstruct our workflow to do what we want to do. And again, let's not just replicate what we do on-premise. Let's think about our workflow differently and how do we optimize for what we want.

And then the other really trivial thing that we realized is 10 gig direct connects in the US became very affordable. If you look at the scale of cost of compute and how much a 10 gig or 100 gig direct connect costs, it's like, oh wow, that's a very investable item that we can manage.

### Production Launch and Evolution: From P4DEs to P5ENs with EC2 Capacity Blocks

So in 2022, we went live in production. And it was amazing. We went live on December 23rd. I was very happy because AWS almost ruined my Christmas, but you didn't. And the person that filled the queue is sitting here in the front row as well. They came on on the 23rd. They came online and within minutes they were full, and they stayed full the entire duration we had them. It was really impressive.

It wasn't great though, I'll be very honest. The challenge that we had with P4DEs was getting them. We had a bit of base capacity, but it was a rare commodity, and we had lots of clever ways to try and get them or find them or just call Hussein and yell.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/910.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=910)

It wasn't great. It definitely wasn't meeting our 10x goal. Did we accelerate projects? Absolutely,  but I would say we were more in the 3x range that we were accelerating. We were doing some great things, but we knew we could do more.

That's where this year when we made the move to P5ENs and Nvidia's H200s, which for our workload were fantastic because that HBM really helped us on our workloads. We also then started using EC2 capacity blocks, and for us that was and remains to be a game changer as far as predictability. We can now, knowing our projects, have weeks of visibility of what we need in weeks. It's not like we wake up in the morning and then freak out and need compute that day. We have some visibility of when these projects are coming, and so now we book them. We have a dashboard and I know when the capacity is coming. I know when it's going away, but that predictability of burst has been a true game changer for us.

The simple way I describe it now is what I can have an engineer do in one click or half a dozen clicks is what historically would have taken me 6 to 9 months, maybe 12 to 18 months if you're thinking about I have to RFP, I have to buy the machine, I have to commission it, I have to bring it on, and then I have to keep it for any number of years as well. Now I click the button and it's there. The scary thing is it's just as easy as buying things on Amazon, so I have to keep my engineers in check because they've gotten very used to spending some very large numbers online. But it is really remarkable to see the power of a capacity block, and I'll talk later about what that's done to our engineers and our actual HPC users, which has been a really fun thing. That's been the journey and now into the flywheel.

Now if I look at when we went live with H200s earlier this year to what we've even done this year, it has been such an increased pace of innovation and optimization. What we're doing now is we're bringing many, many more use cases. We're bringing R&D use cases, we're bringing production use cases, we're bringing some of our AI use cases, some of our GenAI use cases that what we're able to spin up and react to as far as business needs, it is amazing. Are we at 10x? No, we're not at 10x. I will say we're sustainably at 3 to 5x now, so we struggled to get up 1 to 3x on a P4DE. We're definitely in the 3 to 5x range now, but there's still more to come there. That's part of the journey and again I am confident that we will get there, but then we'll just keep going more. I mean we will keep pushing.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/1110.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=1110)

### Technical Architecture: Hybrid Workflow Design and Multi-Region PCS Implementation

The other thing to me is price performance is still number one. What I describe to people as my number one job is to deliver price performance cycles to our HPC users. Ensuring that we get the best performance for the best price and the availability are all paramount to what we do. I'll save that stat for later, but this is what it looks like now under the hood.  Petabyte scale data comes in. We bring it in on premise. The first thing we generally do is depopulate it and you start to shrink the model.

This is where our CPUs still exist. Yes, we do look at AWS for this, but this is a nut we've not been able to crack yet, but it's one that we have value in, so we do it there. We bring it in and we do a lot of the CPU intensive stuff on premise, but ultimately for Full-Waveform Inversion, FWI, that input file goes from petabyte scale to about 10 terabytes. That 10 terabytes then goes on the Direct Connect up to AWS. Then we use a tremendous number of, today P5ENs or P series instances to run that. That's worked tremendously well.

We've just moved to PCS, Parallel Computing Service, as well, so that's SLURM-based. The look and the feel is very similar for us, for engineers. I'll show you what it looks like from a user perspective on the next slide. But this works really, really well. The output then is a terabyte. Yes, there is some egress cost. I tell AWS I'm always annoyed about it, but it's like a ticket fee. But again, in the grand scheme, it is worth the overall outcome. Then that comes back, and then if we need to do any remaining CPU, it happens on premise.

The data gravity is still there, but what we're actually doing is enabling far more workloads. And then at the end, what we're saying, I'll show you, is how we actually integrate this into our overall stack. But this is how it works. From a capacity perspective, we have some dedicated nodes, we have capacity blocks, and what we're also starting to explore is what we can do with spot as we start to see a bit more spot appear in the P series instances. But that's how we actually make it work and how the hybrid solution has been working tremendously well for us.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/1270.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=1270)

This is the Full-Waveform Inversion use case. In other use cases, like where we're doing some fine tuning of a large language model, it just happens in the cloud. There's no, we don't, because you don't have the CPU stuff on either side, we can just have it happen natively in the cloud. Some of our innovation and R&D workloads just happen in that yellow box in the middle natively. So it's probably too small, but that's okay.  I'll describe the important part.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/1300.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=1300)

So on the left is our user environment. That's our virtual workstation. So that's our users logging into their Linux or Windows virtual machines. And the key thing there is that's the same interface they have for on-premise, that's the same interface they have for the cloud. So that is their virtual workstation today that sits on premise. Then on the AWS side, so what you are seeing here is Parallel Cluster Service, PCS,  at work here, which is AWS's managed SLURM offering. But through that API gateway, and I'll call it out because you're probably staring at the, is there a hamster on that diagram? Yes, there is a hamster on this diagram.

Our optimization team has a thing with rodents. The team themselves is called the Rats. That is Hamster. We do have a tool called Mice. The person that is on support every week is called the Rat in the Hole. So yes, it's a hamster. Yes, it's a tool. It's a really important API to make all this work. But ultimately, you have the job submission happening with the API calls with Hamster in there, but ultimately submitting it to SLURM just like we would on premise.

And of course it is not a single pane of glass. The user does know they're running in the cloud, and the biggest reason they know that is because they have to make sure their data is there. So we have jobs that they can orchestrate that move their data there. But some of the other details that this slide doesn't really do justice to, but how we've been able to really leverage the cloud and find capacity and leverage the scale.

This is an architecture that runs in multiple regions. So I can say this as of, you know, this morning, we're running in three different regions. We don't run cross-region yet. That is something that we are actively working with and talking to AWS about, how could we do that. Today we run in multiple regions, but they all run independent of each other. We also run in multiple availability zones. So the joy of our workload not being tightly coupled is we can run the same workload across multiple availability zones in a region. And so it just is another way that we can get access to more capacity.

And then underneath it for data, we have FSx. But what we realized again is we had to think about FSx differently because if we used Lustre on the cloud like we do on premise, it would cost more than our GPUs. And so we had to get, and that's just a nature of how we use Lustre on premise and how we don't have tiering like the cloud does. We just have really, really hot storage, which is Lustre for us, and then we have really, really cold, which is tapes. We don't have in AWS where we can have all this hierarchy.

So what we then do in AWS is we have S3-backed FSx. The way that our job works is you run a whole bunch of computation, you get the model out, it sits on FSx on Lustre. But then you go and you update the model again, and then that's it. But then the older version of the model can go to S3. So as long as just the most recent version is hydrated on FSx, we can keep that FSx to be a small, small fraction of the overall project.

And then again, if you remember how I described the workflow, we just delete it at the end. We treat all of this as ephemeral. Our input data that goes up, we don't bring it back. We have a copy of that on premise. It's, we just throw it away, well, delete it safely and securely. But this architecture for us has really enabled us to be mobile in the cloud. And then now with capacity blocks, it's enabled us to know, okay, on this day I'm moving to this region. And then with PCS, we can spin up a cluster in minutes, maybe, you know, hour max.

And it has just enabled us to go on that flywheel and really make that flywheel spin. It's fantastic to see. So this morning, Matt announced GB300s. I can go ask my data scientist, do you want to test the GB300 tomorrow? We can do it. That pace of what we're able to do and trial and look at benchmarking and look at what works, what doesn't work, where do we need to go invest time with NVIDIA and to leverage the latest chipset, those are things that just the pace that we can do. Because how do we do that in the old HPC days? New chip comes out. Let me call NVIDIA. Let me get a white box in. Do I have the right OS on it? Is it compatible? How do I get it on my network? All of these other steps.

Now, I can just get a capacity block. I can get it on demand or pull it up on spot. So again, the pace of what we're able to do. Yeah, so the biggest thing that I want this diagram to show in the future is multi-region, how can we really play multi-region and chase some capacity and leverage spot more than we can now. Maybe one other plug for PCS, and I can recognize some people saw me talk about PCS yesterday, but having PCS manage our SLURM compared to Parallel Cluster, one of the things that we're now able to do as well is make updates to our clusters seamlessly.

Before with Parallel Cluster, you make a change, you then need to take the cluster down. Where now if we want to update it, so for example, if we want to add an instance to one of these clusters, we can add it, and then automatically the SLURM managed service will update that and that's seamless to the user. Where before you want to add an instance, you want to do anything else, you'd have to redeploy the cluster and take it down. So really great solution.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/1630.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=1630)

### The Integrated Stack Vision: From HPC Foundation to End-User Experience

Now, I have a very biased view of the slide, but what we've been talking about is the bottom, which is obviously the foundation that holds it all up, the most important piece.  But why are we doing all this in honesty is it's to deliver back to that upstream challenge I talked about at the very front. It's how do we deliver an integrated stack. How do we deliver competitive differentiation, how do we deliver market standards. So if you look at what we're providing, and now again this is where it's not the data gravity, this is the HPC is the enabler, and now the conversation is where do we need it, what workflows need it to be enabled, and so then you go up the stack and drive an integrated experience. But Hussein, I'll let you chip in here on what we're doing with you and what you're doing across the vertical as well to help build this whole ecosystem.

Yeah, no, thank you, Michael. And so I just want to bring everyone back up about 10 to 15,000 feet of why we need to look at the forest, not just the trees, because it's important to make sure the foundation and the infrastructure and the business value is driven by the different levels of the stack but not forget about what the rest of the stack looks like and why it's needed, whether it's on the data, it's on the end user experience, it's on the innovation, and it's also on generative AI and the AI capabilities that are starting to get more and more embedded within these workflows. And so for us this represents the big picture. And we are lucky that Shell has been a great partner of ours that keeps reminding us that it's about the big picture, what else we need to do. Don't just think about the compute. Don't just think about the data. It's all of the above.

We still have a lot of work to do around the end user enablement. You saw the slide before where data comes from on-premises to the cloud then goes back. We can reduce a lot of that cost and a lot of that friction when we start enabling the rest of the pieces that are on the top, for example, on the application access to be also present in the cloud at the price performance and at the need that Shell has, as well as many of our other operators that are in the room that we're partnering with as well out in the world. For me this is an all end-to-end journey that takes multiple years to achieve because you have teams that are completely disconnected, to be honest with you, that are working on pieces of this.

We have, you heard from Michael, some of the R&D team is doing a lot of AI and generative AI and models, simulation, etc. on AWS using the same infrastructure that Michael has access to in HPC. How do we make that experience seamless? We have teams that are working on data management and ingestion and deployment and integration with applications. How do we make sure those data sets also impact the results that come out of HPC or go into R&D?

And also some of the key enablement that people usually just forget because they just want to do their day-to-day business is that co-innovation piece. I think one of the best opportunities that I've worked on in my career was the co-innovation we've done with Shell, with Oxy, and others where we've created a whole new way of looking at HPC orchestration and workflow enablement in the cloud without having to repeat the same mistakes that Michael mentioned of doing it as we did it for decades on-premise, and that's with HPC Orchestrator. It's a low-code, no-code tool that we literally came up with together as an idea, we put it into action, and we created something that is now tested in almost every operator that I talk to because it makes HPC a lot more seamless in the cloud and removes that barrier of complexity and adoption that Michael talked about. The first few years you just spin it up, you go in, you build your workload, and you build templates of what you're trying to run, and it could be HPC, it could be AI, it could be any kind of simulation.

That's one area, and then there's also some work that we're doing around agents in the GenAI space that we can talk about maybe next time. But for me, I'm excited because it's a much bigger journey and a much bigger purpose that we're driving together with Shell that also benefits many of the other operators and customers that we have, as well as the partner ecosystem enablement that we put in this as well. Back to you, Michael.

Yeah, no, thanks. The one I'll pick in, and you mentioned it, Ray, around HPC Orchestrator, I think it's a great example of to leverage the cloud you really have to think a bit differently and use the tools that are there rather than simply use the tools that we're used to using. And so if I look at the templates that are in HPC Orchestrator, those templates do some really smart things like driving asynchronicity and using storage in smarter ways. Those are things that help speed, they help price, they help throughput. So thinking about it from a cloud-native perspective instead of redesigning, and the fact that then there's a template as well, is it's an area where there's a lot of things in our industry where we compete, but there's also lots of areas where there's great room for collaboration.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/1970.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=1970)

And so something like a template on how to best use a cloud-native technology is a great area for us to collaborate, and then we can just keep our code that we consider differentiated to ourselves, but we can all use the same template. So I think it's a great example of where, you know, I appreciate what AWS is doing here and helping drive us as an industry to deliver this vision as well.  So let's talk about some of the fun stuff now, and so what? So what we have some nodes, so what we have a bunch of computers, they're great. You guys do a great job of running them, you know, Matt was very proud of you guys' Nvidia reliability this morning at the keynote.

### Business Impact and Lessons Learned: 2.5 Years Saved and Mindset Transformation

So what has that burst done? So unfortunately we're not at 10x, but we are at 3 to 5x, and we were looking at the numbers and thought, all right, how much have we accelerated wall clock time on these projects since 2022? 2.5 years. We've saved 2.5 years of wall clock time because of what we've been able to accelerate with burst capacity. That matters, that flexibility matters. It matters to our business, it matters to how we think about HPC, it gives us variability and capability that frankly we've never had before. But to me, I look at it now and I've been part of this journey in some fashion or form since 2017, and I look at now what we're doing this year and this is where we wanted it to be.

I still want to go, there's still more to do, there's always more to do, it can always be better, but we're doing what we dreamed of. We're doing what that PRFAQ said that we wanted to do in 2017. We're starting to do some of that now. We may have had the date wrong when we wrote that PRFAQ the first time, but it's there, and I think it's the beginning of now you kind of covered a lot of the basics and the foundations to allow you to do a lot more faster next. And we've changed the mindsets, and I'll come to that as well, but the deployment, like I said, I think differently about how to manage HPC demand internally now.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/2090.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=2090)

If I look at what standard lead times are now and what it would take to bring in a machine the scale of what we use, it would take a long time, it would take a lot of planning, it would take a lot of capital. And now I can do that in clicks, and that is a huge difference  that we can now focus on different areas of our business. We can react when we need to react and really focus where we as a company make the most of our investment of time and effort and innovation. It's really interesting, and this comes into the

changing mindset. When we started to talk to our researchers earlier this year and said, hey, we're moving your GPUs to the cloud, a few of them were not very happy with us. Like, well, no, no, no, I like my on-prem node. It's like, it'll be okay, it'll be okay. And now we can't take them away from them because now they say, oh well, can I get a big memory node? Can I get a bigger memory node? Can I get less cores? Can I get more cores?

The heterogeneity and the innovation that we see happening, can I pull this agent up, can I use this service, it is just such a difference. Where before, historically, it would be okay, you want that? Well, yeah, we can get an engineer to spend a few days on it and we can duct tape some stuff together for you. But it has just enabled such a different dialogue with our innovators, and it's changed the mindset. And this is actually both a blessing and a curse as far as changing mindset, because when you have a really big system, the good news is you just try and fill the system and you don't really scrutinize at times, does it need to run, is it the most important thing you need to run, what's the priority?

The challenge now is not is the system full, it's how big do you want the system to be. And really then the fun question I get asked to do is, okay, and how much do you want to spend? And it is a very different conversation that we're having now around what's the value. And the other really fun conversation we get to have now is, do you want it to go faster? How fast? And that's where the 5x comes in, and hopefully 10x, but where do you want to accelerate? So it's just been, you know, like I said, we're doing what we had hoped for when we set out, and then the lessons learned.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/2220.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=2220)

 It comes back to vision. This was both a technical and, like I said at the beginning, an emotional journey. And one of the biggest things that unlocked it was finding how are we going to do things differently. So to me, it's for all those out there, what's your 10x challenge? Maybe it is 10x, by all means take it. AWS has heard it before and they're probably tired of hearing about it, but it's what's going to make a material difference. If you just say I have 100 nodes and I want 100 nodes there, I can tell you that you're going to find a reason to not like the cloud, because you'll do something with your 100 nodes that you prefer versus their 100 nodes, and it's probably not going to be a great setup. But what's your 10x?

For us, velocity. And what I would encourage is I would continue to separate commercials and technical. And AWS loves technical problems, and the teams dive deep and it enables our engineers to go do some really, really fun and hard things. And then we have to figure, of course the commercials are always a consideration, that is a given, but it helped. And then I think the other thing that we learned a lot is de-risking the most important things first and trying to do it in an expedited fashion, and actually do it in an expedited fashion when everyone's in the same room.

So there's a few times, and Amazon has a program called EBA, what's it stand for? Experience Based Acceleration. But essentially, it's one of the things that we did is we said we want to move and be able to run in multiple regions. We brought a bunch of engineers together, we brought some AWS people together. We didn't have it working in five days, but we fundamentally de-risked the most important things. And that's actually one of the most important things was making Hamster work, which you heard about earlier. But then we had the confidence. It was not implemented, but we knew we had de-risked the most important things of how do we get on-premise to communicate to multiple regions, how do we orchestrate all those things. And it was okay, at the end of the week we ran a dummy job.

But that thing gave us the confidence that, okay, off we go. But having all the right people in the room, and that's where the partnership has been really, really great with AWS to me, is if you ask and pull, the response is always there. And then the other one, and this is just to me day in the life of HPC, is relentless optimization. It's never good enough, it always needs to be faster. There's always a bottleneck, it's always too expensive. But it's relentless optimization.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/2380.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=2380)

### What's Next: Multi-Region Optimization and Continued Innovation

So what's next for us? We've talked about it, but integration of workflows and  new workflows. And again, we're already doing that today. We want to fine-tune an LLM, great, let's go. What I can also tell you is we were training that LLM on H200s a few weeks ago and now we're training it on Blackwells. Why? Because we know the workload drives it, we get the price performance, we're there. That rapid response to needs is very, very real.

Integration modulation, we talked about Energy Orchestrator, but to me it's also beyond that. It's getting it into EDI where our data sits. It's getting agentic for energy. It's getting all of that integrated workflow across both our stuff we make and across our vendor and partner ecosystem. It's gluing all that together. And again, it's changing the paradigm of, well, no, we can't go to the cloud because our data is not there. To me, if we give an environment that's integrative and the carrot is there, our users are going to come because it's going to be the obvious place. It's not going to be if and or, it's going to be, well, I have to be there.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/52d83e700c32a633/2480.jpg)](https://www.youtube.com/watch?v=ujO5kQwVHPs&t=2480)

Relentless optimization, we talked about it. Again, that is what we have done day in, day out. There's always something that we're looking at and saying, could we do this better? And so today for us in HPC, that's multi-region. That's our next big challenge that we want to go de-risk. And then price performance. On my long walk over here, I did run into your accelerator lead, and we sort of said, so when do you want to test B300s? When do you want, now that they're out? And so it's looking at that price performance. It's looking at what works for our workloads and how do we then deliver that in the most meaningful way. So that's what's next for us. 

And with that, just one as a closing comment, just a ton of appreciation to AWS for, it's been a long journey. Like I said, the staircase seemed like walls at times, but we're really, really appreciative of the journey. And you know, for me personally, a huge appreciation to all the both the sponsors and engineers that I have on the Shell side. I'm incredibly privileged to be able to be up here and tell this story, but it's through the hard work of all the people that have made this work that we're able to do it. So thank you to everyone and thank you AWS. Of course, thank you for having us.


----

; This article is entirely auto-generated using Amazon Bedrock.
