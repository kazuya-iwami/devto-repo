---
title: 'AWS re:Invent 2025 - Modernize SQL Server & .NET Together with AWS Transform''s New AI Agent (MAM340)'
published: true
description: 'In this video, Khurram Khawaja, Nits Jeganathan, and Vijay Mandadi demonstrate AWS Transform''s agentic AI platform for full-stack Windows modernization. They show how it transforms .NET Framework applications to .NET 8/10, migrates SQL Server databases to PostgreSQL, and deploys on Linux instancesâ€”all in a unified workflow. The session includes a live demo of the four-step process: analyzing database-application relationships, schema conversion, code transformation with ORM updates, and deployment to EC2 or ECS. Vijay explains the multi-agent architecture, including the Windows Modernization Orchestration Agent coordinating specialized agents for assessment, schema conversion, data migration, code transformation, and deployment. The solution uses self-debugging loops and formal verification for query equivalence. Customers like Verisk report 4x faster modernization and 40% cost savings. The service is available at no additional costâ€”users only pay for AWS resources consumed.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/0.jpg'
series: ''
canonical_url: null
id: 3087521
date: '2025-12-05T20:39:03Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Modernize SQL Server & .NET Together with AWS Transform's New AI Agent (MAM340)**

> In this video, Khurram Khawaja, Nits Jeganathan, and Vijay Mandadi demonstrate AWS Transform's agentic AI platform for full-stack Windows modernization. They show how it transforms .NET Framework applications to .NET 8/10, migrates SQL Server databases to PostgreSQL, and deploys on Linux instancesâ€”all in a unified workflow. The session includes a live demo of the four-step process: analyzing database-application relationships, schema conversion, code transformation with ORM updates, and deployment to EC2 or ECS. Vijay explains the multi-agent architecture, including the Windows Modernization Orchestration Agent coordinating specialized agents for assessment, schema conversion, data migration, code transformation, and deployment. The solution uses self-debugging loops and formal verification for query equivalence. Customers like Verisk report 4x faster modernization and 40% cost savings. The service is available at no additional costâ€”users only pay for AWS resources consumed.

{% youtube https://www.youtube.com/watch?v=t1JP8lwG5lU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/0.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=0)

### Introduction: The Challenge of Modernizing Windows-Based Applications

 Modernize SQL Server and .NET Together with AWS Transform. I'm Khurram Khawaja. I head the product management for AWS Transform. I'm accompanied by Nits Jeganathan, product lead, and Vijay Mandadi, senior manager, software development. We are going to discuss one of the problems that almost every enterprise faces: how to modernize Windows-based applications. In this session, we will cover how we help through agentic AI with AWS Transform to modernize not only the UI framework but also the .NET code alongside modernizing SQL Server to PostgreSQL and deploying it on a Linux instance. We will walk you through some of the things we hear from our customers, give you a demo of how exactly things work, and then take a deep dive into how things work under the covers and how agentic AI plays a role. We will also share some customer anecdotes and close with Q&A so we can get feedback from you and answer any questions you may have.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/60.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=60)

 When we speak with our customers, we ask what is holding them back from modernization efforts. The answers across customers are fairly consistent. It is not about the technology or the business. It is about the continuing upgrades and updates required because of end of support for SQL Server and Windows Server, which requires their developers and resources to continue upgrading, updating, and patching to keep the lights on.

Secondly, the monolithic applications running on .NET Framework are inhibiting their progress to adopt cloud-native methodologies like DevOps, AI, and analytics integration. Additionally, since the applications are monolithic, you have to overprovision them to accommodate even smaller amounts of peak traffic. The code is so old that even to continue operating it, troubleshooting becomes more complex and patching becomes much more difficult. Finally, cost is definitely a factor, as licensing costs are one of the biggest line items in most IT budgets. All of these factors combined are holding enterprises back from focusing on innovation and forcing them to continue operating under the burden of these different factors.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/90.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=90)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/200.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=200)

### The Complexity of Coordinated SQL Server and Application Modernization

  If you have transformed a SQL project, the biggest blocker is not just one part or the other. It is the SQL and database modernization as well. If you have ever gone through the migration of a SQL project along with the application, it is not an individual task. It is a coordinated task across multiple personas and multiple different elements that have to come together. For SQL Server modernization, you start by understanding what the database is, what the different queries are, how they relate to the different applications, and what impact it will have if you change the schema. Following that, you have to go into schema conversions and determine the efficacy of those conversions and how much manual effort is required for things that cannot be automatically translated.

Then you modify the application to connect to the new database or new PostgreSQL instance. You deploy, validate, and everything has to be done one database at a time. If you have a large number of databases and applications, you continue this iterative process across multiple applications, one application at a time. But here comes the complexity. It is not one person or a limited number of people doing this. You have the SQL Server being handled by the DBA and the .NET code being handled by the developer.

Then you have the deployment and validation across other teams, so you have to bring all of these different personas together, all of these different teams together in order to make sure that it is done correctly the first time and it is repeated consistently across the remaining portfolio of your applications. Which is hardâ€”it's fragmented. Every team uses different rules. They have different scripts. They have different methodologies, so bringing everything together is labor intensive. It makes the overall process become an inhibitor to modernization across an enterprise.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/350.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=350)

### AWS Transform: A Generative AI Platform for Full-Stack Windows Modernization

This is where AWS Transform comes into play. It's the first generative AI-based platform modernizing the full stack Windows, including the UI layer, the .NET code, the database modernization, as well as deploying it into EC2 Linux instances, bringing all of those personas together on a single platform so that they can work together to first get the modernization done, accelerating this coordination as well as the tedious task of consistency across personas by 5X. 

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/420.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=420)

Achieving consistency across the transformation of the different elements which make up your application, both back end and front end, ensures that it's repeatable across hundreds of applications through that single platform. By adopting cloud-native or modern technologies and open-source technology, you are able to slash the licensing costs, resulting in 70% operating costs and licensing cost savings. 

If you look at what is the historical state and where exactly the target is, you can see on the left side that we start with assets which are .NET Frameworkâ€”really old code, monolithic, most of the time using older technologies. The UI layer contains the business logic and data access layer. Then you have SQL Server and everything is running on top of a Windows machine. Now the target state for this is cross-platform .NET Core 8 or 10 through AWS Transform. Modernizing your UI layer to modern component-driven technologies like Razor and MVC Core, transforming your database from SQL Server to open-source PostgreSQL or Aurora PostgreSQL, and then helping you deploy on Amazon EC2 Linux instances or ECS.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/490.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=490)

How does AWS Transform do that? It's a four-step process. It starts by analyzing the relationship between the databases and the .NET application, understanding the dependencies of the queries, understanding what SQL queries are there and how exactly they impact the application. It builds that analysis and does that assessment followed by the schema conversion of SQL into PostgreSQL, making semantic analysis to make sure that it conforms to what you had in SQL.  And then you can migrate the data as well using Amazon DMS, the Data Migration Service.

The second part, the third step in this process, is where the dependent application code is modified with the connection strings, with the ORM, and any other SQL-dependent application code so that it can start communicating or start working together as an application with PostgreSQL, which is finally deployed and validated on EC2 Linux or ECS instances. To give you more detail on what exactly and how exactly it does this, I'm going to request Nits to come over here and show how these things work.

### Live Demo: End-to-End Transformation from .NET Framework to .NET 8 and SQL Server to PostgreSQL

Hey, everybody. My name is Nits, and I'm a product lead for AWS Transform for Windows. Before we go into the demo, I just want to get a show of hands of how many have played with AWS Transform or have some familiarity with it. I see a couple of hands in there. Good. So for folks who have not used AWS Transform before, let me just quickly set the context.

This is a recorded video, so I will click through and stop at specific points to explain what is happening in each stage. I have the fun part of the jobâ€”I am going to go through a demo. This is the first time for some of you where you would see a full end-to-end experience across the whole stack. You will modernize your .NET Framework applications to .NET 8 or .NET 10, and then we will also show an experience of how you can move from SQL Server to Aurora PostgreSQL as well.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/650.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=650)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/680.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=680)

The first thing you see here is the AWS console. In order to log into this, you have multiple options. You can either create your own identity center or use your own IDP providers to log into that using your credentials. The first thing you will be faced with is a workspace, and a workspace is a container for all your .NET and SQL modernization jobs.  The first step you do is create a job. When you create a job, there are multiple different agents with different capabilities. For example, you see a mainframe modernization agent, and for this particular audience, this is Windows modernization. You come here, select the Windows modernization, and you are faced with two options: one is .NET modernization and the other one is SQL Server. 

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/720.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=720)

In order for customers to migrate their legacy applications, the first thing they have to do is modernize from .NET Framework to .NET 8 or .NET 10. So the first part of the workflow we will see will be transforming your .NET Framework applications to .NET 8, and then after that we will take that same application and run it through transforming the application and SQL Server together so that you get a fully modernized stack. You select the modernization piece and what you are faced with here is essentially the transformation plan.  This is the high-level plan which AWS Transform thinks of as all the steps that it is going to take to transform your .NET applications.

We will first connect to your source code repositories and then assess and identify dependencies across your repositories. If there are projects in repositories that are dependent on other repositories, we will create that dependency mapping and we will do all the transformation. If there are private NuGet packages, we will transform those as well. You will have the option of selecting whether the target should be .NET 8 or .NET 10, or even for some of your class libraries, you want to leave them to .NET Standard. You have all of those options to pick.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/780.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=780)

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/800.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=800)

The first step here is definitely setting up access to your source code connector. We support two mechanisms: either you use code connections with your GitHub, GitLab, Bitbucket, or Azure Repos, or you can upload source code directly using your S3 buckets.  Then after that is set up, we will go and look at the repositories in your account. Here is where you get a chance to select them, which branch you want, what your target is, and this will allow us to perform assessment. If you have hundreds of repos and you want to transform, let us say ten, you select these ones and then let the process go. 

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/810.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=810)

You select the specific repositories you need and then send it for transformation. What we are showing here is the private NuGet packages.  If you have private NuGet packages, we support two mechanisms. One, if you have Azure Artifacts where all your private NuGet feeds are, you can directly interface with that. Or you can download a script from our side and then run it on the same machine as Visual Studio, and we will collect all the dependent packages as one zip file and you can upload it there. You can upload those package files and that way we will be able to resolve all the package dependencies.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/850.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=850)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/900.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=900)

Finally, you get to review. Here is the set of your work. This is our transformation inputâ€”the repos that we want to transform, any dependent repos, and dependent packages.  You review it, accept it, and send it to AWS Transform to modernize. Once the transformation starts, it is essentially an agentic workflow where it will start identifying the order in which we want to transform and will start going through each and every project in those repositories. You can use the dashboard to check the status of what is currently happening. If there are any failed repos, you will get a transformation summary of why we are not able to transform it and the corresponding next steps on how to fix those.  You can then feed those into your coding assistants like Amazon Q to complete those jobs, and then you can come back and run further transformations as well.

So that is basically the .NET Framework portion of the conversion. What it means is that at the starting point, you have a .NET Framework, you go through this process, and at the end of it, you get a .NET 8 or .NET 10 application.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/920.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=920)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/940.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=940)

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/950.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=950)

That becomes the next stage of the pipeline, which is the SQL Server transformation. This also starts the same way.  You create a job as you did before and you select Windows modernization as always. But in this case, instead of .NET modernization, the SQL Server transformation requires you to create a job and come back into Windows modernization, but instead of selecting .NET, you will select SQL Server as an  option. Now it will be familiar with the same kind of steps in the transformation plan, but here you would notice  significant differences between what you saw at .NET versus here. The job plan includes validating permissions and making sure that you have the right access to your databases. You have the option to review databases and repositories together. Before doing all of those assessments and wave plans, there is a sequence of assessment which includes identifying all the reports, all the databases, identifying dependencies between them, creating wave planning, and then after that performing the transformation. The first step is definitely the prerequisites that are needed because we are dealing with databases. We want to make sure we have the right roles, the permissions, and access to the right accounts.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1000.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1010.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1010)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1040.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1040)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1060.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1070.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1080.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1090.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1090)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1100.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1100)

Once you validate all of it, you get started and you approve the connector.  First comes the database connector where your AWS account has all your databases installed, and then the next part will be the source code repositories that you will be installing.  After you do all of that, essentially, let me go back.  Yes, selecting the area there. Apologies for that. Controlling the video is a challenge, so we'll go again from the beginning. But yes, it's a fairly short video, so I'll go through it fast. So here, select the job, go through  it, and as we go through the source code connector and we will go into the assessment, I will stop and talk a little bit more. You get to the prerequisites,  and after that, you'll go into the database. This is where your AWS account is, and after that, you need to send it to an IT  admin for your AWS account to approve it. After that comes the source code connections where your source code repositories that you transformed before and you're able to use both  of them together. That's the true power of the full stack modernization here. You can change the applications and the databases.  Then you have an optional deployment. If you want to deploy those transformed applications, you can do that as well.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1110.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1110)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1130.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1140.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1150.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1150)

Once all the connector setup  is complete, what we want to do is run a validation of all your permissions, making sure that we have the right level of permissions, the roles, and all of that. Once the permissions are checked and everything is good, we would first want to go do a discovery, making sure in your AWS account that we are able to identify  the servers and the corresponding databases, along with the repositories as well. Here you will have the option to select which repositories you want to transform and which databases you want to select.  Then you approve it for assessment, and as part of our assessment we will go and identify  the dependencies between the apps and the databases, and we will create those wave plans that we talked about. You will see those here in this case where you are able to see two waves. Each of those waves has one database each and one repository each because they have a dependency relationship and because they are independent, you have created two waves and they will be transforming independently. That also keeps separation between which teams have to collaborate together and which teams have to work together to resolve those dependencies. You can download the waves and you can edit them as well if you think there has to be more dependency relationship that you have to adjust. Then the first part of the transformation is schema conversion. For the SQL Server to PostgreSQL migration, what we want to do is take a look at your server and identify all the database schemas, and we would want to make sure that we are able to transform them into PostgreSQL compatible schemas. So the schema conversion goes through by leveraging existing AWS services, specifically the AWS DMS schema conversion service, to transform your schemas into PostgreSQL-compatible formats.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1220.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1220)

We will use the target cluster that you created and apply schema conversion to it. 

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1230.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1230)

Once the schema conversion step is complete, the rest of the core transformation and migration will proceed. You can select which target clusters to use and download the converted schemas if you want to make changes after the schema conversion portion is done. 

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1270.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1270)

We perform schema conversion using a generative AI mechanism. We identify schemas in SQL, convert them to PostgreSQL, and run a self-debugging loop. If we identify issues, we fix those automatically. At the end of the process, you receive a summary of all transformations. If the schema is fully transformed, we indicate that status. If it is not fully transformed, you can see what functionality we were unable to transform and leverage our transformation report to fix those items yourself. 

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1300.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1300)

The next part is data migration. You have the option to validate this by migrating a snapshot of your database into the target cluster, which is an optional step. We leverage the existing AWS service AWS DMS, and we provide a deep link into the console if you want to check the status of what is happening. In this case, the data migration is completed. 

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1340.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1340)

Then comes the code transformation step. If your application has Entity Framework or ADO.NET code using an ORM to interact with your database, we make the corresponding changes in the application pointing to the new database. If you have embedded SQL, connection strings that need to be updated, or other ORM elements, all of those things are transformed by AWS Transform. This saves application developers significant time because they do not have to work with multiple database teams to figure out what schema updates occurred, which reduces a lot of back and forth. 

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1350.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1350)

We ask for a feature branch where we commit those code changes for your application, so your original source code is not updated. All transformation reports are available for your review. 

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1360.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1370.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1380.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1380)

Finally, you deploy the stage  and use a deployment connector to ensure that we are deploying to the right AWS account.  You can set the target, whether you want an ECS or EC2 instance as your target, and select those parameters. Once that is sent, we deploy those applications into those accounts. 

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1390.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1390)

At the end of this process, you have a fully validated mechanism. You have apps that are transformed to PostgreSQL, you have a PostgreSQL database with your converted schemas, and you can combine them together and run through validation.  This is the powerful functionality that you have in a single workflow.

That concludes the demo. I was able to show at a high level how it works, and now we will show you what we are doing under the hood so that you have more comfort in terms of what we are doing as a service. For that, I will invite Vijay.

### Under the Hood: Multi-Agent Architecture and the Windows Modernization Orchestration Agent

As you have seen how the product works, I wanted to take this opportunity to talk about how our service works under the hood. Before I get started, a quick show of hands: how many of you have experience working with agents, agentic systems, and MCP tools? Quite a few, right? So this should be pretty straightforward.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1450.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1450)

Under the hood, this is a complete agentic AI solution that comprises dynamic and goal-seeking agents.  The center of this multi-agent framework is the Windows Modernization Orchestration Agent, which acts as the brain orchestrating the entire workflow end to end. It uses connectors, and these connectors act as a bridge between customer resources, AWS accounts, and the agents.

There are three different connectors that agents primarily use. The first one is the source code connector, which acts as a bridge between source code repositories and the agents. The source code connector under the hood relies on the AWS service called CodeConnections, which provides a safe and secure way for the agents to interact with and download code from source code repositories.

Then we have a database connector. The database connector is configured with all the rules and permissions that are needed to access databases in a customer's account, discover them, assess them, transform them, and migrate them.

The Orchestration Agent has access to all the secrets, which allows the agents to connect to both the source database and the destination PostgreSQL database. There is also a deployment connector that contains all the rules and permissions needed by the agents to spin up instances, spin up EC2 clusters, build the application, transform and move all the transformed binaries into these ECS clusters, and actually run them.

The Orchestration Agent also has access to several specialized agents. The Assessment and Planning Agent is responsible for assessing the customer's portfolio to determine what applications are running and what databases they are communicating with. It generates a dynamic plan that is customized based on what the agents discover in the customer's environment, depending on the version of the applications and how many databases they are talking to.

Once a plan is built, the Schema Conversion Agent is primarily responsible for migrating all the database objects across, converting SQL Server database objects like tables, stored procedures, and views into their PostgreSQL equivalent versions. The Orchestration Agent also has access to a data migration tool, which performs all the data migration. Once the schema is converted, the data can be migrated from source to destination.

The Code Transformation Agent identifies the underlying frameworks used for queries based on the nature of the applications, transforms the SQL queries based on how the objects were converted, transforms the code, and then commits it into a specific branch. The Deployment Agent is capable of taking this code, building it, launching necessary EC2 instances or ECS clusters based on customer preferences, copying the binary across, and then building and running the application.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1630.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1630)

The system is completely agentic. All of these agents and tools work together to make the workflow dynamic. Based on the nature of the applications, the version of applications, and the number of databases they are talking to, the plan is customized and generated based on customer preferences.  All interactions that customers have with the system are conducted through chat. The Orchestration Agent has access to a chat agent, which enables customers to provide natural language queries and receive necessary answers.

The chat agent has access to an underlying knowledge base, which encompasses all AWS knowledge as well as how the job is progressing. All reports generated through the process and any errors the system encounters are committed to the knowledge base so that the chat can provide guidance to customers on what the next steps are. The chat agent has access to common errors that customers run into, and whenever there is an issue or error, it can provide guidance on how to unblock yourself so that you are able to move forward. The chat is interactive and knows where you are in the system and what you need to do next, providing you with the necessary guidance.

The workflow is completely composable based on customer preferences. You can choose to migrate data, skip that step, perform deployments, or take control of the deployment process. The entire modernization journey is customizable. Even though these agents are capable of making decisions, they also provide visibility of what they are doing and provide the ability for customers to inspect and interject as needed, controlling the overall modernization process through the hints that it generates.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1730.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1730)

### Deep Dive into Each Phase: Assessment, Schema Conversion, Code Transformation, and Deployment

Now that we have looked into the overall system, let's dive deep into each phase. The first is assessment.  The Assessment and Planning Agent has access to the source code repositories and databases. They use the source code connector as well as the database connector to first validate that all the prerequisites are properly set. In case any prerequisites are missing, through chat, it tells the user what the missing pieces are and how to fix them so that the job can be successful.

Once the prerequisite validation is done, it performs a discovery. It identifies all the applications that are configured in your source code repositories and discovers all the databases that are running, then tries to create a mapping between which applications are talking to which databases. There are several ways in which this is done. The agent tries to parse the source code to figure out if there are any connection strings configured or if there are any environment variables that are set up in the pipeline that can help identify which application is dependent on which database.

It also has a fallback logic where it tries to identify the database objects that are referenced in code, such as stored procedure names, view names, or table names that are being used in code. It is able to detect that in your source code as well as detect that in your databases and then create a cross mapping between these two. It then creates logical units of dependent applications and databases that should be transformed together.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1840.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1840)

Once these logical units are formed, the agent tries to determine the overall complexity of transforming this logical entity from SQL to PostgreSQL. There are different factors it uses, such as how big the source code is, the lines of code, as well as the number of objects present in the database and how compatible they are between SQL and PostgreSQL. Based on this, it computes a score, and once it computes a score for each of these logical units, it  creates modernization waves, with each wave comprising a logical unit of dependent applications and databases that should be transformed together.

The system being completely agentic also gives customers an opportunity to express their own preferences on how they want to transform and the order in which they want to do them. They can express that through natural language, and the agent goes in a loop adjusting the wave plan based on user needs. Throughout the process, a lot of assessment reports are generated, and if the agents run into any errors, all of that data is consistently stored in the knowledge base so that the chat agent can continue to tell the user what to do next or how to unblock themselves.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1900.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1900)

Once assessment is done, the conversion process starts. There are three steps in converting a SQL database to its PostgreSQL counterpart.  First, the agent connects to the source databases, fetches all the networking information, and figures out what roles are needed to launch the target database. Based on the networking information it gathers from the source, it creates a destination cluster and configures the PostgreSQL cluster so that the agents can communicate with it and create all the database objects.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/1930.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=1930)

Once that is done,  the orchestrator uses a schema conversion agent that starts the conversion process and converts tables, views, and stored procedures into their PostgreSQL counterparts. The agent is equipped with knowledge of some of the complexities of this conversion and some of the common errors that customers generally run into, along with examples on how to fix them. Using these examples, it goes into a self-debugging loop, trying to generate the required object, validate it, and if the validation fails, regenerate it using LLMs until the validation is successful.

The validation works by performing syntactic validation once the corresponding PostgreSQL objects are generated, ensuring that the created objects are compatible with what PostgreSQL expects. During this process, it also generates a mapping file showing what the source database objects were and what the converted database objects are. This mapping is important for the agent to update the corresponding queries in the source code.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2020.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2020)

Once the schema is successfully transformed,  there is an optional step where the orchestrated agent has access to an MCP tool, which uses database migration service under the hood to move the data from SQL to PostgreSQL. Once the database conversion is successful, the orchestrator starts the code transformation process. For the code transformation agent, it uses the application source code as well as the mappings that were generated between the source and the destination database as primary inputs.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2050.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2050)

There are different patterns in which applications can interact with underlying databases.  The most popular patterns are using either Entity Framework or, if they are older projects, ADO.NET. Based on what it figures out, if an application is using Entity Framework, the agent is capable of understanding how the application is built and updating the corresponding entity bindings for them to work with the destination database. This is also an iterative process where it uses LLMs to figure out what changes need to be made.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2100.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2100)

There are different ways in which Entity Framework can be configured. Some of the popular patterns are either using annotations or API invocations, or there are also XML binding files and XML configuration files that have information about how these bindings are maintained between the source code and the database. The agent can figure out these patterns and automatically update the code to make it compatible with PostgreSQL. Once the validation is done, and in the case of applications that are relying on ADO.NET,  the agent parses the code, identifies the corresponding SQL queries, extracts them, and first uses a rule-based engine to transform these extracted queries into PostgreSQL.

It then falls back to an LLM-based model where it generates PostgreSQL queries and validates that the queries are accurate and working in the same way. The validation is two-phased. It first does a syntactic validation to ensure that the query is syntactically correct, and then it does a functional equivalence validation where it relies on formal verification mechanisms to ensure that the generated PostgreSQL query is functionally equivalent and generates the same outputs as the original SQL query.

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2160.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2160)

Once the transformation is complete, the agent has validated the transformed queries. It also must ensure that the corresponding surrounding .NET code is working as expected.  For that, it does a .NET build of the application once the queries are replaced. If the build fails, it goes into a debugging loop where it tries to fix the build errors, generate new code, and then retry the process until the build succeeds.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2200.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2200)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2210.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2210)

In this process, if the agent finds that there are unit tests associated with the source code, it also builds the application and then runs those unit tests. Once they are successful, it commits the transformed code into a new branch. Once the code transformation is complete, the deployment process kicks in. The deployment agent primarily takes the  transformed source code from the target branch, launches an EC2 instance, and builds the application. Once the application is built, based on customer  preferences, it either launches an EC2 Linux instance or launches an ECS cluster, copies the built binaries into these, and brings up the application.

The deployment agent then does a couple of validation steps to ensure that the application is up and running. First, it does a health check to verify whether the application is reachable. Second, it does a connectivity check between the application and the underlying databases so that customers can continue to test using either an automated test suite or manual testing. This is an iterative process where customers can continue to check out the code, make corresponding changes, and commit it back to the branch. The agent always picks up the latest code, builds the binaries, and copies them back to the launched EC2 Linux instance or ECS container so that customers can continue to test.

### Customer Success Stories and Getting Started with AWS Transform

This is a high-level overview of how the full-stack Windows modernization works. Based on how customers have been using the system and the feedback they have provided, I will now hand over to Khuram to discuss those details. Thank you BJ. I want to highlight two important things. First, this full-stack modernization is available for SQL Server hosted on EC2 Linux and RDS today. Second, and more importantly, this service is available at no cost to you. You do not pay for using these agents. You only pay for the resources which you use, like the EC2 instance or PostgreSQL cluster, but the entire agentic workflow and access to it through AWS Transform is at no additional cost. Hopefully, that gets you excited to try it out.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2330.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2330)

Based on our customer feedback  and what we have been working with, Verisk is one of them. Verisk has been in the insurance industry providing technology solutions to customers. They expect that with this combined full-stack modernization, they can really see the output accelerating their modernization timelines by 4x while achieving cost optimization of about 40%. Teamfront is another customer who was an early adopter of AWS Transform for .NET. They were able to transform their monolith application from Framework to Core. They even participated in our beta for UI-based transformation, were able to take their web forms to Blazor, and they are now looking at SQL to PostgreSQL transformation to do the complete modernization, which will help them do the full-stack modernization as a combined step instead of multiple steps. Commonwealth Bank of Australia is also looking at the overall transformation of how they can accelerate it, and with AWS Transform, they see a path.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2420.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2420)

AWS Transform provides a path that can help organizations achieve their modernization objectives in months rather than years of effort. We have been working with some of our key launch partners  including Kalin Technologies, Presidio, and Evolved Tech Systems who have been helping us work together on database migrations. The key insight we hear from them is that bringing together code modernization alongside database modernization helps them accelerate work with their customers in a more effective and optimized manner, rather than moving across multiple steps of transformation involving different personas.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2450.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2450)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2470.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2470)

From our perspective, we encourage you to scan the QR codes and go to AWS Transform to understand more about how it works. There is an interactive demo you can look through to see exactly how the solution operates.  You can also attend more specific sessions on this topic. We will be offering workshops where we will walk you through each step of the full-stack modernization process, including how .NET code transformation and SQL Server to PostgreSQL conversion works. 

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2490.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2490)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4be4ca91b12542ff/2520.jpg)](https://www.youtube.com/watch?v=t1JP8lwG5lU&t=2520)

If you want to get more in-depth information, you can scan the QR code and access SkillBuilder to get trained on AWS Transform.  Ultimately, we are here to work with you. Any feedback, comments, or areas where we should be doing better or where we can help you address challengesâ€”please reach out to us. We are always available and will work backwards from your request.  Thank you, and we are now ready to take questions.


----

; This article is entirely auto-generated using Amazon Bedrock.
