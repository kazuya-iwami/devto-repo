---
title: 'AWS re:Invent 2025 - Customize AI models & accelerate time to production with Amazon SageMaker AI'
published: true
description: 'In this video, Eric Pe√±a and Lauren Mullennex from AWS demonstrate SageMaker AI''s new model customization capabilities for building agentic applications. They explain why custom models are growing twice as fast as off-the-shelf foundation models, addressing challenges in domain-specific performance, scalability, and sustainable differentiation. The session showcases the newly launched SageMaker AI model customization agent, serverless reinforcement learning (RLVR and RLAIF), and integration with serverless MLflow for experiment tracking. Lauren provides a live demonstration of fine-tuning Qwen 2.5 7B Instruct for tool calling, improving accuracy from 25% to 67% in 30 minutes using a custom reward function. The demo covers the complete workflow: dataset preparation, serverless training, evaluation with custom metrics, and deployment to SageMaker or Bedrock‚Äîall accomplished in under an hour without infrastructure configuration.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/50.jpg
series: ''
canonical_url:
---

**ü¶Ñ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


üìñ **AWS re:Invent 2025 - Customize AI models & accelerate time to production with Amazon SageMaker AI**

> In this video, Eric Pe√±a and Lauren Mullennex from AWS demonstrate SageMaker AI's new model customization capabilities for building agentic applications. They explain why custom models are growing twice as fast as off-the-shelf foundation models, addressing challenges in domain-specific performance, scalability, and sustainable differentiation. The session showcases the newly launched SageMaker AI model customization agent, serverless reinforcement learning (RLVR and RLAIF), and integration with serverless MLflow for experiment tracking. Lauren provides a live demonstration of fine-tuning Qwen 2.5 7B Instruct for tool calling, improving accuracy from 25% to 67% in 30 minutes using a custom reward function. The demo covers the complete workflow: dataset preparation, serverless training, evaluation with custom metrics, and deployment to SageMaker or Bedrock‚Äîall accomplished in under an hour without infrastructure configuration.

{% youtube https://www.youtube.com/watch?v=WeI-1WB5leE %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### The Growing Importance of Custom Models in AI Applications

Hello, everyone. Welcome to our session today. Before I begin introducing my presentation partner, I would like to assess a little bit of what your experience is with regards to how you are enabling your agentic solutions. So, would you mind raising your hands if you're building agents that use off-the-shelf foundation models, querying API? Raise your hands. Fantastic. And then, how many of you are using or considering using custom models with your own data? That's great. Awesome.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/50.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=50)

So, today,  my colleague Lauren Mullennex, a Senior Worldwide Specialist Solution Architect for Generative AI in SageMaker, and myself, Eric Pe√±a, I'm a Principal Product Manager for SageMaker AI. We are going to introduce you to how SageMaker AI is going to enable you with this new capability we launched yesterday for customizing models so that you can create better agentic applications with increased performance and better results.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/100.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=100)

So, let me begin by setting the stage with a very important trend that you might be aware of. A recent study found that  the pace at which spend in AI applications is growing is twice as much for those applications that rely on custom models. You might argue that in absolute values, of course, querying directly a foundation model, an off-the-shelf foundation model API is still bigger, but the pace of growth is increasing. The pace of growth is twice as much for custom models. So, let's understand why custom models are going to be more and more important.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/140.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=140)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/170.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=170)

 First of all, the obvious reason, right? If you want to have a model that has better performance for a very specific domain, then leveraging even the most powerful off-the-shelf foundation model might not give you the performance and accuracy that you're looking for for your specific application. But not only that, as your applications and agents move from  the proof of concept stage all the way to production implementation and then become a success, scaling those workloads becomes more and more, less and less efficient when you're using just a mere off-the-shelf foundation model API.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/200.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=200)

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/220.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=220)

There are a little bit more niche reasons as well.  Let's say that you are operating in a highly regulated industry, and you have very strict compliance and security requirements, then leveraging just an API for a foundation model becomes less and less feasible. But I think the most important point here is  they're just building a wrapper on top of one API of a foundation model doesn't really give you a sustainable differentiation from a product perspective. Even if it's just providing value for your internal customers, or even more importantly, when you're shipping value to external customers. So as you can see, customizing a model provides this increased value when it comes to how you will build those agentic solutions that you will then provide to your customers.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/270.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=270)

### Challenges in Model Customization and the End-to-End Workflow

Let's put this in context.  How would you go about customizing a model? As you can imagine, this process is not always straightforward. The first is that you have a plethora of customization techniques, right? You can begin with the usual supervised fine-tuning, but then if you want to get an extra edge when it comes to additional performance, you might be exploring additional techniques such as preference optimization or reinforcement learning. But that requires

extra research time and effort. It's not something that is easily available, and you can iterate quickly.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/320.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=320)

The second part is also  you have to have some knowledge about the infrastructure that your use case requires based on the customization technique at hand. So how many GPUs will you use, right? It's always a factor of the dataset size, the customization technique, so it's always a trial and error iteration workflow that can take a lot of time for you to fulfill.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/350.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=350)

 Then, what we have found when talking to customers, many of them have a very good understanding of what the business use case is. They understand what they want to accomplish, but then moving from that particular definition onto a model customization specification is much more complex, right? How do we move from a business use case description to an actual model customization that involves the dataset that you are going to use, the model choice, and the customization technique?

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/390.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=390)

And finally,  you have to be able to do this at scale, right? The very nature of AI and agentic solution building relies on you quickly iterating on multiple experiments. This is not just let's try this and this will be the solution that ends up being used. The more you can do in order to get more experiments running, the better you'll be able to achieve the accuracy and performance goals that you want to get to in order to make your use case feasible and a success.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/440.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=440)

So in order for us to show you how we are going to enable you with the tools to tackle  these challenges, let's put this in the end-to-end workflow for model customization. And then we will see how we are addressing each step of the workflow. As you can see, you begin by defining goals and evaluation criteria. What is it exactly that you want to achieve? Then you gather your data, you craft your datasets based on what you can get, enhancing the data that you have or even generating synthetic data in order to enable your use cases.

Then it's the process of customizing the model itself, right? How can you get to the right technique, choosing the right model and creating that custom model, and then begin assessing based on the evaluation criteria that you have defined in the previous step to whether you get to that end result that you are seeking to achieve. Then it's rinse and repeat, right? This is not something that you do once. It's something that you have to continually experiment to try to find that combination of data and model that makes your application or your agent a success. Finally, deploying your model, roll out to production and begin reaping the benefits of this new custom model.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/550.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/560.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/570.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=570)

### Introducing the SageMaker AI Model Customization Agent

Let's go one by one then. How are we going to help you define your goals and arrive to a better  use case specification? Yesterday we launched the preview of our SageMaker  AI model customization agent. The model customization agent, what it does  is it will take your use case definition in natural language. It will go back and forth with you to define exactly what the use case is and what evaluation criteria is the best to achieve the goals that you have set out to achieve in your use case definition. And finally, it will lay out that use case specification for you to approve and continue forward.

This might eventually end up customizing a model if you choose to move forward with the agent as well.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/620.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=620)

So, this is the UI of Amazon SageMaker AI, SageMaker Studio. When you go to the new  models page, you will see that the models that are supporting this new customization capability will have the customized button, and you have three options there. The one that is labeled in preview will enable you to launch a conversation with the agent. You can also begin the conversation without selecting a model, in which case you will have an opportunity to go back and forth with the agent and then choose one.

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/660.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=660)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/670.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/680.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=680)

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/690.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=690)

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/700.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=700)

So, here's the UI of the agent. The agent  will go into a conversation with you to define exactly what you're seeking. Once it has defined,  once you have laid out exactly what you want to achieve, the agent will suggest  additional information for your use case, and then provide you with options to choose with regards to the model that you can use and the customization technique.  Finally, it will lay out the list of success criteria that you can approve or maybe  go and edit. Maybe you see that you want to add, tweak, or remove some of the success criteria. If you're not satisfied, you can continue iterating with the agent until you're satisfied with the success criteria and approve it, and then you move forward.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/720.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=720)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/740.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=740)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/750.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=750)

### Dataset Generation, Enhancement, and Asset Management

 So that is the way this agentic back and forth in which we're making it easier for you to be able to move quickly from just a raw business use case into an actual model customization specification. Then the most  important piece in model customization, which is dataset gathering and enhancement and curation.  This agent will be able to request you to provide some examples or additional context with regards to the data that you want to provide, and then generate data that matches those examples that you have provided.

But it will not only provide the synthetic data, it will also analyze it based on a set of criteria that include Responsible AI controls so that you are aware and are cautious about the data that you are using to avoid bias, to avoid harmful content. That data generation will require you to choose any underlying compute. It is serverless, end to end.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/810.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/820.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/830.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=830)

And so, here's a sample of this back and forth. The agent tells you  that it's ready to begin gathering data, and then you provide those examples. And then the agent  shares with you the specification of your dataset. Once generation,  you decide exactly how many features you are going to be using or you want the agent to generate, additional configuration items such as the S3 bucket you want the data file to reside on. And you can also provide a history bucket of contextual data you want to have the agent use as a reference to generate that data.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/860.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=860)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/870.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/880.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=880)

You choose the number of rows, the bucket,  and once generation is complete, you can see the quality of your data based on what I was just describing.  Your statistics, your AI quality metrics, including Responsible AI  criteria. But I know many of you are even more prone to do things yourself. Maybe you already have a dataset, maybe you like to use your own data as opposed to just relying on an agent. That's also something that we anticipated customers will prefer to default, and then as a consequence, we have

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/930.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/940.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=940)

created the means by which you can upload or create your datasets and then use them in any customization job that you would like to run in the platform. Here in SageMaker Studio UI, you have the Assets section on the left.  And the first item is, of course, Datasets. You can create a dataset, and then  you will get to a form where you can name your dataset and upload your file, or you can also provide an S3 bucket if you have already uploaded your dataset to S3.

In a moment, we are going to continue talking about what the techniques are going to be. One of which is really important, and I know that you might have already been doing some research on it. Reinforcement learning is becoming more and more important as a means to do model customization for model alignment. One of the things that we consider is what other assets you may also want to build, to upload, and have ready for any job that may come after that.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1000.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1000)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1030.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1030)

We also created the Evaluator asset.  The Evaluator asset has two types. One is a reward function, which is the code that you would create for, let's say, Reinforcement Learning with Verifiable Rewards. You just create it once, and then you can use it in any model customization job that you want to run. The UI will provide you with guidance as to what the code should  look like from a format standpoint. Under the hood, it will be created as a Lambda function. As you can see, you can go to Lambda as well, create your reward function in Lambda, and then just pull the Lambda ARN into the UI in order to create that evaluator that you can reference.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1080.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1090.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1090)

The other is, how about Reinforcement Learning with AI Feedback? In that case, you may be needing to create prompts for your reward models. Here you can create those ones and then use them as you please in the UI. In a similar manner, you choose the reward prompt, and you're guided to the format that you  can use so that you can create those reward prompts and use them on your model customization  experiments.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1120.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1120)

As always, you can do this very simply if you want to use a code-first approach. We have updated our Python SDK to include the constructs that will enable you to create these assets for model customization. I know many of you might be more comfortable using code first, so we are making it easier for you as well in that arena. 

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1130.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1130)

### Serverless Reinforcement Learning and Streamlined Model Deployment

How about customizing the models? One of the things that I mentioned was  the infrastructure configuration complexity. We've talked about this. You need to know what GPU I'm going to be using or I need to use, how many of these accelerators are required to run these jobs. We are abstracting that configuration complexity from the process so that you can focus on what is important for you, which is creating experiments that walk towards your overall performance goals.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1190.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1190)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1200.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1200)

That's why we launched yesterday serverless reinforcement learning. In other words, you will be able to launch reinforcement learning jobs without needing to configure any GPU configuration or the number of nodes. As you can see, the UI will also have, and that's the first option there, Customize with UI.  If you know what you want to do, you just click Customize with UI, and that will take you to a very simple form where you choose your customization  technique. In this case, let's say that it's Reinforcement Learning with AI Feedback.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1210.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1210)

That will allow you to choose a reward model.  In this case we're using Bedrock under the hood. This will be using Bedrock APIs, and after the rollout phase, the reward is being provided using this reward model in Bedrock to provide rewards and then continue the training loop. Of course, the second piece is what prompt you are going to use for the reward model. You can choose custom, and in that case, what you're going to do is choose the evaluator that you created in the other section that I showed earlier, or you can also provide the code yourself in this interface and then create that evaluator at the same time.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1260.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1260)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1290.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1290)

Of course, you can tweak some hyperparameters that we are exposing. This is not, as  you may already have noticed, this is not an exhaustive list. This is, in essence, a very focused set of hyperparameters that we have heard from customers that they normally tweak the most, but we would love to hear from you which additional ones would you think would be useful to have as part of the ones that you can tweak immediately in the form.  We have advanced configuration if you want to go deeper, and some additional configurations such as the MLflow app that we are going to use. I'm going to talk about MLflow in a minute.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1310.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1310)

So once you click submit, this will land you on the training job details page,  which will show you immediately the training metrics so that you can stop the job if it's trending wrong. You can check and troubleshoot using the log stack. You can also gain access to the artifacts tab to see where your model artifacts are going to be stored. But everything, as you can see, is revolving around the single concept, which is the most important piece here, which is the custom model.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1350.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1350)

The custom model is the star of the show, and as such, we have a button up there that you can use to go to the  actual custom model details page. Once the job has run, you can go to the custom model details page so you can see exactly everything about that custom model. The lineage, where you can track which dataset was used for training, and you can also trigger additional actions such as continued customization. You can, for instance, run a reinforcement fine-tuning job after running a supervised fine-tuning and continue improving your performance.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1390.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1390)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1410.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1410)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1420.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1420)

You can also do this in code. So we have the  new SDK classes for you to launch these serverless training jobs from your code. It can be a notebook, it can be a Python file, etc. This is an example of one of the classes that we have created.  What you do next is evaluate your model, obviously. If you want to have another dataset to assess whether you are achieving your performance, you can run evaluation of models  using regular benchmarks, although I know all of you know that that might not be as useful if you're looking for a specific use case.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1470.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1470)

So you can create your own custom metrics. You remember the evaluators? You can use evaluators to create more custom metrics that can be used here for evaluation as well. You can also leverage prompts for LLM as a judge evaluation. At the end of each evaluation, you will have visual scorecards that show you how you evaluated. This is the button, and you can check the evaluation job configuration when you create or choose an LLM as a judge  option, and you can create your prompt from a template in order to run that evaluation.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1480.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1490.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1490)

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1500.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1500)

And then  how do we make this an efficient workflow so you can do it multiple times? Well, we're integrating,  this is the iteration part, we're integrating with MLflow. We launched this year as well serverless MLflow, serverless MLflow.  You don't have to configure or pay anything else.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1520.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1530.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1530)

A serverless MLflow app will be created for you and will immediately begin logging to the app as you run experiments at no additional cost. As you can see, there's a button that says view  metrics details in MLflow. You click it and then immediately launch a new tab in your browser with the MLflow interface that you know and love. 

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1550.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1550)

Finally, once you have conducted your experiments and now have a model that you are satisfied with, you can deploy it. So let's see how we make deployment easier. SageMaker AI now enables you to either launch  or deploy your custom model to SageMaker AI inference endpoints or Bedrock as well. So you can select your option, and Lauren is going to show you how exactly it is done. You can launch and create a model that you can then query through Bedrock APIs. Both are totally possible, and you can do that from the UI.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1600.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1600)

### Demonstrating Tool Calling Customization with RLVR

All right, so this is a very quick pass through all the experience. I think now it's time for Lauren to come and show you exactly how this looks like. Okay, before I show everyone what the new model customization UI looks like  with SageMaker AI, let me set some context. So as Eric just mentioned, the model customization process is often a month-long process. You have to spend time researching what model to use and what fine-tuning techniques should you choose, as well as coordinating with your platform team for compute access. Then you spend time experimenting with different approaches and optimizations. The new model customization features that we have with SageMaker AI help remove friction from this process.

The use case I'm about to demonstrate is for tool calling. So what exactly is tool calling? Tool calling enables models to interact with external systems. So you can do API calling, you can query a database, or you can also execute functions. So this is what makes agents very useful in production-ready environments. Now, base foundation models are often not production ready for tool calling. They hallucinate tools that don't exist. They fail to call tools that are readily available, and they also execute on incomplete information. And this is where model customization can help with this process.

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1700.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1700)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1710.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1710)

So I'm about to show you how in less than an hour you can use the new model customization UI on SageMaker Studio to select a model, evaluate its performance, fine-tune it, evaluate its performance, and then deploy it. So let's get started. Here I am in SageMaker Studio, so I'm navigated to the model section.  You can see here that we have over 1,000 foundation models that we've vetted. We've also provided hundreds of fine-tuning recipes for them. 

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1730.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1730)

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1740.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1740)

For this use case, I'm going to choose the Qwen 2.5 7B Instruct model. It's a good choice for tool calling because it has excellent instruction-following capabilities and it has good performance for its size. So here you can see that you can customize a model with an AI agent that's in preview that  Eric just went over. I'm going to specifically walk through the customize with the UI capability in SageMaker Studio. So all I do is click this. 

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1750.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1750)

And you can see that we have this template for customizing Qwen 2.5 7B Instruct. There are different customization techniques you can choose.  So supervised fine-tuning, direct preference optimization, as well as reinforcement learning techniques. So we have reinforcement learning with verifiable rewards, or RLVR, or reinforcement learning from AI feedback, RLAIF. I'm going to walk through reinforcement learning with verifiable rewards.

What exactly is RLVR? Well, we just talked about how base foundation models often struggle with tool calling precision. So we need to find a way to improve the exact correctness of the outputs. So with RLVR, for each prompt, we generate multiple different responses. And then we have what's called a custom reward function, and that reward function then verifies if each response is correct.

This enables the model to learn and increases the probability of the correct outputs relative to the incorrect ones. This also uses what's called Group Relative Policy Optimization, or GRPO. For each prompt, multiple responses are generated. We're going to have eight in this case. They're in a group, and they're ranked or scored by correctness here. So that's what I'll be showcasing.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1840.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1850.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1860.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1870.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1880.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1880)

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1890.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1890)

If I choose RLVR, you'll see that I have the option to either use a built-in reward function.  We have Exact Match or Code Execution Math Answers, but in this case, I really want to define what good looks like for my use case exactly. So I'm going to use a custom reward function.  And so what this is, I'm going to navigate to our Evaluators page, and here I can create a reward function or a reward prompt.  You can see that I've created one earlier, and all it is is Python code that I have. You can also upload your Lambda  function as well. See if I can zoom in. But essentially, this reward function that we're going to use has, it's going to extract  the tool calls from the model's responses and compare it to our ground truth. And it's in a three-tier scoring system. So what that means here is it's going to  give these different scores here in the middle. So one is a perfect match, which means it's calling the right function and the right parameters. 0.5 is a partial match, so that's the right function but the wrong parameters, and zero is wrong match or it's a missing tool call.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1940.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1950.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/1960.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=1960)

Now, our reward function handles an important edge use case. What that means is sometimes it's correct to not do a tool call. For example, if you have a user who asks the model what's the weather like, but they don't specify a location, then the model should respond and ask for clarification instead of providing a bad API call. So this reward function recognizes and actually reinforces that behavior.  Right. So now I'm going to navigate back to my page for my training setup and I'm going to choose that reward function that I already set up. And then you have the option here to upload  a dataset, have one that's already in S3, which I already do. So I'm going to select this existing training dataset that I have. And this contains 1,500  tool calling samples. So it's a dataset that's derived from the NVIDIA's When to Call. And it has examples of three different types of behavior. So it either executes a tool call when you have sufficient information, it clarifies when there's missing parameters, or it refuses to provide a response when there's harmful information, for example, which I'll show at the very end.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2020.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2020)

You can also see that there's an option to provide your validation dataset. If you do not provide one, SageMaker AI will automatically split your training dataset. So you'll have your validation data metrics that are already provided for you during training. And what this enables is that the model behavior can improve by not seeing these actual validation examples, so it doesn't just learn on the training examples as well. Here's where we set our output model artifacts location at S3.  Here's the hyperparameters that we can configure. So for batch size, I'll leave this at 128. That sounds okay for the size of the model and for the amount of data records that I have. Update the learning rates and I'll have this passed through the data three times.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2040.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2040)

### Serverless Training Results and Model Evaluation Metrics

Also, we have rollouts here that's set to eight.  So that's essentially that we're generating eight responses for each prompt. Also, we are doing experiment tracking with our new serverless MLflow. So I've already created a serverless MLflow app, so you don't have to worry about any infrastructure provisioning there in MLflow, brand new capability we launched. And MLflow automatically tracks all our training metrics. So that includes our loss values, as well as our reward scores in this instance and hyperparameters. So that way you can compare your training runs and then also reproduce results.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2080.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2080)

Now we also have some, of course,  security, top of mind here at AWS, so you can have this run in a private VPC, for example, and then have encryption with KMS keys for storage as well. So, let's just talk about what happens next. You may have noticed that I don't have to worry about any training configuration setup here. So, as Eric just mentioned, one of the key features that we just launched is serverless training.

In fine-tuning, you often have to provision GPU clusters, manage distributed training strategies, and configure communication between GPUs as well as handle fault tolerance and checkpointing. That's often weeks of infrastructure work before you can even get started. We've removed that infrastructure complexity here, so you don't have to worry about that. All of this runs completely serverless.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2140.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2140)

Everything looks good here. I'm ready to submit my training job, my fine-tuning  job. This will take about 30 minutes to complete. Let me navigate to a training run that I've already completed to show you the results.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2170.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2170)

We'll see here this took a little over 30 minutes to complete on our 1,500 samples. I have my training metrics that are loading right here that have already been populated, which you can also view in serverless MLflow. The main metric that I want to focus on,  the key one, is the train reward statistics. What that means here is this is the model's reward score, or how often the right tool was called with the right parameters. It started off here at about only 25 percent. The base model was able to make the right tool calls at that rate.

During the 30 plus minutes of training, we hit 0.67, or are now able to generate 67 percent of the time the right tool calls. We essentially doubled our performance just in 30 minutes doing fine-tuning on this model. This looks pretty good to me so far.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2230.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2230)

What we should do now is evaluate on a held-out test dataset so that we can measure the model's real-world performance on examples it's never seen before. We're able to do that as well with the new model customization features. Here we go to our custom model.  Then we're able to evaluate it. That's what I'll be doing. You can also continue on customization, so perhaps you want to adjust the hyperparameters. If 67 percent of tool calls isn't the right metric you're looking for, you can also train with a different technique as well.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2270.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2270)

We're going to move on to evaluation. Eric just went over that there are a lot of different evaluation types. For RLVR in this use case, it makes sense to have a custom reward function like we've been using. I'm going to choose the custom reward function. I'm going to use the exact same one  and I'm going to compare it with the base model too in our evaluation runs. I want to use the same reward function because that's evaluating the model on the same exact specific criteria that we set for training.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2290.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2290)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2300.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2300)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2310.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2310)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2330.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2330)

I picked that reward function, and I've already uploaded a  similar dataset for evaluation with the similar behaviors that I talked about earlier, so execute or clarify or refuse. This has 300 samples, so  definitely smaller for evaluation. Here are the advanced configuration parameters I need to set. This is just the context window.  I'll have that at 2,046, and the max new tokens is just the tool call responses, so I'll set that to 512 tokens. I'll leave the temperature and the top K and the top P the same. I'll keep this as the default as well. 

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2340.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2340)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2360.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2370.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2370)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2380.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2380)

I'll submit that, and this will take about 10 minutes  to actually run the evaluation jobs on SageMaker, and we'll compare the base model to the custom model. Let me navigate to an already completed run here.  Let's take a look first at the lineage just to show you that this is also available, which is very helpful then  so you can do governance and determine where your actual data is coming from, what you've been doing. You can see your pipeline, and then you could deploy this to production as well. This is all available here. 

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2400.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2400)

You also have logs that you could take a look at to determine what the model's doing. Maybe you need to do some early stopping. Then we have the evaluation metrics that are already loaded for you here as well. These are also available in MLflow by default.  The main metrics that I think are important to highlight are the tool call reward.

You can see that the base model is at 0.35, and we were able to increase that by 20 points to 0.55. So what does this actually mean? It means that the model is making the right decision either to execute a tool, clarify on missing parameters if there are missing parameters, or refuse the request. We're able to do that now 55% of the time, and all we had to do was about 40 minutes of training and evaluation, which is often a multi-month process just to set this up.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2490.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2490)

The next metric that I think is important is the F1 score quasi. The base score you can see here is about 46%, and the custom score, so our fine-tuned model, is now at 64.6%. This actually measures the output quality. What that means is that the model generates the right function name, the correct parameters, and the proper JSON formatting. So essentially two-thirds of the time, the outputs are functionally correct here. You can also view the full evaluation results in MLflow. There's a lot more metrics you can take a look at. 

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2500.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2510.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2520.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2520)

So I can compare the different training runs I had here.  All I have to do is check those and then click compare, and then you can see here's our base model on the left and our, actually I probably just clicked the wrong ones, but the custom model on  the right, and you'll be able to see all the different metrics and compare them. So there's more in here that are system metrics and so forth. 

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2560.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2560)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2590.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2590)

### Real-Time Inference Testing and Production Readiness

Now our evaluation looks pretty good to me, and we're ready to deploy this model and test it for inference on some requests that we have. I have the option to deploy on Bedrock or on SageMaker. So let's do SageMaker for real-time inference. I'm going to create a new endpoint, and here I just type in the endpoint name. By default it's recommending an ML.G5.12xlarge instance. I can update that depending on your needs, and you also have some options as far as updating  the instance count, the max instance count. There's also some auto-scaling that you can set as well. If you notice here, I don't have to build an inference container or write any serving code. We've abstracted that away for you, so it's very easy to deploy here. I'll just click deploy, and this will take about 10 minutes to provision that real-time inference endpoint in SageMaker. 

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2600.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2610.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2620.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2620)

So let me navigate to an endpoint that's already up. You can see that we have our base model and our LoRA adapter, so that's our pre-trained weights combined with our learned adaptation.  And here we also have a playground that you can test the model's responses.  So I'm going to update this to a single prompt mode, and then here you can see that we have a system prompt. This is very important for tool calling  because here you're able to define the tools that are available as well as the expected JSON outputs, and this also includes our three behaviors that we trained on. So that would be the execute, clarify, or refuse.

So let's test out some examples here to see what this looks like in action. We also have some other parameters we can update. We have those output tokens such as the model's response, as well as the randomness and diversity. I want to keep the temperature pretty low because I want these tool calls to be deterministic and not be very creative. Top P looks good to me as well, and then I just enter my prompt here in the bottom.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2690.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2690)

First, I'll start with a tool call that should execute. So I'll enter in what is the stock price for Amazon, and I can run that. And here it's actually going to generate that tool call like we expected, and then it could also  do an external API call and we'll get the current stock price for Amazon. So that's exactly what we expect the model to do. That's that perfect match as well where it's the right function and the right parameters.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2710.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2710)

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2720.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2720)

Now let's do an example where the model should ask to clarify. I mentioned earlier, like what's the weather like?  Well, right now the model has no idea where I am. There's no geolocation enabled, so it's asking for clarification.  It's asking me, could I please specify the city that I'm interested in. I could add that in and it should respond.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5fb166a1110170ee/2750.jpg)](https://www.youtube.com/watch?v=WeI-1WB5leE&t=2750)

Now let's enter in a harmful request that the model should refuse. How do I hack into an email address? Since we fine-tuned our model, it's actually responding and saying that it can't assist with hacking or any illegal activities.  It says please provide a legitimate request for assistance, which is exactly what we want the model to respond with. The results look pretty good to me, so it looks like this may be ready for production, or at least you can test this more in your QA environment and then determine if this works for your use case.

Let's just recap what we did. We were able to use the new model customization UI in SageMaker Studio. We used RLVR and fine-tuned with the custom reward function with serverless infrastructure, so we didn't have to set up any of that training. We removed all the infrastructure complexities there. We were able to evaluate the model's performance with a custom reward function. You could also use more deterministic benchmarks as well as LLM-as-a-judge, for example, on Bedrock. We were able to deploy the model for real-time inference, and we were able to do that in less than an hour.

Instead of navigating the model complexities that often take months, you can really define your use case and focus on that. You're able to iterate in hours instead of weeks or months with the new model customization features in SageMaker AI. I hope that you try it out today and start experimenting with your own use cases. Thank you, and let us know if you have any questions. Thank you all, and please don't forget to fill out the survey. That's very important for us, and we are happy to answer any questions if you want to stick around and chat with us. Thank you all.


----

; This article is entirely auto-generated using Amazon Bedrock.
