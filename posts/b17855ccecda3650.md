---
title: 'AWS re:Invent 2025 - Multi-Region disaster recovery & resilience testing (feat. Fidelity) (COP358)'
published: true
description: 'In this video, AWS Principal TAM Shataksh Sharma, Fidelity VP of SRE Heinz Ludwig, and AWS Principal PM John Formento discuss multi-region disaster recovery strategies. Fidelity shares how they moved 2,000 applications in 9 minutes during the October 2024 AWS outage by regularly testing 3,000 critical applications across 21 business units. The session covers AWS Resilience Analysis Framework, Application Recovery Controller''s Region Switch feature for orchestrating multi-region failover, and Fault Injection Service for resilience testing. Key insights include testing beyond happy paths, avoiding dependencies on primary regions during recovery, and the importance of continuous practice over annual DR tests.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/0.jpg'
series: ''
canonical_url: null
id: 3093225
date: '2025-12-08T21:25:56Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Multi-Region disaster recovery & resilience testing (feat. Fidelity) (COP358)**

> In this video, AWS Principal TAM Shataksh Sharma, Fidelity VP of SRE Heinz Ludwig, and AWS Principal PM John Formento discuss multi-region disaster recovery strategies. Fidelity shares how they moved 2,000 applications in 9 minutes during the October 2024 AWS outage by regularly testing 3,000 critical applications across 21 business units. The session covers AWS Resilience Analysis Framework, Application Recovery Controller's Region Switch feature for orchestrating multi-region failover, and Fault Injection Service for resilience testing. Key insights include testing beyond happy paths, avoiding dependencies on primary regions during recovery, and the importance of continuous practice over annual DR tests.

{% youtube https://www.youtube.com/watch?v=jZIaliFsPIw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/0.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=0)

### Introduction: Multi-Region Disaster Recovery at AWS re:Invent

 Welcome everyone to COP358, Multi-Region Disaster Recovery and Resilience Testing featuring Fidelity. I am Shataksh Sharma, a Principal Technical Account Manager here at AWS. I support some of the most strategic global financial services customers on a day-to-day basis. I'm joined by Heinz Ludwig, Vice President of SRE at Fidelity Investments, and John Formento, who's the Principal Product Manager Technical as part of Resilience Services here at AWS.

I'm glad we all made it till Thursday. A lot of steps, a lot of calories burned. I think we all could use a round of applause. Let's give it. So much information, so many announcements all the time. I think we all deserve it. Before we get into the agenda itself, how many of you all are running production critical apps on AWS today? A quick raise of hands. Awesome. How many of those apps are actually single region today? Okay, mostly. Oh, that's good. You're in the right session. And how many of you all actually had impact as part of the October events? Yeah, yeah. So you're all definitely in the right room.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/90.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=90)

 As part of the agenda today, it's mostly divided into three sections. I'm going to just cover a little bit about disaster recovery and disaster preparedness. Then Heinz is going to come up on stage. He's going to talk about how Fidelity does it on a day-to-day basis and how some of their testing strategies helped them as part of the October event. And then we'll have John come on stage, and he's going to share about the resilience services and how you all can actually use them as part of your operations and SRE functions and ensure business continuity and optimal disaster recovery strategies.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/130.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=130)

 So disaster recovery is a cornerstone of our IT operations. Historically, we would mark dates on our calendars to say we'll do our DR test on that day, possibly annually, and then there is a run-up to that event one month before, two months before, making sure all the configs are there, the data is there, and then we kind of do the test, figure out what the gaps were, and then we work for another year to get there, right? That's the historic process, and it's painful because it doesn't ensure preparedness.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/160.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=160)

 With cloud, with AWS, your apps are changing. You're deploying a lot more changes now than ever. So those annual processes do not ensure preparedness for your functions. As a Principal TAM, I was engaged with some of the strategic financial customers as part of the October events. And the number one reason where the apps could not fail over, even with their planned DR strategies, was because they lacked confidence. They were saying, okay, we have this DR plan, but we don't think, I think we're going to cause more pain if we fail over because we don't know how we're going to fail back. So it's painful.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/220.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=220)

 And I actually got hit on multiple sites as part of this event because I was on the call helping the customers, and then my kid runs up to me and he's saying, Dad, I can't play games on my iPad. What's going on? And my wife is actually outside, and she's saying, I can't open the garage door. What's going on, right? But if you take that situation differently, it could have been an emergency. You could be actually trying to access a critical function across financial services and beyond. With Fidelity here today, you would want to do a trade that's really important. You saw a dip in the market, you want to execute it, but you can't because something is down, right?

So you all are running those similar kinds of apps here, so we have to ensure that we are prepared. This is a famous quote from Steven Cyros: when disaster strikes, the time to prepare has passed. It's obvious, right? But this can be easily translated, and this quote has actually been adopted in a lot of the IT operations and in some of the guidance forces, right? So this is something that we can port over to our mental model when we do it.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/290.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=290)

 So how do we become disaster prepared? These are some of the tenets that you see on the screen. This is not holistic, but these are the key ones: identify your risks, create a disaster plan,

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/320.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=320)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/350.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=350)

build disaster recovery kits and tools, practice your disaster plan, and then establish monitoring and war room functions. What if I tell you these are not IT tenets? As part of the prep for the session,  we got this from the State of Maryland Department of Emergency Management for Natural Disasters. Clearly, there is a lot of best practices that exist beyond IT functions which we can refer to on a day-to-day basis. We can say, how do other functions do it and why aren't we doing it more for our workloads? This whole presentation is going to be around these tenets.  We're going to dive into each one of them, and then Heinz and John are going to describe how they are doing it and how they suggest you all do it.

### Five Tenets of Disaster Preparedness: From Natural Disasters to IT Operations

In identifying your risk, on the left-hand side you see landscape analysis and extreme weather simulations. This is all in the natural disaster side. On the IT side, industry standards for manufacturing and design have been Failure Mode and Effects Analysis. On the AWS side, we have published prescriptive guidance on the AWS Resilience Analysis Framework. If you look at the FMEAs, when you're building your workloads, you do a bunch of FMEA for design. You figure out how you're going to do maintenance and how you're going to do service FMEAs. There are external dependencies. As part of this event, there were a lot of impacts to third parties and external dependencies, so you have to account for this. This can get overwhelming.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/440.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=440)

A couple of years back, I was in an FMEA conversation with one of my customers, and we were chatting about how, if there is a massive event such as a nuclear missile which was sent towards the US, how would we fail over the systems? While we were trying to figure out how to proceed with this, somebody in the room said, guys, if a nuclear missile is en route, this is the least of our priorities. To make it simpler, AWS has published this graph. I love this graph because this helps you to say, let's not get overwhelmed. Let's  figure out what the probability of the failures is for your workloads, because in the end it's going to be diminishing returns over time. You can figure out what is the highest probability of the failure. You design your recovery processes around it, and you get some positive returns off of it. But there are trade-offs. There's cost trade-off and operational trade-off. It's overhead and a lot of work for everyone. You need to ensure what are your most probabilistic events that you want to work around.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/470.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=470)

 The second tenet is create your disaster plan. As you would have seen, I received those notifications that in my kids' school there was a fire drill. They make me aware of it. Initially I thought, why are they telling me every two weeks there's a fire drill? But it makes sense. They have a plan. They're executing it very often, and it's at every level. Folks who are staying in areas where there are frequent natural disasters have a family disaster plan. You have food supplies. You make sure it's stocked and ready to go. You test out the batteries. You have a town level plan, state level plan, and national level plan. We should actually do it for our workloads itself. There are big enterprises, a lot of business units, and a lot of workloads, so you should have a strategy at a workload level, business unit level, and enterprise level.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/530.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=530)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/550.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=550)

The next one is more around building disaster recovery kits.  As I already went over the left-hand side, in terms of the IT on the AWS side, we have Amazon Application Recovery Controller, which John is going to talk about. We have AWS Elastic Disaster Recovery. These are some of the kits that come in handy when disaster occurs. Practice  your disaster plan. That's where I feel, working out in the field with our customers, that we could do a better job in working with our customers to say let's build a better plan. Again, on the left-hand side, the different hierarchy of the disaster plans and the executions. But on the workload level, this is the main slide that we always take to any resiliency conversation with our customers. Pick your strategy. Do you want to go multi-region with active-active? There are cost trade-offs. It's expensive, but you get the benefits. It could be well suited for your most critical applications. Then you have other strategies. But in the end, we take these things, but how do we track it? How do we record it? Are we meeting the bar? Are we not meeting the bar? That becomes part of our day-to-day execution model.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/610.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=610)

One guidance, and this is a very simplistic diagram. Your applications are far more complex than what I have on my screen, 

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/630.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=630)

but if your applications do not have those dependencies, cross-business unit dependencies in coding terms or algorithmically, I like to say it's linear complexity. It's easy. Like one application fails, you fail it over, you're good from a business unit level or enterprise level. But we all have good  workloads, good APIs where you say, okay, that's a good data store. I want to build a dependency on it and not reinvent the wheel. But in this scenario, if it's not well thought out, if your application one fails, it could cause a cascading failure at application two and application three as well. And as we discussed at the start of the session, there were some workloads that were impacted as part of the October service event, so you read about our dependencies as well, right? And Fidelity pushes us a lot. It's like why is it broken? Why is it a dependency?

That's why you have to ensure that you create blocks for your applications at an enterprise level to say, okay, these are a set of the applications that should be failed over together to ensure business continuity and make sure there is no impact at the business unit level. But if you have more correlation between the business units, then you need to have a level up strategy as well. And that's what we have been trying to work with our customers on as part of the strategy itself, because testing it at a workload level is not going to cut it at an enterprise scale. So let's dive deeper into some of these aspects.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/710.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=710)

The final tenet is a lot simpler, monitoring. Yes, you would have seen some of the recent announcements around CloudWatch, the new enhancements that are coming. So you do need dashboards, you do need  KPIs, SLIs, SLOs, and constant monitoring to ensure you know when to fail over or when you are about to miss a KPI for your business needs. As I said, my diagram was very simple. It was not that complicated, so somebody who has figured this out better, so I'd like to invite Heinz on stage to talk about disaster recovery practices at Fidelity.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/750.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=750)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/770.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=770)

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/780.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=780)

### Fidelity's Journey: Testing 3,000 Applications Across 21 Business Units

Heinz Ludwig, Vice President of SRE Productive Services for Fidelity. Fidelity has a long history of innovating  through technology. There's a couple of examples. We bought our first computer in 1965. In 1995, pioneered the first mutual fund homepage. And in 2016, our first production application moving over to the public cloud, and that brings us back to today, nine years later,  75% of our applications are running in the public cloud, over 8,500 applications and counting. As Chaz mentioned today,  disaster recovery starts with a plan, and you need to be able to test that. And what we've been able to do and we take very seriously to ensure our customer experience is to work on resilience testing to plan for those events when they can happen as best as possible.

If you look at the chart here, this is our strategic view of maturity views of starting from infrastructure testing. When teams are actually working on infrastructure and they need to understand how that infrastructure is set up, what kind of mode is it set up in, and the infrastructure that's where most of us are probably running in today are running in multi-region, different zones, different data centers, and even multi-cloud. And you need to be able to understand, are those infrastructures able to handle faults? What it does and when you have a plan for a fault, is it working? Is it architected to handle that fault correctly? And this gets into a very complex scenario and you need to make sure teams are able to measure that, understand and respond to it, rearchitect where needed, and be able to test that out. So when it does fault, it actually responds as expected.

Moving up to the stack, the second tier is application level fault testing. Can the application handle that infrastructure fault? Do you have the right application components set up to handle that fault, and can it live through that? And this is where many teams will stop testing. My application is okay, my components are okay, but unfortunately, we do not live in an app-only scenario when we're looking at customer experience, and this is what brings us to where we are today. And application cross-level testing. This is where we have to look at our workflows across business units from a customer perspective. You're doing trading as Chaz mentioned, looking at your mobile phone and looking at any kind of service that you're looking for, it's not one application, it's many applications working together.

Our challenge around the testing of this is how do you actually test that in real-world scenarios across that end-to-end solution to figure out where do you need to shore up your applications and shore up your experience for your customers to protect them during a fault. This is very complex in some scenarios, and I'll come back to the complexity in a minute. But I want to also go up to the next level, which is one of our aspirational goals: can this be done automatically?

Could possibly AI or advanced-level programming actually be able to take out the human interaction-driven actions to take during scenarios? But again, that's aspirational. Maybe we'll come back in another talk and talk about how maybe AI and automation has fully removed some of the human interactions. But today we're living in these bottom three tiers of application testing, which we constantly test in real-world scenarios. As technology changes and applications change, it's very important to be running through these on a regular basis because nothing stays stagnant from when you've done one DR test to the next.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/980.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=980)

Coming back to the complexity that I mentioned earlier about how we don't just live in one application-only component,  just pulling out your mobile phone, logging in, and checking your account balances triggers 64 microservices across 10 different business units. Think of that. That's the reality we face today: 64 different microservices across 10 different business units at very high transaction rates. We need to be there for our customers without having any interruption when we have faults, and how do you actually test that in reality? And that's what we end up doing.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1010.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1010)

 We do this through orchestration. And we actually build out workloads. When we talk about end-to-end, this isn't looking at just your own application and how it works back with another team. You need to actually take a customer lens and look at that end-to-end solution: if you're trading, looking up the mobile app as I mentioned, making a phone call, working through our call centers. We look through to figure out how do we actually have the orchestration set up by those required workloads to plan ahead and test moving those workloads when needed together to protect the customer experience during any kind of fault.

And by doing so, it's a balance between making sure development teams can actually work with their own autonomy and work as fast as possible, but then also tying together paths that we've built and guardrails for teams to actually build to. And one common path that we've actually built to is Application Recovery Controller, and that's one of our favorite tools to actually go ahead with. John will be talking about it a little later to allow us to detect and move workloads as needed to protect that customer experience.

Once you build out common pathways, it gives the development teams and business teams paths to build to, so it gives us consistency. And once we have the consistency, we can control workloads, moving them when needed, and we're able to measure that. And that's one thing we're able to test for. Once we set up the common paths and we're able to move traffic, either active-active or active-passive to whatever region, we also have the observability and availability metrics around that to understand the data rates, error thresholds, to understand when to make those triggers. And what we do is we practice that, practice that, and practice that under planned scenarios. We also do this in production.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1120.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1120)

 Since we started, we actually do annual, quarterly, and actually with new releases, we'll actually go through with planned scenarios to execute these across 3,000 critical applications across 21 business units. To get here took a lot of practice, a lot of coordination, and figuring out where and when the workloads move, identifying the workloads that need to be pulled and grouped together. This doesn't happen overnight. This isn't magic. This is something we have to build out, work with teams to understand how to move these under real-world scenarios in production before they happen.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1180.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1180)

So we can then, as Jas mentioned earlier, when some of the customers during October 20th hesitated to move over to another alternate or their DR plan, this is something we plan for and work towards every day to protect the customer experience, building out these common paths and testing out real-world scenarios. So by doing so, since we started to where we are now, we've actually been able to improve that by 83%  and keep going. We never stop testing, and our mission always is to protect the customer experience.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1200.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1200)

### Nine Minutes to Recovery: Fidelity's Response to the October 20th Event

You say, great, why does that matter? Well, it matters when it happens in production, when there's a fault. There was an issue, as we mentioned, on October 20th, and when either you need to understand how are you going to respond when there's control and data plane issues. 

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1230.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1230)

Do you have plans around data plane issues? Can you execute on that? Do you have data points? You don't want to be asking each other what do we do, how do we do this, what are our thresholds, when do we make calls to route out or move certain workloads? Once we got our detection and once we made our decision, it took nine minutes from that detect, from making a decision to move 2,000 applications over within nine minutes, and that was to be able to protect our customer experience  so they could continue calling into the call centers, online trading, and so on without impacting their service.

As we do this with testing, we go through testing. Brazil is testing on the weekends, real world scenarios. We're going with a plan. We always come out of that with what worked well, where do we have opportunities, where do we need to shore up things, and even with October 20th, we like to share some of the learnings we've had as well. The one key thing is you might be able to control what you can control, but when you look at your whole ecosystem, you're not alone. You need to make sure you're actually working back with your third-party providers as well, understanding what are their plans, especially when you're relying on their service.

Do you have coordinated planning, DR testing with them? Is it in your contracts? Push it in there. You don't want to be asking during a real life scenario what are your plans, and you have expectation and you have confidence in that plan that you can route out. Once you have the data points coming back to you, you're able to pull back what you've been able to test in production and know that you can swing that traffic over and knowing that you could do that with extreme confidence.

And that's the other part coming down to the remaining part of the region. It's very important once there is a regional event to be able to feel confident. What are all the planning that you put in place that you're able to move that over into an alternate region or whatever the scenario might be for your DR plan? To stay on that and stay on that until the all clear is given. You have to protect that customer experience, and sometimes you might hear that there's improvement going on, and the last thing you want to do is find out you're going back and it might be intermittent or so.

You want to be able to stay in that alternate scenario, have confidence in it, and protect that customer experience. And that's where it goes back into also the regional event recoveries are very complex. Even though they're rare to happen, they do happen. You need to be planned for those. You need to plan for those. And that's where I can't stress enough, practice, practice, practice your DR. Learn from those, always a plan. You do not want to be pulling out what are our playbooks. You don't want to find out later on that something didn't work or you knew something was happening and how to work it.

Once you know that there's an issue that you can't route out, it always comes back and bites you somehow. Once you know it, you can't unknow it, and what you want to do is be able to work with your teams and partners to fix that. And I can't stress enough that to get us down to nine minutes around that, have the right data detection. Very proud to work with many of our team members at Fidelity with a lot of complex scenarios to work through 21 different business units, 3,000 applications as we went through some of these scenarios. It takes a lot of practice and work around that and learnings and with the full commitment to protect our customers' experience.

So from that, I'd like to thank you for my brief overview there, and I'd like to call up John Formento from AWS who's going to help you give an overview of the capabilities that we use and you can use also in your DR planning. So thank you, thank you.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1430.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1430)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1450.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1450)

### AWS Resilience Services: Building Confidence Through Regular Testing

All right, so that's tough to kind of go after nine minutes recovery. It's really cool, but yeah, thank you all for coming. I'm John. I lead the product team in the AWS Resilience Organization, so we focus on  Application Recovery Controller, Fault Injection Service, which we'll cover today, but also Resilience Hub. And so with all that, right, we heard from Chaz talking about how the State of Maryland and just general disaster preparedness best practices, heard from Heinz on what Fidelity  did, but how can we help you, and that's where our organization and my team spend a lot of time.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1460.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1460)

And so the two main areas  are thinking through and defining really a set of resilience tests that you can run on a regular basis. And since we're really talking about multi-region, for multi-region there's really a finite set, right? There's three or four things you can be doing on a regular basis. You want to validate your RTO is a pretty fair one. You want to determine if you're able to operate in, say, your standby region if your primary region is unavailable, or you want to see what dependencies you have. And then really you want to figure out how quickly you can detect and respond to an impairment and different sensitivities.

Because what you may find, it's not always a binary clear cut failure, right? Those are pretty easy to respond to, and if you have health checks configured and things like that, they're easy to detect. But when there's intermittent failures, when maybe some things are working,

or maybe let me phrase it this way, when some things aren't working, say 20% of the time, 30% of the time, what is that threshold for you to trigger your recovery mechanism? And then lastly you need a way to recover, right? And so you need to think about having a highly reliable mechanism to recover in another region.

One of the key points to that is again not taking a dependency on the region you're leaving, and you'd be surprised. I talked to a lot of multi-region customers where maybe they're relying on a pipeline to enact their disaster recovery or multi-region failover strategy. And that pipeline is in the primary region and so you spent a lot of time investing in multi-region. We all know it's not cheap from a cost perspective, from an engineering perspective, a lot of complexities you have to sort through, also operational complexities. But then you go to use it and it doesn't work because you took a dependency on the primary region, so it's an important characteristic you need to think through.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1570.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1570)

And so with those, we have tools that can help, right? But before I go into that there were a few hands I think Chaz asked the  question around who's doing multi-region and so I was just curious how many test their multi-region failover on a regular basis. Wow, not that many. Well, you're definitely in the right spot. So my next question is going to be how many of you tested and actually tested the non-happy path, right? So maybe using some service like Fault Injection Service to test when something is going wrong. I assume maybe less hands, but I'll ask. A couple, cool.

Okay, yeah, it's funny we've, and it's one of the really cool parts about this week, reinvent and talking to hundreds of customers is, as I'd like to sit back and think what are the commonalities I'm seeing across these customer conversations. And one is a lot of testing in the happy path. We'll pick a weekend we're going to say we're going to do our disaster recovery exercise and we go through the motions, and invariably that's going to work, right? I'd say the majority of the times those tests go really well. And the trick is doing those with actually injecting some failures so you can actually get used to that detection, because the detection time is very important.

And there's really two kind of key things to call out. One is multi-region gives you the bounded recovery time that is in your control. So a unique property of AWS is our region isolation, and the benefit to you all is that if something is happening in one region, you can have a high level of confidence that it's not going to impact the other region. And multi-region then allows you if you've detected, if you've practiced your detection and recovery time, to have a repeatable bound of recovery time.

So let's say your telemetry gives you a high level of confidence that within 10 minutes you can detect what's going on and maybe you're using Application Recovery Controller, which we'll talk about in a minute, and you have your automated recovery taking another 10 to 15 minutes, you can say roughly in around 20 to 25 minutes we can recover and that's a pretty powerful thing during events. With the tools we provide you with Fault Injection Service, you can inject those failures and you could simulate or test specific failures such as packet loss within an Availability Zone or DynamoDB. You can pause global table replication as an example and see how that impacts your service.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1730.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1730)

### AWS Fault Injection Service: Moving from Chaos Engineering to Resilience Testing

And then with Application Recovery Controller you can build highly reliable multi-region recovery processes we call those region switch plans, and we'll dive into each of those, but first I'll talk about Fault Injection Service. How many use Fault Injection Service maybe even not for multi-region, just generally? A couple of hands. Okay, cool, cool, thank you.  And Fault Injection Service is really cool, right? It provides you with a lot of primitives that you can inject failures into your application. And we cover things across various services, but one example of a recent one is for DSQL, which is our distributed SQL database.

So we just released a new action to simulate or introduce controlled failures into API calls for DSQL. But in addition, we have over 50 actions across compute, storage, networking, and databases and things like that. And they're also curated scenarios which are really cool, and these scenarios help you simulate or test specific failure modes that we've uncovered over the years and we think it's the best practice for you to test.

Some of these are, I'll just name a couple. One is called the Availability Zone power interruption scenario. And so we've curated a very specific set of Fault Injection Service actions that introduce the conditions of what you can expect if there's a power outage in an Availability Zone. Even more recently, which we're really excited about, we released two new scenarios in the Availability Zone realm, if you will, around gray failures.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1830.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1830)

You can introduce gray failures within an Availability Zone and across Availability Zones. This allows you to really fine-tune your detection and understand whether your thresholds with your alarms are where you expect them to be. If they're not, it gives you a really cool mechanism to test and to tweak those. These are the broader set, not inclusive of everything, but just an idea of the broader set of actions we support. The cool thing is you can construct these for a single  account, you can do testing across multiple accounts, and then if there's an action or a failure you want to introduce that we don't support, we support AWS Systems Manager, which allows you to use an SSM document that you can build your own failure case. It also provides some really cool rollback triggers in case things don't go as planned.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1860.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1860)

How many, I'm just curious, this slide I always find interesting. How many have been in an FIS presentation and have seen this slide? It's a very popular  slide for us. I have it here because I think a lot of folks when they think of FIS, we provide a lot of primitives, and they jump right to this kind of mental model of, all right, let's put some actions together, let's test, let's see what we can have go wrong, let's introduce some chaos. What I really like to think about, specifically for this use case of multi-region, is shifting the mindset from chaos engineering, which is good and it helps you really uncover unknowns, to more resilience testing. That's because with multi-region there are a few tests that you should just run. If you have a multi-region application, you should run these and you should be able to validate that your application can meet the success criteria. Again, one of those could be let's validate my multi-region RTO, and that will allow you to determine how quickly you detect and how quickly you respond.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/1930.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=1930)

That is not chaos. That is something that you've built this multi-region application to be able to do, and just like a unit or integration test, this resilience test should help you validate those assumptions and effectively assert your application's ability to meet those design objectives. What you really need to think about is let's start from a framework,  and this was one, my mental model that I use when talking with customers. It's really starting with defining those resilience objectives. I'll say again, a good one for multi-region is let's define a test to validate your multi-region RTO. That is a great starting point among the few others I've talked about.

From there, you'll go and you'll design your test, and that's the other couple of slides I just talked about. It's looking at the actions that make sense for your application. Maybe if you're using EC2, you may want to terminate some instances to simulate an impairment. You may want to use the networking packet loss action to introduce packet loss in that region. You can get creative of maybe using past experience to craft that specific test. Then what you're going to want to do is align on specific metrics and alarms that will show you your success criteria. For instance, as an example of Fidelity, seeing how many people are checking their account balance every minute is a pretty good indicator that the application is doing its thing. For Amazon.com, they're looking at order rate. They have a pretty good indicator of how many orders they should get every minute.

The point being is whatever it is for your application, find those key indicators of how that application is performing and identify those as the criteria for the test. What you want to do for this specific one, since we're crafting this multi-regional one together, is you'll want to look at, we'll just stick with order rate, order rate in both regions. You'll want to see when you introduce that, start that test, you should see order rate drop in your primary. You should see how quickly you detect that dropping, then enacting your recovery procedure, moving or, as we would like to say, switch regions with Region Switch. Then you should see the order rate pick back up, and all that should be done within your RTO, and that's your success criteria.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2080.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2080)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2090.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2090)

### Region Switch: A Fully Managed Multi-Region Recovery Orchestration Service

You'll define that, and I kind of already jumped to the last one, you select your recovery procedure. Some customers have built their own solutions. Some folks will use scripts. We have Region Switch, which we recently released in August of this year, so it's still relatively new, but use something like that to have a repeatable process and to define a gated process that you can use for multi-regional recovery. With that, we'll switch gears a little bit.  We'll talk about Region Switch. Region Switch is a fully managed multi-region recovery orchestration service.  One of the tenets, as you can imagine for a service like this, is the resilience of it was extremely important. When we started building the service, it was the biggest conversation within our team.

We wanted to make sure that if we were to release this, it needed to work during our worst days when customers could count on this thing to recover. What we ended up building is something I find pretty unique. We built a regional data plan with a global resource. When you build a Region switch plan, you give us some details such as a name, but you also tell us what regions your application is in. When you build your plan, we then replicate that to our regional data plans for the regions you told us.

Let's just say you're in East 1 and West 2, a popular combination. We'll replicate your plan to East 1 and West 2, and then when you go to execute that plan, you'll execute it from the region you're going to. That ensures that you don't take any dependencies on the region you're leaving. In fact, if you try to execute the plan to activate, and we talk about it as activating or deactivating a region, if you try to execute that plan to activate, say Region B, and you run that from Region A, we're actually going to throw an error and say you can't do that. That's our way of making sure that you're not taking any dependencies and we're not taking any dependencies on the region you're leaving, which we've seen quite a few times be painful for customers during recovery.

Within the plan, that's where you build your recovery workflow, and I'll show you what that looks like in a second. You can think about a plan being scoped for an application. For every app, and the definition of app probably varies by customer a lot of the times, but roughly speaking, a plan for an app, so you have a very bounded recovery that you can reason about, that you can monitor, that you can track. For event management, we provide dashboarding so you can see in Region switch all the plans that you have and when's the last time they've been executed.

There's a really cool concept I'll talk about in a minute called plan evaluation, and so you can see if there's been any warnings for your plans. During event or test, you can easily see what's going on, so you can see all your plans, how many are being executed right now, if there are plans being executed, how far along are they, what step are they on, what region are you activating, where are you going to. It gives you a lot of insights and allows you to coordinate. What we find today is a lot of customers do that somewhat manually today on a conference bridge of some sort. They're kind of communicating like, hey, App B failed over, App C, yeah, we're still working on it, give you an update in 10 minutes. This provides a nice central viewpoint of how that's all going.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2270.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2270)

This is a Region switch plan.  I literally just took a screenshot of one in my account. You can think about it as the blueprint for recovery. Again, it's for a single plan for a single multi-region service or application. One cool thing, and this kind of goes to one of the challenges Heinz talked about, it was like, keep me honest, 64 microservices across 10 business units if I remember correctly. We help with supporting nested plans, so you can build 64 plans, and you can then orchestrate those within what we call a plan of plans. You can parallelize as you see here where it makes sense, where things aren't dependent on one another. You can have sequential execution if there are dependencies, and so you can start to coordinate across multiple microservices.

The other really cool thing, and we knew this from the beginning from talking with customers as we were building this product, was we needed to support cross-account and multi-account. You can share plans via Resource Access Manager to other accounts. When you build your plan, we know that your apps are comprised of resources in multiple accounts, so you can have an Amazon EKS cluster in one account, the plan in another account, and the database being a third account, and you could still construct that plan to orchestrate the recovery across those multiple accounts.

The other thing you see on the screen, and it's harder to see over here, is the JSON version of the plan. As you build it, we want to give you a really nice user experience in the console, but we know for the majority of our enterprise customers, no one's going to be building plans in production in the console. We want to make it easier for you to play around, test with plans, but then be able to build them as infrastructure as code, and we provide a nice JSON format that you can take and then build these with whatever infrastructure as code you want to use, and so we support that.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2390.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2390)

### Execution Blocks and Plan Evaluation: Building Reliable Recovery Workflows

Let me go back real quick actually. All those things in the steps we call execution blocks. I just want to kind of paint the context before I go to the next slide. Each of those execution blocks,  I'll go through now, there's roughly four categories. The first is the compute scaling. I always have to preface this, especially for our financial services customers, this does not guarantee capacity.

If you need to guarantee capacity, you should use On-Demand Capacity Reservations and have it running. But for workloads that you want to scale as part of your recovery, we provide what we call execution blocks to do that. It's really neat the way it works. So when you build a Region switch plan and you use one of these execution blocks, we're actively looking at that compute resource in both regions. Every 30 minutes we'll look at the number of pods if it's Amazon EKS, if it's Amazon ECS we'll look at the service and task, and if it's Amazon EC2 we'll look at the instances in that Auto Scaling group. Then we'll store that durably in each of our data planes.

So if you remember, we replicate the plan to those regions and we store that durably in each region's data plane. When you go to execute your plan, we're not taking any dependencies on doing describe calls to understand capacity or utilization. We are reading locally from the data plane and we allow you to basically pick a percentage and scale to whatever percentage you define as related to the max we saw over the last 24 hours. For Amazon EC2, for example, if you had 10 instances as the max in your primary region over the last 24 hours, when you go to execute the Region switch plan, we will scale you to 10 instances in your standby region as part of your plan where you have it constructed. With Amazon EC2 it's pretty easy to reason about the instances. With Amazon ECS we look at services and then tasks, and with Amazon EKS we look at specific namespaces and pods.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2500.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2500)

For databases, right now we support Amazon Aurora Global Database, so you can easily build a plan that allows you to do the switchover or failover  functionality that Amazon Aurora Global Database provides, and you can do that easily within your plan. I should mention you're not writing a script for Aurora as an example. You give us the global cluster name and then the ARNs for each region, then we do the rest for you. The same applies for compute scaling for Amazon EC2. Just give us the ARN for the Auto Scaling groups for each region and help us understand that relationship, like create that pair for us.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2530.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2530)

For networking,  I'm just going to ask how many are familiar with the other product of ours called ARC Routing Controls. Yeah, cool, a couple. So this was our first product we launched for Application Recovery Controller back in 2021. There's no dependency between Region switch and ARC. They're fundamentally different things, but we did want to make it easier for customers that are using routing controls to do the DNS-based failover directly in the Region switch plan. For those of you that are familiar, you can build your Region switch plan, provide us the ARN for what we call the routing control, we'll figure out the five cluster endpoints of the routing control, and we'll call that for you. It's very easy to automate routing controls.

We also wanted to provide a way for customers to do a data plane failover with Amazon Route 53 directly in Region switch, and that's why we created the Route 53 health check execution block. We allow you to provide us the hosted zone, the record set identifiers, and you tell us which region. Then when you save your plan, we're actually going to vend you Route 53 health checks that are managed by Region switch, and you go configure those in Route 53. Then Region switch will manage the state of those health checks. So when you execute your plan and you say I want to activate Region A, we're going to flip the state of those health checks in Route 53 to make sure that the traffic is routed to Region B. It's a really neat way to do a highly reliable DNS flip over if you will.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2610.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2610)

 And then the last set of actions are Region switch actions. We created the custom action Lambda because we invariably knew we wouldn't build all the integrations you wanted, or at least we're not going to build them as quickly as you want. So you can bring your own Lambda function and build a step to do whatever it is you need. We've seen customers using this to do data validations, different kinds of checks. Some that have on-premises dependencies are using it to call on-premises and say, hey, we're switching regions, so you can get very creative with that.

We also have the manual approval execution block, and that really allows you to have a human in the loop. You can scope that to a specific IAM role, and when it gets to that step, the plan is effectively going to pause until someone comes in and approves it. That can be done via the CLI, the API, or the console. Again, customers are using this in a lot of interesting ways. They may do non-destructive steps in the beginning that won't make anything worse, so to speak, but allow you to maybe start scaling. They may do some checks with the custom action Lambda, but then have a human in the loop, maybe some business leader or technical leader to come in and say yes, we validate it, we're actually going to proceed with this recovery. Then that starts to go to maybe a database promotion or a traffic flip as an example.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2690.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2690)

This was the really neat thing I wanted to talk about. So I feel like I've been talking for a while now, but a few slides  ago, I talked about some of the challenges with recovery, right, where maybe you depended on a pipeline in your primary region to recover and then all of a sudden that wasn't available. Another area we've seen customers have trouble with is they built automation or have a script, but then they go to recover and it didn't work because some permission changed over time and it wasn't tested.

What we want to do is create a mechanism that effectively dry runs the plan every 30 minutes. So you create your plan, and as long as it's there, every 30 minutes we're going through and we're looking at the execution role and making sure that the IAM role you provided the plan is able to execute on all the things you've done in your plan. Depending on the execution block, we're going to look at different configuration to make sure that it can execute as you want. And then if you're using any of our compute scaling execution blocks, we're looking at that scaler, the capacity usage in each region, and making sure that we could, if we needed to, scale you up. So it's a really neat way to kind of have that built-in validation.

It's no substitute for testing, and like you heard from these guys, even with Region Switch you should be testing this regularly, right? Region Switch provides you with a very prescriptive way to do that, but if you're using Region Switch, still test, execute your plans, switch regions, get used to that. We find the customers that are most successful are testing regularly and are very comfortable with this.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2780.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2780)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2820.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2820)

### Putting It All Together: Testing, Detection, and Continuous Improvement

This is what a kind of a very simplification of a plan execution looks like.  You can always start these executions manually via API, CLI, or console. We also have a concept of triggers where you can associate a CloudWatch alarm and have conditional logic to build it, if you wanted almost like a composite, I guess you can think of it, to trigger it in an automated way if you want to. And then this is kind of what it looks like of a parent-child plan where the first step goes, the second step is actually the Region Switch plan execution block that links to another plan. That whole plan will execute and then go to step three and then four. And underneath we have recovery dashboards kind of like you see here where you see the number of plans you had.  On the right-hand side you'll see actually one in progress, and you can see which region I'm activating in and so on and so forth, so it gives you a really cool indicator.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2850.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2850)

And so let's pull this all together, right? We talked about FIS, how you can build tests, how you should think about building tests from a resilience objective, how you should set the outcome, find the metrics that you monitor for those outcomes, have a recovery tool to help actually enact the recovery once you detect. And so this is what you should do. You should start the test, you should then be looking at your dashboards  and those KPIs that should be indicative of the health, right? And you should be able to see how long did it take us to detect after we started this test. Then you should implement your recovery procedures once you've hit that threshold, and this should confirm that you have the right thresholds, it's working as expected.

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2920.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2920)

And then you implement the recovery procedure. Maybe you're executing your Region Switch plan. Then you're validating the application of the region once the plan execution has happened. And within Region Switch we give you detailed logs as to every step of that execution so you can really follow along and have an idea of what's happening. Validate the application, and at the end of this you should review it. Did our detection meet our expectations? Maybe you thought we should be able to detect these events in five minutes. If you have a 30-minute RTO, for example, and you know your automation to execute is going to take 20 minutes, that really means you have about 10 minutes to detect and respond. And so you need to kind of work backwards from that bounded recovery time, that RTO, to make sure everything's in line. And naturally you're going to find learnings like both Heinz and Shaz talked about, and take those learnings and kind of continually improve that process along the way. 

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/2930.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=2930)

Yeah, so those are the tools. And so now what I have, a bunch of QR codes here.  I don't know how you're going to zoom in on each of those, but I'll stay here for a minute. The first is the Resilience Lifecycle Framework, so we wanted to put together a document to help customers reason about how to incorporate resilience through the SDLC, and so that's what that first one will talk about. The second one is the Resilience Analysis Framework. Shaz talked about this in the beginning. It's really taking FMEA and applying it to cloud-based concepts. So we came up with five common categories of failure, help you reason about those. We provide mitigations for each of those failure modes and really help you think through probability and scope of impact.

The third one is really interesting. It's the AWS Fault Isolation Boundaries, and so that walks you through how we build our services, so understanding what zonal services are, what's the blast radius of failure, what should you expect from them, then to regions and then to some of our few but important global services. Then the bottom is, interestingly enough, the Resilience Hub product page. So that's our other product which I didn't talk about. So if you want to check out Resilience Hub, which is an assessment tool, so you can build your application in Resilience Hub and we'll provide you with a resilience assessment so you can understand areas for improvement.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b17855ccecda3650/3010.jpg)](https://www.youtube.com/watch?v=jZIaliFsPIw&t=3010)

And then I'll just go to the last one because it's very important for this topic, is the Multi-Region Fundamentals. And so that is our mental model for building multi-region  applications, and it covers things like understanding your requirements, understanding your dependencies, your operational readiness, and how to manage data in state, so very important concepts when it comes to multi-region. By this time I'm sure you all went to the kiosk, but if you didn't, go to any of our in the AWS village. You can get free socks and stickers and stuff, and I'm sure you can get a demo of Region Switch and FIS there as well in our resilience kiosk. So if you're interested in that, I would say go for it. I know it's late on the Thursday, but I'll leave that there.

And so that's what we had. I know we're finished a little early, but by the end of the week I think we're all kind of speeding through things, but we'll hang out if there's any questions, so happy to chat more and thank you for your time.


----

; This article is entirely auto-generated using Amazon Bedrock.
