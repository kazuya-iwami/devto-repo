---
title: 'AWS re:Invent 2025 -From Print to AI: The Economist''s Digital Evolution w/ AWS Architecture (IND399)'
published: true
description: 'In this video, Ahmed Raafat and Professor Bogdan from The Economist discuss how they successfully moved generative AI from POC to production. They address the common challenge that 80% of generative AI POCs fail to reach production due to people, process, and technology issues. The session covers The Economist''s AI strategy, including four main use cases: recommendations, translation, summarization, and the EIU Q&A Copilot. Bogdan demonstrates how they built workflows using Amazon Bedrock and agents, reducing deployment time from six months to just days. The presentation includes a live demo of their RAG mechanism and agent-based system for complex queries, emphasizing that AI supports rather than replaces their human editors.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/0.jpg'
series: ''
canonical_url: null
id: 3088829
date: '2025-12-06T12:09:20Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 -From Print to AI: The Economist's Digital Evolution w/ AWS Architecture (IND399)**

> In this video, Ahmed Raafat and Professor Bogdan from The Economist discuss how they successfully moved generative AI from POC to production. They address the common challenge that 80% of generative AI POCs fail to reach production due to people, process, and technology issues. The session covers The Economist's AI strategy, including four main use cases: recommendations, translation, summarization, and the EIU Q&A Copilot. Bogdan demonstrates how they built workflows using Amazon Bedrock and agents, reducing deployment time from six months to just days. The presentation includes a live demo of their RAG mechanism and agent-based system for complex queries, emphasizing that AI supports rather than replaces their human editors.

{% youtube https://www.youtube.com/watch?v=6vRIVVvCIsg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/0.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=0)

### From POC to Production: Building a Generative AI Strategy with the Right Mindset and Tools

 Good afternoon everyone, and thanks for joining us in the IND399 session. As you know, around 80% of the POCs in generative AI don't actually make their way to production. And it's not about the technology itself, it's about people, process, and technology. My name is Ahmed Raafat, and I lead the machine learning specialist solution architects in the UK. I'm delighted to be here today with Professor Bogdan from The Economist to tell you the story of how they were able to productionize generative AI from an idea to production at The Economist.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/50.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=50)

So let's get started. We're going to go through a little bit talking about how you can actually  envision and build your strategy on generative AI. Then I will be handing over to Bogdan, where he will walk you through more of a deep dive on how we built that, including the architecture, the design, and demos as well to make it more exciting for you. And then I'll be back on stage to close it and tell you how we can actually help you with your next steps.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/80.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=80)

So you've already been seeing so many use cases on generative AI  across the last couple of days and as well as earlier today. I also usually like to categorize or group them under three main categories, but it's different from customer to customer based on your own use cases. Just to make it very simple, you can think about three main use cases: enhancing the customer experience, which you see there on the left-hand side. Then on the right-hand side, you've got two usually internal main categories with so many sub-use cases that come under them, which is how you can optimize your business operations and how you can actually increase your employee productivity.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/120.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=120)

But these are all great ideas, and as I mentioned at the beginning, you can be very excited with a successful  POC, but when you move it to production, you start to face the reality about where is my data, what is the security, how can I take it to production, how do I scale it. And that's why we are here today to tell you the story of The Economist and help you to move fast from an idea to a nice plateau of productivity instead of spending so much time in this area that doesn't add much value.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/150.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=150)

So if we think about building a generative AI and a data strategy, I always think  about it in three main things. First of all, a mindset. You need to always be thinking about business problems. Yes, AI is fantastic, generative AI is super, it's really changing everything we do, but don't forget about the foundation. You always need to start focusing on the business problem and then work backward from that. Always fall in love with the problem, not fall in love with the tech. I fell in love with tech, but you need to fall in love with your business problem. Next is about your people and how you group them and organize them, and technology, which we'll dive deep into.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/190.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=190)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/210.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=210)

One thing that is very important for AI, as you know, is about data. There are two types of organizations  we see when it comes to data. There are some organizations that are very much focused on having everything centralized. The other is decentralized. What we advise is to have some right balance in between. And the idea of that is that you help your entities to have some sort of what we call a data catalog. So if I'm working on data within the  organization, I need to index that and publish this so you, for example, can use and consume this data to build your generative AI use cases. This will transform your organization to be more data-driven, and that will take you to the next step for tools to build with generative AI.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/230.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=230)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/260.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=260)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/290.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=290)

 And that's what we've been building at AWS for many years now with our stack of AI and ML services. As you see, it's three layers. You've got the infrastructure with SageMaker sitting in the middle with Agents and Bedrock, and at the top you've got things like Q and Quick Suite. It's definitely not enough time to cover this stack in more detail, and Swami today and Matt Garman yesterday covered all this in more detail, but quickly a recap about Bedrock. Always remember Bedrock is the best place for building  the platform. It's a platform to build your generative AI from model selection to customizing it, and we announced so many different customizations today as well. Securing this with guardrails and automated reasoning has been covered today as well, and also how you can productionize that and integrate it with tools. And that is actually taking you to Agents, which is a component that sits within  Bedrock, and it actually has six core components, and we added two new components yesterday which are not in the slides here because this space moves so fast. But we are talking about moving from

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/340.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=340)

runtime where you host your agents, securing them through a gateway, adding memory as also has been announced today about the new functionality of memory, and then you monitor your agent and you must have an identity. Then we have two types of tools which is code interpreter and browser tools, and we added two more functionalities yesterday which is very critical. The first one is about how you can actually apply policy, and the second one, how you do model evaluation. So that is enough from me, it's like a quick thing to  just remember that you need to have these three components between the mindset, the technology and tools, and with that to bring it to reality, I will hand over to Professor Bogdan to talk you through the lovely journey of The Economist. Thank you.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/370.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=370)

### The Economist's 180-Year Journey: Implementing AI Infrastructure for Recommendations, Translation, and Analytics

Thank you, Ahmed. Everyone, so I'm just going to give you an overview in the time we've got left on the journey that we went through starting almost 200 years ago, presenting basically  where The Economist started and where we are today. And the kind of infrastructure that we had to put in place and the processes and the people that had to be involved in that to basically achieve the infrastructure and setup and tooling that we've got today to be able to build all this wonderful technology.

So The Economist has four main areas that it operates in, four businesses. The most popular ones that most of you would be familiar with is the newspaper, and then also there is the Economist Intelligence Unit where macroeconomics and other types of indexes and so on are served as a service for other businesses to consume. Those are the two main areas that I'm going to focus today on that basically we've implemented quite a lot of AI now for the last two to three years that we've been looking into.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/430.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=430)

 So the journey basically started in 1843 where we had a businessman, a hatman to be precise, James Wilson, setting up the newspaper trying to look into ways of fair trading and so on and so forth and being able to publish the opinions of many people at that point in Scotland. Since then this has moved quite a lot. About 100 years later we had the Economist Intelligence Unit looking at forecasts and indexes, and eventually it was one of the first newspapers to digitize the content and make it available on a website and applications and so on in 1991. And then today we are employing all these AI technologies which I'm going to talk to you about today.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/490.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=490)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/520.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/530.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=530)

But one thing is very interesting because from  the very beginning, and if I put up the first article that was published, what you will see is that everything is based on data, especially if you look at the bottom there is a table that indicates data from UK exports to the US and shows the differences and so on and so forth, which is very interesting to see that data has been at the frontier of The Economist from the very first article  in order to publish their opinions. So data for us is a key vehicle to basically build our  applications and serve the content to the people.

So today I'm going to focus on four main use cases and then I will go in deep a little bit more on two of them just to show you the two different types of AI workflows that we have developed using the AWS infrastructure. One is recommendations. It is key for us that we are able to basically understand the user journey and how they read the content and be able to provide them with the next article for them to read so that we can keep them engaged and save them time from reading things that might not be as important and so on because today who has time to basically read given the pace that we live in, so making this a bit easier is a key thing.

So the recommendations, for example, is one of the key use cases that I've got another slide to go into a bit more details, but we also looked into translation. And we used AI for that. We built in Bedrock workflows that allows not just to translate using a single function, the content into another language, but eventually also ensure that the workflow contains checkups to make sure that the styling of The Economist remains the same,

that the meaning doesn't change from one language to another, and that it is expressed in a way that is compliant to the policies that The Economist has.

Summarization is another use case similar to translation. It's not just about summarizing. The question would be, is the meaning of the message that this article is trying to put forward remaining within the summary? And how do we ensure that it contains these key elements to provide the story in this short few bullet points? And finally, for the EIU that I was mentioning earlier, we built a copilot because that service can be quite complex and usually it is used by analysts to eventually write reports and it takes them a very long time to search through and understand the data, and we have multi-modal data as well in this use case. How can we build a support tool for the analysts to basically ask questions and generate reports more or less on the fly?

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/690.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=690)

So let's dive deep into one of the UK use cases, which is the recommendations.  The reason I put this up is that traditional machine learning is still a thing, right? So not everything is generative AI, and I wanted to show how similar the process remains between building an architecture that supports machine learning using SageMaker to deploy a model, to then building workflows with generative AI. So what you see here is basically utilizing SageMaker within an AWS account to experiment and build the model. In this particular instance, we used a transformer model that basically checks in a similar way that you would predict the next token. We are predicting the next read of the user, and we're using an attention mechanism as well for that based on all the data that we are collecting from the users. And eventually that model is operationalized, is checked, verified, tested into several accounts, and eventually ends up into the production account where it gets monitored and eventually data is collected, and if needed it comes back to be retrained and deployed again. So this is one of the key use cases that we put together, and of course that will scale to many multiple similar use cases that we have in place.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/780.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=780)

Another one that is utilizing the generative AI approach is the last  use case I talked about, which is the EIU Q&A Copilot. So in this one I'm going to also demonstrate it later on, and I will show you how we extended that to now use agents instead of just a large language model to basically generate the output. So what you will see that is very similar now is that we still have this experimental account where we put together using Bedrock all the necessary workflows to basically try and experiment with the different large language models that are available in the service. But then we create a blueprint out of that, so you will see from Bedrock basically you can extract a JSON that can be productionized in a very similar way that you would do with a model. Obviously there is no model deployment as such. What we are deploying now is the actual blueprint of the Bedrock JSON, so all that can be tested in an operations account and then eventually productionized and made available to the subsystems within The Economist.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/850.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=850)

One other thing I wanted to show here is how we  utilize the Bedrock architecture as well. So basically this is the blueprint I was referring to earlier, where all the processes are built using flows and eventually utilize the embedding models, the generative models, the prompt registry, the guardrails, and everything that is available through Bedrock in order to generate and create this workflow that is then deployed into production. So no click-ops. Everything is utilizing infrastructure as code to be deployed eventually into production.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/890.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=890)

Now if I was to put it all together,  very complex architecture, so I'll put it up there just to show you how we've got everything now together. This is what allowed us to basically build everything AI within The Economist.

The platform that we created enables us to move from what used to take about six months to push a model into production to now taking just days, without even exaggerating, to take ideas, experiment with them, and eventually productionize those to serve the different needs of the business. It's quite an interesting concept. The thing that really allows us to do that, as Ahmed mentioned earlier, is putting together the processes, the people, and the technology. We found the technology through AWS, and eventually we built the processes, and with the team that we've got together now, we're productionizing this in a few days.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/960.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=960)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/990.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1000.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1000)

### Demonstrating the EIU Q&A Copilot: Evolving from RAG to Multi-Agent Systems for Complex Queries

Now this is the demo I wanted  show you, so it won't take more than a couple of minutes. Just to give you the context, for the EIU Q&A Copilot, we run a query using the typical GenAI workflow for creating a RAG mechanism to search through the documents. The query at the very beginning is "manufacturing and retail risks Southeast Asia," which is a very typical query that somebody might want to run.  This is now available for the subscribers. They can type in the query that they want to search for, and eventually, as you have seen many examples before,  answers will start being generated. Eventually there are references to the articles that they can carry on reading about, and they can click through and examine and understand what's going on, and they get an answer to the question they were asking specifically.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1020.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1030.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1040.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1050.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1060.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1060)

This is great because it  helps to a degree with a typical RAG mechanism, but then I thought, what about if we challenge this a little bit more?  Although we have the knowledge base that we have trained, and we tried many different ways, many different embeddings, and so on and so forth, and we created  for the knowledge base all the filters that were necessary, when we run complex queries like this one here, it was becoming very difficult for a typical  RAG mechanism to provide an answer. So what we had to do is simplify the query. If I simplify it a bit, then we were getting some answer.  That answer actually was quite long, and it was kind of getting lost trying to answer everything together, so it was not ideal for us. This is where we start looking into agents.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1070.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1070)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1110.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1120.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1120)

 Now this is not in production yet, so I'm going to give you a peek behind the scenes on our own built-in system, how we now run this query. What is interesting about it is now that there are multiple agents working together to deliver on the different queries that were originally requested. Now what we see is that the agent itself is starting to analyze and think about the process that it needs to do. It's breaking down the different areas that it needs to operate on. This is based on the instructions that we provided,  but it's also going to start communicating with the user to ask questions and say, "What exactly are you looking for?" so that  users can start interacting with the agent in real time to provide the necessary feedback for the agents to carry on working on this, and eventually that is producing quite good outcomes.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1140.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1150.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1150)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1160.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1160)

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1170.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1170)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1180.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1180)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1190.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1190)

Waiting for the  just to show you very quickly a couple of seconds of these outcomes as well. The other interesting thing is  that it's going to start using tooling to query the knowledge base, get access to internal data,  search the web, and so on and so forth until it provides the comprehensive reports. I'm going to skip through now this one  just to also very quickly show you that this is the console. Within Bedrock now we can see all the traces of what's going on and all the spans  on all the activities that the agent has taken, and we can verify that the steps that the agent has followed are all accurate  and up to date with what we would have expected, so that was really great to see.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1200.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1200)

All right, last slide from my side, and I'm going to give you  no seconds to close up.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1220.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1220)

I just wanted to say that we're looking forward now to building even more within The Economist, and one thing that I wanted to mention at the very end is that we obviously have hundreds of editors that are helping us across the world to  build articles, and those are still humans. We're using AI to support them, and we are not using AI to replace them, not at least at any point in time soon. So thank you so much.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1250.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1250)

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1270.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1270)

Thanks, Bogdan, and great to hear that still when I read The Economist, it's been written by humans, not by robots yet. So again, thank you everyone for joining us here today. I would just say, like always, start by working backward from your business needs, select the use case, empower your teams, enable them,  make them feel, as Bogdan mentioned, that AI has helped the business people and the tech people to accelerate and do their work better. Start with an idea, just take it to production, and that will start the flywheel. Then when you get more excitement in the organization, I will close by sharing some references for you here. You can read about data strategy,  how you can secure this production through the Cloud Adoption Framework for AI, an executive guide on Generative AI for your executives that you can give to them, and definitely the details of Amazon Bedrock.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a57bb610a68b9c73/1290.jpg)](https://www.youtube.com/watch?v=6vRIVVvCIsg&t=1290)

With that, I want to thank you. Please do the survey. We really, really  appreciate it if you do the survey for us, and enjoy your lunch. Thanks again for joining us here today.


----

; This article is entirely auto-generated using Amazon Bedrock.
