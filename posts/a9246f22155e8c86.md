---
title: 'AWS re:Invent 2025 - Build, fine-tune & deploy AI models with SageMaker HyperPod CLI & SDK (AIM371)'
published: true
description: 'In this video, Giuseppe A. Porcelli and Arun Nagarajan demonstrate building, fine-tuning, and deploying AI models using Amazon SageMaker HyperPod CLI and SDK. They cover HyperPod''s resilient architecture with automatic health checks and failure recovery, the simplified cluster creation experience through AWS console, and live demos of training a Qwen 3 model with the training operator that recovers from node failures in 6.5 seconds. The session showcases the inference operator for deploying models from S3 or SageMaker JumpStart, HyperPod task governance for resource allocation across teams with priority-based preemption, and the newly launched capability to run IDEs like SageMaker Code Editor and Jupyter Lab directly on HyperPod clusters with fractional GPU support using NVIDIA MIG technology.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/0.jpg'
series: ''
canonical_url: null
id: 3086019
date: '2025-12-05T10:02:07Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Build, fine-tune & deploy AI models with SageMaker HyperPod CLI & SDK (AIM371)**

> In this video, Giuseppe A. Porcelli and Arun Nagarajan demonstrate building, fine-tuning, and deploying AI models using Amazon SageMaker HyperPod CLI and SDK. They cover HyperPod's resilient architecture with automatic health checks and failure recovery, the simplified cluster creation experience through AWS console, and live demos of training a Qwen 3 model with the training operator that recovers from node failures in 6.5 seconds. The session showcases the inference operator for deploying models from S3 or SageMaker JumpStart, HyperPod task governance for resource allocation across teams with priority-based preemption, and the newly launched capability to run IDEs like SageMaker Code Editor and Jupyter Lab directly on HyperPod clusters with fractional GPU support using NVIDIA MIG technology.

{% youtube https://www.youtube.com/watch?v=6hrqologUZE %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/0.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=0)

### Introduction to SageMaker HyperPod: Purpose-Built Infrastructure for Foundation Model Development

 Good afternoon and thank you for joining this session today on building, fine-tuning, and deploying AI models with SageMaker HyperPod, CLI and SDK. I'm Giuseppe A. Porcelli. I work as a Principal ML Solutions Architect in the SageMaker AI service team. In the past few years, I've worked with multiple customers on training and fine-tuning their LLMs on HyperPod as well as running inference at large scale. I'm here today with Arun Nagarajan, who is a Principal Software Engineer and has been deeply involved in the design of HyperPod and the features that we are going to discuss today.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/40.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=40)

 Let's walk through the agenda for the session. We're going to start with an introduction to SageMaker HyperPod to set the context, explaining what HyperPod is and what the key benefits of the product are. Then we will look at the Amazon SageMaker HyperPod CLI and SDK and why we have built this CLI. After that, we will get into a series of demos with live coding on getting started with HyperPod, including how you create a cluster and how you connect to a cluster, then how you run training of AI models, how you do the deployment, and how you optimize resource utilization using HyperPod task governance. Finally, we will also demonstrate a feature that allows you to run your IDEs and notebooks on SageMaker HyperPod, which is something that we launched a few days ago. At the end, we are going to share resources, so all the code that we're going to run today, you will be able to download at the end of the session.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/100.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=100)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/110.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=110)

 Great, let's get started with the introduction to SageMaker HyperPod.  Amazon SageMaker HyperPod is a service that allows you to manage and operate persistent clusters to train and deploy foundation models. You can scale up to thousands of accelerators, and HyperPod comes with improved efficiency. HyperPod has functionalities which help you optimize the utilization of the compute resources on the cluster. You reduce overall the time to train because HyperPod comes with advanced resiliency capabilities which help you recover from failures. When you run training or fine-tuning at large scale, failures can happen on your hardware side, on the networking side, or on the GPU side. HyperPod has built-in features to recover from these failures. Overall, the objective is to lower the cost of foundation model pre-training, fine-tuning, and inference.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/170.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=170)

Let's get into a bit more detail on the benefits of HyperPod.  HyperPod is resilient. The service runs continuous health checks on the instances that are part of the cluster, and in case failures are detected, such as GPU failure or networking failure, the service takes appropriate remediation actions like replacing an instance or rebooting an instance depending on the type of failure that gets detected. HyperPod is highly scalable. We can deploy nodes with a single-spine node topology to get low latency. The nodes come with preconfigured EFA for high networking connectivity across the nodes, which is extremely important, especially when running collective operations. Overall, you can also implement scaling, both scale-in and scale-out of your clusters, even automatically thanks to managed auto-scaling support in HyperPod.

HyperPod is extremely customizable. You can control the entire stack on HyperPod from the hardware side to the libraries and frameworks that you use for running your workloads. You can even SSH into the cluster nodes as needed and do whatever you need to do on the nodes. HyperPod is an efficient service, very efficient thanks to the integration with task governance capabilities which help you allocate the compute resources across teams, defining priorities. We're going to demo that as well. It also integrates with observability solutions like the managed Prometheus and Grafana stack on AWS.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/270.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=270)

 Regarding the possibility to customize HyperPod, you can really modify the configuration of your cluster and do customization at any layer. From the hardware side, you have a choice of compute options and your choice of storage. If you want to train directly from S3 or maybe you want to use a high-performance distributed file system like FSx for Lustre, you can choose the framework of choice from PyTorch, TensorFlow, JAX, or whatever you prefer. You can even connect to your preferred experiment tracking solution.

We offer managed MLflow on AWS, but if you want to use third-party solutions like Weights and Biases, you can definitely do that. What is important to note is that you have a choice of two orchestration options on HyperPod. On one side, you can use Slurm for those who prefer the Slurm experience. On the other side, HyperPod automatically integrates with Amazon EKS to provide Kubernetes-based orchestration for your nodes, and that is indeed the focus of this session.

### HyperPod EKS Architecture and the Need for Simplified CLI and SDK

We are going to focus on EKS-orchestrated clusters for which we have built the HyperPod Alliance. That is the key. At a high level, the architecture for a HyperPod-orchestrated cluster looks like what you see here on the slides on the right-hand side. You see the HyperPod managed nodes, so the service takes care of managing the compute nodes. Then once you deploy a HyperPod EKS cluster, you are also going to create an EKS cluster in your account which acts as the orchestrator for the HyperPod managed nodes.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/340.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=340)



Users will use the Kubernetes APIs provided by this cluster to interact, and the interaction can happen from any client. You can use SageMaker Studio or you can even use your own terminal on your own machine, which is what we are going to do today. For administrators on the other side, you can still use Kubernetes APIs to configure the cluster and set up the cluster. As I already mentioned, you can establish AWS Systems Manager Session Manager secure connections to cluster nodes so that you can SSH into the nodes and configure the nodes as needed.

On the storage side, the storage will be configured in your account because that is where your data resides. You can use S3, you can use FSx for Lustre, or as I already mentioned, you can connect to any file system that might be running in your account. When the HyperPod cluster gets deployed, elastic network interfaces are created in your account in your VPC, and then from that VPC you can connect to your own solutions for storage as needed.

Let us move on to the HyperPod CLI and SDK and why we have built another CLI and SDK. Why another is because to work with the Kubernetes cluster, you can use kubectl, the Kubernetes CLI, and just run commands there. But the feedback that we have received from our customers, from data scientists and researchers, is that using kubectl is definitely fine, but it is very complex. You have to manage YAML configuration files, edit those files, and then submit the workloads.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/440.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=440)



While this can be extremely easy for those who are familiar with this process, sometimes data scientists were looking for a more abstracted interface which is also closer to the type of workload that they are running, which is indeed a GenAI workload. It is not like a generic deployment of Kubernetes. In addition to that, some users prefer a more programmatic interface with an SDK-based interface, while others prefer a command-line interface. That is why we have built this new solution which brings together CLI and SDK. And for sure, to also leverage and better leverage the observability capabilities that are available on HyperPod.

### Understanding the HyperPod CLI: Layered Architecture and Installation Process

The HyperPod CLI is an open-source project which is available on GitHub in the SageMaker HyperPod CLI repository. You can see an example of running the hyp command and CD on the help, which guides you towards the actions that can be executed with the CLI. On the architecture side, what is important to get from this slide is that it is a layered architecture.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/520.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=520)



On the top layer you see the user experience layer which is either the hyp command for running commands with the CLI or the import sagemaker.hyperpod to work with the HyperPod package to work with the SDK. Then you find the core modules of the CLI and the SDK. The core modules of the CLI implement the actual CLI commands. The modules of the SDK are the ones responsible for building the request that will then be submitted to the Kubernetes client.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/540.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=540)



It is important to see how we are reusing the SDK modules for the CLI commands. The CLI has been built on top of the SDK itself, so it uses the SDK to execute the commands. The SDK uses the Kubernetes client to then run the commands against the Kubernetes APIs.

On the right-hand side in pink, you see the templates. The templates are the representation in the CLI of the custom resource definitions that are defined by the various operators installed in the cluster. These templates are versioned so that if the operator gets updated on our side, you can still continue to use the CLI with the previous version of the operator. You don't necessarily need to upgrade to the latest version.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/640.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=640)

To get started, you need to create your HyperPod cluster.  On the cluster creation experience, setting up such a sophisticated environment requires looking at the networking, the storage layer, security, as well as the actual compute. To simplify this process, we have recently launched a simplified cluster creation experience from the AWS console. This cluster creation experience allows you to use quick setup, which comes with opinionated defaults on the infrastructure side, such as how to set up your VPC and how to configure the CIDR ranges for your clusters so that depending on the scale of your cluster, you are sure that you have enough IPs available for your machines.

All these things are pre-configured so you can easily spin up your cluster. However, if you want to tune the configuration, you can move to the custom setup. In the custom setup, you can decide what to enable or what to change on the configuration of the cluster. For example, if you don't want certain operators to be installed by default or if you want to change networking configuration, storage, and so on. For installing the CLI and SDK, you can just run pip install sagemaker-hyperpod in your environment.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/730.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=730)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/740.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=740)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/750.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=750)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/760.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=760)

### Getting Started: Connecting to an Existing HyperPod Cluster

Let me move to the code. Before we get into the actual examples, I want to show you the cluster that we're going to use today.  I am in the SageMaker console, and I can move to the HyperPod clusters. I have three clusters running here: two are EKS orchestrated and one is FLRM orchestrated.  We're going to use this cluster, ML cluster AIM 371.  From the console screen, you can also see the corresponding EKS cluster, which is acting as the orchestrator for the HyperPod nodes.  You can find here all the configuration, including the observability configuration, which shows which metrics we decided to export to Prometheus and Grafana from the cluster.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/770.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=770)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/790.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=790)

What is important to look at is also the configuration of the instance groups.  Here we have two instance groups: one with G5 12xlarge instances, which come with four NVIDIA A10G GPUs on board for each instance, and then we have a few compute CPU nodes as well in the cluster. All the nodes, as you can see here from the details, are running: four G5 12xlarge and six T3 2xlarge instances. 

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/810.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/820.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/830.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=830)

Now, let me move to VS Code. The code that I'm going to show right now, you will be able to download later. It's a modular example that starts from getting started, then goes into training, inference, task governance examples, and then spaces, so running IDEs on HyperPod.  Let's start from the getting started in the first module.  You will see how to set up the cluster and all the things that we have discussed so far. I don't want to spend too much time on this, but how to install the CLI.  I am in this directory where I've created a virtual environment where I've installed the HyperPod CLI. Indeed, if I run the hyperpod command, I should see as output the various help information on the CLI.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/850.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/860.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=860)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/870.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=870)

I can also do something like hyperpod --version to check the version of the CLI that is installed.  As I mentioned, the versioning of the templates that are available with the CLI is important.  Let's take a look at the various clusters that I have in my account. So the list cluster operation will list the EKS orchestrated clusters that are available in my account, and as you can see, we have the AIM 371 and then we have also the other ML cluster that I have. 

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/880.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=880)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/890.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=890)

Now, what you need to do to start working with this cluster is setting the cluster context.  This essentially means updating the local Kubernetes configuration so that you can execute API calls against the Kubernetes API correctly.  When I run this command, you will see that it will update the local Kubernetes configuration, and we are ready to run commands.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/910.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=910)

If you want to see what the current cluster context is, you will just run the get cluster context command and you will know exactly which cluster you're working with. 

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/930.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/940.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=940)

We have pre-installed all the required operators and add-ons on the cluster, so you will skip these steps like setting up the training operator and setting up task governance, and so on.  What I just want to mention is that this cluster has an FSX file system attached where data and the training code is stored.  This is just for your reference for the future examples on training. This cluster has been set up with the FSX configured with the Data Repository Association, which is a kind of sync between the FSX file system and an S3 bucket so that whatever you write in S3 gets made available in the FSX file system and vice versa. It's a bi-directional sync.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/990.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=990)

### Creating a New HyperPod Cluster Using CLI Commands and Configuration Files

Now that we have seen the initial setup, let me go into the steps to create a HyperPod cluster using the CLI. The first thing that we can do is run this command to create a working directory for the cluster creation operation. I can run the hyp init cluster-stack command.  The hyp init operation is going to create in your local directory two files: one config.yaml and the other one is a Jinja template for the CloudFormation template parameters, as I mentioned. The HyperPod cluster creation experience is based on CloudFormation behind the scenes. So when you create a cluster, whether it's on the AWS console or through the CLI, we are deploying CloudFormation templates in your account. This is consistent, so whatever interface you use, either UI or CLI, you get the same experience.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1040.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1050.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1060.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1060)

What the CLI is doing here is downloading the values that need to be provided to the CloudFormation template for deployment, and these are templatized in that Jinja template. Jinja is a templating framework essentially. We can look at these parameters here.  It's a very complex set of parameters which go through the configuration of the availability zone, if you need a NAT gateway within your VPC, the instance groups that you want to deploy,  and the file system configuration, how to deploy FSX in which availability zone through which subnet, and so on. 

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1080.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1090.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1090)

On the other side, you have the config.yaml.  The config.yaml is a much more abstracted configuration file which gives you just about the same controls that you find in the console. You have these available in the config.yaml here. So you can change, for example, the Kubernetes version that you want to use on your cluster, or maybe you can change the operators that are installed on the cluster. You can decide to change the configuration of the instance groups.  For example, in this example, we are just deploying a single T3 medium instance. The instance count is 1 and the instance type is ml.t3.medium. So it's just one instance in one instance group in the cluster, and so forth. You can set the VPC configuration and so on.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1100.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1100)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1110.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1120.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1130.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1140.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1150.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1150)

From the CLI side, you have two options. Either you edit this config.yaml through your preferred editor,  or you can use a command which is called hyp configure to automatically edit that file.  So, for example, here we are updating the Kubernetes version to 1.33. You can see that that has been updated.  After you do that, you can just do hyp validate to validate that your configuration does not contain errors.  And finally you can do hyp create which will trigger the cluster creation.  One thing to note is that in the run directory that gets created here you will also find the evaluated CloudFormation parameters, so the template in which the parameters have been replaced with the actual values for your reference. This is useful, for example, if you don't remember how you had configured the cluster. It's a sort of lightweight experiment tracking solution available on your file system when working with the CLI. 

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1180.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1180)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1190.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1190)

If we go back to the cluster screen, we will see that we have indeed a new cluster being created.  It's here, HyperpodClusterStack, and the creation is in progress. You can monitor the progress by going to CloudFormation.  You will see that there are here a few templates, a few stacks being deployed. This is the main stack, HyperpodClusterStack, which then has a number of nested stacks for the various components that need to be deployed, such as VPC and so on.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1230.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1230)

That concludes the demo for cluster creation. We will not wait for the cluster creation because this takes some time. Let me move on to the next topic, which is about training. 

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1250.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1250)

### Training AI Models with the HyperPod Training Operator: Superfast Failure Recovery in Action

Thanks, Giuseppe. HyperPod ships with the training operator, which is a sophisticated end-to-end training system built from the ground up for AWS. I wanted to highlight a few key features that the training operator supports. 

The first one is superfast failure recovery. In traditional training systems, when a failure happens, the containers and images are torn down so that all the initialization work is lost. With the HyperPod training operator, it is much smarter in that it will not tear down the container when a failure happens. What this means is that when a GPU fault happens or a network issue occurs, your training job is not stuck on initialization for too long. What used to take minutes will take a few seconds.

Secondly, the operator supports something called custom monitoring. What we mean by that is that in addition to monitoring for infrastructure failures, including GPU issues and network issues, the operator can be configured to monitor failures happening in your training job logs. For example, you may have encountered issues where the job is stuck and not making any progress, or you are coming across a sudden spike in the loss curve, or you have other issues preventing the training job from progressing. The operator comes with the ability for you to specify what you want to monitor, and you only need to set up the config and the operator watches your back.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1370.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1370)

Thirdly, the operator also integrates with the task governance system, which we will talk about a little bit later. That will help maximize the utilization of the precious GPU resources. With that background, let's jump into seeing how this comes alive in code. 

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1390.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1390)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1410.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1410)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1420.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1420)

What we see here is a training dataset. We are going to train a model not just for text completion, but also for  reading the query from the user, deciding what sort of tools it needs to invoke, and then reading out loud the thought process which the model went through. This training dataset is a little bit hard to read, so  I pulled up one particular example. You can see that there is a system prompt. Then we are introducing the available tools to the agent. 

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1430.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1430)

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1440.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1440)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1450.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1450)

We have a user prompt, and the assistant decided to invoke a particular tool. Then here is a tool response,  and based on this tool response, the assistant knows to read the various fields it asked for and  construct a user-facing message and then sends it to the user. Then the user asks a follow-up question and this multi-turn  exchange keeps going on. That is one example, and for today's demo we are going to use this dataset and train a Qwen 3 model.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1480.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1480)

Giuseppe showed you how easy it is to create a cluster at the infrastructure level with a single command. But what about the distributed training side? You do need to understand what sort of framework you are going to use, how you are going to get that image  into your cluster, and then perhaps understand how you submit or run a job with Kubernetes. The HyperPod operator simplifies all of that into a single command. I have previously uploaded the training dataset to S3, and let me quickly check if we still have the dataset. We have the training dataset and validation data.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1520.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1530.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1530)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1570.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1570)

I will also check the ECR image. We have the ECR image set up here. Now I'm going to submit this job.  With this command to run the job.  That's it. Once we submit this job, what the CLI is doing is constructing the right Kubernetes spec file and submitting it to the cluster. You can see that we asked for 2 nodes of G5 instance type and 4 tasks, 4 training processes per node. We pointed to how you get your data, and so on. Now let's check what happened to this job by listing all the jobs in this cluster. 

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1590.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1600.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1600)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1610.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1610)

You can see that the job we submitted is already running, which is great. Let's get some more details about this job. You can see that this job asked for 2 replicas, 2 pods are running, and then 4 processes per node.  It was created, pods started running, and the training job itself is running. Now the next level of detail is if you want to look at the pods themselves.  We can do the list pods command, which should tell you the two pods that we asked for. 

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1620.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1620)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1640.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1640)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1680.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1680)

Now we can look into the pod logs, which is basically your training job logs, and see what is going on there.  We can see that initialization completed and then the checkpoint is loaded and the training is starting.  To recap, what this operator did was that with a single command, we specified the training image and the configuration that we wanted. The operator kicked off the various training pods that are necessary to run on Kubernetes. It's doing a lot of heavy lifting by asking the pods if they're doing okay with health messages. All these health messages that you're seeing are health checks happening from the operator side reaching out into each pod asking if they're doing fine. If any of the nodes are not doing well, then the system will help repair that. 

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1690.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1690)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1730.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1730)

Speaking of fault resilience, let's say we want to observe what happens when a fault is noticed in production. The HyperPod nodes come with health agents which will automatically identify if there is a fault happening and flag the nodes for replacement. But for this demo, I'm going to manually do that. Let's see all the pods that are running. These two are the pods that we just kicked off through the training job. I'm going to mark this node as a failing node. I'm going to apply a Kubernetes label which will mark this node as failed and pending reboot.  To repeat, usually this is taken care of automatically by the system, but just for demo purposes, I'm labeling it so that we can observe how the operator deals with it. 

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1790.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1790)

Now let's check what happens to the pods. Remember, we flagged this node, which is ending in 917. Now you can see that the same pod got scheduled to a different node and it's already in running status. Let's describe the job itself to see what status it recorded. When we do that,  we can see that the operator observed the node fault that happened outside of its control and remediated it within 6.5 seconds.

That is the key heavy lifting that the operator offers. You don't have to worry about faults happening in the system, and you don't have to wake up at 3 a.m. to fix a training job that is going to fail because one random GPU decided to quit.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1840.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1840)

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1850.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1850)

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1860.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1870.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1880.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1880)

Let's take a look at the other initialization method for submitting a job, which Giuseppe also showed. In this case, it's for  training. This is a common pattern across all of our operators, where you create a directory and then  call init on the particular type that you want to work on. The CLI creates these templated files for you.  If we open up here, you can see that it has a few details, all blank,  but you can go ahead and fill them either with the configure commands, or you can edit directly with your editor. 

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1890.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1890)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1910.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1920.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1920)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1930.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1930)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1940.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1940)

Now, if we quickly check this config file, the two fields that I specified, job name and image, are already filled in.  I can do validate and then I can do create to submit the job, which I'm not going to do right now. So up until now, we have been talking about the  CLI abstraction. I also wanted to show the SDK abstraction. Here, what you see is that  in addition to the CLI interfaces, we also have the Pythonic SDK implementation  for all of our interfaces. For example, you can import with the standard Python behavior. You can either use Notebook,  or if you have automation that you want to build on top of this, you can write standard Python code for this.

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1950.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1960.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1960)

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/1970.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=1970)

Here is the HyperPod PyTorch job  class that you can instantiate and provide all the different configurations that we talked about. Then finally, you can call create on it to simply submit a job.  You can do the list operation, the status operations, pod log operations, all of the capabilities that we looked at earlier,  they are also available through the SDK interface as well.

That concludes the demo for the training operator. Now, before we jump into the inference operator, I just wanted to recap how quick and simple it was to use the HyperPod CLI or SDK to submit a training job, and you didn't have to worry about fault recovery. The fault recovery is really quick. To recall the layer diagram which Giuseppe shared, this is all a layer on top of Kubernetes, kubectl for example. If you want, you can always drop into the Kubernetes level commands. For example, if you want to get the pods, that's still there for you. If you're an advanced user and want to use kubectl, that's all good.

### Deploying Inference Endpoints: Simplified Model Deployment with Auto-Scaling Support

But if you want to have an abstraction with which you can build on top, we have the CLI and SDK for you. So let's move on to the inference operator. I have a slide to show you what features it has, but I want to kick off creation of the endpoint because it takes a few minutes. Hopefully we will have time to see that in completion. So I'm going to delete the job that we created and make sure that job is deleted. We don't have that job anymore. Now we are ready to kick off the inference deployment.

I'm going to use the config way, and I already have the config bootstrapped, so I'm just going to say validate and create. That's going to kick off the creation. This will take a few minutes, and we'll come back to it in a moment.

Now, HyperPod ships with an inference operator, which is how we deploy and scale AI models. I wanted to highlight a few key features that the inference operator provides. The first is simplified deployment. The HyperPod operator not only deploys custom models that are trained by you or from the predefined models from SageMaker JumpStart, but it also creates all the infrastructure required for production-scale inference endpoints. For example, you need to set up the ALB to expose it to an application, then you need to deal with SSL certificates and termination, and then you need to deal with auto-scaling policies. All of that is taken care of by the inference operator with a single command.

Secondly, as I hinted, whether your model is in FSx or S3, which you have trained, or if you want to use the built-in models from SageMaker, those options still exist for you to choose whatever you want to deploy. Third is the auto-scaling support, where the inference operator is integrated with metrics from CloudWatch or Prometheus, and it watches for scaling events and then scales automatically for you. This operator works seamlessly with the Karpenter auto-scaling with HyperPod and then provisions the instances as they are needed.

Finally, the idea is that for different audiences, you can go through different flows. For example, data scientists and ML engineers can use the HyperPod CLI and SDK, which we will demo in a bit, to create the endpoints. This in turn creates the SageMaker endpoint, the ALB controller, KEDA, and then integration with FSx, S3, and metrics. All of that is taken care of with a single command. When all of this is set up and ready, application developers can interface with the ALB directly or through SageMaker endpoints to do actual invocations on the model themselves.

With that, let's switch over to the demo. I know I rushed through the creation of the endpoint, but let's take a quick step back. What we did was deploy a custom model, and I'm going to show you that the model exists in the S3 bucket from which it is deploying. You can see that this is the model that we are deploying currently. What I did was the config way of deploying, and you can also see that the equivalent CLI way of deploying is also available for you. Where you can specify the endpoint name, where your model is, for example, this one we took it from S3, and what's the bucket name and where the model is located, instance type, and then the image URI. Notice that this image is directly from SageMaker, from the DJL container.

Once the endpoint is ready, we will be able to do inference against it. But let's check what is going on with the endpoint that we created a few minutes ago. I'm going to list all the endpoints in this cluster. It says deployment complete, which means that the pods should be in a running state. We can also take a look at the internal operator logs for the inference operator itself. You can see the metrics are enabled.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2140.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2140)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2150.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2160.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2160)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2250.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2250)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2320.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2320)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2340.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2350.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2350)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2380.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2380)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2390.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2390)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2400.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2400)

         

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2420.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2430.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2430)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2470.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2480.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2480)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2500.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2500)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2520.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2520)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2530.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2540.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2540)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2560.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2560)

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2570.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2570)

You can see the metrics are enabled and the inference endpoint config is all being set up. We can also take a look at the detailed logs from the  custom endpoint  CRD. So what we see here is that the deployment on the pods has successfully completed, but the front-end endpoint, the SageMaker endpoint, is still under creation.  While we wait for that, we can look at some of the pods.   We have a pod which is ready to do inference. We can look at the pod logs and see that the initialization completed and the model has been loaded.  You can see it has loaded the model and then started the worker thread.  The endpoint is ready from the pod. We just need to wait until the SageMaker endpoint is complete before we can do an inference.  There we go. The SageMaker endpoint is now created. We can do a quick invocation on this.  The chat completion came back and the inference endpoint is up. 

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2580.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2590.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2590)

That concludes the demo for inference. We have another way to do inference deployment,  which we probably don't have time to do, but the idea is that just like we did inference deployment from S3, you can do something  from SageMaker's JumpStart as well, where you only need to specify the model ID and it automatically knows to download all the model artifacts and deploy it for you. To recap, what we saw is that with a single command, the system takes care of the heavy lifting required to set up the load balancer, the auto-scaling controllers, the metrics, the logs, and kicking off the deployment for Kubernetes. All of that is taken care of out of the box.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2650.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2660.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2670.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2670)

### Optimizing Resource Utilization with HyperPod Task Governance: Priority-Based Workload Management

We do have some examples here on invoking or forcing an auto-scaling event, but we will skip that for now. The next part of the demo will be handled by Giuseppe again.  Can you delete it?  Great. So as we mentioned earlier in the talk, HyperPod provides built-in capabilities to optimize compute utilization on the cluster.  This is thanks to HyperPod task governance. HyperPod task governance gives you the ability to define a policy at the cluster level and define multiple priorities for workloads. In this example, you can see how the inference priority is higher than maybe experimentation or training priority. This allows your workloads to be treated differently, and higher priority workloads can be configured to preempt lower priority workloads. At the same time, you can decide how to allocate the compute resources that you have available in the cluster to the various teams.

A team corresponds to Kubernetes' namespace. What you do is you say, "Hey, Team A has allocated a certain number of instances." Well, the computer resources of an instance like, I don't know, X GPUs, memory, as well as CPU resources. Or you can go even more granular, so just a certain number of GPUs or a certain quantity of memory, certain number of vCPUs. Both options are available: entire resources of an instance as well as granular resources.

Thanks to this setup, you can also configure how to borrow and lend the compute capacity. You can see that Team A, for example, can borrow 100% of their compute capacity if there is idle compute capacity in the cluster. This means that if Team B, for example, is not utilizing the computer resources in the cluster which are sitting idle, Team A could borrow these computer resources while those are idle. When Team B comes back with guaranteed computer requests, the quota that has been guaranteed to Team B, then the Team A job will be preempted. That's what we're going to demonstrate right now. Let me move back to the code.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2790.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2800.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2800)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2820.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2820)

 We are in the task governance section of the examples. Let me set up some variables.  We're going to use the same training job that was run earlier, but we're going to configure it in a different way to assign different priorities and run the job by different teams. If we go back to quickly check the cluster, we can see that  the cluster has been configured on the policy side. You can see that I have the priority classes that you've seen, almost the same as what you have seen on the screen here, configured on the cluster. And then we have the compute allocations.

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2830.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2830)

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2840.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2850.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2850)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2860.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2860)

 If we go to the compute allocation for a specific team, we will see that Team B has just one instance that has been allocated to the  team. So one ml.g5.12xlarge, which has 4 GPUs allocated to this team. They can borrow 100% of the compute, and Team A  is configured in the same way. They have one instance guaranteed compute for this team, and they can borrow 100%  of their computer resources, so another instance, the computer resources of another instance essentially.

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2870.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2870)

Great, let's go back to the code now. We can show an example.  As you can see, the command is almost the same. What we are setting here is we are defining the namespace that we want to run this workload in, which corresponds to the team that the computer resources have been allocated to. The local queue, task governance works with and integrates with Karpenter, which is a popular scheduler for Kubernetes, just offers a layer to configure Karpenter in the appropriate way. And then we are setting the training priority for this job.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2910.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2910)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2920.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2920)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2930.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2930)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2940.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2940)

OK, so let me run this code.  Let me run this job.  I will check if I copied everything. Indeed, I was wrong.  Great, the job is running now. We can run this list command to see that the job has been running.  And as you might have noticed, we have run the job with 2 instances, but this team, Team A, had just 1 instance allocated. So this means that we are borrowing the compute from Team B at this point.

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2960.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2960)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/2980.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=2980)

Let's check if that is the case.  We can run this command on the queue side. First of all, we can also describe the job and the job details as needed to see if the job is running correctly. OK, indeed it's running. And we can then run this queue command which will  look into the cluster queue, and you can see that we are borrowing 4 GPUs. Since, as I mentioned, this team has access to 1 more instance and this is 4 GPUs on board, now we are borrowing 4 GPUs with this job.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3010.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3010)

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3030.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3030)

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3040.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3040)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3050.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3060.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3060)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3080.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3080)

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3110.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3110)

Let me simulate a scenario where Team B wants to submit a job  with just one node. This is their guaranteed compute, which is their guaranteed quota. Let's run this task. Now we should see what you would expect to see : the Team A job is in suspended state because it was preempted, and the Team B job has been created and is indeed running. Another thing that we  can do is preempt a lower priority task. Within Team B, a higher priority task kicks in , which has the experimentation priority, which is higher than the training priority. So now what we expect is that this task will take higher priority  on the cluster and the other one will be suspended. Let's take a look. We have Team A suspended and Team B with one job suspended. The one which was triggered with a higher priority is  now being executed. Now what we can do is delete, for example, the job with the higher priority. This is the one with the higher priority that we just submitted, and we should see that the Team B job now gets restarted so that Team A is suspended versus Team B is one will be reactivated, resumed. 

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3120.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3120)

Then we can delete, for example, the job from Team B. At this point my expectation is that we will also see we are deleting this job and we  will also see, let me double check. It's called Queen Tim B, I guess, is that the name correct? Yes, seems correct. Let me double check. Maybe a typo and then now we can check the jobs and we expect that Team B's jobs for sure have been all deleted and the Team A job has been resumed, so it's borrowing again the idle compute and has been resumed. So now we can just delete the job to clean up the current status.

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3190.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3190)

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3200.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3200)

### Running IDEs on HyperPod: Interactive Development Environments with Remote and Web UI Access

The last thing that I would like to show you is how to manage IDEs on HyperPod clusters and how to run IDEs on HyperPod clusters.  We recently launched this capability which allows you to execute your IDEs, whether SageMaker Code Editor, which is based on Code OSS Visual Studio Code open source, or Jupyter Lab on the  HyperPod cluster. The idea is that once you have a cluster up and running and this cluster is being utilized for training or inference, you might also want to run your IDEs, your interactive ML workloads, reusing some of the compute that might be available on your cluster at a certain time. You also get very fast startup latencies because you can pre-cache the container images for your IDEs on the cluster, so the environments are bootstrapped with very short latency.

In addition to that, you can reuse the same tooling like the HyperPod CLI to work with the IDEs. IDEs on HyperPod are integrated with task governance capabilities, the same capabilities that we have seen right now. This means that, for example, you can have your IDEs running with a certain priority, but then if a higher priority workload is kicked in, you might decide to preempt those tasks because you might want to use the GPUs for some other task. But at the same time your users can take one GPU, for example, from one node and work on their notebook or with their VS Code using this GPU, as well as even a fraction of a GPU because we have added support for fractional GPUs using NVIDIA MIG technology, so partitioning the GPUs into smaller fractions.

I'm not going to spend too much time on the details of this capability, but you can create multiple templates as well for your IDEs so that each template defines the image that you want to use, some configurations on the eastern side, default eastern type, and the default computer resources that are going to be requested.

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3330.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3330)

This way, when users spin up the environment, they can just use the template without having to provide all the parameters like specifying the computer resources they need. Administrators can set up various templates that are then utilized by users to spin up their environments. 

[![Thumbnail 3360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3360.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3360)

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3380.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3380)

Let me move to a demo for this as well. I need to switch back here. Let me open the last module, which is the spaces one.  So let's set a variable, which is the name that we want to give to the space. Then, as you can see, we are running this command that creates a space with the node selector, which allows us to create this environment, this ID environment on a CPU instance. We don't want to use GPU instances for this task. We can use a simple CPU instance to spin up the environment in this example, and we're also mounting the FSX file system. 

[![Thumbnail 3390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3390.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3390)

[![Thumbnail 3400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3400.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3400)

[![Thumbnail 3410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3410.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3410)

Then we can use the CLI to run commands like listing the spaces. I have two running actually. I've pre-created one space on my side, and then this is the one that is being created right now.  We can also run a describe operation to see the details on the space.  This is all the setup about the space workspace that is starting at this point. 

[![Thumbnail 3420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3420.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3420)

[![Thumbnail 3440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3440.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3440)

What I want to show you now is how to access these spaces. There are two options here. One is to use the remote connection via VS Code.  You can set up a remote connection so you have your pod running on the cluster, but then from your local VS Code you establish a remote connection. This allows you to use the remote compute for your workload from your local VS Code, and that's one option. In order to do that, let me take the name of the space that I have already running on my side. 

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3450.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3450)

[![Thumbnail 3460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3460.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3460)

[![Thumbnail 3470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3470.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3470)

This one is my Jupyter Lab space. So space name equals my Jupyter Lab space, and then you can use this command which is called create hyp space access with connection type set to VS Code remote.  This will give you a secure connection URL to the remote space. Once I run this URL, let's say I run it on my browser, this will automatically open VS Code.  I can open it, and what you can see is that now VS Code will establish a remote connection to the space. 

[![Thumbnail 3500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3500.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3500)

So now I'm running on the remote space. Just to take a look at this, I can do a new terminal. I do LS. This is the home directory on the space, but I can also do FSX. So I'm connecting to the distributed file system, the FSX file system that I have attached to my cluster that I've mounted to the space as well. This way, you can do things like look at the checkpoints. This is the previous example of training that we were running, and here are the checkpoints, the model artifacts, and the scripts that we were executing. This is one way to work with the space. 

[![Thumbnail 3530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3530.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3530)

[![Thumbnail 3540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3540.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3540)

[![Thumbnail 3550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3550.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3550)

The other way is via the web UI.  You can just run this command to get a web UI URL.  Just clicking on this link, I can open this website and now we are connecting to the remote space via the web UI. This was the Jupyter Lab space and we're now connecting to the Jupyter Lab service here. 

[![Thumbnail 3570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3570.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3570)

[![Thumbnail 3580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/a9246f22155e8c86/3580.jpg)](https://www.youtube.com/watch?v=6hrqologUZE&t=3580)

### Session Conclusion and Resources for Further Exploration

That concludes the spaces demo and actually concludes the session for today. Here you can find all the resources regarding the functionalities that we have covered today.  What is important is the QR code that you see there from where you can download all the examples that we have gone through today, even more with additional examples.  I'll leave this for a minute or so and then move to the next slide. We can stay here in case you have any questions since I see that we are right on time. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
