---
title: 'AWS re:Invent 2025 - Architecting AI solutions for mission-critical systems w/ UK MetOffice (ARC406)'
published: true
description: 'In this video, the Met Office and AWS demonstrate how they fine-tuned Amazon Nova foundation models to automate the UK''s 100-year-old Shipping Forecast. Ed Steele and Dinesh Mane present their four-week prototype that achieved 52-62% accuracy in generating forecasts from 45GB+ of atmospheric and ocean model data. They compare LLM versus VLM approaches, detail SageMaker training architectures using P5 GPUs, and share key findings: individual attribute models outperformed combined models by 27.3%, categorical data beat continuous by 25.4%, and full fine-tuning exceeded LoRA by 6.2%. The session includes production architecture patterns, evaluation methodologies using word-based F1 scoring, and practical notebook demonstrations for fine-tuning Nova models on multi-dimensional meteorological data.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Architecting AI solutions for mission-critical systems w/ UK MetOffice (ARC406)**

> In this video, the Met Office and AWS demonstrate how they fine-tuned Amazon Nova foundation models to automate the UK's 100-year-old Shipping Forecast. Ed Steele and Dinesh Mane present their four-week prototype that achieved 52-62% accuracy in generating forecasts from 45GB+ of atmospheric and ocean model data. They compare LLM versus VLM approaches, detail SageMaker training architectures using P5 GPUs, and share key findings: individual attribute models outperformed combined models by 27.3%, categorical data beat continuous by 25.4%, and full fine-tuning exceeded LoRA by 6.2%. The session includes production architecture patterns, evaluation methodologies using word-based F1 scoring, and practical notebook demonstrations for fine-tuning Nova models on multi-dimensional meteorological data.

{% youtube https://www.youtube.com/watch?v=VLjzP9n63mA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/0.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=0)

### The Shipping Forecast: A Century-Old Maritime Institution Meets Modern AI

 Thank you. Now the shipping forecast issued by the Met Office on behalf of the Maritime and Coast Guard Agency at 5:05 on Friday, the 4th of July. There are warnings of gales in Fitzroy, Shannon, Rockwall, Malin and Hebrides. The general synopsis at midnight, Atlantic low 997, moving rather quickly eastwards, expected Pharaohs, 998 by midnight tonight. Low, Iberia, 1016, slow moving with little change. The area forecast for the next 24 hours. Viking, south or southwest, 4 to 6, becoming cyclonic 3 to 5 later. Rain or showers, good, occasionally poor. North Utsira, South Utsira, southerly or southwesterly, 4 to 6. Rain or showers, good, occasionally poor. Forties, Cromarty, Forth, Tyne, Dogger. West, 4 to 6, backing southwest, 5 to 7. Occasional rain, good, occasionally moderate. Fisher, German Bight, West or southwest, 4 to 6. Showers, rain later, good, occasionally moderate later.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/100.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=100)

Hi everyone, good morning, thank you for joining us for this session. My name's Ed Steele, I'm IT Fellow for Data Science at the Met Office, the UK's National Weather Service. And what you've just listened to is the shipping forecast, the world's oldest and longest running forecast, in this case, on the anniversary of its broadcast of its centenary earlier this year.  This is a cornerstone of regional maritime safety and a British cultural institution. It's issued by the Met Office on behalf of the Maritime and Coastguard Agency, the MCA, and it's broadcast to mariners four times a day via NAVTEX and via BBC Radio 4 as well. The forecast, as you've just heard an excerpt of, comprises predicted conditions for 31 different sea areas that must be analyzed by human meteorologists and condensed into text sentences with a very specific length and format, following a very specific and strict set of rules.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/140.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=140)

 So the shipping forecast itself, though, is really a pioneer in the communication of meteorological information. And it can trace its roots back to October 1859, when the steam clipper, the Royal Charter, foundered in a violent storm off the west coast of the UK. Over 450 lives were lost associated with just this ship alone, and the storm itself went on to claim at least a further 800 lives in total, and 133 ships were lost, with a further 90 badly damaged. The founder of the Met Office, Admiral Robert Fitzroy, as a direct result of this, introduced the first British storm warning service for shipping in February 1861. And in appreciation of the valuable help from the meteorological service by the radio reports that were received from ships, a weather bulletin called Weather Shipping was started, and this became the shipping forecast as we know today.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/210.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=210)

 So fast forward to present times and the Met Office continues to push the boundaries of science and technology, helping people make better decisions to stay safe and thrive by delivering the most trusted weather and climate intelligence in a radically changing world. We're one of the few, if not the only organizations in the world that crosses a huge diversity of boundaries, covering weather and climate across all timescales, drawing between science and services, to both government, industry, across civilian and military, both nationally and internationally, and predicting meteorology and extending this now into hazards. And this all happens in one place. But actually pulling this together, a recent report issued last year from an independent analysis showed that the Met Office is poised to deliver 56 billion pounds of benefit to the UK economy and society over the next 10 years. And this gives a return on investment from the taxpayer in the UK of 19 to 1 on our public money. So a hugely important organization and service.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/290.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/300.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=300)

But what does a 100 year old forecast  have to do with AI? Well, in this talk, Dinesh and I are going to give an introduction to weather forecasting. We're going to talk about the last mile in weather forecasting  before proceeding to talk about actually how we architect these solutions to be able to deliver these vital services.

### Weather Forecasting at the Met Office: From Observations to Predictions

We've conducted some unique experiments involving fine-tuning of the Amazon Nova Foundation model, which we'll present some initial results from and talk a bit about some of the evaluation of these so that they can be adopted by other interested groups.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/330.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=330)

So how does the Met Office forecast the weather?  Well, it first starts by understanding what the weather is doing now. We're taking in observations from all over the world. In fact, we're ingesting about 215 billion observations every day that provide the starting conditions for the numerical models that we run to predict how the conditions are going to evolve. This takes place on our supercomputer, where we're computing, using the equations of motion, the evolution of those conditions, using mathematical equations to then give us a prediction of what the conditions are going to be.

The data from this is provided both directly to some users, but also passed to some of our operational meteorologists, who add further value by interpreting some of these complicated fields to be able to distill this information into value-added services for the public. And finally, the accurate weather and climate information is then shared more broadly, benefiting all of the different stakeholders with whom we work, from defense right through to aviation, marine, and others.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/410.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=410)

Now when you step back and think about it, weather forecasting is actually a remarkable achievement.  And over the years, we've undergone what's really termed a quiet revolution in terms of our ability to predict the weather. So physics-based numerical models have seen an improvement in accuracy of between about half a day and one day per decade. So for example, our forecast in 2013, five days ahead, was as accurate as our forecast in 2003, for example, at four days ahead lead time.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/470.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=470)

And these improvements have been driven by upgrades to the observations and the assimilation of those data, the dynamics and physical representation of these processes within the maths, and also improvements in model resolution that have been ongoing through this time. And these physics-based models remain an essential component of our forecasting  approach. But obviously it's remiss not to mention machine learning and the accompanying AI revolution that we've experienced.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/530.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=530)

Now the Met Office has always been a big data organization, and AI and machine learning activities have likewise always been ongoing for many years, although these haven't always been acknowledged under that specific label. But undeniably, advances in deep learning technologies in particular are really rewriting the rules of entire industries and including how we predict the weather as well. And what's really brought us to this point are the unprecedented volumes of data, some of which from observations, some of which from numerical predictions, increasing availability of powerful compute capacity, including on services such as AWS, and then also equipping our expert scientists with the data science tools to really be able to exploit these insights and  information.

### The Rise of Machine Learning Weather Models and the Last Mile Challenge

And the rise in machine learning weather models has been relatively profound. So what I'm showing here is a graph showing the performance on the Y axis of a particular metric for a data-driven model as a function of the lead time when it was published. And the key threshold that we're looking at is this blue line, which is the reference level of a physics-based state-of-the-art model. And what we can see is that from September 2017 onwards, we're seeing a steady rise in our ability to predict weather conditions for a subset of parameters using entirely data-driven processes, so involving effectively pattern recognition rather than directly solving the equations of motion.

Around this time in the Met Office, we were starting to formulate some of our data science framework, so we were keeping an eye on some of these developments, but also laying out how we would develop our own capabilities in this deep learning space to complement our physics-based approaches, but also how we would work with partners and evolve some of our people to support this.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/620.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=620)

But the world changed when we hit December 2022. And my boss often refers to this as the missing Christmas,  because there was a stream of publications of really exciting capability around data-driven approaches in weather prediction that came out literally between about the 20th of December to the Christmas period. And what we can see importantly here is that for the first time for some of these parameters, we're starting to see the performance of these data-driven approaches exceeding that of our physics-based models.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/680.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=680)

If we just look at some of the references here, we can also see that these exciting developments were largely driven by some advances from the likes of DeepMind, from the likes of Nvidia, from the likes of Huawei, so not typical players, but being able to exploit some of these data science capabilities. However, a forecast only actually has value if  a user can derive some sort of decision-making benefit from it. And so while we've had this really exciting development in the underpinning opportunities for predicting what the weather will be in different ways, actually there's been remarkably less attention on what the weather is going to do.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/740.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=740)

And arguably this last mile in weather forecasting, a bit like in delivery, is actually potentially even more significant in terms of the opportunities for adding value and adding benefit for our users, because it enables us to provide even more personalized services. It enables us to provide even more multi-modal approaches for our data delivery, so that we can have data and accompanying narrative, while reducing some of the burden that currently these processes are reliant on human interpretation. So we're super excited about some of this stuff, and  to articulate this, just with a single product example, in this case we're going to use the Shipping Forecast as an example that you heard at the start.

We need to take the complicated data that's output from our weather models and turn this into text at scale. And this isn't necessarily a trivial problem because these are very large arrays of data that we're dealing with that span multiple dimensions, including both in space and time, but also span multiple parameters. So not only, for example, pressure, as I've shown here, but also we heard things like sea state, visibility, weather, wind speed, wind direction, and all of this has to be framed within the very specific and strict wording and format of the Shipping Forecast. So this was the use case that we're going to explore in more detail here.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/800.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=800)

### Technical Challenges: Converting Massive Weather Data into Text Forecasts

 Now just to represent this in terms of some of the technology challenges associated with this, this is really just an example demonstrator of the types of products and services that we're looking to transform using some of these technologies. The reason the Shipping Forecast was chosen was partly because of its iconic status, but actually because it enables us to test a variety of different aspects and a variety of different data characteristics. The forecast itself and the underpinning data comprises both atmosphere and ocean model output, so for example, things like wind speeds, but also wave heights.

The format of this data is both probabilistic and deterministic, depending on which of those variables from which of the models have been considered. And it is multidimensional, so we're considering that spatial region, so latitude, longitude, and time. And what's particularly exciting about this is really some of the data volumes involved. So considering just the output from a single forecast run from one of our numerical weather prediction models, we're typically generating upwards of 45 gigabytes of data per individual forecast from the atmospheric models and about 5 gigabytes of data per individual forecast from our ocean models.

When we then consider the Shipping Forecast, we're able to make some efficiencies and we're able to crop this down to just the particular variables that we need and particular area that we're considering, that Northwest European shelf region. And this results in about 152 megabytes for our atmospheric data and about 7 megabytes for our ocean data. But still

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/950.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=950)

fairly large, particularly considering when we're considering fine-tuning foundation models, then this is about 85 gigabytes of data for a three-month period. An additional challenge from a data science perspective is that some of the formats that we're using here are based on Network Common Data Form. So this is largely an atmospheric and an ocean convention. There are very good libraries for being able to read this in, but it's not a typical format that many outside the weather community will necessarily have encountered. But it's quite a convenient form because it's effectively a pandas or X-array type  format in memory.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/960.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=960)

So the solution that we were then exploring was could we provide this information as output from our weather  models and then serve this into a large language model to be able to actually write this text. We were using both a combination of the Amazon Nova Lite and the Amazon Nova Pro 1.0 models. Version 2 hadn't yet come out. And just to frame this task, it usually takes an expert meteorologist a few hours a day to produce this forecast. And in just 4 weeks of prototyping with the team at Amazon, we taught Amazon Nova to do this with between 52 and 62% accuracy in less than 5 minutes.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1000.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1000)

### Two Approaches: Text-Based LLM and Video-Based VLM Solutions

So before I hand over to Dinesh, I  was just going to say a few words about the process of how we went about this and some of the background, some of the experiments that we conducted. So we're going to talk about two different approaches that we explored. Both of which start with taking the weather sensor data and then that gets run with our models, both deterministic and probabilistic models, to produce outputs. Those outputs are typically, as I say, very large arrays, and those consist of the raw input gridded data. So we're dealing with latitude, longitude, and time in this.

We did for the large language model implementation interpret these through a text intermediary, so we're extracting statistics corresponding to the different regions of the Shipping Forecast itself, as you heard at the start, to summarize those and then pass those into the Nova LLM. And this really provides us with a benchmark. What we also explored, and a lot of the focus of our experiments was around using video capabilities of these types of model. So that same gridded data was first converted and encoded to video. And this was done on a sea area basis typically for most of the parameters.

And then we fine-tuned this using the bulletins that had been issued previously by our meteorologists to be able to interpret these as inputs. And then that was hosted to enable us to then run inference on that in real time. But to dive deeper into the architecture of this, I'm going to hand over to Dinesh to talk about that. Thank you so much, Ed. Hello. Good morning. My name is Dinesh Mane. I'm based out of Luxembourg and I work with Worldwide Specialist Team. I work as an applied scientist over there.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1150.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1150)

### Deployment Strategies and Architecture for Foundation Model Fine-Tuning

Before jumping into that, I want to ask a question. How many of you were in re:Play? Can you raise your hand? Kudos to you. You were able to make the morning session. My manager had to push me out of the re:Play to come to the session, you know, like have a good night's sleep. Okay, let's dive deep into it now. Okay, so I'm going to talk about what deployment strategies and what provisioning we did for the fine-tuned model.  We'll talk about the architecture deep dive. There are two main modules. One is large language model foundation, and another one is vision language model. And third one, we'll talk about one of the notebook samples where you can actually see the code, especially the machine learning engineers, they want to see how easy it is to fine-tune a model. So let's do that one, okay?

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1180.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1180)

So there are 2 choices or 2 flavors you can fine-tune a Nova model.  One is SageMaker Training Jobs, and the second one is SageMaker HyperPod. So which one to choose, that depends on the use case and also how much data and how long you're going to train the job. For our experiments, our training jobs were running for 3 hours on P5.48xlarge GPU using 4 GPUs.

We didn't want to use the cluster. We wanted to make quick, dirty runs super fast, so we used SageMaker Training Jobs. But in production, if you have data for five years and you want to train on several hundred GPUs for weeks, SageMaker HyperPod is the right tool for you. There's one small change: only SageMaker HyperPod supports PPO, whereas everything else is supported by SageMaker Training Jobs.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1240.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1240)

As Aid explained, just to recap  this one for the large-first approach, how we tackle forecasting using foundation models. We used two LLMs. The first one was Claude Sonnet 3.7, and we compared the output with the Nova Pro 1 model. Nova Pro 1 had better accuracy for this weather forecast, and it was not fine-tuned. It was just doing simple prompts and processing data. So if I show you here, the weather sensor data is being picked up. The raw input grid data, what does that mean for us, is you have a multi-dimensional array where X and Y represent latitude and longitude. One value will be maybe one of the weather attributes with a value like amplitude. So you have sea wave height, right? At a certain geographical location, you will see the latitude, longitude, and height of that wave. So that is one example of the raw grid data. There are different aspects, so let's stick to that.

That data preparation is done, as Edward mentioned, and we are passing that raw processed data to the foundation model. There's one more thing: if in a sea region there is a weather attribute like wind direction, and if in this particular sea region 90% of wind is flowing to the north direction and 10% is going northeast, we'll select that entire region as north direction. So that is the raw data to the data preparation, and that we are passing to the foundation model.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1350.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1350)

Let's dive deep into the architecture. This architecture was built within four weeks of the prototyping phase. So we have  the raw input grid data stored in an S3 bucket. As Edward said, one run was around 45 GB, but we had a subset of that, and for three months of data it was roughly around 86 GB of numerical data. That's it, 86 GB just for three months and even for a subset. We actually processed with ECS because of parallel processing. If someone is running 86 GB of processing on a single core, it will take months. These two options, EKS or AWS Batch, are just for showing that data parallel processing can be done by choosing one of the three. Once the data is processed, we are calling the Bedrock foundation model, and the output we are storing into the S3 bucket.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1450.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1450)

Here I want to highlight this architecture is really good to process large volumes of historic data. Foundation models through Bedrock mostly support batch inference, but you need to check which model supports it. The Claude family supports it, the Nova family supports it. So this saves a lot of time and it saves a lot of cost as well. There is one small caveat when you are using a fine-tuned model, but I will explain that when we go to the VLM one, so just keep that in your note. This is very flexible because you are using ECS, and under the hood you can use EC2 instances or Fargate based on how much compute power you want to give. There's one thing in this architecture that is lacking: the orchestration is manual. As this was a prototype, we had to do very quick runs, so that's what we proposed for production.  And of course, from customer to customer, it changes.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1460.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1460)

For this use case, we had the S3 bucket here.  Yes, I wanted to use the pointer. So for the S3 bucket, we have the data. We have AWS Glue Catalog and AWS Lake Formation. As the organization grows, teams want to access the data, and to give fine-grained level access to the data, this data plane will be really helpful with AWS Lake Formation with Glue Catalog. Glue Catalog is really good if you want to get a subset of data for a certain time frame from X to Y. You can run the query and you can get data very easily. We are decoupling this architecture using Amazon SQS. Decoupling is very important in production because what if something goes wrong? We need to know which data failed, which forecast is not being processed.

So this SQS is very important. We can attach a dead letter queue as well, so if the record fails, it's added to the queue and the team can investigate it further. The data preprocessing block is the same, so ECS, EKS, and Batch. You can use any one of these based on your needs. Some customers run their workload mostly on EKS, so they prefer to run the processing on EKS. Some use ECS.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1540.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1540)

There are other architectures I wanted to walk through.  What if the use case uses streaming data? We can use Amazon Kinesis Data Streams. A Lambda function is attached for processing. It depends on how many Lambdas you're going to use. If it's a different processing where one Lambda's output is dependent on another, you might use Batch. Once the data is processed and formatted for the foundation model, you just call the model. You store the results in Amazon DynamoDB and OpenSearch. We're using this if you want to do real-time analysis as well. The last one is a serverless event-driven architecture. In this one, everything is serverless. When the data is uploaded and available in S3, it will trigger a Lambda. It will do the processing using AWS Step Functions, and then it will go to Amazon Bedrock and the data will be stored.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1600.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1600)

### Vision Language Model Architecture: From Raw Grid Data to Video Training

Now the second module, VLM, is much more fun.  I'm going to tell you how the data looks. Every day there are four times the forecast is being generated for 31 sea regions. In the first forecast, we'll have 31 sea regions and each region will have four or five attributes, weather attributes, and for that one we have to create a video. Every hour we are taking the snapshot of the sensor data. So for 24 hours we have 24 frames. The raw gridded data is converted to images. So we have 24 images for one day of records, and we are appending them after each other and creating a one-second video with 24 frames. That is our input data for training.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1660.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1660)

After that,  we're going to walk through the architecture diagram. This raw input grid data is stored into an S3 bucket. It's being processed using similar logic, data parallel processing. It created from 86 gigabytes of raw gridded data, numerical data, to 56 gigabytes of video for three months of data. We also had to clean some of the data. There are some incidents where two sea regions' forecasts were combined. That's how the expert does it. So we have to skip that for training purposes. In this architecture, we have the raw training bucket data in which we have the training data. Now we use Amazon SageMaker AI to submit a fine-tuning job. Once the training job is completed, we get the output bucket with model weights, and we are hosting that using Amazon Bedrock.

Now I want to talk to you about two types of fine-tuning that we experimented with. First one is LoRA. What we do in LoRA is we have a foundation model. We add the weights like an adapter, and we just fine-tune that adapter. That's it, nothing else. But when you do a full fine-tuning of a foundation model, you update the weights of each layer. That takes a longer time, but it has the additional benefit that the model learns more. When you are using LoRA, you can use on-demand inference. What does on-demand inference for LoRA mean? The actual model weight is in a service bucket. The LoRA adapter weight is in your bucket, in your account, and when we deploy it, the LoRA weights will be appended and the model will be deployed. So you can use on-demand inference. You only pay for how many input tokens and output tokens are generated.

However, if you're using full fine-tuning, what's going to happen is the entire model weights are going to be updated. So that model weight is going to be stored somewhere different from the original foundation model. Now the new model weights that are there need to be hosted 24 hours on an instance.

That's why you need to go for provisioned throughput. You need to buy the provisioned throughput so that you can deploy. This is a key difference I wanted to highlight. For this one, when we are using LoRA, we were able to use on-demand inference, but when we use full fine-tuning, we had to do the provisioned throughput.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1840.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1840)

This architecture was built for the prototype in four weeks. It is very simple. It lacks a lot of functionality that can be used in production. So let's go to the production architecture and see how it looks. We have different planes: data plane, compute plane,  and SageMaker training job plane as well. In the data plane, I want to highlight and bring your attention to Amazon FSx for Lustre here also. This is added for super fast access if you have a requirement that requires low latency. The Amazon FSx for Lustre cluster can be used for caching purposes, and it raises similar logic to what we showed in the LLM architecture.

The compute plane is being decoupled with SQS, and we are doing computing on ECS or EKS or AWS Batch. Once the data is being processed, all the videos will be stored. Coming back to one forecast, we have 24 hours of recorded data, so we are creating 24 images. We are creating one-second video out of the 24 images, and we have 31 regions. So we have 31 videos for one weather attribute and for one day. This is quite big data, a large amount of data. We use SageMaker AI. In this case, you can use SageMaker HyperPod as well as SageMaker training jobs in production based on the need. Do we really need to go for SageMaker HyperPod?

Once the model is trained, the weights should be stored into Amazon S3, and then we are deploying it on Bedrock. Why are we deploying it on Bedrock? The first question comes: why, and what does it give additionally? Because once the model is on Bedrock, almost all the features available for Bedrock, you can use it on that model. You can use the Bedrock Converse API without changing anything. You just need to change the ID of the model, and it works. If you want to use the guardrails, you can use that also. So that gives you that undifferentiated heavy lifting.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/1970.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=1970)

There are alternate architectures. I just wanted to walk through them.  The first one is model fine-tuning. It's simple. Your data is in an S3 bucket. Why are we taking everything in S3 buckets? Because it scales really well, and with EKS and ECS, it actually pulls it pretty fast. Once the data is there, the data is processed, it stores into the S3 bucket again, and we are using SageMaker or HyperPod to train. And we are deploying it for inference.

The second one is how we will be calling that model. Because in VLM, there is a huge amount of data processing. For example, when the data comes in, this might take at least a minute to process to create a video, because the raw gridded data needs to be picked up, it needs to be cleaned, then it's converted to images, and then it goes to video. So it takes a little bit of time, and once it is done, we will call Amazon Bedrock to get the inference.

The easiest question is: why not host it on Amazon SageMaker endpoint? Why are you using Bedrock, or why can't we host it on Amazon EKS? If you have an open-source model like from Hugging Face, such as Llama V3, you can do that. You can fine-tune that model and you can host it on SageMaker Endpoint, or you can host it on Amazon EKS also.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2060.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2060)

 We had the LLM and VLM part. Now, how we wanted to show it on the UI is real quick. If the application is hosted and accessible to the public internet, you use CloudFront with API Gateway and AWS Lambda, and decouple the architecture with SQS, and it will call Amazon Bedrock or SageMaker or EKS. The other approach is you put CloudFront, you have Application Load Balancer, your static website or your Amazon Container Service website is behind that one, and then through that you will call SageMaker AI or EKS or Bedrock for processing.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2120.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2120)

### Fine-Tuning in Practice: Training Data, Configuration, and Deployment

We are the Amazon App Runner as also Amazon Amplify to host your website. Now let's go into the notebooks. I will ask a very difficult question now. How  many of you has fine-tuned a large language model or any foundation model? Raise your hand, don't be shy. Okay, now a difficult question. Were you able to get it in the first attempt to fine-tune, or no? Cool, that's it. So now I'm telling you this one, I ran like 25 experiments and one or two failed because I had to stop it because the training data was missed from my side. It runs very smoothly, okay?

Now how the training data looks like. So I just want you to pay attention over here. The system is a system prompt, okay, it says you are an expert in meteorology specialized in UK maritime. Okay, this is our user prompt. We should analyze this one-second video and do this, this, this. And this is our input data. You see Biscay is the sea region, and this is one of the weather attributes. This is our input video, and this is the ground truth, our output. That's it. If you have 3,000 examples like this, it creates one JSON file with 3,000 lines, and this is getting flattened as one line. So that is our training data. We create training data set as well as the validation data set when you are fine-tuning a model, okay.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2210.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2210)

 How you will configure for fine-tuning a model, like what is this? What is the recipe file? So people who don't know, the recipe file is just like a configuration for hyperparameters. So in this one, if you see, we give them which model we want to fine-tune. Replicas is around how many GPUs you want to train. We say the training configuration, how many epochs if you want to control the regularization parameters. Optimizers, there are a bunch of optimizers available out of the box. You can use it straight away, and this is a LoRA configuration adapter. That's it. And you need this YAML file and a training script. Now let's see how difficult is the training script. Okay? For me it was very hard, but let's see.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2260.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2260)

Cool. It's very simple.  Here we define our YAML file, okay. Input data, as I told you, there is train.json file, for example, I select 3,000 lines just in one file. Output bucket where we want to store the model weights. Validation is validation.json, okay. I want to pay attention over here a little bit on validation. Whenever you are fine-tuning a model, use validation data set. Why? Because the model will try to optimize on a training data set. It will always go the training loss low, low, low, but on actual data it might go up. So you need to find a sweet spot where training data, training loss, as well as validation loss is going down. But as soon as validation loss is going up, you need to stop your training, okay? Just keep that in mind.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2350.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2350)

And this one is image URI. This is a Docker image, okay? And it runs very smoothly. There is no issues on the library configuration. It runs as smooth as possible. Then instance type is P5.48xlarge. For Amazon Nova fine-tuning, you can use P5 and P6 as well, and there is a recommendation in instance count is 4 to 8 and 16. You can add TensorBoard output when you're fine-tuning a model as well. Now we create an estimator. This is just like giving all the information what was there,  and here we get the input, the training input, validation input, and we say fit the model. Within three hours, the model will be trained. That was for our data.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2370.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2370)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2400.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2400)

Now how to deploy that model. We have here Bedrock client create custom model, so  it will create a custom model as in it will get the model weights, okay? And this one create custom deployment means it will deploy on Bedrock. It will take an hour or so to do everything. So I want you to pay attention to the custom deployment ARN. This is kind of model ARN or model ID. Now, how we will get the inference? This is a magic code. So we have the Bedrock runtime converse API. We give the model ID. This is a custom deployment ARN, 

The system prompt was "you are an expert in UK meteorology," and the user prompt was "analyze that 12-second video." This included an input video URI from an S3 bucket, and that's it. The model will give the output, and this is the ground truth. So it is very close to this one. Here we did a lot of experiments, around 2,025 experiments we conducted, and we did experiments with LoRA as well as full fine-tuning, like fine-tuning the entire model. That's where the experimentation and the model evaluation will be run by Edward, so I'll hand it over to you. Thank you so much.

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2460.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2460)

### Experimental Results: Optimizing Model Performance Through Fine-Tuning Approaches

Yeah, thank you. Great. Thanks, Dinesh. So having discussed the architecture, we'll just talk briefly about some experiments and evaluations that we conducted to further optimize our presentation of the information  to the foundation model. These experiments in Nova fine-tuning boiled down to four things. Firstly, we were looking at combined versus individual models. We were looking at continuous versus categorical data that's provided to these. We were looking at overfitting versus simulation of early stopping. And finally, as Dinesh also mentioned, full rank fine-tuning versus LoRA approaches.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2490.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2490)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2510.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2510)

Again, just to recap,  for our LLM experiments, this was using the Nova Pro model without any fine-tuning. For our VLM approaches, then this was using the Nova Lite model  with the fine-tuning as Dinesh has just described. So into some of these experiments then. First of all, we considered how we're formatting these videos. So it's attractive on the face of it to have just a single video that we're providing with all of the information about all of the weather attributes, so clubbing together each of the parameters of interest. For example, wind speed, wind direction, visibility, weather type, sea state into a single 5-second video.

But actually, what we found was that if we're instead breaking these down by individual attributes and training models for each of these, then actually we get a much better result. The individual models were outperforming a combined model by on average about 27.3% because we've got increased opportunity in terms of the prompts that we're providing, increased control in terms of how we're doing this, and there's not so much context switching taking place on that. The next question was actually how we're presenting this meteorological data to the model itself.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2580.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2580)

 So typically, we consider, for example, fields of sea state, so our wave height is effectively a map of how high the waves are, with that height being a numeric representation. But given the specific language of the Shipping Forecast, we actually have an opportunity to categorize these into the particular bands that correspond to this particular terminology. And again, changing the color scales from just a continuous color scale for the videos that were provided to these banded categorical values that correspond to the specific terminology of the Shipping Forecast again enabled us to achieve further improvements, with our categorical data models typically outperforming the continuous models by on average about 25.4%.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2660.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2660)

And if you drill a little bit deeper into this, actually the biggest improvements are in areas such as the weather type, where previously just a numeric text label has no real meaning. But actually when we're converting this and labeling this on the y-axis or our color scale on the videos that we're providing, we can provide this additional information to help. What was quite an unintuitive result in many ways  was that in our experiments of overfitting versus early stopping, the overfitting outperformed the early stopping, despite these having a higher validation loss.

And if we trace this back, this is due to a discrepancy between the training objective that we were using in training compared to our evaluation approach. And really, the overfitting is enhancing the memorization of precise word patterns which is specific in the Shipping Forecast, and therefore produces more confident output. So it increases our word-based F1 scores, giving more accurate and more complete forecasts.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2710.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2710)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2740.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2740)

But the early stopping, which optimizes this embedded probability or perplexity, fails to capture these types of nuances.  And the final experiment that we'll talk about relates to this full rank fine-tuning versus the low rank adaptation. What we saw was that typically our full rank fine-tuning is outperforming the use of the adapter in these experiments by about 6.2% across our models. As Dinesh explained, the LoRA approach is basically applying  an adapter to the pre-trained weights that then enable us to make these adjustments.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2760.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2760)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2790.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2790)

But it's worth just drilling a little bit deeper into some of the characteristics of these approaches and particularly some of the cautions in terms of using some of these approaches,  particularly compared to a full weight fine-tune. So fundamentally, if the model has to perform only a single narrow task, full model training can still have a performance advantage. We're very closely optimizing it to learn that very specific behavior. In contrast, LoRA will learn less, and I'd refer you to the paper by Dan Biderman et al., which describes this in nice detail. 

But LoRA learns less, but also it forgets less. So with full model fine-tuning, you can risk catastrophically forgetting the other capabilities that haven't been represented in the training dataset. So you can effectively just overwrite this with catastrophic effect if you're not careful. There is, however, a compromise in some of this space, and as Dinesh said, we were applying this LoRA adapter just to the final layer of the model.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2880.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2880)

However, there's some recent evidence to suggest that instead of just applying it to that final layer, applying a LoRA adapter to each layer of the network could significantly increase performance, albeit at the cost of increased latency. However, this would provide a more regret-free approach to being able to tune these approaches using LoRA-based methods, and in contrast to the full fine-tuning, if you're also increasing the learning rate as well, could potentially achieve parity of performance in that space as well. It's also worth mentioning just in the context of some of the announcements this week that some of the capabilities of Amazon Nova as well might offer additional opportunities for being able to comprehensively tune these types of approaches in future. 

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2900.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2900)

### Evaluation Methods, Model Comparisons, and Key Learnings from the Prototype

So, having talked a little bit about some of the experiments and some of those optimizations, I think it's also worth touching on the evaluations. And I've mentioned, for example, some of the F1-based metrics that we'd used, and I'm just going to dive a little bit more deeply into these before presenting the final model comparisons. 

So throughout our experiments, we used word-based F1 scoring. As we heard right at the start, the text of the Shipping Forecast is highly specific and it's structured in its nature, so it doesn't leave much room for creative writing. We therefore wanted to use a direct comparison metric, so we're counting the number of matching words, the number of missing words, the number of extra words between the generated text and the expected text. And I'm showing an example of this just here.

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/2960.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=2960)

So for example, if our expected text relating to the wind characteristics was northeast veering southeast 3 to 5, and our generated text was east or northeast, 3 to 5, then we've got 4 true positives, 2 false negatives, 2 false positives in this, giving an overall F1 score of 67%. Now an alternative approach that other people might have considered might have been using, for example, a BERT score. 

So this is an alternative to a word-based precision recall, but it provides a much softer match between the words, resulting in an overly lenient impression of performance in applications such as the Shipping Forecast where extreme precision is required. So for example, using that same case that I just showed for the word-based F1 score that we used, if this was a BERT-based score, then rather than having a result of 67% accuracy as we were measuring here, you would result in a performance of about 86%, which is a little bit misleading.

And really to drill down into why these types of approaches are so misleading is if we consider, just for example, the word

north and south. Obviously, geographical direction is fundamental in communicating where weather systems are coming from. But if we generated an expected word north and the expected outcome of north and the generated word was south, then actually this would result in a BERT score of 82%, despite this having the complete opposite geographical meaning. This is because it's working on the embedding level rather than on that word-to-word level in the score. I thought I'd mention that just as a word of caution in terms of evaluating these types of approaches, but also to demonstrate the strictness and the robustness that we applied to these experiments, because we're literally comparing to what the expert meteorologists issued in their bulletins and counting those words on that basis.

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3060.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3060)

Now, as Dinesh also mentioned, we  considered some comparisons between the Nova Pro model and the Claude 3.7 Sonnet model for the LLM comparisons. And again, as Dinesh mentioned, there was a difference in performance, with the Nova Pro scoring an average Word F1 score of 62% to Claude 3.7 Sonnet of 57%. And significantly, from an operational perspective, at a cheaper cost. But I wouldn't drill too much into the actual numbers here, because as we know, within this field, foundation models are being released all the time, and this is only growing.

So the main thing to take from this, and as it relates to the architecture that Dinesh talked about at the start, is the great thing about having architected this in Bedrock means that actually, as new models are released, so for example, Claude 4.5 Sonnet, then actually it's just a one-line change within our code to then be able to test this with a new model. And depending on our application, we might find some perform better or worse, but we have this complete flexibility to be able to take advantage of new models as they're released on this basis.

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3140.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3140)

It's also worth saying something a bit about the  LLM versus the VLM comparisons here. Now, with our LLM-based approach, we were providing information to that through a text intermediary, and we were able to provide a little bit of extra information due to the way we were able to format some of these experiments, and it was on the Nova Pro rather than the Nova Lite model. In comparison, our VLM approach used, for example, a more simple representation of some of our probabilistic information, and this could be significantly improved. But nonetheless, we're getting scores compared to the human meteorologists' written bulletins of between 52 and 62%, and the opportunities for evolving this, particularly in the VLM space, is particularly significant. And we think that this is going to be key going forwards in terms of reducing some of the information bottlenecks that would otherwise be associated with providing information to LLMs through a text intermediary stage as well.

[![Thumbnail 3210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3210.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3210)

So just to summarize then, some of the key learnings  from this. Fundamentally, we've evolved a pattern for architecting scalable AI solutions with considerations of alternatives, advantages, and disadvantages that can be applied to this type of data-to-text conversion that applies to so many of the types of products and services that we deliver at the Met Office. Also, this provides a framework for rapid experimentation and enhancement. As Dinesh mentioned, all of this work was conducted within a four-week period, from spinning up the environment to conducting the evaluations and assessments. So it just shows the types of velocity that you can work at both within these, but also with working practices and support from the team at Amazon.

Fundamentally, we were looking at comparisons of LLM-based approaches versus VLM-based approaches. And as I said, the LLM performs better because we're able to apply some of the extra domain knowledge that we're able to simplify some of its input. But these textual data descriptions do represent an information bottleneck. So in the future, we're anticipating that the VLM will actually outperform the LLM on this task, and we've got some exciting steps to implement to be able to move forward on this.

Finally, we talked in length around some of the fine-tuning experiments that were conducted, and particularly the use of LoRA versus full fine-tuning. So LoRA learns less, so for specialized applications, then actually full model training could have a performance advantage, but it also forgets less, so you reduce the risk of catastrophic forgetting.

As I said, some of the really emergent research at the moment suggests that applying this to each layer in the model could give a regret-free approach, and there are further considerations and opportunities associated with the Nova Forge framework which might provide an additional basis for this.

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3330.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3330)

[![Thumbnail 3340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3340.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3340)

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3350.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3350)

[![Thumbnail 3360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3360.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3360)

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3370.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3370)

I'll leave this up here at the moment, so really just bringing it all together.  We've got the full architecture diagram that we used  for the prototype on the left-hand side. On the right-hand side, we're just showing some of the interface,  and it's recording the interface for the LLM-based approach  that provided us a means of being able to expose this to some of our users. Really excitingly, we're moving  forward with actually being able to provide some of these outputs for evaluation by our meteorologists, not just applied to the Shipping Forecast, but applied to similar workflows as well. So we're really looking forward to this.

[![Thumbnail 3410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3410.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3410)

Just incidentally, what we're seeing here on the left-hand side is we've loaded in the Shipping Forecast bulletin. This is for evaluation. We're running the LLM task here and we can see the progress that is associated with this as it's running. In this case, it's the LLM task. And then on the right-hand side we'll start to see the result returned from the LLM, corresponding to the Shipping Forecast that then gives us this opportunity for evaluation  on that.

[![Thumbnail 3420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3420.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3420)

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3430.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3430)

So we can see this is now generated on the right-hand side. We can compare this directly to  the bulletin that was written by the meteorologists, but we can then also apply our scoring based on these comparisons  so we can actually evaluate the performance of this, which will then feature in the tasks at the bottom down here. Within this same interface we're just able to start to engage some of those types of interested users on this basis.

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3450.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3450)

[![Thumbnail 3460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3460.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3460)

[![Thumbnail 3470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3470.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3470)

And then finally at the bottom  we're just generating some of the, well, we're generating opportunities for translation. Obviously as a national meteorological  service, being able to provide data in different languages or input in different languages is obviously off-the-shelf capability of these types of models,  and we can see the output for human reference that was generated there.

I'd just like to conclude by thanking you for being here. As Dinesh said, really appreciate it after re:Play. But also I'd particularly like to thank the team from the specialist prototyping team from AWS, whose support on this has been absolutely fantastic and absolutely fundamental to unlocking this. So thanking Dinesh, also Emilio and Luis and Amir who couldn't be here today, and Govindarajan, who's the team leader as well.

[![Thumbnail 3540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/3fd124f2d1733e25/3540.jpg)](https://www.youtube.com/watch?v=VLjzP9n63mA&t=3540)

Finally, I'd just like to say please do provide any feedback on this via the survey in the mobile app. That would be much appreciated. And if anybody wants to chat further afterwards, we'll be at the back and very happy to pick up questions. Also point you to on arXiv this week we released a paper describing in more detail some of these experiments that were conducted. So if you're interested in that, please speak to Dinesh or I and we'll be pleased to provide that link. Many thanks. Thank you so much. 


----

; This article is entirely auto-generated using Amazon Bedrock.
