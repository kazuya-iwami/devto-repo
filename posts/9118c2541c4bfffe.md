---
title: 'AWS re:Invent 2025 - GenAI in the Beautiful Game: Data-Driven Success in Football (SPF302)'
published: true
description: 'In this video, Steve Drew, Senior Enterprise Architect at AWS, explores how generative AI and AWS technologies are transforming football operations across English Premier League and German Bundesliga clubs. He demonstrates how clubs like Holstein Kiel, Darmstadt, and West Ham United are building sophisticated data platforms that process thousands of data points from GPS tracking, video analysis, and scouting reports. Key use cases include natural language querying of player statistics using Amazon Bedrock, intelligent document processing for handwritten scout reports, and agentic AI for multi-domain analysis spanning match performance, injury insights, and talent identification. West Ham''s end-to-end GenAI scouting platform exemplifies how clubs are reducing report creation time while analyzing 100,000 players globally. The presentation emphasizes that solid data foundations, combined with AI/ML and generative AI services, enable clubs to gain competitive advantages while augmenting rather than replacing human expertise in coaching and scouting decisions.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/50.jpg'
series: ''
canonical_url: null
id: 3087649
date: '2025-12-05T21:30:18Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - GenAI in the Beautiful Game: Data-Driven Success in Football (SPF302)**

> In this video, Steve Drew, Senior Enterprise Architect at AWS, explores how generative AI and AWS technologies are transforming football operations across English Premier League and German Bundesliga clubs. He demonstrates how clubs like Holstein Kiel, Darmstadt, and West Ham United are building sophisticated data platforms that process thousands of data points from GPS tracking, video analysis, and scouting reports. Key use cases include natural language querying of player statistics using Amazon Bedrock, intelligent document processing for handwritten scout reports, and agentic AI for multi-domain analysis spanning match performance, injury insights, and talent identification. West Ham's end-to-end GenAI scouting platform exemplifies how clubs are reducing report creation time while analyzing 100,000 players globally. The presentation emphasizes that solid data foundations, combined with AI/ML and generative AI services, enable clubs to gain competitive advantages while augmenting rather than replacing human expertise in coaching and scouting decisions.

{% youtube https://www.youtube.com/watch?v=iOxZzRBhrM0 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Generative AI Meets the Beautiful Game

Hello, good afternoon all. I hope everyone's doing well. My name is Steve Drew, and I'm a Senior Enterprise Architect at Amazon Web Services. I work with some of our strategic partners and customers as they're building solutions and using AWS technologies. Today, I really want to talk through the use of generative AI and the beautiful game in football, something that I'm quite passionate about. I've been a football fan for over 40 years now, and I've worked in the data space for a while. I put together the London Olympics data APIs, and what I've been really happy to see is the explosion in the data that is created, how it can be used, and the insights that can be gained.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/50.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=50)

 I thought I'd put this up. Brian Clough was one of the best managers ever from England, and he said, "If God had wanted us to play football in the clouds, he'd have put grass up there." Now Brian Clough was not a man to argue with, and I don't think I'm arguing with him. What I'd like to show is how AWS services are influencing how football is being played. From today, what I'd really like to talk through is a little bit around some of the sports data platforms, the basis of what's going on, and the information that generative AI and AI services can process.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/80.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=80)

 We've got a couple of use cases which are coming from some English Premier League and German Bundesliga clubs, and then maybe a little bit around some of the evolution into agentic AI. Football is one of the most popular games worldwide, from people playing in parks to stadiums with up to 100,000 people to billions watching a World Cup final. But one of the key things to say is that it's not just a game of 11 players versus 11 players.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/110.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=110)

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/130.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=130)

 Modern football clubs are very sophisticated organizations.  We have a lot of different roles in there who are supporting the players to try and drive performance. This shows that results are determined by more than the 11 players who are on the pitch. When teams are competing at the highest levels, there are some pretty fine margins involved, and it's becoming increasingly important to try and find potential edges or to gain an advantage.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/150.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=150)

 Fundamentally, what we're trying to do is win more games. We have to do this by playing by the rules, and we can't just invest in unlimited amounts of money anymore. But more importantly, with the huge amount of community that's involved around football, we want our fans to celebrate success. We want to grow our club in a sustainable way, and building that community is a big element of it.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/200.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=200)

 So if we think around the use of technologies by football clubs, how do we break this down into our main components? We have our use cases of what we want to solve, what problem we're trying to solve, and where we're trying to gain an advantage. This is supported by a combination of data, some traditional analytics, as well as AI, machine learning, and generative AI technologies, which is hopefully then driving us to gaining some insights, getting some value, and hopefully getting a better competitive advantage.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/240.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=240)

 I think this is just a small sample of some of the use cases where we've seen professional football clubs using technologies. Whether that's around a preparation element, such as what our training schedules should look like, or from a performance analysis point of view, such as how specific teams or the group of players who are on the pitch are performing and how they're trending. And then the other big group is around scouting, so for both developing our own players and trying to identify potential future targets to bring in to bring us more success.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/280.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=280)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/300.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=300)

### Building the Foundation: Data Platforms and AI Integration in Football

 I think a few of you have probably seen this slide before, but data is the foundation. It enables us to use those AI and generative AI services to then derive insight and to try and gain that edge. If we think of this in a football context, we have external data sources and we have our own team's internal data sources as well. 

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/350.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=350)

We have videos being recorded with increasing frequency, capturing highlights of specific parts of games. We have text data coming in around coach reports and scouting reports. There's a huge amount of tabular data coming through. When we think through the thousands of data points and data fields available around how a player has performed with their passing KPIs, we have GPS tracking. There's an ever-increasing amount of data with thousands and thousands of data fields and millions of data points. 

As before, we can start answering questions with generative AI on our use cases. At the end of the day, we're still building applications, so we need a database for the user experience outside of invoking the LLM. We have pretty standard data pipeline architecture with data ingestion, where we're bringing in either via batch or real-time structured and unstructured data, setting up the pipelines to process this, to transform it, and to get some initial insight out. We have considerations around a data governance element. We need to ensure the privacy and the security of that data, where we can potentially have players' medical histories in there and other sensitive information. There is PII data and private data in there.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/430.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=430)

Before we can go to that general level, we still have our traditional data processing to do. From the combination of data, AI, ML, and generative AI that we've been working with at the clubs, the AI and ML is still there and still ever-present, really coming through from three parts.  One is around the statistical analysis of our huge amount of data coming through. We're looking at trend and pattern detection, and also feature identification, trying to look at what become the important data fields in this plethora of data that we have. We still see a large amount of use of AI and ML as another layer of processing the data, which can then be used by our generative AI topics.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/490.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=490)

This is just a quick summary before I go into the detail, but where we're seeing the uses of generative AI with clubs, we've got around that data extraction part and intelligent document processing. We're looking at how to handle scouting reports and handwritten reports, extracting specific data attributes that might come out of that unstructured data.  We look at that from an insights perspective. Now that we have our data lakes filled with data, how can we aggregate reports and information from other parts in there? We move to that last part around democratizing our data, where we're thinking about how we can make our data pools easy to look at so that the scouts, the coaches, and the other personas can easily query this data.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/560.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=560)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/600.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=600)

### Performance Analysis at Scale: Holstein Kiel's Data-Driven Approach

A fairly common use case we've seen across a good number of clubs is looking at how we analyze our teams or players' performance.  This is taking the thousands of data fields that exist and trying to work out where we can gain that competitive advantage. That common approach really starts with that data platform, some analysis, visualization, and maybe a little element of generative AI. We're bringing in here, so from this example, one of the clubs in the German Bundesliga. 

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/670.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=670)

The German Bundesliga provides a great example. They wanted to build groups of key performance indicators that could then be analyzed and processed for specific players. This became part of our data pipeline. We start with 6,800 different data points and extract groups of data features, whether that's possession KPIs, tackling KPIs, or other categories. Each of those groups of KPIs can be made up of numerous individual KPIs, and we handle that during our data preparation and preprocessing stages. Once we've completed that, as users put queries in or build dashboards, we can then do longer-term data processing for a single player, multiple players, or player comparisons across a season. This becomes the foundation for our visualization. 

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/720.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=720)

There are many architectures that we've supported clubs in developing. I've tried to pick one of the more simplistic ones to show here, but the components are really the same. We have a number of third-party data APIs in the top right. We have a process to ingest that data, to ETL it, and then store it in whatever data stores we prefer, using Amazon Redshift as an example. We then front it with either a custom-built UI or something like Amazon QuickSight to provide easy visualization of the data that comes through. 

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/740.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=740)

Looking at this example from a club, it's a very simple dashboard, but here I can query and compare players against these groups of KPIs and see how those two players compare. Now I'll move on to how things are evolving. 

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/780.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=780)

We've been working with Holschenkiel in Germany in the German Bundesliga. The key message to highlight here is that data helps us generate important objective information in a short amount of time. Football clubs are generating vast amounts of data, but often struggle to transform it into competitive advantages. The approach that Holschenkiel initially took was thinking about it from a data architecture perspective. Because the data sources should not be viewed in isolation, we've got many linked data types. We started by creating a primary ID for every player they have so they could track the long-term progression of how those players were performing both on the field and in training, and how they were progressing through the different clubs. 

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/840.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=840)

They had identified six key areas that they wanted to look at, whether that was team management, workload management, or scouting. These became the football domains that they found were valuable for gaining insights. From that data model, we set up a data processing environment with a focus on intelligent player analysis. There were five key technical principles that were put in place around scalability and having a secure data foundation, building it as a modular architecture so it could be expanded in the future. A key part as well was approaching this from an event-driven but also logically separated view. 

Why was this important? Because it reduces the technical development or maintenance that's required in the future. At the end of the day, a club like Holschenkiel's focus is on football. They are not a technology provider, as that is not their core business. This example is trying to show how, for a specific player, their attacking performance has changed over the last period of time.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/920.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=920)

### Natural Language Queries and Reasoning Models: Democratizing Football Data

Building on this  from that sort of data platform, Hosecure have been looking at how they could use Bedrock or use OpenAI on Bedrock to gain further competitive advantage, trying to look at those forward-looking technologies. I think the three areas that you may have seen before are trying to use reasoning models such as OpenAI to ask questions via natural language to have the LLMs consider all of the available data points, trying to move on from what is a pretty static dashboard.

I think they are trying to visualize, maybe from that first part, how we can bring in some natural language search into our existing data platform. We have seen a number of cases where it is a fairly easy part to start with. So in our first application point, users can ask a natural language query, such as how has Harry Kane's conversion rates changed over the last six months. This query is taken in via a custom AI running on AWS Amplify. That user query is then passed to Amazon Bedrock Knowledge Base, where we can do both data retrieval and data processing.

The knowledge base converts the user's request into a SQL query, which we can then push to our existing query engine, Redshift in this case, to extract the data. We have a few places where we can have our data either labeled at the field level in Redshift, or we could be using AWS Glue tables to enrich it. As our data is processed and our SQL queries run, we have our data returned back and we can then process it with our LLM and pass it back to the user. With this, we can then go interactively and think around further questions to refine our analysis.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/970.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=970)

There are some more complex reference architectures that we have for looking at this. A common approach becomes adding a RAG type data store here as well, so you can have pre-populated or pre-used queries defined by the data analyst to use. But it really falls back to the use case of how can we democratize our data and how can we get to better usage by what we have of non-technical staff, whether that is the coaches or others. 

I think one of the four considerations, Hodgson Kiel with their choice of using OpenAI or Bedrock, was looking for the reasoning capability that comes in and becomes available. So maybe in that sort of question of why has our striker's goal contribution dropped forty percent, some of the results we may see without heavy system prompting that come back from an LLM is a very one-step answer. With the use of reasoning LLMs where it can be broken down into multiple tasks and go into that multi-step planning, we can look through and investigate more of the data sources that we have collected.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1120.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1120)

If we find conflicting information as we are iterating through the plan, the LLM can revise its future plan to then use tools, whether that is SQL queries via tools or running code to then provide a more accurate answer. I think having that validation step in here, if we are planning to use  reasoning models, becomes quite important.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1220.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1220)

This validation step becomes particularly important if we are planning to use the data to change how we're doing our training methods.  One example that Hush and Kiel went through involved their digital transformation. They were trying to work out what their compass should be, what we might call their North Star. Rather than just introducing new technologies, it's about trying to answer some key questions such as how will our different teams or departments work together in the future. Most importantly, how can we enrich the human knowledge that we have? We have staff members with decades of experience, and the question is how can we enrich their skills with modern technology? And then how do you measure the success of new approaches?

They put together what they call their Data Value Generate Flywheel. The high-level idea here is to start off by looking at the data that they have stored and available, and to think through what the potential use case is. Coming back to that point, it's important to work from that use case. With the technical infrastructure they've put together, the next step is to take a use case, quickly create a proof of concept, iterate on it, and see if they actually get value coming out for what they're trying to do. Then they take that to a full functional solution if that use case is successful.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1350.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1350)

If they do have positive results, they can then start using it more broadly and maybe devote more people's time to using it and more financial incentive to it. But that then contributes back into the overall value of the data platform and AI platform they've put together, because for each iteration when new features are being added, there's better overall insight that comes out.  I think another important quote to bring out is about operational improvements and deploying human expertise more specifically. That's the key part because all the technology that we're using or putting together is there to augment staff at the club.

### Practical Applications: From Penalty Analysis to Handwritten Report Processing

Moving on to a different club, this one may be a little more machine learning focused than generative AI. As an England fan, it always pains me to bring up the topic of penalties, but in the Bundesliga, Darmstadt is an extremely data-driven football club. They try to build data into all of the decisions that they're making going forward. When I was speaking to one of their data scientists, I pulled out an example of where they are using some of these technologies. Something like penalties in football games is not a very frequent occurrence. They've got a pretty good conversion rate at 78 to 79 percent. So they took the experiment to see if they can gain a small advantage here, which potentially modifies something pretty big on the pitch, that goal event.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1380.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1380)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1450.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1450)

  Where we've got our external data providers, from a penalty kick perspective, you can have coaches going through manually watching videos. There are third-party feeds available so you can see for each player which coordinates they put the ball and how they approach the layup. We have that stored in our data storage system. Fabian's view here is that the focus is really trying to gain an edge rather than running and setting up infrastructure. So the choice he went with was using Amazon SageMaker Studio Lab to easily set up his environment, and then using Amazon SageMaker to train the model.

One of the things that scale would help with is this sort of scale to zero approach, which is relatively low cost to run. As they went through models built, the testing is still ongoing, which is not something they're willing to share at the moment. But it was an idea where this use of technology was then used by the goalkeeping coach and the goalkeeper to have that list. You know, that list is still stuck on a water bottle stuck behind the goal, but they've done analysis to see if, well actually, from the opposition striker, where is he likely to go. I think with it, it's trying to be practical with it. We don't have to deliver the most complex architectures to solve or answer questions or to gain insight.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1570.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1570)

 I think one of my favorite quotes at the top here is that emotion drives football but data drives progress. I think that's a really powerful thing for a club to be saying, and this is where Darmstadt are using data at every step. Now, previously you've been talking through how a lot of stats and data analysis come from those sorts of parts. But there are other sets of data that we want to get insights from. We want to augment the tabular data that we have.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1600.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1600)

 Within a football club, you know, we've mentioned that there are large amounts of coaches, scouts, and other parts coming through. We want to take some of those still handwritten reports that come through, some handwritten, some on tablets in other ways, and then look at how we can bring this data in to be queried alongside our tabular data. Again, I've tried to keep it as from our sort of previous data platform. We've got the ingestion, we've got the transformation, we've got some processing in here as well.

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1640.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1640)

 The approach that was taken here was to have photos of the handwritten reports uploaded to a portal and then ingested and stored to S3. We're then bringing it in through step functions to initiate the processing. Through experimentation and some comparison, the use of an LLM was brought in to do both the OCR and intelligent document processing part of it to then extract some key insights and key entities from that information. As part of that process, we do some validation on things like player names and other parts, some of those fixed known variables. That information was then written back into our data stores so it could be visualized alongside the existing dashboards that had been created.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1730.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1730)

### The Evolution to Agentic AI: Multi-Agent Systems for Football Intelligence

 We had some document ingestion and document processing. As we start building and increasing the types of data that we have available, we start looking at the next iteration of our application. How can we start searching across multiple diverse data types and data stores? Because having data stored in one specific place, we always try to pick the best data store for the right type of data for better performance that way. The next iteration was then looking at rather than the use of, say, QuickSight for dashboards, we've got our custom app written on Amplify and hosted there. We're then looking at how we can use Bedrock Agents to take the queries coming in from the user. For example, how is Harry Kane's conversion rate for the last six months?

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1830.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1830)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1870.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1870)

Our agent then connects to our structured or unstructured data stores to extract the relevant information, combine it, and present it back to the user.  We then move on to a topic where there is still experimentation rather than full production deployment, at least from what I'm seeing. Agentic AI is a hot topic in this space. We're trying to take our three key inputsâ€”our memory of what's been done before, the tools that are available, and the goals of what the user is trying to achieveâ€”to then take some actions to query our data, make observations, and iterate. 

I wanted to bring out how that single agent approach could work, coming back to maybe one of the tasks that were discussed. So what becomes the task? We're looking at match analysis. We want to look at the previous game and think about how our pressing play performed today. We then pass that request into our agent, which looks at it from a planning perspective based on the user's query. What tasks do I need to break this down into? This includes assessing the user's ask, evaluating responses, and analyzing the data. The planning steps can be modified and iterated as needed.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/1990.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=1990)

We then call out to some tools or actions, which commonly include querying our data stores to see what the historic attacking performance looks like. We want player-level analysis or team-level analysis. Our agent then iterates through the plan, ticking the points off and maybe adding new ones. The idea is that we now have a more flexible and autonomous approach. As new tools and information are discovered, we move away from fixed coding or fixed prompts. From the architecture perspective, many people are now talking about AgentCore for hosting our agents. 

From our user front end hosted on the left within AgentCore Runtime, we want to deploy our agents. Within this, we have the instructions for what we would like the agent to do. If we think of it from a football data analysis perspective, we have some of the rules and guidance of what it should look like. We can build out some of the historic reports that we had available, such as player reports, team analysis, or player stats, and make them available as local tools to the agent. These are actions that can inform and query our different data stores.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2070.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2070)

As it iterates through, we can make calls out to Amazon Bedrock and the large language models of our choice. We want to work out what to do, process our information, and move toward the goal that our user has prompted us for.  I wanted to bring this back to the six domains that were discussed. As our systems evolve and we want to generate more insight or gain more visibility into our information, we can approach it from a multi-agent approach, thinking around those more specialized agents that are focusing on specific areas.

This gives us modular agents coming through. For example, one could be focused on match analysis. This approach gives us flexibility because we can quickly add or remove agents and functionalities from a workflow perspective. We're using an orchestrating agent at the front that can pick and choose the right tools or agents to use to fulfill the user's request.

Sometimes we only need to call a single agent, maybe if we're looking at how a player has performed. However, there could be queries where we need to make requests against four or five of these domains to aggregate the results and come back with a comprehensive answer. This gives us the flexibility we need. Rather than having previously rigidly defined rules, we're now presenting a huge amount of data and a huge amount of tools, and the actions taken can be customized or planned automatically by the LLM either based on the user's request or as new information is discovered.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2250.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2250)

For example, if we get something back from injury history in the medical insights where Harry Kane's conversion rate went down because he injured his left foot eight weeks ago, that might change some of the further analysis being done and change the reports presented back to our end users. As we start deploying  multiple agents, it doesn't become much more complex. We're still using AgentCore Runtime to deploy those individual agents. The pattern we see is for common tooling and common functionality to expose that either as an MCP server via AgentCore Gateway.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2320.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2320)

The idea is that we can have our common functions and common reports hosted in AgentCore Gateway to be exposed to all of the agents running there. With this, we can take either our existing API endpoints or our existing Lambda functions, interface them with AgentCore Gateway, and then they become available to be used by any of our agents that are running. I've talked through some data,  some AI, some machine learning, and some generative AI as well. Now from this point with agentic AI, not everything needs to be agentic AI. We really need to think about what makes sense based on our use case.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2400.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2400)

We can end up using a combination where we can use generative AI for information extraction from documents or report summarization and reports correlation and summarization. We can combine that with maybe some predictive AI where we're looking at trends in our sports data. Then maybe we consider using agentic AI if we're looking to plan but most importantly to take actions depending on what we discover. There's always going to be some components of workloads that make sense just to be run as traditional code. There is no problem with that. 

### West Ham United's End-to-End GenAI Scouting Platform

From a use case perspective, there are really two sets of guidance and teams that I would like to bring out which can help you on your journey. The first is where you can build with the AWS GenAI Innovation Centre, or secondly, looking at building with an APN AI Competency Partner. Both groups have a huge amount of experience with all the technologies that we've discussed today to help you deliver those projects faster, and I'll call out a number of our partners as well because in the partner network there are some with real deeper industry specialization.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2450.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2450)



So let me walk you through the final use case for today. West Ham United in the Premier League has a long running tradition around talent development, known as the Academy of Football, and they have a strong background in identifying, nurturing and building talent. West Ham's technical recruitment team have been looking at building an end-to-end GenAI scouting platform. They are the first club in the Premier League looking at using GenAI end-to-end. West Ham is partnering with both AWS and with APN partner Crayon to make sure they're using the best technologies available for their specific use case.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2520.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2520)

When I say end-to-end platform using GenAI, let me walk you through how the scouting process can look for a Premier League club.  You're starting with a worldwide database of 100,000 players across 180 countries of available players who are professionally playing football or coming through youth teams. Using initial data analysis, they try to shortlist some of those candidates based on statistics, reports in newspapers, and other data sources that come through. This breaks it down to a first level watch list of players to investigate in more detail.

From that watch list, scouts are looking through watching videos and doing further analysis on players' performance, their trends, and other components. The scouts then build text and visual reports around that player. This can become quite time consuming. Based on that shortlist, reports are created and a further priority list is identified. However, scouts then go out to view these players in real life because statistics and video clips will not capture everything. Once a player has been scouted, a much longer report is put together, typically 20 to 30 pages, as clubs start thinking about whether they should proceed, spend more time, or potentially spend money on this player.

With all of that, there are statistics and data analysis going through, but there's a huge amount of human element in it with the report creation. This is where generative LLMs and AI become very powerful at both helping create those reports faster and also helping summarize reports. For example, if you have 30 players to prioritize or 100 players to prioritize on your desk, you can use the tools to help bring out the most relevant information. The time spent writing reports, analyzing reports, and gathering that data can be significantly reduced.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2720.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2720)

This is really where  West Ham wants to get to with their use of generative AI: using the technology to help scout and recruit players. The goal is to complement their existing scouting methods, make this process faster, and get better visibility across the number of players they can evaluate. By using the right technologies, we can achieve that better scope of player coverage.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2770.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2780.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2780)

I think this is where we can use generative AI and where clubs are using it to give those productivity boosts, help with scale, and bring out further detail and insights.  Trying to summarize where West Ham is with their talent identification platform, we've covered where they want to get to: reduce the time and effort from the scouts and coaches they have available, increase the level of analysis they have available, and grow that player coverage. 

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2800.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2800)

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2840.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2840)

The considerations and data inputs that go into our environments include human-crafted scout reports, match reports, and ratings. We've got a combination of other modes of data: event-level data, tracking data, and physical data. There's a large amount of information going in there that we want to consider to help advise further decisions.  The generative AI component comes in here to process our different modes and different types of data.  It provides reasoning capability to think through what is being asked of it as new data, new variables, and new insights are extracted, helping to increase the coverage of reports and the level of detail in the reports.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2880.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2880)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2930.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2930)

Coming to the final part of this, we have evidence-led decisions to augment existing practices.  The technology is there as a tool to provide either more detail, a higher level of analysis, or to save time that could be better spent in other places. We're getting to the point where we have automated creation of due diligence reports, which can then be used in a scouting or transfer context. 

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/2970.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=2970)

What I'd bring out about this is that there are many dimensions that go into identifying the right player for our club. Some can be technical, some can be financial, and some can be a human part of it. The idea is that where we can use data and generative AI services, we can increase the quality of the reports being created to allow us to make better data-driven decisions.  Much like a large language model, my recap might be slightly different each time, but there are four points I'd call out. What we've seen across many clubs is that a solid data foundation is really key, where we need to store the right types of data with the right technologies and then funnel it into a single interface where it can be queried depending on what our needs are.

### Key Takeaways and Q&A: The Future of AI in Football

The use of GenAI by the clubs is there to enhance users' experience rather than replace them. We have a game in football, and there is still such a human element in it. Those decades of experience serve as a complementary set of knowledge.

By collecting more diverse types of data available, we can make better targeted decisions when we're looking at our analysis and where we want to get to. One of the key things is that as our number of data fields start expanding and getting bigger, the use of LLMs will help us identify what is the right data to look at or the right data to consider. As we're looking at this with the use of agents as well, as they're working through processing the data and narrowing down the right information to use, they can take corrective actions if they discover new data or if they get an answer back that doesn't look right or requires further investigation.

So that's all I had for today. If anyone has any questions or any other topics they'd like to talk about, feel free to come and see us at the side or ask questions now, however you would like to do it. Thanks, that is a really interesting story. It's really weird to hear my voice come back at me. Is there anything going on in refereeing that you've seen as well?

Not that I've seen. It's been more of a relationship with the clubs rather than the FA sort of level. I would suppose where your area of interest might be is maybe analyzing past performance thresholds around where yellow cards go. Yeah, I think as play breaks forward, they've all got a monitor in their ear with real-time data going into their ear. I don't know if it's something like a player going into the box with a 90% chance of diving or that real-time data going through, but I don't know if that helps them or if there's anything.

There are real-time analytic solutions available, none that the public can talk about today. But from that idea of adjusting that real-time data, like when I was talking through the data sources, some of it's pull back looking and some of it is that real-time screening based data. You know, with the GPS data that's coming through, you could analyze the players. I'll pick on Harry Kane again for some reason. He has a 60% chance of going down when there wasn't proper contact, and then you feed it back. From a latency perspective, the services exist that could be built and put together because you'd be talking with not necessarily subsecond, but within several seconds processing, imagine from capturing an event to then having it available to then correlating it against the existing statistics.

That could make a very interesting value add for a sports broadcaster as an example. But yes, interesting question. I'll have a think on that one. Where do you see wearables? Many teams are using Catapult and Whoop and some other things. Are you starting to see clubs requiring or asking for data associated with wearables in today's market?

Yes, so from two of the clubs mentioned today, wearables data from a tracking location on a pitch perspective is being brought in and being ingested and used for analysis on things like performance stats on miles run and other metrics.

There was a use case where they looked at how a club's defense linked up during a game to see if players were getting out of position. That analysis comes more from GPS and player tracking data rather than individual joint movement data, but it falls into the data lakes that have been built. At this point, it comes up more from AI/ML type analysis rather than generative AI, but it is something being considered.

I wanted to go back to AI and football moving towards real time, so rather than having it used for decisions at event time, it becomes more forward-looking or for previous review. This involves direct communication with coaches and assistant coaches. On a football ground with video monitors available, instead of a scout putting out a handwritten note, you now have real-time analysis showing what you're seeing that might change the tactics you're employing.

I think there would be uptake for this, though there's still a lot of focus at the moment around the preparation elements. Clubs are using past data during film sessions to set up differently for the next game or change a player's schedule based on historical data and scouting. As we've seen from other industries, it will move towards more real-time information. Some clubs are doing this in training, and there may be clubs doing it live that I haven't spoken to yet.

Do coaches open to that? Most coaches are from what you might call a legacy generation. There's still a huge amount of human element involved. Some managers who've been in the game for fifty-plus years may be less forthcoming to it. However, if you look at the data flywheel concept, the idea is that if you can show and prove with a return on investment that this process is giving you an advantage, it gains more acceptance. The manager is still likely to hold very strong opinions, but he may be more open to it.

[![Thumbnail 3560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9118c2541c4bfffe/3560.jpg)](https://www.youtube.com/watch?v=iOxZzRBhrM0&t=3560)

Thank you. Well, thank you all. If anyone has any further questions, feel free to come see us afterwards. There are a couple of the team from Crayon here who have been working with West Ham  and some of the Bundesliga clubs. Enjoy the rest of Reinvent.


----

; This article is entirely auto-generated using Amazon Bedrock.
