---
title: 'AWS re:Invent 2025 - Adopting AI within Streaming Architectures (AIM265)'
published: true
description: 'In this video, Redpanda introduces their Agentic Data Plane, built on their Kafka-compatible streaming platform. The presentation explains how streaming infrastructure intersects with AI agents, emphasizing trust, governance, and observability through immutable logging. Key challenges addressed include agent authorization, context building, multi-agent orchestration, and replay capabilities. The speaker advocates starting with secure VPCs, documented processes, and low-latency use cases like manufacturing IoT, where agents can provide real-time context for predictive maintenance decisions by monitoring equipment telemetry and reasoning across multiple data sources.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/0.jpg'
series: ''
canonical_url: null
id: 3093336
date: '2025-12-08T22:56:36Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Adopting AI within Streaming Architectures (AIM265)**

> In this video, Redpanda introduces their Agentic Data Plane, built on their Kafka-compatible streaming platform. The presentation explains how streaming infrastructure intersects with AI agents, emphasizing trust, governance, and observability through immutable logging. Key challenges addressed include agent authorization, context building, multi-agent orchestration, and replay capabilities. The speaker advocates starting with secure VPCs, documented processes, and low-latency use cases like manufacturing IoT, where agents can provide real-time context for predictive maintenance decisions by monitoring equipment telemetry and reasoning across multiple data sources.

{% youtube https://www.youtube.com/watch?v=iieR8_xsEXw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/0.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=0)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/20.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=20)

### Redpanda: A Streaming Data Platform for Enterprise Data Movement

 Hello everybody, thanks for taking some time and peeling off of your expo expeditions to sit down. If you're anything like me, you've walked about 20,000 steps in the last few days, so sitting is probably a welcome respite. So let's jump in. Today I'm going to talk to you a little bit about Redpanda  for those of you who are not familiar. For those of you that have experience in the streaming world, we're not talking about Netflix streaming and videos. This is streaming data and data movement throughout organizations, so we'll educate you there. We'll get into the intersections of AI and streaming and where we've seen our early research find intersections that we think can be very useful when you're adopting AI into organizations.

We'll actually dig into some architecture, so we'll try to keep it high level, but for the people that have experience at the infrastructure level, we'll actually talk a little bit about how we deploy these within ecosystems, within data centers, things like that. Lastly, we'll try to provide you some actual practical guidance. I see a lot of use cases thrown around, but we'll try to give you some actual actionable intelligence into how we're integrating these, where to start, and where to go.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/70.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=70)

So,  Redpanda as an organization is traditionally in the streaming space. For those of you who are familiar with Kafka, we operate on top of the Kafka protocol, and we help organizations like these move data throughout their organizations. This could be large financial services like the New York Stock Exchange where we do very highly transactional workloads, moving millions and millions, as in tens of gigabytes per second of data throughout their trading platforms. This could also mean retailers where we are building systems that help them federate their data and spread out session captures throughout and essentially keep all this data moving through durably, reliably, and deployed where their customers typically deploy their workloads. This could be self-hosted, this could be in any cloud or hybrid nature.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/120.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=120)

Now  this is just a quick look. This is our typical form factor, so we have a single binary, but this can be deployed anywhere. We're cloud agnostic and this includes self-hosted. So this allows us to move throughout many organizations and deploy across a wide spectrum of use cases and verticals, all the way from very security conscious organizations that need very high restrictive class zero data, things like security, PII, PHI for healthcare, all the way to just typical retail data.

Now, over the last few years, you can see on the left and right, we've taken great effort to expand our ability to extend into ecosystems, pull data from source systems like typical databases, MySQL, Postgres, takeover use cases like CDC. In the middle there, we are providing you a durable log, ways in which we are capturing everything that's happening through those in an immutable, in an append-only way, and then storing that for safe access. That'll come into play with where we're headed with our AI products as well.

Lastly, on the right-hand side, you'll see we can sync to most common solutions. A lot of our customers need to pull data from a very sparse set of databases and microservices and tools, federate them out to another set, maybe Snowflake for analytics, maybe Databricks to use Iceberg to do analytics, things of that nature. And this is really the foundation for which we're building our AI products, the interconnectivity, the ability to collect all these logs, and the ability to deploy these anywhere.

As far as form factors, again, all of this runs typically on, for self-hosted bare metal, but then in our cloud environments, this is all on top of Kubernetes. So we can scale this horizontally or vertically very easily. It makes it easy for us to handle DevOps and acclimate to whatever the workloads we're working with.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/230.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=230)

### Introducing the Agentic Data Plane: Infrastructure for AI Agent Deployment

Now, let's  get into the AI and Agentic. We've released our Agentic Data Plane. This is built on top of the form factor you just saw. To give you a little bit of a dive in, you're going to have agents, and the simplest definition for agents is you have a body that has intent in the form of a prompt, it needs access to a model, an LLM, and then it needs a set of tools that you can use to build complicated processes or functions to get outputs.

So in our world, what you see in this box here runs on AWS inside a private VPC or it could be running in your self-hosted data center. And we provide all of these services that run at runtime. So your agent is a separate service that runs, we have gateways that allow you access to LLMs, various types, and as you can see, we're not choosing a specific vendor. We believe we're not going to try to build your specialty agents. We think that that's going to be where you come in, but we're coming from the infrastructure up.

So if you want to change out models, if you want access to Bedrock, for example, if you want access to OpenAI, you want to use Anthropic or Claude, it doesn't matter to us because we think changing those out is actually a superpower to allow you to really dial in your agents. As all this data is moving through, you see all the lines.

There's a lot of interconnectivity in the streaming domain. We're used to handling all of this disaggregation. We're used to having to track down and get access to all of these source systems that could be either inside your organization or outside of it. As you can see, there's some public internet traffic here. We're used to egressing and ingressing very sensitive information. We know how to do VPC peering. We know how to do all this interconnectivity. So we're baking this into how we approach AI and how we integrate it.

Lastly, you'll see at the bottom there all of these services are dumping to a log or observability log, which would provide you the ability to do things like replay through as agents are being built, and we're already very good at this. This is our bread and butter. This is something we've been doing for the last seven years. We know how to protect these. We know how to distribute these logs, and we really know how to make the best use of them in a very cost-efficient manner. So if we apply that to agents and all these gateways, we learned that agents send out a ton of messages into its entry system between each other using agent-to-agent protocols, every call it does to an MCP, for example, and all of that can be written to these logs durably for safekeeping and for replayability.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/380.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=380)

 So I keep hearing a lot about what are the big challenges with this and where to start, and I hear governance, I hear security, I hear access. But really, once we start digging into this, it becomes about trust first. How do you trust this entity that you're going to give autonomy to, to make decisions, to potentially trigger events that may go off in your organization? And the best way, I'm no writer, Ernest Hemingway said it best, but the best way to find out is to extend some trust at first. So I'll talk to you a little bit about the practical approach of how we do that, how do we provide an environment where you can give it some trust, see how these agents behave, track all of this, and then start to figure out how do we implement this more widely and how do we take it to production.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/420.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=420)

 So first off, in our ADP you can see we have a layered approach. In our thesis for how our products are coming to fruition, we have an action layer, which is where the agents are actually taking action. These could be security agents that are handling very sensitive data. This could be coding agents, which are very popular right now and we see the heaviest use of. Of course, it's very approachable. But then when you get some more esoteric cases like manufacturing IoT where you have different types of agents that are taking action, they're all going to pass through this data plane. And if we can provide a way for you to track all of that, log all of that data responsibly, and then also collect all the reasoning layer access as well. So every time an agent's calling out to an LLM, we want it to be passing through this single plane, this data plane, and allowing you all the tools to track that.

It's meant to really start to target, I mean, common term we hear is shadow AI. If you've been around for a while, you've heard it as shadow IT back in the day. But we see a lot of this, people just going out, spinning things up and running it, and we believe that to trust your organization, we start with a single plane, allow people access, do proper authorization, authentication, do proper governance, and people will just use it through this plane. And that way we can actually effectively track, effectively make policies that make sense for your organizations.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/510.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=510)

### Applying Streaming Principles to Solve Agent Governance and Orchestration Challenges

So back to streaming and why we're layering this in, streaming is already everywhere. We stream for pretty much every single  vertical, and our use cases span cybersecurity, very sensitive data, all the way to non-PII data, including like hospitality data, loyalty programs, analytics, things of that nature. So where we're coming from is, look at how can we give you effective tools to build into this existing infrastructure. Why retool things when we already have data flowing through an organization that's already been authenticated with all of your end users, all of your database admins have already created service accounts for access to all of this? How can we use this connectivity to provide agents better context and better ways to operate?

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/550.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=550)

In the streaming world, we already  have, we understand that there's some primitives for agents that don't apply, right? There are primitives that we work with to move data very rapidly throughout organizations, but that doesn't encompass things like agent-to-agent or MCP protocols. So how do we layer those in? Well, we already have very effective methods to distribute and do asynchronous operations, right? We already have this publish-subscribe model. For those of you unfamiliar with the streaming domain, we're publishing messages, they're getting written to brokers, which is collecting this immutable log, and then we can have any number of subscribers subscribe to this information to pull it and go anywhere else.

Now, if you really think about it, the same archetype can be applied to agents. They're going to become distributed in nature. They're going to be sending messages. They're going to be subscribing to many different services, tools, et cetera. So we're applying the same principles that we've learned in streaming to the agent deployments.

Architecturally, we already know how to decouple services, and if you think about agents, we have tools that are decoupled, we have LLMs and reasoning that are decoupled, but you still have these intersections in which you want to collect all this information. You need to do tracing on all those little pieces, and you actually want the outputs from all of that stored durably and readily. The last thing you want is a very high critical agent to go down and not be able to replay data, understand what happened to it, and understand what happened to the data. Do you understand how to move it and do disaster recovery responsibly? These are the same concepts that we already do in streaming very well.

The last thing is we're already built for real-time. We help customers migrate from large batch jobs all the way to very fast real-time applications. There are cases in which agents are good and bad for either batch or streaming, but over time, we believe that more agents are going to be utilizing streaming information either for event-driven architectures to help trigger or kick off, or to help provide the most up-to-date context because that's one of the more powerful features that we've seen for agents is real-time context. Can we get it to them and can it influence their decision making right now?

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/670.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=670)

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/690.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=690)

 So in the problem space, we see context building and maintenance being an issue. Authorization, governance, we see auditing be an issue with agents. How do you audit these? Replaying and validation of agent actions, and then multi-agent orchestration. And again, these are all data streaming problems that we've essentially solved in one capacity  that we're now augmenting to solve for agentic issues that we come across.

Especially the governance and replay, we already are at all these intersections. As I said, we're collecting all these tracing and we're storing it durably and immutably. So you can't go in and alter these logs. That way you have an already ingrained auditing pathway that you can use. And the last is multi-agent orchestration. We're going to start to see these systems distribute very widely, and it's going to be hard to track which agent is doing what. Well, it turns out we already do that in a capacity. Most customers use us for logging activities where we're collecting logs across maybe 15 different services, passing them through, filtering them for them. So we already have this framework set up to be able to take this action from this DevOps infrastructure perspective.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/740.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=740)

 So just to hammer that home, in the context abroad, you can see here in this little diagram, I have agents deployed. In our system, agents are deployed on Kubernetes and scalable within your VPC, so we have them running on pods, but they still need to get access out to databases. They are egressing your VPC out, and we start to see a spider web create, right? All these points of interconnectivity. Our thesis is that enforcement on that single data plane is going to be easier to manage from a DevOps perspective and provides uniformity. So you can actually track, especially when you get to multi-agent orchestration, you have many agents accessing the same MCP tools potentially, or the same data sets. So that is where we're really coming from with a single plane, a single data layer.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/790.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=790)

Just to summarize that, we really think that the intersection of data, that means access, authentication,  authorization to those sources, and this includes MCP servers and just general connectors, because you'll have both at your availability. The AI side of it means access to reasoning models. This could be LLMs or this could be other classification models. And then governance, the ability to govern all of this in one space, and we want to be at the intersection of that.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/810.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=810)

### Practical Adoption: Manufacturing IoT as a Low-Latency Agent Use Case

 Now, let's talk about practical adoption. How do you actually get this stuff into production? What we've seen when we speak with customers, we get a lot of questions on how do I get data access. Even if you're the best at building, let's say, a Model Context Protocol and building MCP servers, you still have to go to your database admin to get access to this. They can't just spin it up and say, go grab data. Which data can it get access to? What are the user permission structures? So it really starts with organizational conversations. Talk to your infrastructure teams about what data should be available. Your security teams will know what are lower level or non-class zero, class one data that may be used to start testing these, but that is the slowest piece.

It's really not that hard to stand up Kubernetes, get your services running, and get connected. It's really hard to start working through how do I connect them? How do I get data to this agent? Is that a database query through an MCP protocol? Is that actually a connector doing a CDC workload? That piece is taking the longest, so I actually advise people to start there. Start asking, where can I get access sooner and what types of access can I give it.

On deployment, we're recommending starting with secure VPCs and private networking just as a safe rail and safeguard, right? This goes back to that trust component. You really don't want your agent egressing and making decisions out, especially on the open internet, when you're not sure of its non-deterministic behavior. So lock it down, log it within your local VPC, and then until you really get a good understanding of what the actions it's taking.

How does the particular LLM behave, especially when you're changing the prompt iteratively? Typically when we're developing agents, you're doing it very quickly. You're changing the prompt, seeing what the behavior is, and seeing if it matches your measurable criteria. Lastly, this is kind of antithetical to what I've seen, but I see people searching for new use cases and new things to solve with agents. I come from a processing background, and that's the use case we'll dive into: IoT manufacturing.

When you really want to automate something, you don't start with an amorphous thing that you don't understand. You should pick something that you know already, that you already have well-documented steps for. That way you can actually understand, okay, is the agent behaving in a manner that we already understand? Rather than take a really crazy problem and try to throw it at it, start with the actual document that has 30 steps in it and see if your agent can replicate it. That's much easier to track, much easier to implement, and really gives you a good starting point, especially because you understand how to measure it.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/980.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=980)

The number one question I get when we're speaking to customers is, what are the use cases? Well, let's start with something. Let's augment a process and help one person become three within your organization by figuring out what are the processes they understand very well.  So for us in the streaming domain, we're targeting low latency use cases because that's a strength of ours. This doesn't mean that agents can't be used for longer runtime or more heavy deep thinking, but if we're being realistic, we like to look at ad tech, manufacturing, and gaming, where seconds matter and you want inferences coming through agents and decisions being made quickly. That's going to help drive value sooner.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/1020.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=1020)

These are some of the common latency domains today. We're looking at manufacturing, which is where I have a background, and we'll look at a use case. In a manufacturing example, pardon my little arrows there, they're supposed to be going sideways.  You have a lot of equipment on a factory floor. It's a common use case where we're aggregating all this data through maybe a protocol like MQTT. We're pulling it in, and we're doing a factory management system, and then we're doing an operational database that we're coalescing this data to.

So within this use case, where could we deploy agents? Well, there's a couple of places. Since Redpanda can run at the edge, we can run them very closely to the machine equipment. A common use case would be they're augmenting a technician and helping guide them through a process. All right, something happened at the edge. An agent has been monitoring the stream and already has access through maybe a vector store or maybe even just raw text about a given manual. A technician will physically go stand there and be given information and say, hey, based on this information, here's the decision criteria or the RCA you should work through to repair this machine. So that's a physical use case.

Now, when we get more towards the operational database side where we can have more agents running, there's where we can start to provide context. Things like, let's say it's common to have multiple machines of the same type. Let's say we have 50 of them, and I install spare parts into one from a specific provider with a SKU. We already have systems in place that can give you indicators that a machine is failing, but what it can't do is reason and think and say, all right, we installed the same spare part in two different machines. I'm going to go ahead and recommend you stop the second machine that hasn't even displayed problems yet on any of the telemetry metrics and do remediation based on knowledge that I gained through context, right? And that could be just a PDF that was uploaded with their spare parts when they did change of management.

So that's one of the use cases that actually makes sense to have an agent that's running 24/7 that has human in the loop and is helping you uncover and stop something from happening that you wouldn't discover through just traditional preventative maintenance or traditional PID controllers. So that's an actual bare bones use case that gives you an idea of where you can deploy these that actually create value. So that's all I have for you. Thank you. I really appreciate you taking some time out.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/90153232ec12c9d8/1160.jpg)](https://www.youtube.com/watch?v=iieR8_xsEXw&t=1160)

If you need to talk to me, we don't have a Q&A here, but I'll be at our booth, which is  right over there by the Redpanda Data booth if you have questions about the session. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
