---
title: 'AWS re:Invent 2025 - What''s new in AWS cost Optimization (COP202)'
published: true
description: 'In this video, AWS optimization product leaders and Delta Airlines'' FinOps team present new cost optimization features. Rick Oaks introduces EBS optimization automation in Compute Optimizer, enabling automatic snapshot and deletion of unattached volumes and gp2 to gp3 upgrades with rollback capabilities. A new Cost Efficiency Metric in Cost Optimization Hub balances rightsizing and commitment optimization, providing 90 days of historical data. Bonnie Fernstall shares Delta''s journey achieving 27% monthly cost savings through automated EBS cleanup, volume conversions, and EC2 rightsizing, emphasizing communication and cost champion programs. Mary Naimov unveils Database Savings Plans offering up to 35% discount across nine database services including RDS, Aurora, and DynamoDB, demonstrating the Purchase Analyzer tool for optimizing commitment purchases based on usage patterns.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/0.jpg'
series: ''
canonical_url: null
id: 3093226
date: '2025-12-08T21:25:56Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - What's new in AWS cost Optimization (COP202)**

> In this video, AWS optimization product leaders and Delta Airlines' FinOps team present new cost optimization features. Rick Oaks introduces EBS optimization automation in Compute Optimizer, enabling automatic snapshot and deletion of unattached volumes and gp2 to gp3 upgrades with rollback capabilities. A new Cost Efficiency Metric in Cost Optimization Hub balances rightsizing and commitment optimization, providing 90 days of historical data. Bonnie Fernstall shares Delta's journey achieving 27% monthly cost savings through automated EBS cleanup, volume conversions, and EC2 rightsizing, emphasizing communication and cost champion programs. Mary Naimov unveils Database Savings Plans offering up to 35% discount across nine database services including RDS, Aurora, and DynamoDB, demonstrating the Purchase Analyzer tool for optimizing commitment purchases based on usage patterns.

{% youtube https://www.youtube.com/watch?v=R1yO7HwTB9I %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/0.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=0)

### Introduction: AWS Optimization Tools and Audience Engagement

 Hi everyone, thanks for coming. My name is Rick Oaks, and I have a few other people here with me to help present this session, COP 202, What's New in AWS Optimization. I lead the product management team for AWS optimization tools including Compute Optimizer, Cost Optimization Hub, Savings Plan Analyzer, Savings Plan Recommendations, and I know I'm probably missing some. I also sit on the FinOps Foundation Technical Advisory Council helping AWS represent how customers optimize and operate with financial management. Bonnie Fernstall, I lead Delta Airlines' central FinOps team. Mary Naimov, I lead the Savings Plan purchase experience. Thank you.

All right, so we have a lot of stuff to get through today and we're really excited to have you all here. Just before we dive into any sort of specific content, I'm curious how many of you folks are here from FinOps organizations? Show of hands. Awesome, that's great. How many are maybe more FinOps adjacent, maybe engineering leaders or engineering folks? Okay, so it's about actually fairly split. That's awesome.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/70.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=70)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/80.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=80)

 Okay, so how many of you have, before I dive into specific features and announcements, how many of you have used some of our AWS  optimization tooling such as Compute Optimizer? Awesome, that's great to see. Okay, we've got some new things in Compute Optimizer to show you. How many of you have made Savings Plan purchases with our recommendations or analyzer tooling? Wow, we've got a lot of optimization aficionados in the crowd. Cool.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/110.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=110)

### New EBS Optimization Features in Compute Optimizer

Okay, so first on the agenda we've got some new features that we'll talk about. We're going to talk first about EBS optimization. How  many of you heard of this new launch? We just launched it about a week ago, right before Thanksgiving. No, okay, you're going to love it. So in Compute Optimizer we offer recommendations for those of you that haven't used it yet, recommendations for rightsizing, modernization, idle detection, and cleanup for things like EC2, EBS, RDS, and quite a few other services.

Inside Compute Optimizer, we kept hearing from customers, I like these recommendations, but there's so many of them. You keep giving me more, it's really hard to get after the ones I already have, and you keep launching more, so help us take them. And it's great feedback. We love building and launching new capability here, so optimization has been pretty high on our priority list for a while now. We decided to focus on the low hanging fruit, easy to do optimizations to get started here.

In this case, EBS idle cleanup, which is EBS volumes that are unattached to any instance and are really not adding any value to your environment. So we have optimization that will automatically snapshot those unattached volumes and delete them for you and maybe even add tags if you wish as well. We also will automate the upgrade of volumes, and that means moving from gp2 to gp3 for a 20% cost improvement or moving from io1 to io2 for resiliency improvements. And so this optimization capability allows you to apply these recommendations in your environment automatically.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/200.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=200)

 You can apply them immediately by selecting them and hitting execute or implement, or you can create a policy which will allow you to schedule the cleanups and do it repeatedly over time to keep your environment clean. We're really excited about this because as we were talking to customers there's a lot of these volumes to go clean up and we understand that the scale problem took a lot of time from FinOps teams and engineering teams and so this will allow you to set up these automation rules and move on to other more interesting or impactful things where cleaning up storage volumes isn't always the most fun and engaging.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/240.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=240)

### Automation Rules, Safety Nets, and Implementation Tracking

So a little bit more details.  Here's a screenshot of our console. I'll also go through a demo here in a minute. You'll be able to see the recommendations that are applicable for automation. You can multi-select and click the little review and apply button right there in the top and save money immediately. And I hope before the end of the week maybe open up your laptop, take a look at this, maybe try a few and start saving money right away. My hope is that at the end of re:Invent you've all saved a little bit of money with these optimization tools.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/270.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=270)

You can also create  these automation rules. Now automation rules are very flexible. You can create an automation rule for your entire payer account and organization. You can create an automation rule for a subset of accounts, say non-production with different rules or configurations or maybe an individual linked account as well, so you can kind of scope out and figure out what specific scope or organization or team is okay with execution of automation of recommendations and create rules just for them.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/310.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=310)

You can create rules for specific sets of recommendations and customize them just for those resources. You can also add criteria to the automation.  How many of you have tags for environment like production and non-production? These environment variables are awesome. So if you want, you can create an automation rule that applies only to certain tag combinations, and you can even select multiple different tag combinations or set rule criteria based on region, capacity, or quite a few other options as well.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/340.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=340)

 Lastly, you can set a schedule. If your organization likes to make changes in your environment during downtime windows or maintenance windows, you can select a window for these recommendations to be implemented, which allows you to target those maintenance windows. Even though the recommendations that we have for you with automation have no downtime or no necessary impact, we understand a lot of organizations still do have change control windows that they would like to respect.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/370.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=370)

 Once you take the recommendations, do you have a question? Sure, I'd take a question. Can you do that? You can do it either across accounts or in an individual account, and I'll show that in the demo as well. Tracking results: once you have implemented recommendations, you can look at how many were implemented and how much savings you've accomplished or achieved from taking these recommendations. Now this graph is basically a sum of the monthly potential savings. We're not tracking it month over month and accruing it over time. This is just a very simple sum of that potential savings metric.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/420.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=420)

One of the most important parts of how we automate optimization is safety nets.  Engineering teams do not want to get called at 2 in the morning because of incidents or outages or issues, so we've built a lot of these safety nets into the automation pipeline to help protect from any sort of impact. You can roll back any recommendation that you've implemented for up to one year after the automation has been implemented. Because we automatically take a snapshot of a volume before we make a change on it, we have that snapshot available to use in a rollback event for one year.

We also have other safety nets. We automatically check if the reason why the volume was marked as unattached is still true before we implement the recommendation. So there's another safety net, and if it fails that check of still being unattached, then the automation will be marked as failed. Anytime you have a failed automation, it will make sure to not implement the recommendation, thereby leaving you in a healthy state.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/500.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=500)

### Live Demo: Configuring and Executing EBS Automation

So before we get to the next one, I'm going to switch over here to my laptop and we're going to demo this automation feature.  For those of you familiar with AWS Compute Optimizer, this should be a screen that you're used to. The new thing here is on the left side of the screen you'll see an automation section. I'm going to go ahead and click on recommended actions. These are the recommendations that are eligible for automation.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/520.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=520)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/540.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=540)

In this case, we have two recommendation types:  snapshot and delete unattached EBS volumes, and upgrade EBS volumes. In this case, I have some gp2 to gp3 volumes that need to be upgraded, and I can see the estimated savings for each one of these, and I can see exactly what recommendation type I have. I can multi-select some of these recommendations, and at the top of the screen  it'll tell me how much money I'll save per month by taking these recommendations, and I can click review and apply. So you can save money that quickly. How many clicks was that? Like four or five?

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/550.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=550)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/580.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=580)

 So the idea here is to make optimization very easy for everybody to implement. If you have some environments that just have a lot of aged EBS volumes and you want to just clean them up right now, this is a great option. If you have a lot of accounts where you want automated or recurring implementation of automation, you can create an automation rule. Now you can create an automation rule at the organization level or at a linked account level,  and you get to choose which ones get implemented first, so you can kind of set priorities here on if you want an individual team to run their own automation or if maybe you have a central FinOps team that manages optimization for everybody. You'll have the ability to have organizational level automation rules. Let's save questions to the end. Thank you.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/610.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/620.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/630.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=630)

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/660.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=660)

So once I look at these automation rule options,  I can create this automation rule, and I'm just going to create an organizational rule and select my linked account in my organization. And I can choose which recommendation  types I would like to apply in this policy. Then I'm going to add some rule criteria. In this case, we talked about tags.  Let's say I have an environment tag, and say I want the value to be development. How many of you have had tags where some of them are capitalized and some of them are lowercase? Okay, a lot of nods. Okay, so I'm going to type in capital D development, and so you can have multiple tag values here in your criteria to catch it all. And then you can also add additional criteria. So say if I have maybe  estimated savings or volume type criteria, you can add that here as well and really customize your automation rules.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/670.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/680.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=680)

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/700.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=700)

 There you can do ORs by creating additional criteria and picking tag again, so you can do ANDs or ORs. Now once you've selected your criteria,  you'll get a refreshable list of the volumes that match your criteria. In this case, I actually don't have environment tags, but you'll see the list of all of the volumes that apply to the criteria that you've selected and the estimated savings for those recommendations. And then you can set your schedule.  In this case, you can set midnight on Saturday or something like that, and you can set it to run for however many hours you have your window. Note 100 recommendations will be implemented at a time, and they can take a variable amount of time depending on the size of the volume. So just note if you have a lot of very large volumes, they'll be a little bit slower to implement.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/720.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=720)

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/730.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=730)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/750.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=750)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/760.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=760)

 And implementation will stop beginning as soon as the window completes. You can also add new tags  to the newly changed resources, so this is great because you can tag something as optimized. I've already optimized it, and this allows you to understand how many resources you've optimized later on or use those tags for other FinOps purposes. You name your rule, and then you can set the rule as active or inactive if you'd like to save it for later. And once you  implement this rule, you'll be able to go into Compute Optimizer automation events and look at the historical number  of recommendations implemented and the amount of savings. You'll see any recommendations that completed or if they failed and then rolled back, and you can see those by month. And then you can see the savings by month as well.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/770.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=770)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/790.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=790)

### Idle NAT Gateway Detection and Recommendations

 So that's automation. If you have any questions on this feature, we will definitely be available after the session to answer questions. We also have a blog post and a lot of great content online to help. Next up, we have a new recommendation for you in Compute Optimizer.  How many of you have NAT gateway usage in your environment? Okay. Understanding if a NAT gateway is being well utilized or not is a complex task. One of the things that we understand is a lot of customers run NAT gateways, but they're designed to be failover or backup NAT gateways in case something happens. So we designed a new recommendation to detect idle NAT gateways, and we look at 30 days of historical utilization.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/820.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=820)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/850.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/860.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=860)

 And we also look at if the NAT gateway is referenced in a route table or associated with the route table. What that means is we check to see if it's a failover or backup, not just if it has zero utilization. This means we know that there's no route table association and there's no usage, so this is a great opportunity for you to talk to the network team and go figure out if this NAT gateway is left behind and really needed or not. Once you view  NAT gateway recommendation in Compute Optimizer, you'll see them populate in the idle recommendation list, and then you will be able to click into it and see the utilization  details that validates that this NAT gateway is not being consumed and may be an option for you to clean up.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/870.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/880.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=880)

### Cost Optimization Efficiency Metric in Cost Optimization Hub

Next up, cost optimization efficiency.  We in the Cost Optimization Hub launched a brand new metric that we're really excited about. We heard from a lot of customers that there's  not very many metrics or KPIs for optimization success in the FinOps space. We have a lot of cost visibility KPIs, but the optimization space has kind of lagged behind on a maturity curve a little bit. There are some metrics that we've heard from customers like effective savings rate, average CPU utilization, but they only show part of the puzzle.

Focusing solely on one type of optimization will push commitments really high, but then may block you from right-sizing or cleaning up idle resources. The same applies to resource optimization. If you only optimize resources, you may be leaving commitments on the table. So we wanted to come up with a new metric that equally measures and balances the value of right-sizing and resource optimization with commitment optimization.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/940.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=940)

We heard from a lot of customers that they've tried to make metrics on their own, but convincing leadership of the value of the metrics and coming to a common agreement on the definition is very challenging. So we wanted to make an attempt at this, and we created the Cost Efficiency Metric in  the Cost Optimization Hub product. This is available today for you. We launched it about a week and a half ago. This metric is automatically generated and populated with 90 days of daily historical granularity in Cost Optimization Hub. This will allow you to set targets and benchmark your cost efficiency over time and ideally save more money by helping convince leadership that efficiency is a very important investment for your organization.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/970.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=970)

Here's what it looks like.  In the Cost Optimization Hub, you'll see a brand new graph on the right side. It is your cost efficiency. In this case, my environment is very inefficient because it's a demo account. You'll see it trended over 90 days, so you can see as you're building new resources whether they are efficient and effective, or maybe new resources are not so efficient and you're seeing the number drop. This is a great way to talk to individual resource owners or account owners. Maybe some resource and account owners have really great efficiency and have some stories to tell about how they were able to do that, and you can do a little bit of culture work and knowledge sharing with teams that are really good at optimization with teams that maybe need more help.

So let's talk a little bit about how the efficiency metric works. We take the total potential savings of all of our recommendations in Cost Optimization Hub, Compute Optimizer recommendations, right-sizing, idle resources, Reserved Instance and Savings Plans recommendations. We combine them all together. We deduplicate the savings so we don't overstate savings, meaning if there's a bunch of idle resources, we won't tell you to buy a Savings Plan for them, for example. We take that number and we divide it by the total potential optimizable spend. What that means is we take some things out of this number like credits, fees, support costs, things that we can't optimize, so we have a more stable baseline amount of spend that is optimizable that we provide recommendations for.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1090.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1090)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1100.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1100)

Then we divide one by two and we have a percentage. In this case, if you have maybe $10,000 of potential savings on an account with $100,000 of optimizable spend, your efficiency is 90%. The idea is to make this very easy to explain to leadership, and every single individual percentage point of efficiency relates to a real dollar value that is on your invoice. So driving this metric up will be an actual real business impact to your cost. You will be able to look at the metric by account,  by region, and you can look at the top five highest efficiency, lowest five efficiency. And again, you could look at specific date  ranges as well. If you had very specific moments in time where a Savings Plan expired or you bought a new Savings Plan and your efficiency took a drastic change, you'll be able to zoom in on that and understand how the activities you're doing today in your FinOps practice will influence your overall efficiency.

Thank you all so much. I'm going to invite Bonnie up from Delta to tell her story about how they use Compute Optimizer to optimize their estate, and I really appreciate all of you attending. If you have questions on these new features, we'll be available afterwards. Thanks, Bonnie.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1150.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1150)

### Delta Airlines' Journey: Building the Foundation for Optimization

Thanks so much. You know, personally, I'm most excited about the efficiency metric. It had been part of my 2026 roadmap, so I've got one done now, right?  So when I stepped into my role, Delta was about three years into a massive cloud migration program. An early FinOps practice had been established, mostly focused on showback with a little bit of purchase commitment as well. FinOps became a hot topic for us for the same reason I imagine it has for most of you. Our cloud bill is growing rapidly, and we need levers to pull to rein it in.

Now at this point, all of my FinOps knowledge came from a web search I did prior to accepting my position, so I had no idea what those levers would be. Luckily for me, Compute Optimizer had a set ready to go. Getting people to adopt those optimizations in an organization our size has definitely been a journey. We learned a lot about the gap between intent and reality, turned to automation to help along the way, and ultimately ended up achieving measurable change. So today I'll share a little bit of that journey with you.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1210.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1210)

I'm a product person by trade,  so when I first saw the recommendations, to me it looked like a large number of stories that needed to be written, refined, assigned, and worked by a large number of teams. Those stories and the desire to reduce our cloud costs predated me, so it was pretty obvious that my job was to figure out how to turn those opportunities into reality.

I started, as I think a lot of people with my background would, with the five W's. Where were people supposed to be picking up the work from? Developers log into their accounts daily, but the people managing the backlogs don't, and they need a central place to go to see all the recommendations across the 1,200 account ecosystem we have. Who is supposed to be performing the work? If we're going to ask these busy teams to put even more work into their backlogs, we couldn't be asking them to also figure out which work was theirs, nor could we establish any sort of accountability mechanisms without that information.

What was the work? I went in with the assumption that even though I may not personally understand the optimization opportunity, the teams performing the work would. But as I mentioned, we're in the middle of a migration journey, and for many of our developers, the cloud was just as new to them as it was to me. When was the work going to get done? As an organization, we had quite a few priorities, and we were going to have to figure out how to get the visibility to move up the ladder if we wanted the work to get done. And then finally, why? The work was there, the desire was there. So why wasn't it getting done?

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1330.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1330)

This probably sounds pretty familiar, as I found it echoes some of the key findings in the 2025 State of FinOps report by the FinOps Foundation. Competing priorities, lack of accountability, and lack of understanding are challenges that a lot of us are facing.  We decided to start by tackling where and who through the creation of the opportunity dashboard. The recommendations are centralized in a single database and then enriched with organizational context like leader or application name to provide clear lines of ownership.

Then the report's broken down into two levels. The summary report lets you view across both teams and opportunities where the greatest savings lie. So whether you own a single application or an entire development organization, you can quickly filter and see where your team's efforts can best be spent. Then each opportunity has a detailed report, and that's really more relevant to the people performing the work. You can see at a resource level things like what account the resource is in, what the actual savings estimate for that particular resource is, and usage metrics that can be helpful in understanding why the recommendation was made.

Then to help with the understanding of what, each opportunity has its own opportunity runbook. This runbook takes in a five minute or less read all of the essential points that you need to understand, validate, and implement an opportunity. This is to help with capacity constraints. The FinOps team has the time and frankly the mandate to go deep dive into these opportunities and understand what they are and their implications. So by consolidating that information into a Cliff Notes version, you can enable your developers to spend less time reading and more time optimizing.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1430.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1430)

 I'm going to pause here and touch on our tagging strategy. While I can't take any credit for it, I think it's really lightweight from a maintenance perspective and results in a high quality of data, so it's worth a mention. Instead of requiring teams to maintain a bunch of tags on each resource, we use a ten character code as a primary key that represents the business capability the technology supports. The reason this works so well is that while the exact functionality or the ownership is likely to change over time, that core business capability generally does not. Each code gets registered in a central architecture repository, so teams have a single place to go to keep the contextual information that we're so reliant on up to date.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1490.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1490)

 I don't know if any of you have experienced what I did. We have foundational reporting now where you can go into a room of leaders and talk about the savings opportunities you've found and watch the dollars roll off the cloud bill in their eyes.

### Incorporating Team Decisions and Addressing Implementation Delays

It's fantastic until you realize that not all opportunities can be turned into savings. Early on, I had to roll back some pretty big numbers I'd reported on after the teams told me that the opportunities wouldn't stay valid for long due to upcoming changes in usage patterns. Now there was nothing wrong with the recommendation, it was accurate. So the key learning here is that teams have critical information about planned changes, about their roadmaps, and if you want to present an accurate picture of savings, you have to incorporate that.

So with that, our dashboard changed from purely a reporting mechanism to also incorporate a decision from the teams. Were they going to implement or reject an opportunity? Or did they need to test in a lower environment prior to deciding? If they chose to reject it, they also had to provide a justification, and that was to allow us a starting point for conversations later if teams were rejecting opportunities that stayed valid, maybe just based on a lack of understanding.

So with this information you can now go in and talk about the savings that your teams have said they will implement. I did that. And then I went in the next month, and the next month, and talked about many of the same opportunities and many of the same numbers. And our leaders started asking one of the same questions I started with, when is this going to happen?

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1620.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1620)

That led us to add our final two decision points: a timeline for implementation, and then a reason for delay if the recommendation was expected to stay idle for more than 30 days. Now you have a system that provides clear line of sight into those five W's.  Each week the recommendations get refreshed, and depending on their change in state, if they still exist, they roll up into one of three reports.

Unactioned opportunities are rolled up by justification and timeline. This is to provide insight into the cost opportunity of prioritizing other work. Rejected opportunities go into a validation report. If teams are rejecting opportunities that continue to show up week after week, there should be a sign off that those are costs that our organization is willing to carry.

The fun one is implemented opportunities going to the tracked savings report. For this one, we baseline the cost of the resource the month before the optimization is performed, and we compare that to subsequent month costs, which allows us to track savings even if a team's account or environment or system is still growing.

So what this transparency and the conversations that we had after it showed to us was that we had a fundamental misalignment between team and leader timelines for implementing these changes. Leaders want the savings now. Teams agreed to implement the savings. But we still had teams migrating workloads. We still had traffic coming in. The business certainly hadn't stopped asking for new capabilities, and teams knew that this was important, but it just wasn't translating to those stories getting committed.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1740.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1740)

### Automation Strategy and Cultural Transformation at Delta

So my team turned to automation. If we knew they're okay with it and they just couldn't get to it, we could automate some of that implementation work and let them focus on those other efforts. We started at one of the same places the AWS Compute Optimizer team did, cleaning up unattached EBS volumes. 

We chose this for a number of reasons. First, there were zero autonomy concerns. We weren't making changes to systems and there was a broad agreement that no one was abandoning these volumes on purpose. Second was the number of volumes. We knew this was a place that we could demonstrate measurable savings, both in terms of dollar and time. And then third, we had an on-premise policy to replicate, so the behavior wasn't new.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1800.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1800)

Our flow is pretty much the same as what you saw and honestly if the automation had existed back then, we would have used it rather than build it. But if you're looking for a place to start automating cost optimization, I highly recommend you check this one out. Especially with the ability to create snapshots, though I've yet to hear of a single one being restored, it's a good way to build trust in automation and save some money along the way. 

We must have been cosmically aligned or something, because we also looked to volume conversions next.

For us, it was mainly io1 to gp3. There was definitely a little bit more concern here because we were going to make changes to the system, so this required us to do some more education. We used metrics to show teams that gp3 would more than meet their system requirements in most cases. We also had to let people know that not using gp3 hadn't been a design decision. It was new technology that simply hadn't existed when they were deploying their systems.

The automation approach is more aligned with that rules-based approach in that this automation only runs in accounts where teams have agreed to opt in, and it only runs against resources that teams have agreed to optimize. This lets teams that are doing all of their updates through pipeline keep their environments clean and ensures that we're aligning with testing timelines and things like that. Now, the immediate concern here was that we were going to limit our savings by allowing teams this latitude, but that's mostly mitigated by the fact that the reporting is agnostic of how you choose to implement. Whether you solely use automation or perform every single update yourself, what's important is that you align with your leader's timeline for getting the work done.

With this automation, for the resources that it does run against, after updating the volume directly, it also loops back and updates drifted stacks. This further helps build buy-in because you're not creating tech debt for teams to clean up later, and you're also ensuring that it doesn't just pop up on the report again the next time the system is redeployed. Finally, we follow everything up with an email letting teams know what work was completed and what work, if any, remains.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/1920.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=1920)

 All right, so this set us up to go after the big one: EC2 rightsizing. Now, we knew we had to tread carefully and we had to get this right, because another team had made an attempt at rightssizing with a different tool's recommendations, and it was pretty disruptive. We were confident that we could do this with AWS Compute Optimizer for a couple of reasons. One was simply the trust that we had built in both the automation and the recommendations in our previous efforts. And two is the ability to customize those recommendations.

For our first pass, we set the highest threshold for both CPU and memory utilization headroom, letting teams feel comfortable that they had room to grow. We also excluded some instance types that weren't compatible with agents on our base build. The first run at this was pretty simple. We looped through the recommendations, further refined them to low-risk options with no architectural changes, generated an email with an approval link for the managers, and then executed approved changes during our standard change window. And I think I had about three approvals through the system, so once again we had to take to the teams and try to figure out what we were missing.

Getting buy-in for this required us to adapt to design and resiliency standards in our organization. This was things like identifying boxes that needed to be sized identically for load balancing purposes, creating different patterns for different environments or different resiliency tiers, and allowing an opt-out option for teams that maybe have vendor software that has specific requirements behind it. This was definitely a heavier lift, but what it allowed us to do ultimately was shift from an opt-in to an opt-out approach, providing greater savings.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2050.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2050)

Now, what I haven't stated explicitly, but I hope you've heard through all this,  is how key communication is. And I want to stress how critical it is in this space to establish and to adhere to a communication pattern. For us, it looks something like this: inform, often more than once, then open the door for feedback. Don't go in assuming that you know why an unoptimized decision was made. When you do take action, make sure you're transparent about what's being done with that feedback. Teams need to have confidence that you've heard them and that you understand the potential implications of whatever decision is finally made.

For our organization, this has also been aided by the implementation of a cost champion program. I have a really small team. There's four of us, and so getting the scale of work done, we had each cost center owner appoint a champion or two, and we meet with these people on a biweekly basis.

We educate them about new opportunities, tell them about new processes, changes to reporting, and more recently, and my favorite, they've started coming to us and educating us and the community about optimizations that they've actually found. They trust us, their teams trust their knowledge of their domains, and those two things combined let us get far greater reach than we could have on our own.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2140.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2140)

And along that journey, we've managed to see a 27% monthly cost savings.  Just as important to me, if maybe not to my boss, is the cultural transformation that we've achieved along the way. Teams, as they become more educated about these optimizations, are starting to shift left and incorporate them in their design phases, which is allowing FinOps to become a part of how we work rather than the extra work it seemed to be initially. That partnership is so key in building trust to make that happen, and it ultimately leads to the teams being more likely to accept new optimizations and new automations. All of this combined provides the foundation for our leaders to make data-driven decisions that thread the line between cost optimization and the 100 other priorities that cross their desks every day.

### Introducing Database Savings Plans: Coverage and Purchase Strategy

Now, I mentioned at the beginning that we have some purchase commitments, but we've kept a modest baseline to this point, and that's been an intentional strategy. We didn't want to commit ourselves to resources that were maybe oversized or entirely unnecessary. But now that we're reaching a much better state, we intend to look to this in 2026 as a big lever to control our costs. So I want to invite Mary up onto the stage to tell us more about some developments in that space.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2260.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2260)

Perfect. Thank you, Bonnie. Thank you so much for the insights and the stories. Hi everybody. I have a key question for you. What do you do when you have different types of databases running across your environment, you want to optimize your cost, and you want to do it fast? From this week, you can purchase a Database Savings Plan.  Thank you. This is one of the most exciting launches we have at this re:Invent. We saw a lot of excitement already in the news.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2290.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2290)

My name is Mary Naimov. I lead the Savings Plan purchase experience at AWS, and overall, I'm just obsessed about creating simple experiences for technically complex products. Today, I'm going to show you, basically we'll go through an introduction of what are Database Savings Plans.  Then we'll see how the Savings Plans will apply to your database services usage. And lastly, I'll show you how to purchase a Database Savings Plan with our own native AWS tooling like Savings Plan Purchase Analyzer and Savings Plan recommendations. Can you raise your hand again who used these tools before? Okay, perfect, great. So you can utilize your knowledge to purchase the new Database Savings Plan. And if you never purchased a Database Savings Plan, don't worry. After this talk, you're going to be an expert and you will be able to teach other people how to purchase a Database Savings Plan.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2330.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2330)

Okay, so let's start with the basics.  A Savings Plan is basically a dollar per hour commitment for a long-term period. You can purchase, for example, one dollar for one year, and in exchange for this long-term commitment, you will get discount pricing. It's the easiest way to start saving in your AWS environment because it doesn't require any architectural changes, and the best part is after you purchase it, it automatically applies to all the eligible usage you have in your environment. So you don't really have to do anything after you purchase. You just monitor the success of that, then you can repurchase more and more as you have consistent usage in your environment.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2370.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2370)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2400.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2400)

A little small history lesson about Savings Plans.  Today we have all these types in our environment. We launched Compute and EC2 Savings Plans in 2019. SageMaker came out in 2021, and this week, as I said, we are really proud to introduce Database Savings Plans in 2025. The Database Savings Plan gives you up to 35% discount.  It's a one-year commitment, and it's no upfront cost.

There's no upfront payment, which means that you're going to pay for this on a monthly basis. It's very easy. All this information is also in the UI, so you really don't have to memorize it. The UI will guide you through the purchase experience.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2430.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2430)

This is one of the key slides that a lot of our customers are curious about. What will the Database Savings Plans actually cover? We have nine types of database  services that it will cover, like RDS, Aurora, DynamoDB, and ElastiCache. You're welcome to take a picture. As you can see, the serverless has a higher type of discount, so this Savings Plan is actually encouraging you to modernize and move from instances to serverless. Once you do that, the savings that you will get from the same purchase will be actually higher because it will cover the serverless types.

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2460.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2460)

After  you purchase the Savings Plan, it will start to apply to your usage. In this example, we show that there is Aurora Serverless with 35% discount, ElastiCache, and DynamoDB. Once you purchase the Database Savings Plan, it will start to apply to the service with the highest discount. So in this case, 35% on serverless Aurora. Once it applies to that, it will trickle to ElastiCache and finally to DynamoDB. In this way, it maximizes the savings you get from the purchase because it applies to the highest discount first.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2510.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2510)

You might say, I have all these different services running, how much should I buy? How should I decide what to buy? You can start with a conservative purchase of 50% coverage.  It's a great start. If you have no Savings Plan currently in your environment, you just start with something and you start covering 50%. In this case, we're covering most of the Aurora and most of the ElastiCache.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2530.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2540.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2540)

You can then move on and maximize your savings. This is a watermark line of the maximum  savings you can get from Savings Plan purchases, because if you go above this line, you actually don't have enough on-demand  to justify a Savings Plan purchase. That maximum savings, we actually in our tooling will help you to find that watermark line, and you can build up to it with small purchases over time. Or if you have an environment that is always growing, you can start from maximum savings and just keep purchasing more and more as your usage becomes consistent.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2570.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2570)

Lastly, before we jump to the demo, I want to show you the workflow that our customers do for all their Savings Plans.  First, you analyze your environment and you use your recommendation or Purchase Analyzer to decide on which one to purchase. You add it to the cart with a few clicks, you purchase, and then you start monitoring it with coverage and utilization reports. Coverage and utilization reports will show you how much of your on-demand is covered and how much your Savings Plan is being utilized in the environment. Lastly, after you review your purchase in the reports, you can again do the same thing. Use the tools to analyze a new purchase, repurchase, and just see your coverage grow over time.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2610.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2610)

### Database Savings Plans Demo and Closing Remarks

With that, I'm really excited to show you a demo on this topic, so let's  do that. Once I'm in the Billing and Cost Management, I'll scroll down over here and there's a Savings Plans area, and we have all these pages: overview, inventory, recommendations. They supported all the Savings Plans we had until now, and now they support also Database Savings Plans. So if you used anything like this before, you can just continue building upon your knowledge and use the same tools.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2640.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2650.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2650)

I'll start with clicking on recommendations.  Here I see all the four types of Savings Plans we have: Compute, Database, EC2, and SageMaker. I'm going to click on Database  and I'm going to see that it's already selected for me one year and no upfront. So that's the offering, as you can see here, and I have a selection based on the past. Here I need to select the lookback period that most likely is going to be reflecting my future, because we're going to find that maximum savings waterline based on the historical usage patterns.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2680.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2680)

Let's take a look at what I have for the last 30 days. So I'm going to click here on view details  and I'm going to see a visual chart that we'll go over to explain how we found this maximum savings number. In this case, I see that my recommendation is $2 per hour and it will save me around $300. It's very low. In red, it's the on-demand usage. So you can see that we were testing this feature before re:Invent and we spun up a lot of database services

up and down to see that everything is working correctly, so you can see that usage. But since I don't have a lot of things in the last 30 days, the recommendation is really low because it's targeting the consistent usage. So in this case I want to modify it and that's why we have Purchase Analyzer.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2730.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2730)

If I click here, I'm going to Purchase Analyzer. You can also start from Purchase Analyzer  if you scroll down here and click straight to Purchase Analyzer. Here you see the same kind of selection. I have the four types of Savings Plans. I'm going to click on Database and I need to select all these things here. So the analysis level payer will take all the usage across all the linked accounts and we will build a recommendation on top of that. If you want to focus on one of the linked accounts, you can also click here and you will have a linked account selection.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2760.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2760)

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2770.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2770)

 Once you select that, one year, no upfront, no selection over here. And here I'm going to focus on an area where I had a lot of usage.  It actually started on the 28th, where we had more consistent usage, which is also my birthday. So I really like to select this date and up until the 4th. Here you have the flexibility to select any lookback period in the last 60 days, so it's more flexible than the recommendation. This is actually a simple way to think about it. This is a superset of anything we generate for you in the recommendation.

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2810.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2810)

So recommendation is a good starting point for you to see what is available, but then you can go to the Purchase Analyzer and really modify many things and have a lot of simulations. So in this case I'm going to select the 28th until the 4th.  The next selection here is the recommendation where it finds the max savings waterline or I can also do custom. Custom, I can enter any number, so maybe the waterline mark is $10, I can enter $5. So let's start with the recommended amount.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2850.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2850)

Now, since I clicked on this, what it does in the background, it takes every hour in my lookback period, it collects all the services that are eligible for Database Savings Plans coverage and it starts to look for what dollar amount will give the maximum savings for this type of usage. In this case,  you can see that we went from $2 to $28 per hour because now I selected a short period of time with a lot of consistent usage, and that's my estimation right now of what I'm going to use in the future. This will give me 14% of savings and I can also take a look at my coverage and utilization.

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2870.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2870)

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2900.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2900)

 So coverage, right now, I don't have a Savings Plans purchase, so I'm actually starting from very low, from zero. And then I'm going to reach 100% coverage for this purchase. You can actually go over and see hour by hour. There is a small amount of coverage because we did purchase a tiny Savings Plans just to test that it works. So you can see that here. You can see hour by hour what's the  coverage and same for utilization.

So here I see there are some peaks and troughs in my environment and at some point my Savings Plans is not going to be utilized, which is okay. Still have 76% utilization. So I can narrow it down or I can just take this into account. Okay, this is my weekend drops. I want to see what I'm going to optimize with my weekend drops since it's a yearly commitment. In this case, I can go here, click on custom, and let's do $15 and see what, maybe I'm not ready for the max savings commitment.

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2940.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2940)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2960.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2960)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/2970.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=2970)

So I'm going to click on $15  run analysis, and now it's going to do the same thing. It's aggregating all the hourly usage and it's starting to apply the $15 per hour each hour and then we'll see the results for that. Okay, I decreased it by 50%. So now I see that I have more  on-demand uncovered, so the red part is on-demand and I have some coverage which is the blue area. If I click on coverage, I'll see that now I'm not at 100%  coverage anymore.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/3010.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=3010)

I actually reduced it to an average of 65% coverage and maybe that's a good start for me. I just want to purchase one and then I'm going to grow with my coverage over time as I see my usage grow and I'm getting more comfortable with Savings Plans purchases. Lastly, the utilization is going to be 79%. Again, because we did testing, we have a lot of ups and downs. Usually for customer environments, you will see more steady usage for database services. So that's how you go. After you run a few simulations with all these options, you can add it to the cart and  continue to purchase the Savings Plan.

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/3020.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=3020)

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/3040.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=3040)

After you purchase the Savings Plan, you can also go over here. You have a utilization report and a coverage report. In the utilization report,  I want to show you exactly how to find the database utilization. Once you go over here, you will click on Savings Plan type and you will select database, then apply. In this case, we have a very tiny purchase and it's 100% utilized, and I have more details down below  on how exactly each of the database Savings Plans is utilized so that after you purchase, you can track your utilization. When we look at customer environments, many times it's 100% or 99% if you have a little bit of going down over the weekend.

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/3060.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=3060)

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/3090.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=3090)

 And you can also click on the coverage report. Over here, when you click on service, you need to select all the database services, and then you can see exactly what coverage you have. In this case, because we have a very tiny amount of Savings Plans purchased, we're going to look at the last seven days. So it's not a lot of coverage, but the cool thing here is that I can see the coverage per different services,  so it gives you a lot of insights of where the Savings Plan is going to cover and which services it's going to cover for you. So you track it over time, and then you go back and you start purchasing the Savings Plan again.

So we just want to thank you all for coming today, and one last thing we wanted to talk about before we end the session is just to thank you all for the feedback you give us as we build new FinOps tools and cost optimization features. We use that feedback that you send your TAMs and log as product feature requests in our roadmap planning and as we plan out the next set of launches and features we do. That feedback is really crucial for us to make sure we can help you optimize your AWS spend as effectively as possible, and honestly it's a really fun job because we get to come to re:Invent and other conferences like FinOps X and places and bring a lot of new free savings tools. I think we have maybe some of the best jobs in the world that we get to build tools to help you save money, so it's a lot of fun.

But it really depends on quality feedback from all of you as you look at your cost and as you implement your own cost optimization programs to let us know how we can serve you better. So please provide feedback. We put an email alias up here on the screen for you if you would like to give us feedback on any of our products, tools, or experiences you have. Feel free to just reach out and let us know. We love talking to customers. It's the highlight of our day as we go through product management activities, and thank you also on behalf of all of the developers and engineers back home that work tirelessly on all of these features.

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d8d9a5a36151a41b/3240.jpg)](https://www.youtube.com/watch?v=R1yO7HwTB9I&t=3240)

The road and lead up to re:Invent is very difficult sometimes as we have to scramble to get Savings Plans working and everything, and there's a lot of people behind the scenes that come together to pull this together, especially across so many different service teams. So on behalf of all of those engineers, thank you so much. Your usage of these tools validates and is rewarding for us as we see savings roll in and usage for these products and tools, so thank you so much. And again, we'll be available afterwards for questions if you'd like to stay after and learn more about any of these services or Bonnie's story about how they implemented and  really moved the needle on saving 27% off of their cloud bill. Really exciting story. So thank you all.


----

; This article is entirely auto-generated using Amazon Bedrock.
