---
title: 'AWS re:Invent 2025 - How Netflix Connects Product Experiments to its AWS Bill (IND388)'
published: true
description: 'In this video, Netflix engineers Chris and Tushar explain how they connect product A/B testing experiments to AWS infrastructure costs. They present a framework called "cost-aware decision making" that uses distributed tracing for attribution and machine learning for cost estimation. The system tracks how experiments impact request volume across microservices and projects the full cost if rolled out to all users. For example, their hypothetical "Smarter Pre-fetch" experiment showed a 40% increase in requests to the metadata service, translating to $750,000 in projected costs. Key challenges included ensuring trace completeness, handling evolving infrastructure with continuous model retraining, and classifying services by scaling behavior. The framework enables proactive decision-making by displaying projected costs alongside business metrics in dashboards, transforming cost from a surprise to a design constraint. Future plans include expanding attribution to storage costs and batch services including GPU workloads.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - How Netflix Connects Product Experiments to its AWS Bill (IND388)**

> In this video, Netflix engineers Chris and Tushar explain how they connect product A/B testing experiments to AWS infrastructure costs. They present a framework called "cost-aware decision making" that uses distributed tracing for attribution and machine learning for cost estimation. The system tracks how experiments impact request volume across microservices and projects the full cost if rolled out to all users. For example, their hypothetical "Smarter Pre-fetch" experiment showed a 40% increase in requests to the metadata service, translating to $750,000 in projected costs. Key challenges included ensuring trace completeness, handling evolving infrastructure with continuous model retraining, and classifying services by scaling behavior. The framework enables proactive decision-making by displaying projected costs alongside business metrics in dashboards, transforming cost from a surprise to a design constraint. Future plans include expanding attribution to storage costs and batch services including GPU workloads.

{% youtube https://www.youtube.com/watch?v=ulFeSnJlRvk %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/0.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=0)

### Introduction: Netflix's Experimentation at Scale and the Hidden Cost Challenge

 Hi everybody. Welcome to Reinvent 2025. It's good to see you all here. Thank you for attending this session. Picture this: it's Friday night, and you're scrolling through Netflix, trying to find a new title to watch. If it were me, I would be scrolling through K-drama listings, sci-fi listings, rom-com listings, and Netflix just magically surfaces the one title that catches your eye, along with thumbnail and artwork that catches your eye, makes you click, you watch one episode, two episodes, and by the time you're done, you've binge-watched the entire season of Squid Games.

This is not magic, to be honest. It's a product of disciplined experimentation. Netflix is constantly running experiments in the backend using A/B testing to see what titles you click, how long you view these titles, and whether you come back to watch those titles tomorrow. What we should also realize is that a good experiment is incomplete if it only speaks about member engagement because for every new row in the Netflix home page, for every artwork that gets stored, for every smart recommendation, there is an infrastructure footprint on AWS. So there is a cost to running these experiments, and at Netflix scale with over 95 billion hours of content viewed just in the first half of 2025, the cost of those experiments can be pretty huge.

Netflix has worked to connect its product experimentation with the cloud spend, treating the AWS bill almost as an experiment metric. The real win here is not just for me being able to binge-watch the entire season of Bridgerton, but it's about the way in which the overall experience works, where it keeps the member viewership experience great, it makes decisioning data-driven, and it keeps the cloud spend intentional. My name is Manju. I'm a Solutions Architect with AWS. Today Chris and Tushar from Netflix will talk to you about how their teams have connected product experiments to the AWS bill.

By the end of the session, you will leave with concrete ideas on how to run experiments, not just to delight your users, but to show up cleanly and predictably in your AWS cloud bill. So your product teams can continue to innovate and deploy new products, and your finance teams will actually thank you for that process. Over to Chris.

### The Core Challenge: Bridging the Gap Between Product Innovation and Infrastructure Costs

Thank you very much. I'm very curious to see with the launch of Stranger Things, the new season, who's dabbled, who's started to watch the new season. We got a few hands. Well, we appreciate it. Good morning everybody, and thank you for attending our session today. I do want to start with a little bit of a loaded question. I'm curious to see from the audience how many of you have opened up your AWS bill and promptly closed it out of fear or out of just a visceral reaction.

I do see a few people put their hands up, a few people looking away. That's okay. We've all been there, I think, in some way. So if that kind of resonated with you, I think you're in the right place as we're going to dive into aspects of that today. I'm Chris. I'm a Data Scientist at Netflix. I'm joined today by Tushar, my colleague, who we'll hear from a little bit more later.

Over my 12 years at Netflix, I've had the pleasure of seeing our product evolve from shipping DVDs in little red envelopes to streaming video and now into new frontiers of live streaming and video gaming. Over those 12 years, our products evolved, but so has our infrastructure, and our infrastructure has grown increasingly complex to handle not only the scale but the complexity of the product. To further challenge that, we move at a very fast velocity, and as mentioned, we use A/B testing as that engine of product innovation. This empowers our team to be data-driven, to validate ideas, and to ship features fast.

We come to a little bit of an interesting challenge: we tend to have a mismatch in our granularity when we look at how we report infrastructure metrics and costs, which is at the service or application level, but when we do innovation, we're working on a feature or experimental level. So we have this fundamental kind of data and cost attribution problem.

We can't just look at our Amazon usage and cost data and get an intuitive understanding of how much our experiments are costing us, but also how much a future experiment will cost us if we want to think ahead. We have spent the last couple of years building a system to try and resolve this tension and solve this problem, and we're excited to be here today to share this with you and tell you a little bit about our journey.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/340.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=340)

Now to set the context, I want to make  an illustration of a hypothetical feature and A/B test. This should give us a little bit more context about the problem we're trying to solve. Let's imagine we have an idea for a brand new feature called the Netflix Smarter Pre-fetch. We believe by intelligently preloading and caching data, content, box art, and images on the devices, we can improve the overall experience of our members. We can reduce the app loading time and improve the overall responsiveness of the app. Remember, this is hypothetical.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/400.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=400)

Now let's say we run this experiment and by all traditional measures it is a big success. Our members are engaging more, the overall experience feels snappier, and so we decide to roll this out to all of our members so you can have moments of joy. A month or two later, a different story starts to emerge.  One of our finance teams takes a look at our monthly aggregated costs, and they have a visceral reaction. They notice what appears to be an anomalous increase in our total compute costs, and they have no immediate reason why this is out of trend and they don't have any context regarding what has changed.

In addition to that, an engineering team responsible for a mid-tier service somewhere within our complex environment has also started to see an increase in demand for their service and application, and similarly they don't have an understanding of why. Neither of these groups suspects that it's due to this feature, the Netflix Smarter Pre-fetch, one because they lacked the data to understand that, but two and more likely is they weren't involved in the experiment or the feature development at all. We have a fundamental problem or disconnect here. On the surface and by traditional measures we have a very clear product win, however, there's this hidden impact to our infrastructure that is meaningful to our business.

What we would like to do and our problem that we're trying to solve is to prevent these hidden costs, to prevent these surprises. We don't want our finance team in a monthly review asking the question what caused this. We want to give them the data and the tools, as well as our experiment owners and service teams, what they need to know to make better product decisions up front and what that impact on the business is going to be. This hypothetical example led us to build a framework, and we're going to share that with you today.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/510.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=510)

### Netflix's Approach to A/B Testing and the Cost-Aware Decision Making Framework

Over the next 40 minutes we're going to take you on a journey regarding our approach. First, we're going to set the ground and talk about what we call the core challenge, and this is that at Netflix scale attribution becomes an increasingly challenging problem.  I'm going to then hand it over to Tushar who's going to deep dive into our framework and talk a little bit more about how we do this in practice. We'll discuss some of the outcomes and how we take this data and turn it into decisions and how we use it to make better product decisions, balancing the trade-off of innovation and efficiency.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/570.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=570)

We'll talk about the real lessons we learned along the way, the hurdles we had to overcome, and give you a glimpse of the roadmap for us, what is in the future. Finally, we'll leave some time for Q&A. If you're interested in asking some questions at the end, feel free to raise your hand and someone will come by with a microphone, and we'll try to answer your questions as best as possible. Now, to understand generally our approach, we need to understand a little bit more about how we approach experimentation at Netflix as well as product testing. 

We rely heavily on A/B testing as our engine for innovation at Netflix, and so our teams are testing everything from simple UI tweaks to new recommendation algorithms to other changes across the product every day. We don't run these experiments one at a time. There are hundreds if not thousands of experiments running simultaneously at any given time.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/610.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=610)

All of these experiments operate on shared infrastructure.  This is our shared cloud infrastructure. In doing so, we have an interesting challenge. When these experiments run on shared infrastructure, the requests and information from each of our members, whether they are in a test or not, fan out across this very large and diverse ecosystem we have. They touch systems for authentication, to get recommendations, to get their synopsis, and so on. Because of this, it can be very challenging for us to attribute or associate the impact or usage of any given test or members in a test to the actual infrastructure. This infrastructure is not only supporting one test; it is supporting hundreds or thousands along with our regular baseline production traffic.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/690.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=690)

We have this granular attribution problem. We want to be able to link, for any given experiment, its usage and eventually cost back to its impact on our infrastructure. Once we are able to solve this fundamental attribution problem, we will have a better idea of the impact to our business.  To work in reverse here, what we have built is a framework to solve this problem. It is developed around an attribution and projection framework that we call cost-aware decision making. At Netflix, we run at the scale of hundreds, if not thousands of tests. We have had to build this automated framework that in real time, day to day, monitors the impact of all of these experiments and converts their usage into a projected impact on our business.

We showed this year in a mock dashboard one of our experiment results. A test owner or service owner running a new product experiment gets to see the impact on our core business metrics. For example, in the hypothetical experiment we had called Smarter Prefetch, we see that in the middle column there is an improvement in our overall streaming. People are engaging more. However, we see right beside it that there is a cost to the business of $100,000. This is the cost if we decide to productize this and roll it out from 1% to 100% of our traffic. Just to be clear, this is not the cost of running the experiment; this is the cost if we decide to roll this out to everybody.

This type of information allows us to move from reactive decision making, as we saw before where we were reacting month to month when we had these surprises in our bill, to being much more proactive. We can make much better decisions when we have this information up front and when it is presented alongside our core business metrics. Our test owners now have the price of a feature right in front of them so that they can make better decisions trading off innovation and cost efficiency. At the end of the day, this has allowed us as a company to start thinking about what a successful experiment looks like and how we balance velocity, cost, and other factors with so many experiments running.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/820.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=820)

One of the interesting challenges we have is that with a company our size, our infrastructure is very diverse and heterogeneous.  I imagine for a lot of people in the audience you have very similar infrastructure. Our infrastructure spans machine learning pipelines, internal tools, and everything to the core critical infrastructure that powers our consumer product. From day one when we thought about solving this problem and approaching this framework, we knew we could not boil the ocean. We had to start somewhere, and that place we decided to pick was our consumer-facing systems. We picked these systems to start with because they tend to auto-scale dynamically in response to changes in our customer behavior. As people get engaged more and launch the app more, our systems tend to scale dynamically with that, and that has an impact on our cost.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/900.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=900)

### Attribution Through Distributed Tracing: From Traces to Signals

With that context, I am now going to pass it over to Tushar, who is going to walk through the actual details of how we got to designing and launching this framework. Thank you, Chris. Hello everyone. My name is Tushar, and I am a Senior Data Engineer at Netflix. I would like to take you behind the scenes at Netflix.  I will admit it would have been nicer if it was for one of the famous series like Stranger Things that released last week, but for now, we will take a dive into the cost of air experimentation.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/920.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=920)

This is the framework that allows us to connect the dots between product experiments and the AWS bill. Before we get started, here's a quick overview  of what's ahead. We'll start by discussing the two main components of the framework, then we will take a closer look at attribution, which we also call traces to signals. We'll also go into the foundations of distributed tracing, which is a key pillar of our approach. Then we will explore the estimation side of the framework, and finally, we're going to see how we put this framework end to end into practice at Netflix today.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/960.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=960)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/980.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/990.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1000.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1000)

So let's start with attribution. When we run an experiment, we have a treatment group, which is the set of users who are experiencing a modified  or a new feature experience, and a control group, which is a set of users who are experiencing a standardized or a default experience. As users from both groups interact with the Netflix application, we collect a wide variety of metrics across different layers of the system.  Sometimes these interactions produce a noticeable or a statistically significant change in the key business metrics.  Attribution is this process of determining the root cause of these changes, essentially identifying which action or which feature led  to that specific outcome.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1020.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1030.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1040.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1040)

Within the context of our framework, attribution helps us pinpoint what's driving a shift in the infrastructure metrics, which ultimately translates into the AWS cost. Now once we have measured the significant  change, let's move on to the estimation. This is where we use machine learning and statistical methods to estimate the  true impact by translating these metrics into key business relevant insights.  In short, estimation is about translating these technical changes which we saw in the attribution to the quantified impact which helps us determine the true business effect of an experiment.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1060.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1060)

Now let's get into the details of the attribution. As we mentioned in the earlier slide, the goal of attribution is to identify a  core significant change. We also saw that to identify that core significant change, we need to have members interacting with the Netflix application. Let's call this interaction by members with the infrastructure and the data collected as the usage data. The term usage here in the context of an online request response applications refers to the amount of work that is done by an application as it serves the traffic.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1110.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1110)

However, the issue that we're dealing with here is we're not only interested in the usage data, but we are looking to compare that usage data between a treatment and a control of an experiment. So let's rephrase that usage data to a usage delta.  The graph here represents a usage metric. It's a generic usage metric for now between a treatment and a control. The important thing to note here is we're more interested in the delta between the pink and the green trend lines rather than the absolute values of those usage trends.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1140.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1140)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1160.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1160)

More concretely, these usage metrics in the context of a request response application can be request latency, request payload size,  request volume, or even error rates. So we may want to use one or more, or a combination of these usage metrics to achieve what we really want to get. Now with that, we have seen what usage metrics delta mean, but we also need to have an understanding of  how we can use these to derive what we want to, and in this case, the change in the cost prediction.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1170.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1170)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1200.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1200)

In the real world, most  applications scale based on the defined policies by the engineering team. Here at Netflix, majority of our online request response applications scale based off the CPU. So that means an increase in a usage metric like a request latency, we need to add more server or we need to scale the application up to be able to meet the rising demand and bring down the request latency to an acceptable value. More importantly, by measuring this  increase or decrease in the application scaling, we are able to project that change into AWS cost change.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1230.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1230)

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1240.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1240)

An increase in the number of applications needed will lead to an increase in the AWS bill, or a decrease in the number of application servers needed will help the business save money. We've seen how usage metrics can be used to predict cost changes, but let's examine which metric we  decided to use. After extensive analysis and testing, we determined that most of our applications  scale based on request volume, or the number of requests inbound to an application. This is primarily because our CPU is directly proportional to the request volume. In the context of an experiment, if we measure the request volume for a treatment and also measure the request volume for a control for the same time period, we can use that delta to project a cost difference.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1270.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1270)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1290.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1290)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1310.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1310)

Now that leads us to a critical question: how do we get the request volume, or more specifically, how do we get the request volume separately for treatment and separately for control?  This is where we enter into distributed tracing. Imagine a complex microservice architecture where requests flow from one application to another.  This architecture represents a single user request coming in from one of our members. In this case, application 1 calls applications 2 and 3, and they in turn call applications 4, 5, and 6 respectively. This architecture in the form of a distributed trace can be represented as  a single trace, which is a collection of rectangles that are technically called spans. A span is essentially a single request flowing between one application and another. For example, application 1 to application 2 corresponds to one span.

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1390.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1390)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1400.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1400)

A span itself has two key properties: a start time, which is the start of the rectangle, and latency, which is denoted by the length of the rectangle. In this diagram, the purple span has the maximum latency, probably because it's waiting for all the other requests beneath it to finish before returning a response to the user. More importantly, a span can also have certain tags, which are key-value pairs used to store information about the request itself, such as the nature of the request, payload size, or request status, or even about the application itself, such as node information or cluster location. This single microservice architecture for every request turns into something like this  at the scale of Netflix architecture, or more precisely,  at the scale of our experiments.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1450.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1450)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1460.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1460)

An important thing to note is that we are dealing with billions of requests flowing through thousands of our applications. Each of these requests represents a user action, and the application may want to treat it differently depending on whether it's from a treatment or from a control. The sheer volume and complexity is exactly why the entire process of attribution is so challenging, especially at the scale of Netflix. Even though the diagram represents blue for treatment and orange for control, in reality, we are trying to measure a single change that ripples through a massive interconnected ecosystem. Of course, tracing is not without its own challenges.  Given the magnitude of traffic that Netflix receives, the first and foremost challenge is that we cannot log every single request, so we have to rely on sampling.  This means we only log a fraction of the traffic requests that come into Netflix's ecosystem.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1490.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1490)

What makes it even more challenging is that different business domains may want to have varied sampling rates. For example, logs mostly at 0.05%, 1% for web traffic, and maybe a higher percentage for sampling for emerging domains like ads. Even with these sampling rates, we have to deal with inconsistencies in the data and spikes in volume.  These may be because of a launch of a famous series like Stranger Things or a live event coming in.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1520.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1520)

The data collected from these applications is usually unstructured and dirty, based mostly on the best effort from the application itself. Additionally, like other big data problems, we have to  deal with the size of data for processing, which is usually in the billions of rows, even at a very low sampling rate. All these factors make statistical rigor and careful analysis even more important.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1540.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1540)

Now that we have seen the attribution components, let's put them all end to end.  We will start with getting the sampled data, which is collected via distributed tracing at the top left. We will combine this with the anonymized member data, which includes the devices. We will also use the application's metadata itself. All these datasets go through an ETL process where the data is cleaned, enriched, extracted, and made in a way that it can be analyzed. The final output is the A/B test usage data, which is now ready for statistical analysis.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1580.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1580)

To better understand how we use attribution in real practice, let's revisit the experiment that Chris talked about in the beginning.  As a quick reminder, the hypothesis was that if we are able to prefetch the content the user is likely to watch next, we can make the entire Netflix experience faster and more responsive. The treatment is where we are fetching the content the user is likely to watch next in the form of text, images, videos, or anything else. The control is where the data is being loaded on demand as the user scrolls through the UI.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1620.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1620)

Applying attribution to this particular Smarter Prefetch experiment, we were able to identify the true impact of the experiment.  We found a clear and significant signal that was not a random fluctuation. Members in the treatment group of the Smarter Prefetch experiment made 40% more requests to one application in particular, which was the metadata service. Even though we could have guessed something like this based on the hypothesis of the experiment, it's important to note that this was not a hunch anymore. It was a data-driven result that provided us a clear direction. We had confidently isolated the primary impact of the business in a massive interconnected network and away from all the noise in the system.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1680.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1680)

### Estimation: Converting Usage Deltas into Projected AWS Costs

Now that we have a significant signal from attribution, let's move to the second component of the framework, which is estimation.  In this step, we will be using the significant signal obtained from attribution and convert it into a real prediction, in this case, the actual dollars or bills being paid to AWS, only if the treatment is rolled out to the entire global audience of Netflix. In the context of the Smarter Prefetch experiment, this means the 40% increase we saw in the request volume needs to be converted into the actual dollar figure if we decide to launch the treatment to the entire global audience. Estimation is a three-step process.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1730.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1730)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1770.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1770)

In the first step, we learn from production.  We train machine learning models based on historical production data. These models learn the complex relationship between the usage pattern, which in our case happened to be request volume, and the infrastructure cost for each application. This is the core engine behind our cost estimation. The graph here represents the AWS hourly cost and how it increases with the increase in request volume for the metadata service. The second step is the simulation. Once we have the machine learning models for each application,  we use these trained models to generate two parallel universes. The first scenario is where we get the cost based on the original historical usage. This is our baseline cost and is represented by the green line in the graph for the metadata service. The second scenario is what we imagine the cost would be like if we were to apply the increased usage across the board. This is represented by the red dotted line in the graph here.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1810.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1820.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1820)

The difference between the two universes  gives us the incremental cost of the experiment, which is denoted by this up and down white arrow. Once we have obtained this cost  difference for every single application that had a statistically significant change, we need to move to the third step, which is aggregating and analyzing these data. This gives us a comprehensive view of the total cost delta we can expect if the experiment is launched to all users.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1850.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1850)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1870.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1870)

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1890.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1890)

Applying this entire three-step process of estimation onto the Netflix Smarter Prefetch experiment,  we were able to see a significant increase in the cost by $750,000 as a projection if we were to launch the experiment to all users. As we had expected based on the results of the attribution, where we saw a 40% increase in the request volume,  the biggest increase in this cost came from that single metadata service where we had seen the increase earlier. But it is important to know that now we know the true impact and how much the impact is. For the first time ever, we can see the true cost of a product win. 

This framework really enables Netflix to make smarter, data-driven decisions. There are two main ways in which Netflix consumes this entire framework. First, it acts as a safeguard, helping us catch and prevent any unexpected spikes in our infrastructure spending before they even become a problem. Similar to what Chris showed in the beginning, we will have a dashboard or even have alerts where people will be notified of a significant increase in projected cost.

Secondly, it gives us the ability to have meaningful trade-off discussions. We can directly compare the user benefits of a new feature or even the key core business metrics with the projected infrastructure cost. This means we are able to make much more informed decisions even before rolling out an experiment to our entire audience. As we had highlighted earlier, the goal of the framework is not only to help reduce AWS cost, but to allow us to make much more informed, data-driven decisions.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/1990.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=1990)

### Lessons Learned and Operational Excellence Beyond Cost Attribution

It is mostly about having that operational efficiency with the innovation pace. And with that, I will hand it over back to you, Chris, to take us through the lessons learned and the roadmap ahead. Thank you very much, Tushar. Now building the framework that Tushar talked about is one thing, but trusting the data is a completely  different beast. So I want to share three specific learnings and hurdles that we had to overcome to make this not only production ready and viable, but also that our users could trust.

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2010.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2010)

The first and foremost, and probably most important, is trace completeness.  As Tushar mentioned, the idea of tracing and attribution is the core to this entire framework. If there is something wrong here, then everything else is questionable. Our attribution framework is based on the assumption that we are able to trace the requests of a user throughout the entirety of our infrastructure.

What happens if one of those pieces of our infrastructure service does not propagate trace headers well? That ends up in a blind spot in our infrastructure. We are now missing parts of our ecosystem, and we are blind to big parts of the infrastructure spend. Within the context of our framework, that means we may be underreporting costs or underreporting the impact. Attribution and trace completeness here is a very crucial step for us in building this framework.

To do that at Netflix, there is no silver bullet, so we had to work very closely with our engineering teams, specifically our observability engineering team, to ensure that we had accurate request tracing. On our side, that included setting up automated data audits and alerts to tell us when we had these gaps or these potential visibility issues. If you are interested in attempting this within your own organizations, this is where you want to start. You want to start off with getting high quality infrastructure tracing and attribution in place.

The second challenge is that our infrastructure is always evolving and constantly changing and drifting.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2110.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2110)

We knew ahead of time that we would be chasing a moving target. Our teams at Netflix move fast, as we mentioned prior, and these teams are also upgrading infrastructure constantly. Teams may be upgrading EC2 instance types, migrating service architectures, and even pricing can change. We had to take this into account and couldn't build just one model and apply it across all of our services. We have a variety of different services, but also the behavior of those services changes over time.  If we had built a single model once, it would be out of date very quickly.

The second challenge is that with evolving infrastructure, we need to be cognizant of edge cases, which we'll discuss in more detail. To address the first issue, we implemented continuous retraining for the machine learning models. We retrain our models daily or even hourly to ensure that we capture the most recent state of our infrastructure. With regard to those edge cases, we also had to implement robust fallback mechanisms within our modeling. We can fall back when we detect drift in our infrastructure to simpler heuristic-based cost models. They don't necessarily have the precision of the ML models, but they are directionally accurate in scale. This was important to us because given the number of tests we're trying to inform and the rate at which our infrastructure is changing, we want to always be able to produce an estimate for our end users.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2220.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2220)

Finally, one of the other hurdles we had to overcome was that not all of our services are created or scale equally.  Earlier we mentioned that we decided to focus on our consumer-facing product, and the reason we did that is because they mostly auto-scale. However, the reality is a little more nuanced than that. Even within our online consumer-facing applications, some applications are statically provisioned, meaning that with an increase in demand there is no change in their cost. There can be a few reasons for this. Maybe they are scaled up to a high water mark to absorb bursts of traffic, or maybe they're just over-scaled and not as efficiently scaled.

In contrast to our dynamically auto-scaling workloads where there is a clear relationship we can learn from, in the static case there really isn't a relationship we can learn. The cost is almost always constant until we hit a tipping point. To ensure our framework was able to handle these cases, we built logic to classify our services based on their different scaling behaviors. We were able to say that this service is an auto-scaling service, this service is statically provisioned, and so on. The framework is able to use this classification to perform its cost estimation and modeling. This ensures that we only attribute costs where there is an actual increase in usage that will drive our bill.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2320.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2320)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2340.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2340)

Overcoming these technical hurdles was a significant effort.  However, during that time when we were solving this, we realized that the data we're collecting and the models we're building can also unlock a whole other use case for operational excellence. The first is that the data can be used to proactively perform capacity planning. The same usage data that Tushar talked about, where we're collecting information at an application level and estimating the difference, can be useful for helping plan the rollout of the experiment from the service perspective.  For example, imagine an experiment where we are implementing a very new computationally expensive process. Using the framework, we're able to understand within the experiment what its potential usage pattern will be and turn that into a potential capacity need.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2400.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2400)

This turns it into a proactive discussion, whereas before we would launch the feature and potentially have a situation where we might be under-scaled. The power of the framework is that as we collect this usage information to project cost, it's also very valuable for helping inform capacity planning as well. The second use case is that it also enables what we call shift left validation. Imagine a case where you have a product team implementing a new feature  and due to many layers of abstraction or obfuscation they take a dependency on a legacy system that they shouldn't have.

Before our framework came along, we would roll out this feature and teams would now have a dependency on a service that we're trying to deprecate, or worse, a service that wasn't scaled correctly to handle the demand. Given that this framework and the attribution model we designed allows us to get insight into the usage across our entire environment, the test owner and the service owners involved in the product experimentation can now have insight regarding their architectural implementations. We're able to bring this idea of testing not only our actual product feature but the infrastructure implementation into the actual experimentation phase as well.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2450.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2450)

Finally, with this granular  information as Tushar mentioned with usage, we're able to get better performance insights. This goes beyond just the performance of the test on the business. Are we improving streaming? Are we improving retention? This gives us insight into the health of the actual experiment and the feature itself. Recall back to what Tushar had said about measuring latency. As part of this, we can now measure what is the impact from a latency perspective of any given feature. This allows us to tie the impact on the infrastructure to the member experience, giving not only our test owners but our service owners a more holistic picture of the impact and health of the feature.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2500.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2500)

### Future Roadmap and Key Takeaways: Expanding the Framework to Storage and Batch Services

This project has been a journey for us, and while we're still in the middle of it, the core framework we presented today offers a blueprint that you can get started with today. As we talked about, we focused primarily on our online consumer-facing systems because they mostly auto-scale with demand. However, that's only a fraction of our overall cost, and if our goal is to really understand the total cost of innovation, then we need to increase our scope and include other drivers of cost. Our next step in that journey is to now consider storage costs. While compute is transitive, storage is mostly cumulative. A new feature may be increasing storage costs, or more likely a new feature will require new amounts of data. Think, for example, of generative AI. 

In these cases, we want to be able to expand our framework to not only include our online services but also take into consideration data storage technologies like Cassandra, S3, EBS, and others. This requires different attribution models and mechanisms because we can't necessarily rely on tracing, as Tushar had mentioned, to be able to tie this all together. The second area that we want to focus on is our batch services, and arguably this is a little more challenging. Our batch services power everything from offline machine learning algorithms to new recommendation and personalization systems, and this is a challenging area to approach given that a lot of these systems are disconnected from our consumer product system. However, with the rising costs of GPUs and other technologies, this is a crucial area for us to expand our framework into, as these are contributing greatly to our overall spend.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2610.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2620.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2620)

Now as we work to apply these methods and attribution processes to our new domains, we want to leave you with a few key takeaways.  First, attribution is extremely important.  As we've discussed throughout the presentation, this is the foundation, the bedrock to everything we've discussed today. If you want to start to untangle and understand the impact of an experiment on your infrastructure and then ultimately project that, you need to start with attribution. Cloud costs are typically at the granularity of applications and services, but features are at the level where innovation happens, so there's a granularity mismatch there.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2650.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2650)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/f5b425ddc957d23c/2690.jpg)](https://www.youtube.com/watch?v=ulFeSnJlRvk&t=2690)

The second and more interesting point is that once you have the attribution in place, you want to turn that into what Tushar mentioned as the cost of success.  You want to be able to use your attribution data to then project into the future what this feature or this product is going to look like when you roll it out. This really does help transform the decision-making from a reactive process to a more proactive one. It helps teams then balance the trade-off of innovation and efficiency. Third, you need to cultivate an environment of shared ownership with this data. You don't want to lock this data within dashboards and in finance reports. You want to democratize this information to all the users who are going to be using it or should use it to help make decisions. 

You want to be able to place this information right beside key business metrics as we showed in our mock dashboard to allow test owners and service owners the ability to make those smarter decisions, and you don't have to police them. In closing, what we presented today is a framework that has allowed us and enabled us as Netflix to redefine what we think a successful experiment looks like, helping us balance those trade-offs of innovation and efficiency and bringing cost into the decision framework. This empowers our teams to catch issues early and to prevent surprises to our bill. Most importantly, it turns cost into a design constraint, allowing us to build better products. Thank you very much.


----

; This article is entirely auto-generated using Amazon Bedrock.
