---
title: 'AWS re:Invent 2025 - What''s new with Amazon SageMaker in the era of unified data and AI (ANT216)'
published: true
description: 'In this video, Sean Ma and Iris Sheu present the next generation of Amazon SageMaker, which unifies machine learning and analytics capabilities into a single integrated experience. They address the challenge that 74% of companies face in AI initiatives due to complex toolsets, data sprawl, and fragmented governance. The session covers key components including SageMaker Unified Studio (a single development environment), SageMaker Catalog (for data governance), and an open data lakehouse architecture built on Apache Iceberg. Recent launches are demonstrated, including one-click onboarding using IAM roles, visual workflows for data engineers, unstructured data support in S3 buckets, QuickSight integration, and tag-based access control. The highlight is a live demo of the new serverless notebook with an AI-powered data agent using model context protocols, showing how users can explore data, generate SQL and Python code, and build machine learning models without manual infrastructure provisioning.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - What's new with Amazon SageMaker in the era of unified data and AI (ANT216)**

> In this video, Sean Ma and Iris Sheu present the next generation of Amazon SageMaker, which unifies machine learning and analytics capabilities into a single integrated experience. They address the challenge that 74% of companies face in AI initiatives due to complex toolsets, data sprawl, and fragmented governance. The session covers key components including SageMaker Unified Studio (a single development environment), SageMaker Catalog (for data governance), and an open data lakehouse architecture built on Apache Iceberg. Recent launches are demonstrated, including one-click onboarding using IAM roles, visual workflows for data engineers, unstructured data support in S3 buckets, QuickSight integration, and tag-based access control. The highlight is a live demo of the new serverless notebook with an AI-powered data agent using model context protocols, showing how users can explore data, generate SQL and Python code, and build machine learning models without manual infrastructure provisioning.

{% youtube https://www.youtube.com/watch?v=V2wFP-qzahY %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/0.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=0)

### Introduction: The Next Generation of Amazon SageMaker

 Hi everyone. Welcome to A&T 216 where we are going to cover all things new with Amazon SageMaker. Before I get started, I am going to take two quick questions. I want to raise your hands. How many people here are managing or processing large amounts of data for analytics? Can I see a show of hands? Okay, about half of you, something on that order. Okay, second question: how many people here are trying to prepare and leverage data for AI initiatives? Okay, more of you, more of you. Good. And that is a pretty good overlap. Well, the good news is if you do one or both of these things, this is the right session to be in.

Today we are going to talk about how the next generation of Amazon SageMaker brings together widely adopted machine learning and analytical capabilities into a single integrated experience with unified access to all your data. My name is Sean Ma. I am a principal product manager with the Amazon SageMaker Unified Studio. Hi, and I am Iris Sheu, a senior product manager with Amazon SageMaker Catalog. Okay, so let me give you a quick overview of what is happening.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/60.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=60)

 If you were here last re:Invent, you may remember that we launched the next generation of SageMaker at re:Invent 2024. So the goal for today's session is to bring you up to speed on everything that has happened since our launch all the way up until this moment in time. We are going to start off with an overview of the next generation SageMaker, just to level set there, followed by launches during the year, and then we will move into the most recent launches that just came out over the last few days and weeks.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/110.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=110)

### The Challenge: Why 74% of Companies Struggle with AI Initiatives

Okay, so let us set the stage. Why are we here? What is the purpose of the next generation SageMaker? Why did we solve it? Why did we introduce this next gen? Well, it is because the data landscape is changing and more and more people are investing in AI as part of their core enterprise strategy. The problem is, according to Boston Consulting Group,  over 74% of companies surveyed are not actually set up to be successful with this initiative. That is a huge problem, a very concerning statistic.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/130.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=130)

So why is this the case? Well, after many discussions with our customers, we learned that the challenges primarily fall into three big buckets.  First, they told us that the rapidly evolving nature of the data and AI landscape means that there is a lot of new and complex tool sets that customers have to manually stitch together. This impedes progress and increases costs. Instead of becoming more efficient and agile, it is actually slower and more expensive.

Second, they said they have a challenge with data sprawl. Well, this is actually not a new problem. We have always had a challenge with data sprawl. When you are dealing with complex environments, whether data on premise, in the cloud, SaaS systems, databases, data warehouses, data lakes, that has always been a challenge. But now we are also seeing increased demand for unstructured data, so it has just multiplied greater. This data sprawl naturally results in more data silos, which then leads to a fragmented governance experience. This is challenging because now you have to make sure you have access to the data, but also the right data and how do you ensure that is the case?

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/200.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=200)

All bottom line is all this means is that customers are having difficulty utilizing their main competitive differentiator, their data, to be effective and drive the business forward with AI initiatives. So how can Amazon help?  Well, as you all know, AWS has an unmatched experience for reliability, security, and performance, with many services enabling more than two million customers innovating with data and AI. It is in our DNA to listen to you, our customers, on how we can make our products better.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/240.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=240)

So customers such as yourselves have been pushing us to go beyond just more breadth of capabilities and more services and more capabilities inside of them, but really invest in how these services work better together. To do this, we focused on how the nature of work is actually changing for our users and what we could do better to improve that.  What we saw was there was an urgent need to integrate data, analytics, and AI at scale, which allows various teams to work together across an end-to-end workflow, whether it be SQL queries, ETL jobs, building models, generating applications. All of those need to come together in multiple services in an integrated development experience, plus combine that with centralized data governance and data management.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/290.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=290)

Therefore, our challenge was to select the right set of services that will work well together in a unified experience. And that is why we took Amazon SageMaker, which was already used by hundreds of thousands of customers to build, train, and deploy machine learning models, and we introduced the next generation of Amazon SageMaker by expanding what SageMaker had to add data and analytics capabilities into that.  This next generation includes virtually all the components you need for fast SQL analytics, big data processing, search, data preparation, AI model development, and generative AI all

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/320.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=320)

### Three-Layer Architecture: Unified Studio, SageMaker Catalog, and Open Data Lakehouse

on top of a single view of your data in a managed and governed manner. This new offering consists of three layers. On the top, you'll see a single development  experience with multiple tool sets called the SageMaker Unified Studio. In the middle is the SageMaker Catalog, which is built into the unified studio, so you get end-to-end governance for your data and AI workflows. Finally, the foundation is built on using a lakehouse architecture that enables access to all your data through an open Apache Iceberg standard interface.

I'm going to break these things down a little bit further so you can understand what each part means. Starting at the top, the unified studio removes the pain associated with separate development experiences. As you can see here, it's integrated for individual use case-specific tools, bringing that all together in one end-to-end complete experience. Not only are these different interfaces available, but it also integrates with the underlying AWS data processing and compute engines such as the original SageMaker, now called SageMaker AI, along with other services such as EMR, Glue, Athena, Bedrock, and Redshift. All of this is done to bring it together into one seamless experience.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/390.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=390)

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/400.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=400)

You also see that we will continue to expand what we support inside the unified studio, including streaming, business intelligence, and search analytics.  This is something that we continue to invest in and continue to build out.  With this unified studio itself, teams can collaborate without the burden of switching context. You log in once, it's one setup, and then you can go between these different interfaces. Your data is all accessible in one place, and you can switch between them without having to lose your train of thought.

In addition to this, to cater to a wide range of users, we have multiple different interfaces for customers to use. Whether you prefer to write Python code, we have a code editor. If you prefer to query with SQL, we have a query editor. For people who prefer the drag-and-drop boxes and line interface, that's all there. And on top of this, as expected, we also support natural language through our generative AI assistant, so you can just ask questions and interact with the services.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/460.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=460)

I'm going to move on to the next component: how to access data in a secure manner. It's nice that we brought all these tools together,  but really it's not enough if you can't get access to your data while ensuring trust and security. What's the point? The challenge at this layer is that customers are trying to find the right balance. How do you democratize data access for your end users and give them the datasets that they need to complete their job while still maintaining compliance with your regulatory obligations like GDPR and HIPAA?

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/490.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=490)

Getting the right data at the right time to the users is critical.  To help customers move faster with the data, we built the SageMaker Catalog, which is built into the unified studio, so you have a single place for data and AI governance with all the data access controls in the studio itself. Using this catalog, you'll be able to securely discover and access approved data, models, compute, and generative AI assets through a publish-subscribe workflow, or you can assign individual assets without having to go through IT. It's all done through that one interface.

You get semantic search across all the assets in the catalog, which means easy collaboration and a comprehensive view of what data has been accessed, the data quality, sensitivity, and end-to-end lineage. On the data preparation process, we have built-in data quality, sensitive data detection, and lineage that makes sure it's constantly tracking from end to end, really streamlining that process. This is the core data governance capability that the SageMaker Catalog now has.

### Responsible AI and Customer Adoption Since Launch

On top of this, we also want to address some of the unique challenges that arise because of generative AI in this world that we're dealing with. We've also added the ability to support Amazon Bedrock guardrails. With this, you can implement responsible AI filters in natural language, which includes blocking sensitive information and reducing hallucinations. These guardrails work with any models, not just the ones from Amazon Bedrock, but anyone that may be available through a third party.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/590.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=590)

Now, the last part I want to talk about is the open data lakehouse, really the foundation of where  the data is being stored. Often customers use workload-specific storage in their data warehouse or in their data lakes. However, this means that the data is split naturally, sometimes in the data warehouse or sometimes in the data lake. This is only a problem because each tool and each engine tends to access the data differently based on that format.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/630.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=630)

The problem with this is that it can lead to inconsistent data access, or worse, it can lead to performance concerns later down the road that are hard to debug. So to address this, the SageMaker platform is built upon an open data lakehouse architecture, which is fully compatible with Apache Iceberg. 

What this means is rather than you having to worry about setting up access to each of the different kinds of storage, this unified data access using Apache Iceberg goes across S3 data lakes, Iceberg tables in S3, as well as Redshift managed storage. Plus, it allows Iceberg compatible engines such as EMR, Redshift, Trino, and Spark to query and access the data in a consistent manner. That's how we ensure consistency in terms of what you're accessing.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/670.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/680.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=680)

Now, what about the data itself? You also have access to bring in operational and enterprise applications  using over 150 ETL integrations without having to build or manage ETL pipelines. And for data that's sitting outside, you can use  the federated query connectors as well as AWS Glue connectors to tap into on-premise systems or third-party applications, so that you can query all your data in one place, all available through the lakehouse itself.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/700.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=700)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/720.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=720)

So this is a summary of everything we announced at re:Invent last year and we eventually made it available generally available in March.  As with any new product launch, we didn't stop investing in this platform once we launched it. In fact, since then we've made dozens of enhancements and expanded the availability, so this is now available in over 15 different regions. 

We've also had the privilege of having a variety of customers across different industries, whether it's healthcare, tech, automobile, or financial services, use SageMaker to address their enterprise data and AI needs. It's been really humbling and exciting to hear how these customers have been using the next generation of SageMaker and really benefiting from this unified approach. So we're going to spend the rest of this talk talking about how we've continued to improve it and some of the key launches through the year, and with that, I'll hand it off to Iris.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/760.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=760)

### Visual Workflows and Job Monitoring for Data Engineers

Thanks a lot, Shawn. So now that Shawn has established what SageMaker Unified Studio is, let's talk about a bit more about  our launches and what our team has been busy building. Before I really dive into the launches though, I just want to share the framework of how we think about how we build in our product roadmap, and that's really with our data team and our customers and data teams in mind. Whether you're a data engineer maintaining ETL processes and pipelines, a data scientist building ML models, a data analyst building dashboards and analyzing data, or a data steward responsible for data governance and access control and security, our vision is that SageMaker is the single tool that you need.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/800.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=800)

So with that context, let me walk  through four key launches that represent how we're addressing each of the users that we just talked about in the previous slide. Fortunately, we don't have the time to talk about each of the over 40 launches we've had this year. But I'm focused on these today because I think they do a good job to represent all the different capabilities and each layer of the stack of SageMaker. So we'll start by discussing visual workflows and job monitoring for data engineers, and then move to how we onboard unstructured data stored in Amazon S3 and the SageMaker Catalog. Then we'll talk about our Amazon QuickSight integration in SageMaker and finally talk about S3 table governance with tag-based access control.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/850.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=850)

So let's start by talking about our data  engineers who are responsible for maintaining complex pipelines that involve multiple steps and ETL processes, often using tools like Apache Airflow. Today, thousands of our customers are using Glue jobs as one way to automate these ETL processes, and we've brought the visual ETL interface into SageMaker that our customers love. We've also expanded upon this with visual workflows that allow you to automate Glue jobs alongside other tasks. Our goal with this launch was really to expand the accessibility of who can create these automated workflows and pipelines without needing to know the nuances of Python or Apache Airflow code, and we think this is something more advanced data engineers will also appreciate because you can easily visualize all the steps by uploading already created files you have.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/910.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=910)

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/920.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=920)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/940.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=940)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/970.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=970)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/980.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/990.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1000.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1010.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1020.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1030.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1040.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1050.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1050)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1070.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1080.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1080)

Since this is such a visual feature, let me go through a quick demo of how this works.  I'm starting here in the SageMaker portal, and I'm going to create a new workflow.  As you see here on the left-hand side, I have a bunch of tasks that I can choose from. When we launched in March this year, we just supported 3 tasks, but we've now expanded to over 100. These tasks range in categories from analytics to various machine learning tasks, so they're Bedrock operators.  You can also use the quick filters on the top for specific services. It's pretty easy to create a workflow. For the purpose of this demo today, I'm going to start with a simple example of a three-step workflow where we're going to take a Glue job I've already created that processes sales data per store, and then I'll run a notebook that takes the output after the Glue job has completed.  It'll run a sales forecasting model and then send the S3 result at the end. As you can see here,  I'm simply dragging and dropping the Glue task here, and I can search for the Glue job that I've already created in my list. However, if I wanted to, you also have the option to actually create a Glue job from scratch within this interface as well.  Now that I've selected that,  I'm going to select the notebook. It's pretty simple here. I will search for that and select it, so it adds the next node here that will start only after the previous task has completed.  In the interest of time, I've fast-forwarded. I've added all the additional nodes as well and updated the parameters. As I've been updating, what's been happening is that the code is automatically updated as well, so I could download this and use it for infrastructure as code in pipelines if I wanted to.   If I have previous code that I've already saved, I could also upload it here so you can visualize it, make changes to it, and save those results as well.  Once I'm happy with this workflow, I'll save it, name it, add any descriptions I want to, and then click run.  We're going to make sure that it's running so you can see that the workflow has started. I'm going to click view runs here on the right-hand side.  When I click on view runs, I get to all the historical runs that have happened. I had a previous run that failed, but I'm going to click on the one that's currently running, and then I can see all the different tasks.  My Glue job has completed, and it's now currently running the notebook. As the notebook was running, I get the logs for it, and then I could troubleshoot any errors if I wanted to from there as well.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1110.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1110)

### Unstructured Data Discovery: Bringing S3 Buckets into SageMaker Catalog

 Next, I'd like to discuss how we made it easier to discover and govern unstructured data. In this age of generative AI, I'm sure we all know how important it is to have a data foundation that's built on all types of data and not just structured tables, but also your unstructured photos, images, videos, and so on. However, our customers have shared that working with all these different data types can be challenging because data is separated across different environments, and they often need to use different tools to work with different types of data. That's really what we're trying to solve with this launch.

We've brought S3 buckets into the SageMaker catalog in order to support unstructured data. This allows unstructured data to be a managed asset in the SageMaker catalog. Data producers can then enrich the unstructured data with business metadata and semantic meaning so that data consumers, such as your data scientists and developers, can search for the right data they need for their machine learning or generative AI model development. After that, they can also write results back to S3 buckets if they have the right permissions, which are controlled by S3 access grants. This allows access through a user's corporate identity. This allows unstructured data to be discovered and shared in the SageMaker catalog and work with it within the same SageMaker environment.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1220.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1220)

With structured data, you can work with it within the same SageMaker interface for querying and processing data. With launches like this, what we're really trying to do is create that single unified view for your data assets in the SageMaker catalog. 

### QuickSight Integration: Streamlining Dashboard Creation and Sharing

Now, let's talk about how we help data analysts build dashboards to answer their business questions and derive business insights. One challenge we've heard in creating the right dashboards is that it can be difficult to find the data they need to answer their questions. For example, a marketing team might want an analysis built off of sales data, but needs to request access, which could take days. Once they get the access, it can be difficult to understand what the columns are, the schema, and everything else.

On the other side of this problem is too many ungoverned dashboards. This is a problem I personally faced as well. There could already be a dashboard that exists that answers the questions I'm looking for, but I don't know where to find it or which one is the right one. That's what we aim to address by bringing QuickSight into SageMaker. As Sean mentioned earlier, we are looking to bring BI capabilities into SageMaker, and that's coming soon. But in the meantime, we took this first step to launch QuickSight from a dataset in SageMaker.

Here's how this works: you can click the option to visualize a dataset that you have access to in a SageMaker project, which opens up QuickSight. Behind the scenes, what we do is create a QuickSight folder with the same name as your project and also provision the same role and access credentials so you have the same access to that data as well. This takes away the steps of setting up the right permissions. Once you create any dashboards, those can be saved back to the project itself where you can add business metadata and add some context that makes the data more easily searchable by others. You can publish it to the SageMaker catalog and allow others to discover and request access to the dashboards as well.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1370.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1370)

In this way, we're trying to make it easier to find the right data you need, create dashboards off of it, and share those dashboards with others so that you can more quickly get to those business insights. 

### Tag-Based Access Control: Scaling Permissions Across the Lake House

Finally, I'm also going to talk about how we've been expanding support in the SageMaker Lake House. We previously supported tag-based access control for resources in the Glue data catalog, and we've since expanded that across S3 tables, Redshift data warehouses, and federated catalogs as well, such as SQL Server and PostgreSQL. What this allows data administrators and data stewards to do is define permissions via tag-based grants that are automatically inherited by resources, so you no longer have to specify permissions at the resource level. You can also create multiple Lake Formation tag expressions to grant combinations of tags to multiple users.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1420.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1420)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1450.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1450)

I'm going to walk through a bit more detail of how this works.  To start, you'll define Lake Formation tags, and you can assign these based on your team. As an example here, we have a tag with a key of "team" and values of "developer," "SRE," and "QA." You can also control who can actually create these tags and assign them. 

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1470.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1470)

Next, what you'll do is attach these tags to the resources. You can assign them to databases, tables, columns, as well as S3 tables and federated catalogs. As you can see here, our sales database isn't tagged, so none of these teams will have access to it. However, this on-call database is tagged to allow developers to have access to all the tables. 

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1490.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1490)

One of these tables allows read-only access to our QA engineers. It's a pre-production data table, and SREs have access to one of the columns. So you can get pretty granular with the access controls. Once those tags are assigned to the resources,  you can create policies for IAM principals such as your users and roles and scope those permissions based on tags so that as you scale your resourcesâ€”let's say add, remove, or delete resourcesâ€”you can do so

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1520.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1530.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1530)

without worrying about updating policies each time, as long as you're tagging the resources appropriately. This really allows you to scale across hundreds of thousands of resources.  This works in parallel with attribute-based access control. In tandem with attribute-based access control,  you can define a tag ontology that matches your users' attributes, such as your departments and the level of data sensitivity they can access. You can tag the IAM principals, and when the tag values match the same tags as resources, access can be granted or denied. Actions on certain resources can also be controlled with these tags.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1590.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1590)

These options show how we're providing more flexibility in the SageMaker Lake House to scale access permissions across multiple users. This is also helpful because as users change departments, leave the company, or join the company, you can now assign tags to users to control their access rather than having to make policy updates each and every time.  With that, I'll now pass it back to Shawn to talk about some of our more recent SageMaker launches. Thank you, Iris. I hope all of you folks are paying attention, as there will probably be a quiz at the end. Iris has brought you up to speed until literally this event, talking about all the launches so far. What I'm going to cover are things that have come out in the last few days with the new SageMaker platform.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1630.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1630)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1660.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1660)

### One-Click Onboarding: Getting Started with IAM-Based Domains

While there are many things to talk about here, I'm going to focus on four key improvements.  The first one is getting started faster with our new one-click onboarding. Following that on the far right is our new serverless notebook experience, which is an all-in-one notebook for developers to interact with their data. Below that is a purpose-built data agent specifically for SageMaker notebooks. And last but not least, we're always investing in performance at scale through our new Amazon Athena for Apache Spark engine.  With that, let's begin. I'm going to start off with getting started, right? Getting started with SageMaker Unified Studio has never been easier than with our new one-click onboarding. Our goal is to get customers into the product and working with their data in minutes.

So you may ask, how did we do that? What's the trick here? Well, we focused on the one thing that all AWS customers have in common. Can anyone guess? It's AWS IAM policies, roles, and permissions. If you're on AWS, you're definitely using it and probably built your data around accessing data through AWS IAM. So what we did was add the ability to reuse your existing IAM role. We take this role and automate the rest of the setup and configuration for SageMaker Unified Studio. By using that role, the Unified Studio now has the same access to data and resources that you had in that IAM role that you gave it. If it worked outside, then it'll definitely work because it's using the same IAM role.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1750.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1750)

This includes S3 buckets, S3 tables, Glue data catalog, and Lake Formation policies. All of these are accessible through the Unified Studio using the IAM role provided to it. Now, let me show you what I mean by this.  At the top, you'll see the Amazon SageMaker console landing page where you'll see a new getting started option. This is our new default getting started option, that orange button there. If they click it, you're actually going to see one setup page. While it's one full page, you really have one primary decision: choose your IAM role. This is how you determine what permissions and privileges the Unified Studio has access to.

You have two options. One is to auto-create a new role. If you have the right permissions to create a role, then we'll create one for you and give you default permissions to that account. Alternatively, you can use an existing role, something that you've already got working, something that's already been approved by your IT, and you can use that to set up your SageMaker environment. This will set up a new SageMaker environment, and it's appropriately called an IAM-based domain because you're using your IAM role to set it up and log in with it.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1810.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1810)

After a few minutes, you'll be taken into the product where you'll see our new updated visual design. When we said we were improving onboarding,  we didn't stop with just the setup process. We also want to streamline how you access the various tools and interfaces available in the product itself.

On the left-hand side, you'll see a navigation menu with all the various tools you have access to, whether it's general capabilities, notebook workflows, data analytics, AI and ML tools, as well as code developers which I mentioned earlier. All of these are now easily accessible through this left-hand navigation. In the middle, we've included quick actions for common tasks that customers take, including discovering ML models and exploring your data. For those users who are new to SageMaker and want to get started and understand what it can do, we've provided sample data with pre-built notebooks ready to go, so you can just click and try it out from there.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1870.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1870)

The important thing to take away from all this is that using the one-click experience and an IAM role, you didn't have to do additional setup to get access to your data, which means you can start working with it faster.  What do I mean by that? This is exploring the data inside of this without any additional setup. You can go in and see what S3 buckets are available. This is, for example, what's available based on the IAM role that I chose. You can see what's inside your Glue data catalog because that will all be available. You have S3 tables and things of that sort. There are so many more that you can explore as a result of having access to this data.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1910.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1910)

Now that we've had this core improvement to SageMaker Unified Studio, we said, "Hey, maybe others can benefit from this too." So as a bonus, we recognize that there were several other customers already using standalone AWS services  that could benefit from the same type of simple onboarding. Especially those using individual consoles like Redshift, Athena, or S3 tables. If you're using one of those, then it's an opportunity where you can launch the same one-click onboarding from any of these consoles. As you can see here, this is the Athena console, and when you launch it, it will also take you through the same one-click onboarding where you choose your IAM role.

The benefit here is that if you use the same IAM role that the console is using to access the data, so in this case, Athena is using an IAM role to query data, you can use that same IAM role to set up SageMaker Unified Studio. And guess what? SageMaker Studio can continue with your process. It can also make the same queries because it has access to the exact same data. All of this is available on Amazon Athena, Redshift, and Amazon S3 tables. We made it hopefully easier and faster for you to get onboarded, whether as an admin or as an end user.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/1980.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=1980)

### Serverless Notebook Experience and Purpose-Built Data Agent

Now let's talk about how we've also improved the developer experience within the product itself.  We took the learnings from our existing interfaces. You saw we had a whole range of interfaces available, and we identified three key areas that will really benefit customers, particularly those users who are new to this space and trying to learn about data, AI, and analytics and how they combine together.

One area of emphasis that we heard consistently was interactivity and visualization. Today, customers typically have to load additional libraries or, in some cases, additional libraries for simple visualizations, or in other cases, actually have to go to another tool to better understand and visualize their data. This takes time and effort and is often error-prone. The next part of this is supporting multiple languages. Supporting things like Python and SQL is not particularly new, so that's something people expect. The challenge is actually as you're interacting with different languages, using the language that's most effective for your tasks. Often you have a problem where you're trying to bring data from SQL into Python and vice versa, and transitioning that data is actually more difficult than you think. So the challenge is not supporting multiple languages, but actually transitioning the data between the languages so it can be processed and used by them.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2090.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2090)

The last thing I want to call out is how customers often get started with their development experience. Many customers start off with a local environment where they've set up themselves, set up the data access, and start processing inside of that. Then when they want to deal with larger and larger datasets, they have to connect it to a new engine and set up the infrastructure and provision it all upfront. We looked at this as an area we could improve and make easier for our customers. Which is why we're happy to introduce a new notebook for Amazon SageMaker. This is a fully serverless web-based  workspace designed for users to act on their data using an immersive interactive experience. For the end user, it supports Python and SQL natively, right out of the box, plus it has a built-in AI agent so you can use natural language to develop your code.

But the main thing to call out is that because it's serverless, you don't have to pre-provision it. You don't have to worry about managing any of it or tuning it. It automatically starts as soon as you launch it and automatically scales to the environment based on the task that you need. So there's nothing you have to worry about from there.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2150.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2150)

Since it's part of the IAM-based domains, it has access to the same data assets that are available, and it has built-in visualization libraries so you don't have to worry about importing additional libraries. Here's a classic example of the new notebook experience.  As you can see, it uses a popular layout. On the left-hand side, you'll see a data explorer where you can access and view all the different datasets that you have. In the center, you'll see the popular cell-based interface where you can choose to write SQL on the top or Python on the bottom, all within the same notebook.

In this example, we're using SQL to query and get some results inside of that. Then at the bottom, you're going to use that data that you queried in SQL to process with Python. I mentioned that one of the challenges between these two is not just having SQL and Python support but actually making sure the data gets passed properly. What you see here on the arrow is that the notebook will automatically store the dataset that you queried and make it available in a variable so that you can then reference it without having to do any additional steps through the rest of the cells. This is done automatically.

In this way, it makes it very easy to switch between Python and SQL, whichever language is best suited for the task. Under the covers of all this is the new Amazon Athena for Apache Spark engine, which is scaling out to support all those types of data as well as process the code here. I did mention there's going to be an AI agent for this notebook, so let me talk about that next. But before I do, I just want to say at this point, I think everyone's expecting a chat interface for an agent. That's pretty much a standard set. We didn't want to do just that.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2240.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2240)

We said  we want to go beyond just a standard chat interface and really have a purpose-built, data agent that is context aware. So we're introducing Amazon SageMaker data agent available in that new notebook. This agent uses model context protocols, or MCP, to explore the metadata stored inside your data catalog and understand what type of assets you have, and use that to help generate the code. When it is generating SQL and Python code, it can be very specific because it's actually using the metadata inside your data catalog and can find out what columns and attributes you're focusing on and what information is associated with it.

In addition to this, we expanded its ability. Instead of just generating individual cells worth of code, it actually can break down sophisticated end-to-end workflows into individual manageable steps. It can break down one whole workflow into multiple steps that you can inspect and review individually. Ultimately, our goal with all this is to really make the agent much more aware of the data and of the task that's completing, so it can give you targeted guidance on how to proceed in your end-to-end solution.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2330.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2330)

### Live Demo: Exploring Customer Lifetime Value with the New Notebook and AI Agent

The best way for me to show you this is not through more slides, but through a demonstration, which I'm going to switch to right now. We're going to start off from the same  Amazon SageMaker console page. We're going to start off from this point here and build a new environment from scratch. Obviously, this demo is focused on new things, but if you have an existing SageMaker domain, you can also access it from here as well. Let's click on Get started.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2350.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2350)

Once you click on Get started,  this is that single page for configuration. The main thing to focus on is what IAM role you want to use to set this up. By default, it'll create a new one if you have the permissions, and we'll use that. I'll show you the default setup in this example here, but once you've done that, that's really the main decision you have to make. The rest of the choices are optional things that you can proceed with. You click Set up, and that's it.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2390.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2390)

Under the covers, SageMaker Unified Studio is automating the rest of the setup. It's provisioning whatever compute is necessary, setting up the environment, setting up the permissions necessary, and pretty soon it'll take you into the new experience. And this is what you'll have access to.  I mentioned on the left-hand side you have the various tools available to you, and for today's demonstration, I'm going to focus on a notebook, the new notebook capability that we just launched.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2410.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2420.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2420)

Now in this example, I'm going to show you an exploratory case where I'm starting from scratch. I don't have anything preplanned.  I don't have any data sets I had already in mind. I'm actually going to show you the full life cycle of exploring data, analyzing and understanding what's there. I'm not going to use any  of the sample data notebooks. I'm just going to launch something new. So here's where we launched it. This is that experience of brand new where you have something new to try out. You can see the cells are available for Python, SQL, and markdown. From there, as I mentioned, since this is new, using the new AI-based domains, probably the first thing that I'm going to want to do is actually take a look at what data I have access to. So I'm going to navigate over to the Data Explorer and understand what's inside of that.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2470.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2470)

So the first look is to check if I have any connections. I don't have any. This is a new environment, but I do have my AWS data catalog. This is my AWS Glue data catalog, and I can see some of these things that are available on my account that I could take a look at. That's great. Now, I mentioned I don't actually know what's inside here. So one way I could actually explore this is I could use the cells and type SQL  and try to query and understand what that is. But since we have a new data agent that is context aware and can understand what's inside my data catalog, why don't I ask it to help me figure out what's going on?

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2490.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2490)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2510.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2520.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2520)

With that, I've launched a new data agent here.  It comes with a pre-canned set of prompts up front. As you can see, you can ask what this agent does, you can analyze other things available to it. But one of the pre-canned questions is, "Hey, list to me the tables that are available inside my sample database." So I'm just going to use that and see what kind of results it gets me. I just clicked it. It's using that prompt to generate some results.  Pretty soon, it's actually going to be able to understand what's in my data catalog, go through and explore the individual parts of it, and now it's generating  a summary which covered three areas. It had three core data sets, including a weather data set and this digital wallet LTV. Well, let me take a look at it. Let me look at the summary that it provided for that. It says it's a digital wallet customer lifetime value dataset with about twenty columns.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2550.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2560.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2560)

Let's say this is the one I'm interested in. I've not seen it before. I don't know all that much about it. Why don't I try to see if I can explore and understand it?  The fastest way for me to do that is probably going to be SQL, but I don't actually know the syntax of the exact table I'm looking for, so I'm just going to ask the data agent, "Hey, can you get me a sample  of this and let me take a look at it?" Okay. While it's doing that, I'm actually going to look some more at the summary and I realized, "Oh wow, it's got several various metrics inside of that. LTV has transaction metrics, engagement metrics, satisfaction scores. All this is related to the long-term value of a customer based on their interactions with your company."

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2590.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2590)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2610.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2610)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2630.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2630)

Okay, great. Here it is. It's generating a SQL query based on what I requested.  Right? And now what I'm going to do is I'm going to let it generate and I'm going to run the SQL query that it created based on the data set. It's running that again. Under the covers, this is all done serverless, so all that setup, all the provisioning under the covers was done automatically and it generated a quick result for me so I could  see the top ten rows. I'm going to scan through it. It's made it easy to understand the distribution of it, go across the various columns and rows available here. Just taking a look and seeing what's interesting to me. So you're going to see here some interesting support tickets, issue resolution. Oh wait, there's a customer satisfaction score.  So I actually am interested in this customer satisfaction score and I'm thinking I have a hypothesis that customer satisfaction and long-term value are probably correlated. So I'd like to understand what that is and it's really hard for me to do it from this tabular mode.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2670.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2670)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2700.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2700)

So I'm thinking maybe I can ask the agent to help me and help me visualize this in different ways. It's a hypothesis. These two things are related. Let's have it visualize the impact between the customer satisfaction score and long-term value. Okay. So from there I'm learning. I'm just typing it in.  And in this case I'm just going to let it generate options for me in terms of what that is. But before I do though, I wanted to run on the full data set, so instead of just a limit of ten, I go back and edit the original data set here and now I'm generating against the entire data set. Then I kicked off that analysis for comparing the two. Okay, so while that's working and trying to build those visualizations,  if you recall I mentioned that that output, that full output is now available as a variable so that way that SQL output that I just generated can be used in the rest of the cells being generated.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2720.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2720)

Here's the sample code that's being generated based on that.  It's also generating some key insights on the right-hand side. What it's doing is suggesting different ways for me to visualize the data. I didn't specify how I want to understand the data, and it didn't necessarily know, so it's providing me different options. I can just rely on the options and see if that's something useful to me and if it gives me any indication of what the data is. I'm going to hit run and accept this. Let's take a look at the results.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2740.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2740)

It's generated multiple plot points for me based on the customer satisfaction score and the overall long-term value. Looking at it, it's okay. It has some data and some correlation. The red line in the first left-hand side box is the overall LTV compared to the customer satisfaction score. It shows some correlation, but it's not particularly strong. When I look at the rest of the graphs, I'm not seeing anything that really stands out. There's some correlation between the two, but probably nothing that would make me say that is the one correlating feature towards LTV. 

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2800.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2800)

That makes me think maybe it's not just one factor causing this. Maybe it's going to be multiple factors to consider. I'm going to look at some of the summary of what's being generated here and see if there's anything else that stands out.  I don't see anything standing out based on what's being generated so far. My original hypothesis probably isn't so great, and I want to expand how I want to analyze this data. We had mentioned there were 20 other columns inside of that dataset. I had just picked one at random and tried to visualize it, and it didn't work out. Maybe I'm going to leverage the power of AI to actually look at the entire dataset and find other factors and features that are going to impact my long-term value.

I'm going to leverage the data agent again, but I'm going to give it something more complex. I'm going to ask it to put together a model that will analyze all the data, find all the various features that are part of that, and train that model to predict long-term value based on the multiple features available in the dataset. To do this, I'm going to use the same dataset I had in the beginning. I'll use about 80 percent of it to train a new linear regression model and 20 percent of it to test it. I'm going to see what kind of results I get. There are three things I want to understand. First, what are the features that are relevant for my long-term value? Second, how well does this linear regression model match up to the actual data itself?

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2890.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2890)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2910.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2910)

I'm starting to work on this and build on this linear regression, taking into full context of what's going on. As it's working through this, it starts to generate the code into multiple cells, and now comes a multi-step  process where we're generating multiple steps: data preparation, model deployment, model training, all the way through this entire thing. I'm going to quickly scan through this one here. It's clearly multiple steps that are broken out. So let's walk through each one and try to quickly understand what it's doing. This is fetching that model. It's generated the code associated with that. 

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2920.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2920)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2930.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2930)

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2940.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2940)

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2950.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2950)

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/2960.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=2960)

From there, it's actually going to split the datasets that we need into the training datasets as well as the testing sets. It's finding what some of the features are available as part of this.  Generally, this is looking pretty good for that. It's going to apply the linear regression and then execute a training job for that, and finally it's going to visualize the end result. I just quickly scanned it, and it looks okay.  I'm going to accept and run this suggestion. While it's running, you can see the results.  It's actually gone through and identified a set of features. It's figured out the 80/20 split in terms of datasets. One interesting note is that because of this, it's actually identified 5 more  features that I had not considered earlier. So now there are 23 features to consider. It's going to train that model based on that initial training dataset,  and then it's going to generate the end result. We're going to see how well this model does, how well this new linear regression model does against the test dataset.

At this point, these are visualizations that it suggested, but notice I didn't have to specify any of this. It did all these multiple steps of getting the model, deploying it, separating out the datasets, training it, and then eventually looking at the results here. It did it all on its own with me just accepting the results. Of course, I could have gone back and tweaked any of these things, but for the sake of this demo, I'll take the default suggestions. What it's showing me is that this linear regression model is actually not bad.

It has a relative correlation between LTV and predicted LTV. It has some correlation at the lower end, but the higher end is not matching up as much. Not bad for a great initial try. However, I'm still curious about my initial hypothesis that it was customer satisfaction scores. So what exactly were the key factors to be associated with this?

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/3030.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=3030)

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/3040.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=3040)

In addition to this part here, I actually want to understand what was being generated and what  are the main factors that affect long-term value. So if we keep going down, what we'll see is that we can actually explore some of the factors that were available to this model.  This is probably going to be the most immediate insight for me just based on this demonstration. Income level is the most impactful towards LTV. This makes sense. I didn't see it obviously, but it does make sense in that regard, considering how much value has come in.

On top of that, my customer satisfaction score, while impactful, is nowhere close to as impactful as the overall income level. However, something that did pop out is support tickets raised. As support tickets are raised, LTV has been decreased overall. Now this is kind of the end of this demonstration, so I'm going to recap what we saw in this.

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/3080.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=3080)

We saw that  starting from scratch, you built a brand new SageMaker Unified Studio experience using the IM-based domains. Then you were able to use the new notebook interface without provisioning any infrastructure. It was all built out. You were able to plan together, discover data, and plan an end-to-end flow which deployed models, trained them, and executed a result.

With that, I'm going to switch back to the presentation and just summarize this part. These are new capabilities that were just generally available recently, and we had the privilege of having several beta customers try out our product. Thankfully, they were able to give us some feedback. In this case here, Sachin Mittal from Deloitte was able to apply these new capabilities to their ML lifecycle, and really they saw faster delivery and experimentation because it was a developer-focused environment that more and more of their users felt comfortable with.

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/3150.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=3150)

That's what hopefully you will also benefit from. We encourage you to try it out now that it's much easier to get started  with SageMaker Unified Studio. And with that, I'm going to pass back to Iris, who's going to summarize and close out the session.

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/3170.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=3170)

### Closing Remarks: Commitment to Customer-Driven Innovation

I know we've covered a lot today, but hopefully you've come  away with some ideas of exciting things that you might want to try out. If there's anything to take away from this session, we hope it's that this product really is about you, our customer. Since going generally available in March, we've been continuously listening to and hearing feedback, and as a result of that feedback, we've been doubling down on ease of use and bringing consolidated capabilities into the SageMaker Unified Studio. We're also really committed to continuing to innovate and bring new experiences to SageMaker, as you can see with the data notebook and data agent, so not just bringing existing capabilities.

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ee2dc10d002205a8/3220.jpg)](https://www.youtube.com/watch?v=V2wFP-qzahY&t=3220)

With that, we're at the end of our time today. Thank you so much for coming to the  session all the way in Mandalay Bay. I know it's really far, but I really appreciate you taking the time.


----

; This article is entirely auto-generated using Amazon Bedrock.
