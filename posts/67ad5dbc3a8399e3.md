---
title: 'AWS re:Invent 2025 - Building Prudentialâ€™s microagent platform with MCP and A2A on AWS (IND3302)'
published: true
description: 'In this video, Moon Kim from AWS GenAI Innovation Center, along with Rohit Kapa and Subir Das from Prudential, discuss building Prudential''s MicroAgent Platform using MCP and A2A protocols. They address challenges FSI enterprises face when scaling AI agents, explaining how Prudential deployed a Life Insurance Advisor AI Assistant serving over 100,000 advisors. The platform uses orchestration agents with specialized subagents for quick quotes, forms, products, and book of business management, reducing turnaround time from 6-8 weeks to 3-4 weeks. Key architectural components include microservices-based design, context engineering frameworks, MCP gateway, A2A protocol for inter-agent communication, and centralized governance. The team emphasizes three critical takeaways: scaling with time requires modularity, ensuring fungibility through separated concerns, and implementing a centralized platform with core and distributed layers to prevent silos while enabling plug-and-play AI solutions across business units.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Building Prudentialâ€™s microagent platform with MCP and A2A on AWS (IND3302)**

> In this video, Moon Kim from AWS GenAI Innovation Center, along with Rohit Kapa and Subir Das from Prudential, discuss building Prudential's MicroAgent Platform using MCP and A2A protocols. They address challenges FSI enterprises face when scaling AI agents, explaining how Prudential deployed a Life Insurance Advisor AI Assistant serving over 100,000 advisors. The platform uses orchestration agents with specialized subagents for quick quotes, forms, products, and book of business management, reducing turnaround time from 6-8 weeks to 3-4 weeks. Key architectural components include microservices-based design, context engineering frameworks, MCP gateway, A2A protocol for inter-agent communication, and centralized governance. The team emphasizes three critical takeaways: scaling with time requires modularity, ensuring fungibility through separated concerns, and implementing a centralized platform with core and distributed layers to prevent silos while enabling plug-and-play AI solutions across business units.

{% youtube https://www.youtube.com/watch?v=9UTzSY40e9I %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/0.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=0)

### Introduction: Challenges in FSI Industry and the Need for Modular AI Architecture

 Hello everyone. Thanks for coming. I hope you're off to a great start this week. My name is Moon Kim, and I'm a Lead Machine Learning Engineer at AWS GenAI Innovation Center. Today we're going to be talking about building Prudential's MicroAgent Platform with MCP and A2A. We're happy to be here. I've been very fortunate to co-build this platform with Rohit and Subir, and we're happy to share the details.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/40.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=40)

 I'll kick us off with common challenges that our customers in the financial services and insurance industry are going through and the approaches they are taking to address them. Then I'll hand it over to Rohit and Subir, and they will talk about Prudential and what it means to manage their AI initiatives across all lines of businesses. They will also talk about a specific use case called the Life Insurance Advisor AI Assistant and how they built the platform around it, which can support many more use cases. Then we will discuss impact and future directions and close it off with some takeaways.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/90.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=90)

 Enterprises, especially within the FSI industry, have huge ambitions, and they're only growing. There are so many opportunities with GenAI: intelligent document processing, analyzing data using agents, and simplifying workflows for your customers. These are only the beginning. Some enterprises are speculating to have thousands of agents as part of their business. Thanks to the market, they want to deliver GenAI solutions with urgency. They want to be at the top of the game and naturally be part of it.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/130.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=130)

 Naturally, developers and teams want flexibility and autonomy so they can bring POCs to production quickly. All of this has to align with one enterprise mission, meaning they have business goals, constraints, and branding to protect. You want to make sure that all of your GenAI solutions are within governance.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/150.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=150)

 What a lot of our customers end up doing is building multiple solutions and multiple agent tech solutions within one repo, which might be the same repo that you developed two to three years ago for a RAG application. But this starts to break down when there are too many agents, and there are such things as too many of something.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/190.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=190)

 Agent implementations become entangled, and some components are shared, so the boundaries of ownership get blurred. I've seen deployments take days instead of minutes because making one line of change might trigger security alerts from multiple teams. It also makes it difficult to properly manage sensitive data like PII and PHI because each of them might require special logic to treat the data properly. On top of these, ensuring that your use cases and platform scale with time is critical for long-term success.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/230.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=230)

 Your solutions need to adapt to the latest GenAI trends. There are always new technologies coming out every week, and you want to see if it fits your solution and if it does, you want to be able to apply them. You need to maintain optimal performance by updating your models, their prompts, and sometimes the implementation itself so you can adopt reasoning capabilities and more.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/270.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=270)

 Naturally, the architecture shift becomes inevitable. We want to take some components out and separate the concerns for business logic, runtime execution, governance, and scaling. That's why modularity is key.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/280.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=280)

 Modularity allows you to separate concerns and easily swap components when necessary. Identifying key components like runtime, memory, code interpreter, and browser tool will go a long way. This will allow you to adapt and move quickly in the industry.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/310.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=310)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/340.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=340)

Growth also needs to come with governance so that you can stabilize  using a centralized platform. Enterprises need to manage the access control of their users and agents for their resources. We also need to govern and observe the solutions end to end to make sure that your developers and AI agents are following their standards and policies. This is where folks like myself from AWS GenAI Innovation Center  came in to help our customers, especially Prudential, build their platform that is scalable and future ready. We start by identifying the problem and sharing some best practices, building a solution and bringing it to production within their environment.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/370.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=370)

### Prudential's Scale and AI Opportunities Across Business Functions

So with that said, I'm going to hand it over to Rohit who's going to talk about Prudential and how they scale their AI agent technology platform.  Hi, everyone. I'm Rohit Kapa, VP of Data Science at Prudential. For those of you who don't know what Prudential is, Prudential Financial is an American financial services company that offers insurance, retirement, and investment management. We cater to individual and institutional businesses across the globe and operate in 40 plus countries.

Naturally, for an enterprise like Prudential of that scale and size, you expect customers to have different expectations on different things like personalization, speed, trust, and superior customer service, among many others. That's the high bar that our customers have and the typical expectation set on a large financial services company like Prudential. With that said, let's take a deeper look into the businesses that we have and how AI is helping in addressing some of these challenges.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/440.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=440)

I'll try to relate back to what was said earlier about what typical FSI enterprises are facing, and within Prudential's lens, I will try to share a view on what that means from AI use cases and scaling those AI use cases. Overall, the slide is a matrix that represents different challenges and the opportunities that AI can provide.  For Prudential, you have three different business units: retirement, group insurance, and life insurance. I'm not talking about investment management here for this particular session, but these are the three primary components within the US business.

Each of these businesses, if you look into the business functions, they are probably similar in nature. All these business units have distribution and sales, underwriting and risk, customer and claims service, and then product development and marketing. Although every business unit has their own innovative products or different customer journeys, every one of them has a similar kind of challenge. For distribution and sales, you have fragmented advisor workflows and limited personalization. For underwriting and risk, you have manual, slow underwriting and rigid risk models. For customer service and claims, you have high servicing costs and claim processes that usually take longer. For product development, you have slow innovation cycles, and for marketing, you have generic campaigns and low conversion rates.

These are not only potential challenges. You can relate them to typical financial services challenges in general across these different functions. From an AI outcome or goal standpoint, where businesses are looking forward, how can AI create hyper-personalized advice at scale as a sales enablement tool? If you look at the impact perspective, these AI solutions, if you try to build them for each of these business functions, they directly impact the people who are working in these organizations.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/580.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=580)

For example, for distribution and sales, if you want to build hyper-personalized advice, better sales enablement processes, or next-best-action models, they directly impact your advisors and brokers. Similarly, for real-time risk assessment, if you want to build it, it directly impacts your underwriters and risk analysts. From a customer service and claims perspective, if AI can help in creating self-serviceable processes or automated claims triage, these directly impact your call center agents and claims specialists. If you think of product development, if you try to create AI-driven market insights or personalized riders or solutions that you build inside your products,  these directly impact your product development and product management folks, or these actually impact actuaries. Same with marketing as well.

So what you're seeing here is every business function has a set of challenges that can be solved using AI, and that AI goal or outcome has a direct impact on the people who are working within each of these functions. That's the business strategy. That's where we are seeing the value.

And that's where we are seeing the industry heading towards. Today's discussion, I would like to share one such use case around distribution and sales, and how we are actually using an AI financial advisor for life insurance business as an example. We are trying to help our advisors in delivering personalized, better sales enablement and a better process compared to where they are today.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/650.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=650)

Although Agentic AI can deliver all of these things overall, we are hoping that it can deliver all of these things from a value standpoint. The biggest question is: can we scale with time? Can we actually scale with these many use cases? Are these realistic? Those are bigger questions, but I'll just deep dive into the use  case straightforwardly.

### The Life Insurance Advisor Challenge: Fragmented Systems and Manual Processes

When I say life insurance advisory assistant, one thing I would like to note here is from an advisor lens perspective. If you break down what an advisor does on a day-to-day basis from a life insurance carrier perspective, an advisor typically engages in client engagement for prospective clients. They need to do need assessment and solution design. They need to identify which kind of product is more suitable, show illustrations to the clients, and try to sell. For example, they might say this kind of product might be good for a 30-year-old or a 50-year-old, and they might probably look into a separate IUL kind of product.

Different product presentations and illustrations showing how much benefit clients acquire need to be shared with the client. Then if the client agrees, the advisor typically pushes them to application and underwriting support where they have to fill an application, apply for life insurance, get the policy delivered, and once the policy is delivered, they have to do service and follow-ups for the next 10, 15, or 20 years of the policy.

If you look into this whole end-to-end loop of what an advisor does on a daily basis, there are five or six different steps. Each step, the advisor has to log into one particular system or has to deal with one particular IT system in the back end. Overall, what is happening is from an advisor lens perspective, if they have to do this for one carrier, and if they have five or six carriers that an advisor is working with, they have to interact with 50 or 60 different IT systems.

This is not a real value of an advisor, and it is probably breaking the value chain of what a financial advisor is supposed to do. Ideally, they should be providing good advice or probably helping clients in delivering something. What they ended up doing is tangling with multiple backend operations which may not help them in a better way. This involves looking into all different tools and systems, and advisors are required to maintain long-term interaction with the clients for 10, 15, or 20 years on the life of the policy.

What is happening in the process is that for the first client they started with 10 years ago, they probably forget the context, like what they actually talked about 10 years ago, why they gave that particular advice, and what is happening on a periodic review basis. These are the problems that an adviser is typically facing, and I will share with you how we are trying to solve this from an AI assistant standpoint.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/790.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=790)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/810.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=810)

 This is a view in terms of sharing how we build the advisory assistant, the financial advisory assistant, and how it is transforming the distribution and sales process. On the left side of the screen, what you're seeing here is the original experience today. When I say today, it used to be the experience in the past where an advisor  has to actually log into different systems for different things.

On the very left, what you're seeing is a manual process on QuickCodes. QuickCode is more like an informal process where, let's say people reach out to advisors, or advisers actually reach out to carriers saying, I have a prospective client, they have a medical condition like diabetes, they have hypertension, or they have cancer. Can you offer me what is the best quote? They typically give medical information in the request, and what happens in the back end is an underwriter from a carrier perspective actually takes one or two days and gets back to the advisor in one or two days with the answer.

This is because the volume of these quotes is high, and underwriters are stacked with a lot of requests coming from advisors asking for multiple requests. This is a big challenge with one to two days delay happening in the process. The same thing applies if you look into forms or getting product-related information. For example, if an advisor is looking for a customer to fill up an application and they expect the client to fill like a 1030 exchange for a tax purpose benefit, or they want to get some kind of rider benefit or some kind of disclosure sign, they need to find this form.

They need to either call some carrier system in the back end like customer care service, be on call for 10 minutes to get what is the right form, or do the search in the manual system on their own and try to figure out if this will work or not. The same thing is with product-related information. Usually, insurance products are complicated in nature. If they want to know if a particular product feature can be applied to a particular client or not, they need to have a lot of knowledge.

Advisors must request a product specialist, which takes 10 to 15 minutes even in the best circumstances. Looking at the inefficiencies on the left side of the space, we can see significant challenges that advisors face today. What we have done is replicate those processes with AI-driven systems on the right-hand side.

### Building the Multi-Agent Advisory Assistant: From Quick Quotes to Book of Business

With a multi-agent AI system, we are delivering all of this information instantly to our financial advisors on the go. We have built an advisor agent that provides a chat experience with natural language-driven and rich, context-driven conversation. Advisors can ask questions as a one-stop resource and request different kinds of interactions they typically deal with. The foundational product we have built includes an orchestration agent, which serves as a single point of entry for advisors.

The orchestration agent takes different sorts of inputs from advisors, understands intent, and routes requests to multiple subagents based on context. It is non-deterministic in nature, meaning it adapts to the specific situation. The subagents in the background include the quick quote agent, forms agent, product agent, illustration agent, and book of business agent. Each agent is a specialized entity trained to deliver accurate responses back to the advisor.

I would like to highlight the quick quote process, as it represents how we use AI to replicate an underwriter and provide instant decisions to financial advisors. When an advisor receives a request such as "I have a client with diabetes and cancer, can you give me a medical quote?" they can use this chatbot. The chatbot is intelligent enough to ask follow-up questions based on our underwriting documents. It has been trained on several hundred documents, each containing detailed descriptions on how conditions should be risk-rated from a risk perspective.

This approach correlates to risk assessment processes across financial and underwriting operations at different insurance companies. Every underwriter has manuals of this kind, and we have trained those manuals to answer these questions automatically. We have built training and validation pipelines that automatically perform prompt optimization, validation, and determine what level of information is needed to make a decision. When an advisor sitting in front of a client says their client has diabetes, the bot immediately asks follow-up questions about A1C values, blood sugar levels, sugar ratings, and medications the client is taking.

The bot provides an assessment of the risk level from a diabetes perspective. Similarly, if the client has cancer, the bot asks follow-up questions about remission dates, treatments performed, chemotherapy, and the cancer stage. The bot is intelligent enough to ask these follow-up questions back and forth, gathering the necessary information from the advisor to make the appropriate decision. The forms agent is also built on hundreds of different forms across various areas, allowing advisors to get forms on the go.

The forms agent can ask follow-up questions about the type of transaction and the specific form needed for a particular state. The product agent works similarly as a smart search feature where advisors can ask deeper questions from a context standpoint. They can inquire about what products are available for their clients, what features exist, and whether they can apply specific features to their clients. The agent returns responses based on these queries.

Overall, we have built an orchestration agent with subagents, and context is shared between them. At the agent level, we have established guardrails from a functional standpoint at each agent and at the orchestration agent level. These guardrails handle invalid queries, subqueries, and follow-ups. With respect to the illustration and book of business agents, think of these as APIs being served through agents.

Your book of business typically means an advisor's book, which includes hundreds of thousands of policies they have placed in the past. These agents can provide information on next best action, what a particular policy would look like, and what actions advisors need to take on a particular policy in their book.

It gets really hard for advisors to actually manage this whole thing, so we are using agents to help them in this process. Overall, what we are seeing is a multi-agent system that is helping advisors. We are live today with more than 100,000 advisors actually using this, and it's more like a foundation structure that we have built where we can actually add more and more agents on top of this.

This is one particular example of how we are using AI and how we are actually transforming our sales and distribution within life insurance. What we are expecting is that, in the previous slide, you have seen a bigger picture of where we want to drive AI across different business functions. That's our goal, and my colleagues will be here to explain how we are trying to get towards that goal and what is needed from a platform or what is needed from a solution perspective to actually get there.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1280.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1280)

### Deployment Challenges: Scaling to 100,000 Users with Secure Microservices Architecture

Good evening, everyone. I'm Subir Das, Director of Machine Learning Engineer. As you have heard, Rohit explained the use case that is already developed here and the model is ready. Once the model is ready and the business accepts the model, the biggest challenge that we faced  is how to deploy the model in production and at the same point in time, scale with time.

The deployed model should be able to scale up to 100,000 users. It should be able to support intent-driven orchestration. The agents that are there and the components within the agent should be able to be reusable, and at the same time, the entire solution that gets deployed should be secure as well as compliant. These were certain challenges that came from this particular approved model.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1330.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1330)

In addition to this deployment model, we also faced some challenges on how to scale for other solutions in order to make sure that this is reusable across the entire enterprise.  This is our microservices architecture, but before I dig deep into it, let me explain something about scaling in time. Scaling in time means that in the generative AI space, the information and frameworks are changing at a very rapid speed.

In order for the system to address correctness, completeness, and accuracy of a particular system, and at the same point in time, we would like to take advantage of those enhancements and frameworks, whether from a prompt management standpoint, context engineering standpoint, SDKs, pipelines, or anything else that is coming up. That is basically the problem of scaling with time. Along with that, we also saw something from other teams about how to democratize the entire solution development so that the development and deployment of these various agentic solutions can be given to other teams based on minimal touchpoints with the platform.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1420.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1420)

Definitely, we would like to standardize the frameworks, the pipelines, the tooling, and the tech stack that is used there. With these challenges, we reached out to AWS Innovation Center and we came up with a microservices-based architecture which you see on the slide.  In the slide right here, you can see the user interacts with the UI app where SSO authentication happens based on the user's home access level control.

Once the user is authenticated, a secure token is generated and that particular token is passed to the agent layer. The agent layer revalidates the token. Once the token is validated, it generates a context-specific window ID and maps it back to the secure token. These two pairings are passed to the orchestration agent, and the orchestration agent builds that entire thing and uses that particular pair of context ID in order to maintain session and context engineering for the entire agentic discussion purpose.

The same context ID as well as the secure ID is passed to the form agents and a particular agent, whether it's forms, products, or others, uses the LLM gateway, which is another secure way of accessing the LLM in order to get a response. If an agent wants to use knowledge management, it uses the knowledge management system to get the relevant documents and generate its response back to the user. While all this is going on and this entire system is deployed to scale up to 100,000 users,

one of the pieces that we have seen here is that the context engineering around this 100,000 user base, which is paired with a secure token and the context window ID becomes very challenging. What happens is that models in a typical sense sometimes drop performance for unknown reasons, and debugging this context engineering framework in order to separate it out and find out exactly why it happened is challenging in this environment. We will talk about more in the next slides, but another component that I want to add is that while all these things are going on, we are actively developing and adding more agents and adding support for MCP-based tool access as well as API access for the agent layer.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1570.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1570)

### Internal Agent Architecture and Platform-Based Approach with MCP and A2A

We are also adding support for the agent layer to have monitoring and observability. We are also adding support for the agent layer to have the evaluation framework attached to it. This is our internal  orchestration of our architecture of an agent. The previous diagram was the deployment architecture of the entire LM system. This basically boils down to a specific agent, how it is architected and internally how it is sharing the data across different segments of an agent.

You can see that in a particular region on the left-hand side, it accesses the data plane and then uses MCP as well as the React framework. It uses the memory management for short-term memory as well as long-term memory access and the A2A protocol in order to move from one agent to another agent. If this particular agentic system needs to transition to and access some other different agentic systems like PlanProvision, which is a retirement multi-agent system, and IDP, which is another multi-agent system, it uses the A2A protocol.

Along with this, you will see that the context engineering framework is under active development. This is based on the ACE framework which came out recently, and we are actively developing on this to see exactly if we can enhance the context engineering that is needed in order for us to scale at this level. In order for us to maintain such complexity as well as scale with time and implementation of MCP and A2A, the only solution that came into our place is that we have to take a platform-based approach.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1670.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1670)

That is what we built: our agentic platform.  You can see that traditional model space is being catered by the SageMaker inference, which the agent uses via an MCP. On the GenAI side, you will see GenAI which has a vector store available to it for knowledge management system. It also has a Bedrock-based agent and an LLM gateway and is actively developing a management and evaluation framework for the platform for its enterprise needs.

The platform uses enterprise services, be it for agent ops or for GitHub actions. Basically the entire DevSecOps is being available through the enterprise services. For its data needs, it relies on enterprise data and for its infrastructure and tech stack, it uses the entire AWS system. The platform is predominantly used by data scientists as well as machine learning engineers for developing and deploying models, whereas from a consumer standpoint, it is consumed by business users and apps.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1730.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1730)

Now that we have built the platform and taken a platform-based approach,  I would like to share a few benefits that we have seen from taking this approach. One of the benefits that we have seen directly is that since we have a platform-based approach, we are able to ensure that responsible AI is engaged at that level. That means anything and everything that is running on top of it can be going through this responsible AI framework.

At the same time, the time to value, which plays an important role in benefit realization, can also be achieved. What happens is that if you develop a multi-agent system and deploy it bespoke, you will not be able to share the common layers. That means every time you have to rebuild and you spend more time in rebuilding the common layers. However, platform-based services allows us to share the common layer piece.

From a monitoring and agent monitoring and evaluation standpoint, we are evaluating and deploying something which basically helps us to measure the entire agent span as well as agent traceability and observability into it. With this, I would like to pass it to Rohit again in order to share some business outcomes and lessons that we have learned.

### Business Outcomes: Reduced Turnaround Time and Standardized Solutions

Now that Suvi has explained why we are building a platform, you've already seen where our business ambitions are, and you've already seen how one use case has evolved into different use cases and why a platform-based structure would help us get there, and what the general benefits of an agency care platform are from a common solution standpoint.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1830.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1830)



[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1840.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1840)

I would like to highlight some of the business outcomes that we have seen from this approach, and later, I would dive deeper into where we are heading from a future standpoint.  Overall from a business standpoint, we have observed that benefit realization is happening. It is happening because we are able to reduce some of the key challenges we are facing across turnaround time and the time to actually get business value generated from an AI use case itself. We have reduced the turnaround time from 6 to 8 weeks to 3 to 4 weeks now.

What is happening is that once we have built a foundational advisory assistant system, we are able to add multiple agents on top of this and deploy into production in 4 to 5 weeks' time from both development and deployment standpoints. Some requests that are coming from advisors, such as wanting to add a particular product embedded inside product-related agents, we are able to deliver quickly and deploy them as stand-alone solutions. This is helping us from a turnaround time perspective and our ability to integrate with existing IT applications or workflows.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1910.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1910)

Similarly, on standardized solutions, we have observed another benefit where we are able to scale from one use case to multiple use cases now. What we are seeing is that if we have built one particular use case in one particular business area, we can now reuse this development in the same way across other business units.  Another benefit from this approach is that we are reducing tech debt. Many components within multi-agent systems are changing rapidly in the market. We see new context frameworks arising, new A2A frameworks arising, and new SDKs arriving. All of these things are changing very drastically, and it is not easy for us to keep up with those paces. Having a standardized solution set from a platform standpoint is helping us upgrade for longer runs. For example, if there is a new model released or if there is a new change in observability-related systems, we are able to keep up with those changes.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/1970.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=1970)

Another thing we are observing is business feedback.  We are able to incorporate business feedback very well early in the game. Earlier, what used to happen is that data scientists or engineers building the systems would focus not only on building agents but also on surrounding components, such as how to handle multi-context and how to handle the system moving from development environment to stage or UAT environment. They would usually see many issues in terms of how they move from dev to stage or UAT, and things would usually start breaking apart when you add scale into the context picture. A lot of LLM performance used to drop.

Now with many of these standardized solutions, we are able to dig deeper and get insight into tracing or debugging a particular issue, understanding the chain of prompts from an observability standpoint, and monitoring your solution once it goes into production. These things are becoming easier. We are able to incorporate business feedback quickly, which is helping us gain business trust in AI systems. In practicality, this has helped us focus more on developing a solution from a performance standpoint rather than looking into all other engineering-related aspects. This is probably a bigger win for us because now our data scientists can focus on improving performance. In financial services, especially the area that we work in, performance is a key part. A lack of performance usually adds distrust from an underwriter's or user's standpoint, such as advisers, underwriters, or risk agents.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2070.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2070)

### Lessons Learned: Agent Limitations, Performance Drops, and Context Engineering Challenges

Overall, I would like to share what lessons we have learned.  One thing that we have absolutely learned is that agents are not suitable for all business problems. Some problems, like Quickode that I mentioned, are not an agentic solution by itself. It is more like a stand-alone LLM application where for a particular task, you are trying to replicate an underwriter using a complex LLM application system by design. Similarly, if you think of building an IDP solution, a simple IDP solution may not work for some of the use cases that you have. If there is complex handwriting or some complex information that is being handed over to your agents, you probably need a separate process for that. A generic agent system may not work.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2110.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2110)

The other thing that we observed is that the solutions we are building  need to be built from an end-to-end value chain perspective. For example, if I take a "not in good order" use case, which a lot of businesses are trying to solve, and if I try to build an agent to solve an IDP-related issue, it may not work by itself. We may be able to solve the IDP solution, but from an end-to-end value standpoint, you probably need other components on top of your IDP itself.

For instance, if you have a workflow where you are trying to determine whether a case is good or not good, you need IDP plus a business rule processing system on top of it, or you need some kind of workflow management on top of your IDP processes to understand, fully implement, and execute the end-to-end NIGO process and then see the business value benefit. Business alignment and end-to-end value creation is actually crucial in gaining the value benefit realization from these use cases.

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2160.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2160)

The other part that we have learned is there's an unpredictable drop in agent performance, and it happens a lot when you try to scale in production.  Traditionally, we have observed one or two reasons. One is that model upgrades are happening, which are probably affecting the way the agents actually perform. The other thing that we observed is your training and validation dataset may not have seen all the cases that you are typically seeing in the outside world.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2200.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2200)

There are also some issues around how much memory and context engineering can actually come into play, and that is also an issue. That is probably one of the bigger aspects that we are trying to solve. When you are looking into scaling to  multiple advisors, multiple underwriters, and especially from an A2A standpoint, if you are trying to reuse some of the agents that another system has built, context engineering becomes a key challenge. How do we actually maintain that? How do we actually log it? How do we do the debugging or tracing? This becomes a key important aspect when we are trying to solve these kinds of use cases.

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2230.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2230)

I would like to talk about the future direction, where we are heading.  One thing that we have felt or realized is that integration with other line of business agents is probably a key win to success, because some of the use cases that other businesses are solving, we can reuse some of those agents in other business systems as well. Having that reusability component actually addresses a lot of the issues.

The other piece that we have observed is having a standardized approach on memory and observability, and also context engineering might help in scaling in the longer run. For every multi-agent system, it is hard to actually recreate its own database, its own cache memory, its own short-term, long-term memory, or static memories and building context engineering around that. It may not scale in the longer run. Maybe one pattern for one particular agent system might work, but if I switch from advisory assistant to maybe some real-time system or from there to IDP or from one use case to another, it becomes very harder to scale.

Things like agent core, which actually provides some of these features inbuilt, which most of the industry is heading towards, and if they are actually doing all of the groundwork, maybe it is better to just adopt this as part of your platform and try to incorporate those features into your platform and reuse them in your systems. That is probably a better approach for the longer run. If you have some edge cases or use cases which are very complicated, then probably you can try to re-engineer that, but for all the generic purpose use cases, it is probably better to stick with something the industry is heading towards.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2330.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2330)

### Future Vision: Three-Layer Platform Architecture and Key Takeaways on Modularity

With that said, this is a macro view into where we are heading in terms of scaling AI agents with time and trend.  You are seeing a bigger picture. We have a bigger ambition from our multiple agent systems. If you have to build multiple agents, what we have or what we are thinking is you need to have a different agent development layer on the very top, and then you need to have a core platform layer, and then you need to have an enterprise services layer on the very bottom.

What do I mean by agent layer and what do I mean by platform core layer? You need to standardize some of the things which are probably foundational in nature for building any kinds of agents. On the top layer, what you are seeing in the agent development layer is a layer that is supportive in building or probably democratizing the way agents are being built by using any kind of SDK or by using any kind of framework in different business environments. Not only data scientists, but this kind of framework should actually support data scientists, software engineers, and AI enthusiasts to build their own agents, be it agentic or non-agentic solutions.

Our fundamental thought process is that people should be able to build their own multi-agent systems, and this particular layer on the very top from a platform standpoint should actually support building up these layers on the go. Capabilities like deep research agent, IDP, call summarizations, customer service summarizations, and image recognition could all be built on top of this agent layer.

The core idea is to provide core services like interpreter, execution, and browser tools. These tools should be made self-discoverable and reusable across the agent layer and agent development layer itself. This way, people at the top are focused solely on development, not on the other platform components.

The core platform in the middle should become a foundation layer for the agentic development systems being built on top. The core platform should handle centralized Bedrock environment, context engineering environment, and development environment with SageMaker Unified Studio and Enterprise data stack. Some of the core pieces to add include your MCP gateway, your A2A gateway, and your GAI gateway. Additionally, we are thinking of adding a registry for agent discoverability, agent management, and agent report card, as well as a registry for MCP management.

Fundamentally, any agent built on top will use the core functionalities from your platform layer and will adopt agent configuration patterns, agent registry patterns, discovering those agents, identifying their performance, and deploying those agents. Your core platform will become a foundation layer for how core services can be provided in a modular way. These modular solutions will help people on the top layer deploy things at scale. We can reuse agent monitoring, agent development, deployment frameworks, and any of those frameworks from the bottom and deploy them on top.

We are envisioning two different layers, with the infra layer sitting in the middle and your enterprise infrastructure services, such as your Splunk service, staying at the very bottom as the crude base layer of the entire thing. This is how we are envisioning how a hybrid modularized platform will shape up and help us in attending our ambitions of creating different multi-agent systems for different business functions in the longer run.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2570.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2570)

With that said, I'll give back to Moon to conclude and give the takeaways. Almost there. Thank you, Rohit. Before you go, I just have three takeaways. It was pretty hard to boil down to three, but first, scaling with time is not trivial.  Everybody can scale nowadays with Kubernetes or any scaling platform, but scaling with time requires you to think more deeply about how you're going to prepare yourself for the future. Understanding what the latest AI trends are and how you're going to optimally maintain performance requires you to make your solution modular. Modularity is key for your long-term success.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2610.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2610)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2640.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2640)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/67ad5dbc3a8399e3/2680.jpg)](https://www.youtube.com/watch?v=9UTzSY40e9I&t=2680)

This will enable the first point, ensuring fungibility.  Fungibility allows you to have building blocks and replace some things if you need to exchange them for something better. It also means separating concerns to ensure that each unit can operate independently and scale independently. Lastly, we want to have a centralized platform to stabilize growth.  Just like Rohit said, you want to have multiple layers: one core layer and then on top of it a distributed layer. This will allow you to provide governance and prevent silos between teams and between AI solutions. It will also make it easy for your AI solutions to plug and play within your platform.  With that said, that's the end of our slides. We're open for questions. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
