---
title: 'AWS re:Invent 2025 - Optimize gen AI apps with semantic caching in Amazon ElastiCache (DAT451)'
published: true
description: 'In this video, the speakers explain how semantic caching with Amazon ElastiCache can optimize agentic AI applications by reducing latency and costs. They demonstrate how multi-agent frameworks compound expenses through repeated LLM calls, and show how semantic caching using vector search in Valkey can achieve 750x cost reduction compared to uncached systems. The presentation covers HNSW algorithm implementation, achieving 95-99% recall rates, and practical configuration of TTLs and similarity thresholds to balance accuracy with performance in production deployments.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/0.jpg'
series: ''
canonical_url: null
id: 3085386
date: '2025-12-05T05:26:50Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Optimize gen AI apps with semantic caching in Amazon ElastiCache (DAT451)**

> In this video, the speakers explain how semantic caching with Amazon ElastiCache can optimize agentic AI applications by reducing latency and costs. They demonstrate how multi-agent frameworks compound expenses through repeated LLM calls, and show how semantic caching using vector search in Valkey can achieve 750x cost reduction compared to uncached systems. The presentation covers HNSW algorithm implementation, achieving 95-99% recall rates, and practical configuration of TTLs and similarity thresholds to balance accuracy with performance in production deployments.

{% youtube https://www.youtube.com/watch?v=i_lLnV6NgPg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/0.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=0)

### The Challenge of Scaling Agentic AI: From Demo to Deployment

 Today we're going to talk about optimizing your agentic AI applications with semantic caching and Amazon ElastiCache. Let me start with a quick poll: how many people are really happy with the time and cost of your agentic AI applications if you've deployed any? I don't see many hands. So if not, we're going to show you the tools to get started.

Today we're going to cover four main topics. First, the journey of agentic AI and where we are today. Second, we'll discuss why traditional caching typically fails for agentic AI applications. Third, we'll explain how semantic caching works. And finally, we'll cover implementation and best practices in agentic AI applications.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/50.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=50)

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/60.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=60)

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/70.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=70)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/80.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=80)

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/90.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=90)

 Agentic AI continues to evolve. Early on, it was really about building strong foundations. In 2022, there were introductions to conversational AI types of tools that we're all familiar with today.  As we moved from 2022 to 2024, the focus shifted to quality and handling multimodal data like images and videos.  Then in 2024 to 2025, it started shifting to chat to action and natural language to tool invocation.  But where are we today? We're really past the experimentation phase, and the mindset has changed from "does it work" to "does it work at scale."  We're now thinking about security, governance, latency, and cost containment. Today we're going to focus on latency and cost containment with semantic caching.

There are really three barriers to move from demo to deployment. The first is scale. As these applications become more complex, you have more agents, more tools, and more different steps. The second is speed. Each of these individual steps requires more LLM calls, more API calls, and more tool calls generally. The third is cost, as many of these steps require LLM invocations, which is extremely expensive for a single transaction or single-turn conversation, and even more so for multi-turn conversations.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/140.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=140)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/170.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/180.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=180)

 One of the biggest issues with agentic AI is compounding latency. Think about assembling all that context, retrieving memories, structured data, and user personalization. Then you need to plan what to do, which agents to call, in what order, and which tools to call.  Then you actually invoke all these tools and agents, and finally you re-rank and assemble the final response.  The biggest thing here is that in these applications they can cycle over and over for a single transaction, and your latency and costs can be boundless without proper guardrails.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/190.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=190)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/210.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=210)

 When you look at the cost associated with agentic AI applications, these applications are growing fast. In fact, over 79% of companies have deployed agentic AI applications in their production workplace. But as we add more agents and more steps to increase the intelligence of these applications, we increase the complexity.  What do I mean by that? I mean planning and orchestration, more tools, and more specialized agents. Overall, this does improve the quality of your agentic AI applications. However, it drives up costs and latency.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/230.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=230)

 So practically, what do we need to do to find those optimization techniques to really flatten this curve and make it palpable at scale while maintaining that intelligence? There are many different techniques you can follow. You've probably heard the concept of prompt caching. You can use cheaper models, do model distillation, and fine-tune models for specific tasks with better routing. But a tool we're going to introduce and explain in more detail is semantic caching. This really allows you to squeeze every bit of intelligence out of your application without ballooning costs and maintaining an efficient cost profile at scale.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/290.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/300.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=300)

### Multi-Agent Frameworks and the Compounding Cost Problem

Let's think about these implications in agents. We're all familiar with the concept of single agent frameworks. When we think about this in practicality, let's do something simple. You might have come to Vegas today or yesterday and thought a question like "What's there to do in Vegas?"  A single agent framework would basically go out and maybe fetch information by doing a web search, looking at databases on events, and searching across the internet, then grab that context, serve it to an LLM, and give you a response. 

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/310.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/320.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=320)

The cost and latency are essentially the cost of the time associated with latency or associated with LLM  APIs and all that. But when you think about the complexity that has really started to be introduced with multi-agent frameworks, you know they're including more LLM calls, more tool calls, which means more latency and more costs. 

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/340.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=340)

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/350.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=350)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/360.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=360)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/380.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=380)

Just to give you a quick snapshot of the types of multi-agent frameworks that you see today, thinking about first supervisor models, so thinking about a primary orchestrator fanning out to all these specialized agents. Every single one of those circles is another agent  call which includes a bunch of different tools, a bunch of different API calls, all more costs, more latency. You think about network models, all  these agents working in collaboration, going back and forth to complete a specific task, more LLM calls again, more tools, more costs, more latency.  And you think about hierarchical, which is kind of a derivation of the supervisor models. Similarly, you can draw those same analogies. And finally, think about sequential models, agents that hand off to specific agents after they complete certain tasks. 

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/380.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/390.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=390)

Today I'll walk you through a quick example of a sequential multi-agent framework and how semantic caching could fit in there.  Let's make it real. Think about a common question you might be asking yourself. Because for me, I love Italian food and I love vegetarian food, I might be asking a question like, "Find Italian restaurants near the Bellagio with vegetarian options at 4 p.m. to 7 p.m."  This is a realistic question that everyday users ask to their travel or dining agents. It may seem simple on the surface, but underneath lies a really complex web of requirements thinking about the cuisine, the distance, the menus, and the availability.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/440.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=440)

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/450.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=450)

When you think about what you as a person do to actually accomplish this, you do all the reasoning. You decompose the question into multiple steps. You look at all these apps and find Italian restaurants in the Bellagio, look at Google Maps and all that. You might look at all the menus on various websites. You might search all the apps to find availability, and you might make that final  decision based on all that data you have. 

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/450.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=450)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/470.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=470)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/490.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/500.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=500)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/510.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=510)

So when you think about agentic AI applications, how does it handle this?  You think about a planner agent, like what you would do, that decomposes that question into individual steps, asking specific subagents to complete certain tasks. You could break that down into finding all the Italian restaurants near the Bellagio.  Ask a search agent that might go across all the distance calculations, might do web searches, various API calls, and then grab and find that list. Then I might hand off to a review agent that might look for the vegetarian options and all of those menus through web searches, through RAG calls, database searches, and funnel that down into something more manageable.  Then finally searching maybe for that subset of restaurants and looking at all the availability they have, searching across all these different  applications that you commonly look at to find availability. And then finally using another LLM to essentially take all that data, merge it together,  and give an answer back to that user. But as you can see here, the latency and the cost really starts to compound at scale.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/530.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/540.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=540)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/550.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/560.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/570.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=570)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/580.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=580)

Think about another user that asks me something that's very semantically similar, veggie items, different time, all that. In this use case, the user's  question would be decomposed in a very similar fashion. A search agent would run, that review agent would run, that availability agent  would run. But in practicality, you're basically doing the same thing over and over to get the same response with a slight variance in the availability. But what do you really notice here?  You notice that patterns really start to emerge at scale. You think about that search agent's response, you could actually fetch that user one's  response, grab all those Italian restaurants near the Bellagio, figure out the review agent's response, grab the veg options, and then also use that availability agent's response  and grab what availability was associated with those restaurants. So there's a pattern here. So what we think is that caching is the solution here. 

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/590.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=590)

### Traditional Caching Meets AI: Introducing Valkey and ElastiCache Vector Search

We've seen these patterns before in traditional workloads. You see this typically in applications to offload queries from a database to cache to really save time and money.  You typically first check the cache to see if the key or that query was executed before and you get that result within microseconds. If not, you run a query against your database, which might take hundreds of milliseconds.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/610.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/620.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/630.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/640.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=640)

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/660.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=660)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/670.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=670)

 You cache that result into cache, which occupies maybe less than one kilobyte of data, speeding up that following user's query that might be the exact same.  We have a solution here, which is Valkey. Valkey is a community replacement for Redis. It's open source and permissive.  It's been well adopted across many different cloud vendors and organizations.  We launched support for Valkey in ElastiCache last year, so you get all that managed goodness around observability, security, scalability, and more. ElastiCache has a ton of use cases.  Today we're going to focus on one use case around machine learning and AI and one specific new capability. Vector search for Valkey is now live.  As of November 17th, vector search for Valkey offers the lowest latency vector search at the highest levels of recall and throughput, with the best performance and price performance across popular vector databases on AWS.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/720.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=720)

This really supports your traditional RAG use cases, which you're all familiar withâ€”grabbing context and serving that to the LLM to generate a response. We're thinking about real-time semantic search for recommendation engines, agentic memory to personalize your agents based on a user, and finally today we're going to talk about semantic caching and how ElastiCache can really help you reduce your cost and improve performance for your agentic AI applications. ElastiCache for Valkey offers the speed that really allows you to build super fast, cost effective, and accurate agentic AI applications.  With that, I'm going to hand it off to Allen who's going to walk us through all the details and explain how you can build this and why this high level of recall is really important for you as you navigate this world of agentic AI next year.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/730.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=730)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/760.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=760)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/790.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=790)

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/800.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=800)

### Understanding Semantic Caching: From Concepts to Vector Embeddings

The first question is, if we're going to build a semantic cache, what is a semantic cache?  I showed you a picture of a classic database cache, and a semantic cache is very similar to that. The basic operations are similar. You put things in, you look for previous answers, and so on. In a classic example, what you're indexing the cache and what you're looking for is typically a SQL query. You're turning that SQL query into a piece of text and asking the cache, have I seen this piece of text before?  In the semantic world, we're going to do all the same things with caching, but we're not going to use exact text for the query.  We're going to use the semantics of the user's query.  In short, a semantic cache is simply a cache that's indexed by semantics, not by SQL.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/850.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/860.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=860)

To operate your cache, the first thing you have to do is take the query and extract the semantics from that. Once you've got those semantics extracted, you're going to search through the cache through all of the previous answers that you've stored in it and see if you can find an answer that's close enough to the query that you've gotten. If you find one of those, then you have a cache hit and you use the answerâ€”classic caching. However, if you can't find one, then you're going to have to go off to your LLM, generate a new answer, and then put that in cache.  The back half of this is cache, which you're familiar with. That part operates the same way. 

Semantics are a great human concept. We all know what it means, but computers don't work that way. They work on bits and bytes. It turns out that you can turn semantics into numbers and represent those numbers as vectors, and that's exactly what we're going to do now. In your typical application, the vectors that you use to represent your semantics are going to be very largeâ€”hundreds or thousands of dimensions. In my examples here, my tiny little brain can't go past three dimensions, so that's what I'm going to show you here. But where you see three dimensions, think hundreds or thousands.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/920.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/930.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/940.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=940)

In my little three-dimensional world, I can take some items and convert them into the semantics and then plot them in this three-dimensional world. The thing to realize is that things that are similar  group together in the three-dimensional space.  What does that mean? It means that things that are similar semantically have a proximity, have a small distance in vector space, and that's something that we can compute on. 

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/950.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=950)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/970.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=970)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/980.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=980)

So what does it mean to say that the semantic distance is the same as a vector distance? I have another three-dimensional plot. In this case, I have some queries  here, and as you can see, the dining and things are kind of grouped up into the right. Let's say I get the first piece of the query that Sanjeet was talking about here. Somebody asked, "Where are the best Italian restaurants?" Well, semantically that lands about there in this simple three-dimensional world.  I can look at the distance between that and the other vectors that are near it, and I can establish  a similarity threshold. Now I can say things that are within this distance basically mean the same thing. That distance, that similarity threshold is the key to getting high quality results, and we're going to return to the effect of that near the end of the presentation.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1010.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1020.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1020)

So how do I actually do that? Well, it turns out, how do I turn a question like  "best Italian restaurants" into a vector? This is actually old technology. People that have done natural language processing for years will recognize this.  We use a set of models that are called embedding models, and you may have heard the term vector embeddings or embeddings or vectors. They're basically all the same thing here. I don't know why we use the word embedding. Somebody pulled that down before it stuck.

Embedding models take in queries and turn them into vectors that are semantically equivalent. We can do that on Bedrock using any of the embedding models that are available on Bedrock. Amazon offers two embedding models: there's a text embedding model, or if your queries are not texturally oriented, if they're sound-based or image-based, you can use a multimodal model. The process is exactly the same after that. You can also use other models from other vendors, or you can even train your own. If you have a query environment that has a domain-specific nature to it, you can improve the quality of your system by training up your own embedding model.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1100.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1100)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1120.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1130.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1130)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1140.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1150.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1150)

The key item for the embedding models is they are dramatically faster than  large language models. 750 times cheaper and faster is actually not hard to achieve. That number is going to come back. So we actually have all the pieces now. We can put it together. To extract our semantics,  we convert that into a vector. Our semantic search is now a vector search. We're going to look through vectors and find vectors  that are close within our similarity threshold. Things that are within that threshold are hits.  So we can reuse the answer that's associated with that vector. If we can't find a vector within that, then we have a cache miss, go off to the model, generate the answer,  and put that answer into the cache so that we can reuse it in the future.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1200.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1200)

### HNSW Algorithm and the Economics of High-Recall Vector Search

Now that you've put that all together and you've built it, you run into a problem, which is it doesn't scale very well. The reason, when you dig into it, is you had your computer science hat on and you did your vector search using the exact vector search algorithm. Find me the exact right answer. There is no known algorithm for exact vector search that's better than brute force. It's a 75-year-old problem. No one's ever solved it better than that. But it turns out that if you're willing to give up on getting exactly the right answer, you can do a lot better.  There is an entire family of algorithms called approximate nearest neighbor.

You can think of these as kind of like a bloom filter. With a bloom filter, you put something in and if you look it up, you know that if it's not in the bloom filter, you've definitely never seen it before. But if you look it up and you do see it, maybe you've seen it, maybe you haven't because of collisions. So that fails with a false positive. The approximate nearest neighbor algorithms sometimes won't find you the nearest vector. That's what it means by approximate. And in the caching situation, that's okay because what it means is you didn't find the best answer.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1280.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1280)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1300.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1300)

But if it's within the similarity threshold, it's an acceptable answer. Or if what it found is outside the similarity threshold, then it's a mess. And instead of getting the wrong answer, you're simply just a bit slower than you could have been otherwise. So the question then boils down to how good is my approximate nearest neighbor algorithm. Well, we have a way of measuring that and we call that recall.  And recall is exactly what you would expect it to be. What percentage of the time does it get the right answer? You can walk through the math, but let's say for example I do 10 lookups and 8 of the times it gets the answer right and 2 times it gets the wrong answer. That's a recall of 80%. 

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1310.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1310)

There are lots of ANN algorithms out there. If you start digging into this space, there are dozens,  may even be hundreds. I don't know. I quit looking after a few dozen. What's interesting about this is that in standard computer science, we always talk about time and space trade-offs. It's a two-dimensional space that you can trade off with different algorithms. With ANN you now have recall as a third dimension. So you can trade off time and space and quality recall.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1380.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1380)

And it turns out there's lots of different algorithms that trade these off in different ways that apply to a lot of different problems. But the caching problem itself, what matters number one is the recall, because that directly affects your ability to find a high quality answer. And second is time, because after all, the whole point of caching is to save time. So the best algorithm that's out there for that corner of the trade-off space is called hierarchical navigable small worlds, which I hope to never say again. I'm just going to call it HNSW going forward.  HNSW is a graph-based algorithm. You take your vectors in and it builds a graph of them, and the graph has linkages based on the distance, so vectors that are nearby are linked together.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1420.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1420)

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1430.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1430)

But the people that built HNSW took a page out of the people that did skip lists, and they made the observation that if my graph gets too big, if I've got 100 billion entries in it, it's going to take me forever to search the thing. I don't know where to start. Searching around in it takes a really long time. But what I can do is take a subset of that data.  And make a graph out of that. And now I have a much smaller problem. And in fact I can do that again, and now I have a very small problem.  So what you've built is like a pyramid here, and that's exactly the way a skip list works.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1460.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1460)

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1480.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1480)

So let's see how this works in action. For this example, I'm going to look for this query vector which is the yellow dot down at the bottom. I want to find the vector that's closest to that yellow one, so I'm going to start at the top and randomly pick one of the spots, one of the vectors at the top here, and now what I'm going to do is I'm  going to look around in the graph. I'm going to search my neighborhood and find the vector that's the nearest to the query vector, so I look all around my neighborhood and it turns out I can spot the one in the middle there is the nearest. So I go to that one and then I drop down, use that and drop down to the next layer below it and now it's rinse and repeat.  I have a starting point on this layer. I look all around there, not all around, but just in that neighborhood. I find the smallest vector there and then I use that drop down again. Rinse and repeat, and in this case I'm on the bottom layer. I find the one that's closest to that, and because I'm at the bottom, we're done. This is a logarithmic algorithm.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1540.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1540)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1550.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1550)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1560.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1560)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1570.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1570)

This is a logarithmic algorithm that scales. The implementation in ElastiCache can deliver answers in hundreds of microseconds or single-digit milliseconds, which are very typical response times for fairly large datasets containing tens of millions or hundreds of millions of entries. This approach is very effective in a caching environment. HNSW gives you excellent throughput with really high recall. Getting recall in the 95%  to 99% range for HNSW is very easy to accomplish.  Now the algorithm itself has a bunch of parameters. I'm not going to go through them for you because it's like a PhD thesis to understand how they all interact, and this is only a 400-level course.  But the key takeaway, and Sanjeet talked about this before, is that high quality for your results matters at every step. 

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1580.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1580)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1600.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1600)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1620.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1620)

Let's take the example that Sanjeet showed us: find Italian restaurants in the Bellagio with vegetarian options at 7 p.m. for 4 people.  He broke that down into three agents that then gave you the overall response. Now there are some very inexpensive searching algorithms and databases that use these algorithms that will give you 90% recall on that, and that sounds pretty good until you realize that these all multiply each other.  That gives you an overall 73% accuracy, and I have to tell you, I think a system that gives me the right answer only 3 out of 4 times is no good. So let's repeat that at 99%, and the answer that you get now is 97% accuracy.  That's pretty acceptable for this kind of application.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1670.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1670)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1680.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1680)

Now, I mentioned before that embedding models are 750 times faster than your large language model inference. You've got your caching model which is running in microseconds or milliseconds. What happens when you put all that together for your wallet? Here's what happens. First of all, the numbers that I'm going to show you here are detailed in a blog. I'm not going to do that because we only have limited time here, but if you're interested in that, by all means go to the blog and it describes the basis for all of these numbers in detail. So here I've got cost per day  on the y-axis and the cache hit rate on the x-axis, and that's really where the win is going to come, as you'll see. 

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1690.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1690)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1730.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1730)

The white line here is an uncached system. Obviously the cost doesn't change regardless of the hit rate. But now I'm going to plot that system  with caching, and you can see if your cache hit rate is 0%, this actually does cost more. You've got all the costs for your model plus the cost of the embedding models and the cost of the caching system itself. But when the embedding model is 1/750th of the cost of the regular model, it really doesn't add much. In fact, the break-even point is 2.5%. So if you can achieve even a 2.5% hit rate in your model, you will break even, and of course things just get better after that. In fact, the cost reduction almost exactly follows the hit rate. 

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1740.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1740)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1770.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1770)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1800.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1800)

### Building a Semantic Cache with ElastiCache: Technical Implementation

Now, how do I actually build this on ElastiCache?  I'm going to walk through some examples here, but first, ElastiCache with Valkey is a key-value store. How do I do vector operations on that? What does that mean? How do I map that? Conceptually, the search module in ElastiCache looks a lot like a relational table. In a relational table, you've got  rows and columns. What the search module does is say that every key that's in the database is a row in the table. Since it's a key-value store, I'm going to use values that have multiple parts in them. In this example, JSON, but you can use hash. Each of those fields is an element inside of the key. So for example here,  you can see that the relational model maps fairly well into the key-value store.

For our example problem here, I want to build a cache that has a vector component and then it points to the answer. This is the minimal cache that you can do. In this particular example, I'm not actually going to store the answer. I'm going to say that storing the answer, the answer could be a few kilobytes of data, and storing that in my cache gets a little more expensive. The price per bit of the cache is a bit more than some of the external storage, so your answer storage could be something cheaper like maybe DynamoDB or S3, or you could stick it in a file system on an SSD or even a hard drive if you wanted to. Any place really doesn't matter. All that really matters is that the value for each key has a vector and it has a reference to where the answer is.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1810.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1810)



[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1870.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1880.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1880)

So I want to define an index. This is one command, but I'm going to break it into pieces just to make it bite-sized. I'm going to create an index and give it a name.  You can have multiple indexes, and each one has a name. I'm going to put it on JSON. I mentioned before you could use hash.  The prefix is the subset of the key space for this index. When I create an index, it is constrained to a portion of the key space. The reason you might do that is you could host multiple different caches inside the same instance of ElastiCache. It makes your management easier and cuts the overall expenses. So you can create an index. In our example here we had potentially three caches. You could put each of those logical caches into a single ElastiCache instance using different portions of the key space.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1890.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1890)



[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1940.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1940)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1960.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1960)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1980.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1980)

Now I'm going to define the schema for this index.  Since it's JSON, I have to have the JSON path, and in my query language I have to give that a name. It's a vector field. It uses the HNSW algorithm with 32-bit floating point. I've got a three-dimension vector for my example.  You're going to have something that matches your embedding model. Typical embedding model sizes are 256, 512, 1536, or if you build your own, it's whatever size you choose. ElastiCache doesn't care. It supports vector sizes into the tens of thousands.  And I have to establish a distance metric.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/1990.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=1990)



[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2000.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2000)

So how do I load data into the index? Well, the answer is you load data into ElastiCache. When you set the data using the regular ElastiCache value key commands, set and get,  in this case JSON set, that data is automatically indexed because the key matches the prefix in the index. When that gets written, it's automatically loaded. That right here will complete in a few hundred microseconds or a millisecond or two. And unlike a lot of other vector stores, the query that data is in the index immediately. So you can issue a query that will include that data right after the set has completed. A lot of other vector indexes have a waiting time that can be minutes or even hours from when you insert something before it becomes queryable. We think in a caching environment that's not really going to be effective, especially if your application is temporal or subject to a flash event. Then you're going to want those cached responses to be available immediately to help you deal with that peak spike in the load.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2070.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2070)

So you could put a couple of vectors in, and since it's a cache,  and Sanjit is going to come back and talk about this in more detail, you can make the data expire on a time interval. So if for example something like a table reservation, you might want that to expire fairly quickly, whereas say like the best restaurant, that might not expire for more than a day or two, and you can pick the interval of time that matters for you. So now that my index is defined and I've put some data into it, how do I query it? Well, the query command, the syntax for that is a little baroque. I'm not going to go into it in detail. The red pieces are the parts that you care about here.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2110.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2110)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2140.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2140)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2160.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2160)

I'm going to search a particular index.  I'm going to search the vector component of that index. The search command can actually also do hybrid searches. We can have other field types besides vector. You can have tag fields and numeric fields in ElastiCache and do what's called a hybrid search. I didn't get into the complexity of that, but it supports it. And then you have the reference vector, which is that last piece, and the query language is independent of whether it's  in RESP or JSON, so that comes in in binary. Generally, you're using a language client like the Glide client or some of the Python clients or whatever your application language is. Those clients for Valkey take care of most of this for you. 

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2170.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2170)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2180.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2180)

When you get an answer from that query, the things that you care about are which key. That may not be useful, but sometimes you can encode  information into the key. I know the distance. That's what's going to drive my hit or miss logic. I'm going to compare that to my similarity threshold.  And I get back the key, or excuse me, the value, which in this case has the reference to the answer for me. So once I get this result, I can compare it distance-wise to my similarity threshold, decide whether or not I want to go get that answer, or unfortunately it's a miss, and now I have to go to the model and generate a full answer.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2220.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2220)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2260.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2270.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2270)

### Best Practices: Balancing TTLs, Similarity Thresholds, and Continuous Optimization

So now I'm going to turn it back over to Sanjeet, who's going to go over some of the implementation and best practices. Thanks, Alan. So now that we've gone over how to actually build your semantic cache, let's take it back to that prior use case and think about  how that practically would be built here. So if we think about this complex question, this multi-step composite question, we could have actually taken the decomposed questions around the Italian restaurant near the Bellagio, the underlying restaurants that have vegetarian options, and the availability associated to those restaurants, and actually taken those subquestions, created the embeddings and the vectors, and actually stored those embeddings and the answer within a semantic cache like Valkey. As a result, the second user that came through with a very semantically similar question could have then run a simple vector search and fetched those answers  from the Valkey semantic cache and then returned those results back such that you never had to invoke any of those agents for the final response to user 2. 

So basically, you're avoiding all these expensive LLM calls, these tool invocations, and you're just following the vector search path and the embedding generation path, which, as Alan noted, is significantly cheaper. And the other big piece is that as Alan noted in that one example around maintaining a high level of recall, you're still able to maintain a high degree of accuracy in that final response to the user while still saving time and saving money. So one thing to note here is that when you think about agents, they all follow a very similar framework, but they're tuned to very specific goals. They're using different tools, prompting models, and have access to different underlying data. When you think about semantic caching, you need to do the same, especially when you think about it in terms of agents. You need to think about the right embedding model to use, the right search algorithm as we discussed, and then finally you also need to think about similarity thresholds and TTLs, so the expiration of the keys.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2350.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2350)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2370.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2370)

So one thing to note is all data is not made the same. When you think about a lot of these three different agents, you think about each of these agents using the underlying data and the underlying tools have a different tolerance of the volatility of how  quickly that data is changing. Thus, you need to really set specific thresholds on expiration of that data or those agent responses to be able to reuse that data. By following those strategies, you're able to actually really see a dramatic drop in those LLM calls, thus dropping your cost and your latency in  the sequential multi-agent framework example that we walked through. Obviously, this will depend on your use case and how you deploy it, how you configure your semantic cache, and all that good stuff. And you can actually apply it to a lot of those other multi-agent frameworks we discussed earlier. I think the big takeaway from Alan's point is the breakeven of deploying a semantic cache is extremely low, but yet you can yield significant benefits. So there's really two knobs here to really balance the freshness, the accuracy, and the performance, and those are TTLs and similarity thresholds.

TTL, or time to live, is basically how long you maintain the keys, agent responses, or vectors in your cacheâ€”whether that's in minutes, seconds, days, or however you want to configure it. Similarity thresholds determine how similar a response or subsequent user query has to be to something already stored in the semantic cache for it to actually be a cache hit.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2490.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2490)

The key thing to note is that a lower TTL, or how quickly you want to kick things out, means a lower cache hit rate, so you're going to have more freshness, but you're going to invoke all those agents over and over again. You're going to invoke all those language models, you're going to have a lot more cost and latency, but more accuracy. So you need to really balance this and how you want to configure it. Similarly with thresholds, when you require higher similarity, you need your subsequent user's question to be very close to the question already stored in the cache. Obviously you're going to have another lower cache rate, more agent calls, more language models, more tools, more cost and latency, but better accuracy. 

However, with something like ElastiCache for Valkey and the vector search capabilities, you're still able to maintain a high level of accuracy by maintaining a recall level of 99 percent. Conversely, you can see that higher TTLs and lower similarities means higher cache hit rates. Therefore you're going to save a lot of time and money and you're not going to invoke those agents.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2530.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2530)

One thing I'll also leave you with is that, as is the case with any traditional caching, you really need to cache based on your application use cases. You need to understand the tools and the data that your underlying agents really have access to. For instance, if your agent has access to real-time or compliance data, you need to be cautious about actually reusing a lot of the underlying responses for subsequent users as it might be fast changing, or you might fall out of compliance. 

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2550.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2550)

When you think about slow-changing data like your menus and your restaurants, those types of things are generally very safe to cache and reuse if all the tools and the data that the agent has access to fits under that category. The final thing I'll note as a key takeaway is that as you deploy your agentic AI applications, optimization from a cost and latency perspective is really a continuous level of effort.  As you add more tools and more agents to improve the intelligence and add more users, you're going to increase your cost and your latency, so you really need to employ these types of techniques whether that's semantic caching with something like Valkey or whether that's employing smaller language models, model distillation, better routing to your underlying agents or your tools. Really consider what best fits your use case. Optimization is really a journey you need to think about as you scale up.

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2590.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2590)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2600.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2600)

With that, I'll leave you all with a few blogs. The first one is a quick semantic caching blog on how you can actually get started. There are some good code samples and some good performance specs.  There's also our technical documentation on the actual underlying APIs and how to actually launch this for yourself. 

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ace94b2ab7a62128/2620.jpg)](https://www.youtube.com/watch?v=i_lLnV6NgPg&t=2620)

Thank you everyone for your time. Me and Allen will be outside, so if you have any questions feel free to come ask us. Please fill out your survey as we really do appreciate the feedback, and thank you everyone. Please do stop by our AWS database booth if you have time and thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
