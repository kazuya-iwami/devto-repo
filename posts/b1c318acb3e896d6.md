---
title: 'AWS re:Invent 2025 - AI-Driven Design at Scale: Tempo Labs'' Move to AWS Bedrock (MAM325)'
published: true
description: 'In this video, Vaidy from AWS introduces Tempo Labs, a Canadian startup that migrated from GCP to AWS while building a design-to-code platform. Kevin and Brandon from Tempo demonstrate their productâ€”a visual tool that bridges Figma and IDEs, enabling designers and PMs to ship production-grade React code directly. They explain how a $250,000 monthly LLM bill drove their migration to Amazon Bedrock for better rate limits and cost efficiency. Through AWS''s Migration Acceleration Program and partnership with OpsGuru, they rebuilt their Kubernetes infrastructure, reducing cloud costs from $25,000 to $5,000 monthly. The team also collaborated with AWS Gen AI Innovation Center on a PRD agent that extracts product knowledge from codebases. Vaidy concludes by highlighting MAP and Gen AI Innovation Center as key accelerators for migration and AI innovation.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/0.jpg'
series: ''
canonical_url: null
id: 3088684
date: '2025-12-06T10:58:29Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - AI-Driven Design at Scale: Tempo Labs' Move to AWS Bedrock (MAM325)**

> In this video, Vaidy from AWS introduces Tempo Labs, a Canadian startup that migrated from GCP to AWS while building a design-to-code platform. Kevin and Brandon from Tempo demonstrate their productâ€”a visual tool that bridges Figma and IDEs, enabling designers and PMs to ship production-grade React code directly. They explain how a $250,000 monthly LLM bill drove their migration to Amazon Bedrock for better rate limits and cost efficiency. Through AWS's Migration Acceleration Program and partnership with OpsGuru, they rebuilt their Kubernetes infrastructure, reducing cloud costs from $25,000 to $5,000 monthly. The team also collaborated with AWS Gen AI Innovation Center on a PRD agent that extracts product knowledge from codebases. Vaidy concludes by highlighting MAP and Gen AI Innovation Center as key accelerators for migration and AI innovation.

{% youtube https://www.youtube.com/watch?v=xFNz-UdB-Q4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/0.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=0)

### Introduction: The Evolution of Cloud Migration and Gen AI Adoption

 Awesome. Hello, everyone. Thank you for joining us today. My name is Vaidy. I'm the Worldwide Tech Leader for Migrations at AWS. Today, we have a customer with us. The session is a customer spotlight, so they will walk us through the innovation that they are doing with Amazon Bedrock, and they will also walk us through the journey of how they migrated from another cloud provider to AWS, and how they are doing continuous modernization and innovations in Gen AI.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/50.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=50)

Before I introduce the customer, I would also like to give an overview of how the landscape has evolved for migration and modernization, as well as Gen AI over the years.  I joined as the very first migration SA in the team and still remember the very first large-scale migration that we did. The primary drivers for the large-scale migrations back then were cost and efficiency. Cost as in the total cost of ownership, and efficiency as in operational excellence. So it's primarily around having better security, reliability, performance, and scalability.

The conversations have evolved over the years since then. Especially the board of directors and customers like you want outcomes that are much more than just cost and efficiency. They want business outcomes. They want to transform both the business and the customer experience. They want to deliver digital transformation and modernization outcomes. The thing is they also want to reinvest some of the cost savings that they have and be able to achieve innovation. And with Gen AI, they want to do this at 10x the velocity and 10x the agility.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/130.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=130)

 In the Gen AI space, back in 2023, it was completely a new landscape. Everyone was trying to understand mostly the concepts, the basics, and the potential of it. 2024 was more about the implementation. They were asking the right questions to get into the implementation mode and getting into the practicality of it. 2025 was taking things to production and making it a core essential for their business excellence.

### AWS Application Transformation and the Tempo Labs Story

A good example I can walk through is AWS ourselves. We had a lot of legacy Java stacks that needed to be upgraded. They were nearing end of life and needed to be upgraded. Back in 2023, we were exploring Gen AI on how best it could be, creating the low-touch ability to upgrade the Java stacks at scale. In 2024, we productionized it internally and we got a lot better at it. We were able to upgrade Java applications in a matter of every hour. We were able to upgrade Java stacks.

Today, 2025, fast forward, we have our first Gen AI service called AWS Application Transformation, which can do this at scale. Every customer can leverage it, not just for Java, including .NET and mainframe modernization. AWS Application Transformation so far has saved about 800,000 hours of developer productivity and has also been able to look at a billion lines of code from a modernization perspective, and it's not a small feat.

This is not just true for AWS or cloud providers or enterprises in the space. It is also true for startups who are in the space. They have been given the same access to the accelerators and technologies, and they are also innovating at the same rapid pace and building the primitives that the enterprises can readily consume.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/250.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=250)

 With that said, we're excited to have amongst us Tempo Labs, who is one of Canada's fastest growing startups and a trusted AWS customer. Tempo Labs has been using AWS to power their next generation design and engineering platform. The thing is they have the ability to transform a spec into a real-time production application that is ready to use. They are also a great example of what builders can do with AWS. They will demonstrate the innovation that they are doing with the Gen AI applications, and they will also walk us through the journey of how they evolved, migrating from another cloud provider, continuously modernizing and innovating using AWS.

### Introducing Tempo: Bridging Design and Production Code

I welcome the Tempo Labs team to join us and walk us through the journey that they had. Thank you for being here. All right, so we have a few fun things planned for you guys today. It's a little different given the silent room, so we're going to do our best to kind of echo through that, but before we really jump into things, I wanted to do something a little bit fun and play a video for you guys that really introduces the scope of Tempo but also hopefully hypes you up in the sense of understanding what we are doing today.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/360.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=360)

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/370.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=370)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/380.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/390.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=390)

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/400.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=400)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/410.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=410)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/420.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/430.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=430)

        All right, so I hope you guys enjoyed that little clip there. We have a few things planned for you today that are going to take it beyond the traditional, move a little bit into the danger zone as we actually show you guys what this is all about. But before we really jump in, let me briefly introduce myself. I'm Brandon. I'm the Head of Growth here at Tempo.

So what exactly is Tempo? Tempo is the best design tool for design engineering. It's a visual platform like Figma or Webflow, but under the hood, it works on top of any production React codebase, just like Cursor, VS Code, or any other IDE. I like to kind of simplify this and say it's almost like Figma and your favorite IDE had a baby. We bridge that gap between those two.

Tempo completely eliminates the traditional handoff between designers, PMs, and your development team, where effectively your design team or your PM is able to open their own PRs and push production grade code. What this has represented is that teams that are deploying and using Tempo effectively are seeing on average an acceleration of shipping features at 40% greater than what they were before. But given that we have Kevin here, our co-founder and CEO, I'm going to let him dive into that a little bit more because frankly speaking, he is much better suited to talk about that. I like to call him our chief product, if not prompt officer, because frankly that is what he does best.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/530.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=530)

 So a little bit about Tempo. Tempo is a venture backed early stage seed company. We released our product in December of 2024. Since then we've had over 160,000 people use that product with an average growth rate of about 22% month over month. I'd be remiss to not give a shout out to Kevin here to add a little bit of credibility to the team, but also our other co-founder Peter, who operates as CTO. Before founding Tempo, they were part of the founding team at a company called Perpetua, which sold for 100, 150 million. I can't remember the exact number, but somewhere right around that.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/580.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=580)

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/590.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=590)

And lastly, I would definitely be remiss if I didn't do the shameful shout out that we are a YC company, so in true traditional fashion, if  any of you are from the Bay Area, I can guarantee that you've seen almost every single founder have this picture.  It wouldn't be an introduction if I didn't take the opportunity to at least show that.

So before I really jump into this, you know, today I actually saw a tweet and someone said, vibe coding is dead. What is happening in 2026?

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/630.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=630)

Frankly speaking, this is what I want to differentiate ourselves from. Tempo is not just another vibe coding tool. We're not out here marketing, you know, what if you could just build or design a SaaS dashboard? Because frankly speaking, we already know that's very possible. This has already been highlighted. There's a lot of tools out there that can get you that surface level interaction. What we built is a solution that goes beyond the  superficial.

Tempo is a space for designers, developers, and PMs to actually collaborate. And more importantly, enable your teams to ship production grade code. AI has really gotten good enough for PMs and designers to ship real code, but this isn't happening at an enterprise level. Why? Because, frankly speaking, designers like using Figma. PMs, they like using the traditional scope of Linear or Jira. But that isn't helping you at a business level, that's not helping you move faster. And so, this is the problem that we are solving at Tempo.

To show you we aren't kidding, we're actually going to take this a little bit of a step further, do something a little bit dangerous, which generally speaking, you probably shouldn't do when you don't control all the factors, but we're going to do a live demo. So we're actually going to show you guys right now. I'm going to call Kevin up here, and we're going to pull up our app, and we're going to do this in real time. Kevin was supposed to record a fallback video in case the internet didn't work. He didn't do that, so we are living extra dangerously. Kevin, are you ready to go? We're good. Okay, swap it over to the other one.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/730.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=730)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/740.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=740)

### Live Demo: How Tempo Integrates with Production React Codebases

Alright, thanks Brandon, nice to meet you everyone. As Brandon mentioned, my name is Kevin. I'm co-founder and CEO here at Tempo. And when it comes to Tempo, like Brandon mentioned, there's really two key differentiators versus a lot of the other sort of vibe coding tools that are out there in the market. The first is that we integrate with  production code bases, so it's not just about generating sort of prototypes in a silo that are effectively throwaway, but we actually sit on top of your existing production React code  base.

And so what you're seeing here again, it looks very similar to a design tool, right? You have an infinite canvas on the side, you have a layers and properties panel, and you have a styles property editor on the side over here. But the most important and interesting thing is that this is a live running React app and it's running on a Docker container in the cloud so that this whole environment is actually collaborative just like Figma, right? So just like you can send a Figma link, multiple people can edit things at the same time. You can actually send a link in Tempo and a designer, a PM can actually edit the same code base because it lives on a container in the cloud. And so we have a lot of cloud infrastructure we'll talk a lot about which powers this experience.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/810.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/820.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=820)

But that is the high level scope of Tempo. Now when you go to Tempo, you'll notice, you know, just like a traditional vibe coding tool, of course you can start from scratch or you can enter a prompt and build something from scratch, but the most  value and our biggest power users all import their existing code base from GitHub which you can do in one click. Now of course  you can vibe code. You can use AI, but the second key differentiator with Tempo beyond the fact that it integrates with production code bases is that we give designers and PMs familiar tooling and we give them the level of control that they're used to from a tool like Figma.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/850.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=850)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/870.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=870)

So designers can use Tempo to control every single pixel by hand, and I'll give you a quick demo of what that looks like. So here in this case you can see, let me just refresh this here. Let me just give me one sec. Well that might happen to me.  So here you can see I have a sample Airbnb code base. Right now let's say I want to add to this home section a testimonials page. What I can do is I can actually go to an online marketplace, which such as reactcomponents.com, which is another property that we own, and I can look for something  like a testimonials page.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/880.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=880)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/900.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=900)

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/910.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=910)

I can look at what other designers have created in the community, and what I can do is I can actually just copy  this component. Let's say we want to use this testimonials component, looks pretty good. And now what I can do when I go back to Tempo is I can paste it on the canvas just like I would in Figma. And so there you go, here's my testimonials page. Now if I want to add this into my code base, I can just drag and drop it and what you'll notice is if I go to the actual underlying code,  you'll see that Tempo has added the code for this UI component in the correct place under the hood. Now, obviously this doesn't look that great because I need to customize it and make it look better for my  application.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/930.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=930)

So I'm going to reduce some of the padding that's here. I'm going to edit the copy here, and actually it's duplicate. We have another hero section, so let me delete that altogether. One sec, let me delete that altogether. Let me edit this copy to say come to Vegas for  re:Invent. And then I can make changes such as, for example, let's say I want to change the color of this element. I can change this to amber and maybe I can add, just for fun, a blur. Okay, that's too much blur. Let's dial it back down a bit.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/960.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=960)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/970.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=970)

And when again when I look under the hood, you'll see that every single change that I made, even though I haven't used AI once, I haven't prompted once, I've  just been controlling every detail by hand, you'll see that Tempo is editing the code correctly under the hood, and all of the code changes are actually staged here for me to be able to just enter a commit message.  I can push this up to GitHub, or if it's not all the way there, I can just share the link with an engineer and they can sort of take it from here. And then, the beauty and the power of this is that you don't have to, for example, this is not ready to push this to production, but it's much better than a Figma design, right?

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1010.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1020.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1020)

But what we're seeing is about 60 to 70% of the code that PMs and designers generate in Tempo is actually being able to be reused by front-end engineering teams, and that is a dramatic change to the traditional design handoff. And teams are shipping about 40% faster using Tempo. And the whole thing is collaborative, so I could send this link to Brandon  and I'll show you another, you know, I sent this link to a friend of mine, a colleague of mine earlier and they created this beautiful animated tabs component.  I won't go into the full demo today, but you know, Tempo also builds very deeply on top of Storybook.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1040.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1050.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1050)

So if you have an enterprise design system that's in Storybook, every single one of your stories is ready to be drag and dropped and reused just like an asset in Figma. And so here in my Storybook, someone from my team added an animated tabs component  and so again now, I'm sure some of you guys remember the beautiful Airbnb release that they did, I think in the summer. I can just drag and drop this new animated tabs component  and boom, you know, here's my beautiful new animated tabs component. So the whole thing is collaborative, the whole thing is built on top of your production-grade, your production code base and your production design system built in Storybook.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1090.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1090)

And lastly, just to sort of end the demo, there's a lot more that I'd love to show you, but just to end the demo, some of you might, and a lot of customers come to us saying, hey, this looks great, you know, a lot of the vibe coding tools they're good for little toy apps, but can it actually handle a production code base, right? Can it actually handle a large enterprise-grade code base? The answer is yes. We have been working extremely hard for months and months, and my favorite demo to prove that is actually  Tempo on Tempo. So what you're seeing here is actually the Tempo editor recursively inside of itself.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1120.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1120)

And so Tempo, as you can imagine, is a very heavy, very complex front-end app, and our team and our designers and PMs actually use Tempo to contribute code directly to our code base. And so if you can handle our code base, most likely can handle yours if it's not too legacy. So yeah, that sort of wraps up the sort of demo. Hopefully that gives you a taste of what is possible in Tempo, but what about AWS and the migration to Bedrock?  Let's talk about that. And so with that, do you have the clicker? And it's not there so I'm not covering you. Okay, perfect.

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1130.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1130)

### The Infrastructure Crisis and Why Tempo Chose AWS

So now it's  where does AWS come into play? So as I mentioned, you know, we have these cloud dev environments, right? We're spinning up containers that are running your production code base in the cloud where multiple people can edit similar to Figma, right? With designers and PMs, you know, they don't necessarily want to keep their Git up to date and set up their environment variables and maintain the dev environment. It's a lot of friction. So we always believed from the beginning that it had to be a cloud-hosted experience that felt like Figma. It's just a link that you could send so that we could make collaborating on code feel like Google Docs, you know, so people can just, code is just a relay race, right?

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1200.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1200)

It's something a PM generates something, a designer generates something, and then hands it off to an engineer, and they take it from there and ship it to production. So cloud was a big, there's a lot of cloud infrastructure that powers Tempo, and our team was previously on GCP because we built our previous company on GCP. We love GCP primarily for BigQuery. We were in the ad tech industry in our previous startup, and so we were building on GCP. And as any startup, you know, you do things that don't scale, and so this infrastructure that I mentioned, you know, you could imagine you want to spin up pods and containers very dynamically to handle, you know, large changes in load and manage costs and  stuff. We didn't do any of that.

What we did is we just had a massive machine, the biggest machine we could possibly find on Google Cloud Platform, and we hosted all the containers basically on one machine. And then when we grew we were like, oh snap, okay, we're going to fix up our infrastructure later. We didn't, and then we went kind of viral in March, which I'll talk about, and we basically just ultimately got five huge machines and just routed users between them. But we were at this point in March where we were basically completely screwed. Our infrastructure was costing us way too much money. It was not scalable. It was a real pain in the butt to manage, and more importantly, our cloud inference bill for LLMs, we hadn't even optimized our AI agent for economic costs. So how much a customer pays us and how much it actually costs us in terms of LLM costs, I'll just say we weren't very disciplined about that in the early days.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1270.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1270)

In March or April, we basically got hit with a $250,000 LLM bill, and we were not generating $250,000 from our customers in that month. And we were really, really scratching our heads being like, okay, what exactly is our strategy going to be?  Now, most of that LLM spend was with Anthropic, and as I'm sure you're aware, Anthropic and AWS have a very, very special relationship, much deeper, at least at the time, than really any of the other cloud providers. And so that's really what brought us initially to Amazon Bedrock. Two of the challenges that I mentioned that we had were costs, but the second was also rate limits. And so we were looking for how can we scale up our availability, our capacity. Anthropic was giving us first party support because we're a YC company, but still, even they were struggling to scale and whatnot. And so that's sort of how we got involved with the Bedrock team and Amazon, and we were kind of like, hey, help, where do we go from here?

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1310.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1310)

 My turn again. The risky part of the presentation is over, all done in a live demo. Okay, so transitioning now in terms of talking about the AWS journey, and it's really important to consider Tempo under the scope of the startup lens simply because a $250,000 LLM bill for an enterprise company probably isn't going to cause that much anxiety, so to say, but for a seed stage company, that's massive. And so when we think about the bigger picture here, and when we think about cloud computing, it's relatively become a level of a commodity. Access to cloud computing is quite vast. There's a lot of different resources, options available, especially being a YC company. We're afforded a lot of luxuries in the sense of promotional kind of levels of pushing us in a certain direction or them trying to give us credits.

And so what led us to being such an outspoken supporter of AWS, it's honestly fairly simple. We chose to work with AWS because they were willing to invest in the future of our tech, but more importantly, they were willing to invest in us. And I'm not talking just about credits or resources. I'm talking about the fact that when we have a problem and we're texting our AWS rep at 10:00 p.m. at night saying, hey, there's something wrong with our rate limits, they're responding that night. They are truly operating like a partner, and not just from a technical perspective. Being Head of Growth, obviously I'm looking at the bigger picture. How are we deploying, how are we growing? Working alongside the AWS startup team has been instrumental for us, especially as we start to think about our more upmarket trajectory and working more on the enterprise level, which I'll talk a little bit about more here in a second.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1440.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1440)

So  it's more than just credits and resources. It's opened up significant doors for us in terms of our go-to-market motion. As we improve the product and as it's become more stable, we first launched it in December 2024. The video that we showed earlier was actually that initial video, and one of the things why we want to show it is because it's a truer beta product. But what happened was, within three to four months, we had literally tens of thousands of people testing that app, and we quickly realized that we needed to grow up. And as we've grown up and as the product has stabilized, as we become more confident in it, we've started to expand our scope in terms of working with more enterprise customers.

Having access to the AWS Marketplace has also enabled us direct access to some of those groups, not in addition to obviously the networking events, the conferences. But a concrete example that's a really big, big opportunity for us is the fact that we're now working alongside a Big Four consulting firm to deploy this at scale.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1520.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1520)

A lot of the facilitation for that project is going to be with access to the AWS Marketplace. All right, now to actually talk about some of the more technical details,  I'm going to pass it over to Kevin.

### Migration to Amazon Bedrock and Building the PRD Agent with AWS

Awesome. So migrating to Bedrock was very easy. Frankly, number one, we had a ton of YC credits that we wanted to use, and Bedrock was actually the only big cloud provider. Just to be clear, as a YC company you get about $250,000 to $350,000 in credits from each of the different cloud providers, but Bedrock was really the only one that allowed us to apply those credits towards Anthropic LLM inference, which as you can imagine is a huge financial incentive. If you look at all the data, Anthropic is by far the best LLM for enterprise API-oriented use cases. So that was the initial huge value add.

But the second thing I mentioned was support in terms of rate limiting. So now the support that we've been receiving from the AWS team has been incredible. Whenever there's a new model released or anything like that, we're working very closely to ensure that we have a smooth, timely, and swift transition from an older model to a next-generation model and that we have the sufficient rate limits to be able to achieve the capacity that we need.

And lastly, this is another really cool thing. We build on top of the Vercel AI SDK, which is an open source SDK. Even in the earlier days there were certain new features that were getting added to LLM APIs, like for example prompt caching. For those of you that aren't familiar with prompt caching, it's something that you can use to really reduce your cost of LLM inference. The Bedrock APIs at one point in time when we transitioned didn't support the exact prompt caching APIs that were needed for use within the Vercel AI SDK. We contacted Amazon and really within a few days they were able to rectify that whole issue, so we were able to continue building on top of the Vercel AI SDK on top of Bedrock.

So I have nothing but good things to say about our migration to Bedrock, and really the Amazon AWS team has just been incredible. I've worked with both of the other cloud providers as well for these types of workloads, and really no one compares. A large part of that is the very deep relationship with Anthropic.

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1640.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1640)

Beyond that,  as I mentioned, we had this huge LLM inference bill, but we also had this huge cloud infrastructure bill for our containerized Docker cloud-based collaborative dev servers. Amazon, as part of their Migration Acceleration Program, actually invested quite a large sum into helping us build that out. So they said, hey, not only are we going to give you incentives to come over to Amazon AWS, but we're actually going to invest in resources to help you modernize your architecture and make it truly scalable.

So we moved from these routing between huge machines to a proper Kubernetes-based containerization infrastructure that allows us to scale up and down and auto-heal and set really good pod-level permissions and security and high availability, all of those different things. And that was actually code that we didn't write. It was actually one of the Amazon partners that we invited into our code bases, and they worked really closely with our team. They did all of that work over the course of about six to eight weeks, which I can talk about. And not only did it allow us to be more scalable, more secure, all of these different types of things, but actually reduced our cloud cost by five times. So we went from about $25,000 in monthly cost to $5,000 in monthly cost to power the infrastructure for our user base, which was just incredible. I have nothing but good things to say. It was incredible that they invested in helping us do that.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1730.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1730)

 And OpsGuru, some of you may or may not have heard of them. OpsGuru was the specific partner that Amazon invested in that helped us build that whole migration. Now the whole, like I said, they completed the whole project in about six to eight weeks. Our team then took on all of the code, all of the infrastructure. We now own it and maintain it and continue to build on top of it ourselves.

It's funny, most of our competitors use more managed cloud solutions like Fly.io or some of those other types of solutions that you might be familiar with. In our case, we decided to, with the help of Amazon, we were always planning to basically handle our infrastructure by hand. That might be either the best decision we've ever made or the worst decision we've ever made. Time will tell because it takes a lot of investment and a lot of maintenance, but we're already starting to see really cool benefits.

For example, we run TypeScript language servers and Tailwind language servers to power things inside of our app. So for example, if a designer wants to change a prop on a React component right in Tempo, you just have a dropdown of here's the different props that you would want. We actually run a TypeScript language server, which is what powers IntelliSense in VS Code. We actually run a TypeScript language server on our pods, and that's what we use to then power a WYSIWYG props editing experience that feels like Figma.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1830.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1830)

Again, we believe that the scope of things that we need to build won't be possible in the fully managed, fully hosted solutions, and so we did feel it was important to own and manage our infrastructure in-house. Amazon and Kubernetes really made that possible with speed and tremendous cost efficiencies. 

And that's like, again, the partnership with Amazon has only been getting deeper and deeper. So I talked a lot about our cloud infrastructure migration, but they've actually been co-building with us on these true R&D projects that are focused on innovation. One of the projects that we did was around what we call the PRD agent, which I'll explain in a second, but essentially a new type of agent to help product managers specifically. And you can see there's a screenshot of a paper that we actually wrote together, Amazon and Tempo, explaining essentially the work that we did. And so, beyond just migrating and making our cloud infrastructure scalable, we actually work together with Amazon to do R&D on what's truly cutting edge workflow within the industry.

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/1880.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=1880)

So what is this PRD agent? Essentially,  again, it's a lot of information there, but essentially at a high level, every product manager or every person who's an expert in a certain product has a knowledge graph in their head, right? This is my project. Here are the features that exist. Here's the scope of the functionality. Here are the limitations. Here's what's supported. Here's what's not supported. There's all of this context, right? And very, very few organizations other than maybe very large disciplined enterprises have all of that knowledge and that context written down somewhere in a way that it can be parsed by an LLM, right?

All of that context lives in your code, but there's so much noise around the implementation details, and it's a different layer of abstraction from product knowledge. And so the project that we kicked off with AWS was, hey, can we take a code base and basically run a dramatic amount of LLM preprocessing on top of that code base in order to parse out and extract out essentially each of the PRDs that explains the scope of functionality within this application, right? And we did a test with a large open source code base, and the output from that code base was essentially a set of, I think it was 15 or 20 PRDs that in detail explain what's implemented, the limitations, but at the right level of abstraction.

And that output is now used as context for a PRD agent, a PM agent that we're building, such that you can now interact with an agent that knows everything about your product, right, as if they were a senior PM that was there from the beginning of your code base. And that level of context I think just accelerates the nuance and the types of tasks and the types of PRD writing that you can do with AI. And so, anyways, super exciting project that's going to be released early next year. But we've, like I said, been working with the AWS Gen AI Innovation Center team to develop this out. We've had tremendous results that we've been seeing, and please give it a try early next year or contact me if you'd like to be part of the beta.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/2000.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=2000)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/2020.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=2020)

And, you know, I just put the screenshot here, but  the level of depth that the AWS R&D team was able to go is much farther than us as a startup would have even been able to go ourselves. And so not only is AWS helping us do this, but they're taking the project further beyond what we would have even been able to do just on our own. So with that, that sort of describes our  migration journey to AWS, to Bedrock, and our partnership with AWS. And with that, I'll pass it over to Vaidy.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/2060.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=2060)

### AWS Accelerators: Migration Acceleration Program and Gen AI Innovation Center

Awesome. Thanks. Thank you, Kevin and Brandon. That was an excellent walkthrough, yeah. So the thing is, we would allow everyone, every one of our customers in here to have the same journey as Tempo Labs, right? And we want to provide you with some of the accelerators that can get you started in the same journey, be it able to get in the path of migration, continuous modernization, and innovation, and innovation at scale. 

So one of the accelerators I would love to introduce, if you're not already familiar with, is the Migration Acceleration Program, or called the MAP program. So AWS, over the years, I would say 19 plus years of migrating tens of thousands of customers, we have compressed the experience that we have learned into a framework to be able to support you on large scale migrations and continuous modernization. To be able to move at scale, you definitely need multiple aspects. You need a proven framework or a methodology on how you go about carrying out a large scale migration and a continuous modernization so that you can unlock the innovation side of things.

You also need a set of purpose-built services, whether you are dealing with compute, storage, or the latest Gen AI services. How are you able to move from one to the other without having to risk and also be able to remove some of the undifferentiated things as you are carrying out this transformation? There are also some of the services involved, and as Kevin mentioned, there are certain specialists that are involved in guiding you about moving from one technology to another or moving from one provider to another. So it could be a set of AWS Professional Services, the experts that are in-house who would be able to come provide the blueprint. And there are also a larger partner ecosystem who would be able to take some of these blueprints and be able to apply it at scale.

The thing is, I know both Kevin and Brandon mentioned it multiple times, any migration modernization journey means you are keeping the lights on while you are carrying out the transformation. So there could be double cost, triple cost if there are licenses involved or not, and you need to be able to offset it. And that is where the AWS funding comes into play. We look at the long term and are able to offset some of the cost that goes into this transformation. So AWS as an organization is investing in the long term to be able to be successful in this journey.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/2200.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=2200)

One accelerator is MAP, the Migration Acceleration Program. You can reach out to your account counterparts  on how to leverage MAP when you have migration modernization projects. The second accelerator that I want to talk about is the Gen AI Innovation Center. The thing is, as you heard from Kevin, a lot goes on, especially when you have Gen AI-based workloads. The first thing is around the model selection, the right model that you need to choose, and doing a whole evaluation around, hey, is this the right model for the use case that I have? Do I need a race car or would the utility car be good enough for the use case that I have in hand?

And after the model choice and the model evaluation, you would have to take the model to production. So maybe in time to market, the race car would have addressed the use case, but it might not work well from a cost and efficiency standpoint. You would have to either, in a race car maybe the braking system was really good and that was the only thing that you need for the use case, so maybe you could use the braking system from a race car and apply it in your utility car to be able to save costs. So that is the whole thing around model distillation, right? Could you have a teacher-student training model be able to do this model distillation so you can take some of the models to production?

And there is also the fine tuning. One of the things that Kevin mentioned was around being able to use the knowledge that you have and be able to have the LLM agents be able to use the knowledge base. So the thing is, this is around fine tuning, taking the model, doing fine tuning, being able to do the model distillation, and doing this continuous optimization. There are new models coming out every other week from some of the key model providers. So how do you stay on top of these purpose-built models for some of the use cases that you have, and how is this end-to-end journey going to look like?

So the thing is, the Gen AI Innovation Center brings together this one team, a team of data scientists, a team of specialists, a team of solution architects who would be able to work with you either hands-on or through advisory, strategic advisory or technical advisory in a five to eight week time period at no cost for you to go through and work with to make your use case a reality using Gen AI. I would recommend this as another accelerator for your teams to consider, to engage with us. Reach out to the account team on how to bring in the Gen AI Innovation Center.

So the way they engage is, it is two prompts. One is the use cases where you're using primarily the model inference. Another use case is where you are actually doing the model building from the ground up. So based on the two use cases, there are multiple engagement models that are available. It could be just at the advisory level where it is a couple hour workshop, getting into things and providing you with the right next steps or the pathway on how you can self-service the use case implementation, or it could be a deep dive on a five to eight week engagement.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b1c318acb3e896d6/2390.jpg)](https://www.youtube.com/watch?v=xFNz-UdB-Q4&t=2390)

Thank you, yeah, and everyone, we'll be around here  if you want to come chat for a few minutes or whatever. Awesome, thank you. Thanks. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
