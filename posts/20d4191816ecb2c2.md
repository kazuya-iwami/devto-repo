---
title: 'AWS re:Invent 2025 - SageMaker & MLflow: Innovate faster with no infrastructure management (AIM3340)'
published: true
description: 'In this video, AWS introduces serverless MLflow in SageMaker, eliminating infrastructure management and offering it at no cost. The session covers how MLflow addresses AI governance challenges including traceability, reproducibility, and collaboration across distributed teams. Key features include automatic version upgrades, centralized experiment tracking via RAM Share, and native integration with SageMaker Pipelines and model customization. Rohan from Intuit shares their journey from self-hosted to managed MLflow, handling 16 million daily LLM invocations and 43,000 evaluation runs in Q3 2025 alone, reducing model development time from 3-4 months to under 2 weeks while supporting 400+ production ML models generating 60 billion daily predictions.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/0.jpg'
series: ''
canonical_url: null
id: 3093257
date: '2025-12-08T21:55:32Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - SageMaker & MLflow: Innovate faster with no infrastructure management (AIM3340)**

> In this video, AWS introduces serverless MLflow in SageMaker, eliminating infrastructure management and offering it at no cost. The session covers how MLflow addresses AI governance challenges including traceability, reproducibility, and collaboration across distributed teams. Key features include automatic version upgrades, centralized experiment tracking via RAM Share, and native integration with SageMaker Pipelines and model customization. Rohan from Intuit shares their journey from self-hosted to managed MLflow, handling 16 million daily LLM invocations and 43,000 evaluation runs in Q3 2025 alone, reducing model development time from 3-4 months to under 2 weeks while supporting 400+ production ML models generating 60 billion daily predictions.

{% youtube https://www.youtube.com/watch?v=xgdmK3bMarg %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/0.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=0)

### Introduction: The Critical Role of AI Governance in Machine Learning

 Hi everybody, welcome. Let's start with a quick show of hands. How many of you are either using MLflow or are you familiar with MLflow in some capacity? Okay, a few of you. Hopefully this session today will be covering not only MLflow and what we've already launched, but a brand new launch that happened earlier this week. I'm also going to be joined today by Rohan from Intuit, who's going to be sharing his customer journey and his use cases, how they're using MLflow in their lives and how their journey was. So, with that, let's get started.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/50.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=50)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/80.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=80)

 So I like this statement for a couple of reasons. One, I think it focuses on not only what people often misconstrue as reasons for failure, which is the model wasn't built right, maybe there was something wrong with the training data set or with the test data set, but in fact the root cause of many of these problems lies in the lack of governance. So why are we talking about governance, right?  Whether it be a classic ML or a GenAI model, you have these discrete five-step processes typically that are followed. Starting with the onboarding and then you have the slightly different variations depending on if you're building and training or pre-training, fine-tuning. And then registration, deployment, and ultimately monitoring. And in all of these across all of these five different steps, this is where AI governance comes into play.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/110.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=110)

 Without governance, what we often see customers complaining about and challenges they often run into is a lack of standardized infrastructure. This leads to slow onboarding. You need to provision the infrastructure first, and there are often multiple risks associated with this as well, whether it be security risks. And in this day and age we have teams that are distributed across the world. So to be able to collaborate effectively without having any risk of sharing experiments and data, and we want models that are explainable, reproducible, and you need that audit capability associated with this, and this is where governance comes in place, a big part.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/160.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=160)

### MLflow Fundamentals and the Evolution to Managed Solutions

 So MLflow is an open source tool. We have a SageMaker managed MLflow capability. And it's designed to address these four key challenges. Addressing that traceability and repeatability problem so that way you're able to consistently deliver the same results whether it's the data scientist who's doing that or the auditor later on. Being able to package these up via the CI/CD pipeline, be able to take that all the way to production is where MLflow is starting to play. MLflow has a native model registry capability. SageMaker also has another model registry option. And with both of those you're able to have that full lifecycle, the associated approval processes that go along with that and be able to track that all the way through. And because it's a fully managed capability within SageMaker, it also plays well with other aspects and other areas of SageMaker itself.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/230.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=230)

 So as I mentioned, MLflow is an open source tool so you're able to get that self-hosted, you're able to run that on your local machine if you so choose to do so and a lot of customers start in that process. It's a good way to do a proof of concept, get familiar with the tool itself, and that works well. But where customers usually start coming to us is because they run into these three challenges. Self-hosting and managing an MLflow instance leads to this operational complexity. Version upgrades are done almost on a monthly basis at this point and keeping up with that is an overhead that they frankly don't want to deal with. Building these integrations that we'll be talking about with the SageMaker security, building it into the Studio IDE, all of those are additional overhead that often you don't take on if you're self-hosting. Overall, customers cite that the total cost of ownership of self-hosting as your team sizes grow larger, as your use cases get more robust and complex, is actually higher and pace of innovation slows down.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/310.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=310)

 So that was the reason why about a year and a half ago we came up with the fully managed MLflow instance in SageMaker. You still have the full flexibility of choosing any IDE that you need. There are purpose-built tools built into the product itself. And it's secure because it's sitting on the AWS infrastructure and so bound by all the security that goes along with that. And there are still apps that we can plug into from a specialized partnership perspective.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/350.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=350)

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/370.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=370)

So I threw this up there just to show you the evolution of the product. This is  version 2.13 way back when, and as we can see in the new demos you'll see how the pace of innovation is with the latest 3.4 version that we have. But the product is changing. It constantly is evolving. So within SageMaker, this is a typical workflow.  Within your code as a data scientist, you need to provide an ARN, a resource identifier, a unique identifier that tells the code where to go log the metrics and model information into the MLflow environment. From that you are able to navigate to the MLflow UI. You are able to see the information, the metrics and the information that was logged, and then you are able to register and deploy that model. So it's a typical seamless process.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/410.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=410)

### Introducing Serverless MLflow: Infrastructure-Free Experiment Tracking

So as I queued up earlier, on Tuesday this week we launched a serverless version of MLflow.  What this does is it actually takes away all that infrastructure management and underlying things you have to do. Just to give you some context, previously with the fully managed capability, you had to select the server size like a small, medium, large, and you had to make sure that that was all provisioned behind the scenes. Now with serverless, all that goes away. And what I don't have up there, serverless MLflow is also free. So it comes at no cost. So that is a pretty significant change in our capability and offering.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/450.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=450)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/480.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=480)

 So just to give you a context of where we came from and where we are now, you can see the original version that I showed up there, version 2.13 that we had launched about a year and a half ago, to where we are today with the launch of the serverless capability along with version 3.4 that we're supporting right now. So with the serverless capability as I queued up, we don't have that infrastructure management anymore.  So there's no longer a need to select a size because it's serverless and it's going to scale up when the demand is there and it's going to scale back down when the demand goes away.

As I mentioned, MLflow is an open source tool, so the version upgrades are quite frequent. With serverless MLflow now you will have automatic in-place version upgrades. We'll still be making sure that those versions are backward compatible, make sure that they are up to standards, and once that is deemed so, then they will get automatically pushed in all the serverless capabilities. We also have centralized experiment tracking. So what often happens in deployments is that you have a multi-stage environment. You have dev, QA, production, or you have multiple dev environments for distributed teams.

For each of these instances, what we often see and customers were asking for is the ability to share a tracking server, share visibility for these teams to be able to collaborate better. So now we're also going to be supporting centralized access via RAM Share or Resource Access Manager. And last but not least, we've been making contributions to the import export tool, an open source product that allows for easy migration from your existing tracking servers or older version over to the new serverless MLflow capability.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/570.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=570)

 So just to put it in context and see where MLflow fits in, you can see right in the middle we have SageMaker AI with MLflow. On the left you can see that you're able to bring in your local IDEs and your notebooks whether it be within Studio or outside of it. We have native integration now into SageMaker Pipelines, as well as the model customization capability that was also recently announced this week. You're able to take all that, take it into the SageMaker Model Registry in your top right.

Where there's an auto-register capability. If you choose to use that, you're able to automatically register your models from MLflow into SageMaker Model Registry, and then from there deploy that using SageMaker Inference. Or you could also deploy these to other target environments.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/630.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=630)

So we talked about the benefits  for the admin. What does all this mean for data scientists? And this is where I'm really excited by this. What this means is that you no longer have to worry about, as a data scientist, provisioning a tracking server or worrying about whether MLflow is running for you. One's already going to be available by default, so as soon as the domain gets created, an MLflow App, which is what we're calling the serverless version of the tracking server, is going to be provisioned automatically. So there's no longer the need to wait for an admin to go ahead and do this process. It's already going to be done. This leads to rapid experimentation. As a data scientist, my job to be done is to go build models. I don't need to be worrying about infrastructure.

And as I mentioned, this MLflow is now starting to get integrated with other areas of SageMaker. So when you do a model customization, which was recently talked about, behind the scenes we're going to be utilizing MLflow, and the model customization outputs are automatically going to be logged to your MLflow App. Same thing with a pipeline job. If you create a SageMaker Pipeline, which is a multi-step process, behind the scenes an MLflow App will be used, and you're able to automatically log information there. So taking away all those additional friction points that typically you would have to think about or build into your models, you're going to have that automatically out of the box.

Because it is serverless, it's always going to be on. As I mentioned, it's at no cost, so there's no concerns there. Behind the scenes, we're taking care of the scaling up and scaling down, scaling down to zero when it's not in use, and this allows you that flexibility to be able to scale this out. And we talked about this briefly, that cross-team collaboration, to be able to see these MLflow Apps not only from the elements of my domain that I created, but also from other domains or other accounts as well. We have that cross-account sharing.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/760.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=760)

 So just to put that in perspective, you can see at the bottom here, this is our existing flow as it existed today. As a data scientist, what I would have to do is I would have to log into the Studio, navigate to the MLflow page, create a tracking server, which could take anywhere from 20 to 25 minutes, a good cup of coffee or maybe a lunch. Later I have my tracking server up. I need to copy that and then go to my pipelines, do the same thing, and model customization. It becomes a little bit more challenging. You can see that could take a while. In the top, this is going to be the serverless world. As I mentioned, as soon as you log into your Studio, we already have a default app created for you, so there's no provisioning, no creation. You just have the ARN and you're good to go. Or if you're using Pipeline and Model Customizations, we detect and use that default MLflow App natively, so there's no longer need to even copy the ARN, and you don't even have to worry about MLflow running it behind the scenes. So as a data scientist, I'm able to get from point A to point B the quickest way possible, and that's the intent behind serverless.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/840.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=840)

### Live Demonstration: Serverless MLflow in Action

I love this  quote from the Wildlife Conservation Society. I know it's a long one, but I want to focus on the last piece of it. The reason behind doing the serverless experience or utilizing this managed MLflow is primarily so that they could accelerate their time to development, being able to get critical cloud-driven insights. Just to provide the context here, the Wildlife Conservation Society, in this MERMAID project, they're actually taking photographs of coral reefs and then analyzing that. And being able to do that with a small team, they were very interested in utilizing this managed MLflow capability, specifically the serverless, so that way they're able to keep their costs down as well as make sure that their scientists are able to focus on the job to be done.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/890.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=890)

 So with all that being said, I wanted to do a quick demo. I will be honest, I did struggle to find content for the demo because the serverless capability is so easy. There's little to show, so this is going to be a short one.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/910.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=910)

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/920.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/930.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=930)

So bear with me here.  All right, so as I said, we're going to start with the admin persona. For what it's worth, you're going to go in as an admin.  As soon as you go into your domains and you create a domain, you can see that Amazon SageMaker MLflow is going to be enabled by default for you. So you don't actually have to do anything as an admin.  Behind the scenes, IAM roles are going to be created, permissions are going to be set up, everything is going to be ready, and that's it. That's the job of the admin to enable MLflow.

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/950.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=950)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/960.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=960)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/970.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=970)

So now I'm going in as a data scientist. This is the SageMaker Studio, and you can see as soon as I click the MLflow,  a default MLflow app has already been created for me. So I didn't actually have to create anything. I didn't have to do anything. One's already there.  If you're an existing customer for Managed MLflow, you can always access your tracking servers. It's in a separate tab, so you're good to go there. You can see I can continue to view the details.  I can copy the ARN here that I can then use in my experimentation and notebook jobs.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/980.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/990.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1000.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1010.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1010)

As I go back, you can see that you  still have the flexibility of being able to edit the name, make it customized, and you're also able to create other MLflow apps as well, right? So you're not just restricted to the default one.  We're defaulting the IAM role, the S3 bucket behind the scenes just to kind of make that life easier  as well. So it's now a one-click process. Provisioning an MLflow app on average takes about two minutes, right? So long gone is your lunch break. Now it's kind of more of a  quick coffee break maybe.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1020.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1030.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1030)

Here I'm showing you the other capabilities. You're able to kind of see other MLflow apps. They may be in other domains. You're able to see that here  and you're able to see that information. So while that's creating, we're going to go into the pipeline step here for SageMaker Pipelines, and you can see this new integration option has  now been popped in. So this allows you to automatically register your metrics and information during your training, fine-tuning, and notebooks. You can see here we've automatically detected the MLflow app, and you're able to use that. You're able to provide a unique experiment name so it's easier for you to go find that after the fact.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1060.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1070.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1080.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1080)

And for those not familiar with SageMaker Pipelines,  you can see that it's a very easy drag-and-drop experience using the DIVE framework. So to show the other integration  option here, this is kind of that model customization capability that was recently announced. As we go in here, you can use the customized option.  And as you're customizing this model, you're providing all this information. Under advanced configuration here, MLflow is already built in. You can see again we've already detected that a default MLflow app is there, so that's not even a selection that you need to do, but you can always overwrite it with something else if you so choose to do so.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1100.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1100)

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1110.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1110)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1130.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1130)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1150.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1150)

 So once I would submit this job, once the customization job is completed, at the end of it, you'll be shown MLflow metrics and information.  So if I go in and open MLflow here, this is the new MLflow UI. You can see it's a stark contrast from where we started, and here I'm kind of showing that now not only is it traditional ML, but it's also GenAI agent use cases.  In this particular example, this is IT support, sort of example here. So not only do they have different information about like what type of ticket, in this case how do I resolve this ticket, we have the correctness in the top right, but also a breakdown of the actual  way in which the lineage, how did the agent go about answering this question. So from a traceability perspective, from an observability perspective, you're able to diagnose these issues.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1170.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1170)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1180.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1180)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1190.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1190)

So if you look at this other example here,  you can see that in addition, for this one experiment, we have multiple runs. And for each of them, you're able to kind of utilize the inbuilt  visualizations to compare metrics across all these different runs to identify the optimal model and execution that you need to do.  So there's inbuilt version tracking as well for each of these models.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1210.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1210)

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1220.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1220)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1230.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1230)

As I mentioned, MLflow has a model registry capability built in,  and in addition to that, each one of these models can be auto-registered into the SageMaker model registry as well.  With the latest release of version 3.0, I believe prompts were also added. Here I'm demonstrating the ability to compare prompts across each other.  You can see that it highlights in green any changes and in red any deltas.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1240.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1240)

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1250.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1250)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1260.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1260)

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1270.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1270)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1280.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1280)

Just to complete the cycle from an admin perspective,  not all is lost. You have stuff to do if you want to do cross-account sharing. I'm just demonstrating what that Resource Access Manager capability looks like.  You're able to go back in and select which MLflow app you want to share across accounts. You're able to give it specific permissions,  whether you want the ability to create, edit, and so on. You're able to identify which  accounts you want to be able to share that with. That's it. That's all you would have to do from a sharing standpoint. 

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1290.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1300.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1300)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1310.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1310)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1320.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1320)

Next, I wanted to also show the import-export tool that I talked about.  This is an open-source tool that's available. You can see from a high-level architecture perspective, the idea is that you're able to take a tracking server that you may have  self-hosted on SageMaker or anywhere else, and you're able to move that over to MLflow Apps or serverless MLflow.  Our recent contributions here have allowed us to ensure that we're able to keep up with the latest versions of MLflow as well. 

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1360.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1360)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1390.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1390)

### Intuit's Journey: From Predictive AI Velocity to Generative AI Scale with MLflow

That's, like I promised, a short and sweet demo. With that, I will turn over the stage to Rohan to talk about Intuit, their journey, and how they're using MLflow today. Hi all, good afternoon, good morning. My name is Rohan Tangadpalliwar. I'm a Staff Software Engineer at Intuit. Today, I want to tell you our MLOps platform story,  from velocity to scale in two different acts, like the journey and how it proceeded. 

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1400.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1400)

Act One is about how we built our world-class MLOps platform. We were able to use that for predictive AI and then solved it for velocity. Act Two is about how the generative AI revolution forced us to reimagine the same platform for scale. Let me introduce you to Intuit. Our mission is powering prosperity around the world. Our strategy  is to be an AI-driven expert platform that serves our 100 million plus consumers, small business customers, and mid-market customers, such that they make more money, save time doing less work, and when they make financial decisions, they do it with complete confidence.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1430.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1430)

Before I take you into the journey,  I want to talk about where we are today. This is the state of the art at Intuit. These are Intuit AI agents embedded into our products. These aren't just regular chatbots answering questions. These are agents that can perform and complete complex tasks. Imagine a small business owner asking about their finances and a finance agent proactively tells them about their expenses being up, or their accounting agent categorizing their transactions and expenses automatically. Then there's a payment agent which gets the invoices ready for the businesses to go. Also, the project management agent creates the new project, gets you started, and then you can just dig into the further details of that project as a small business owner. Another one is the customer agent,

which sources leads, drafts personalized emails on your behalf, can schedule and suggest meetings based on the data it compiles, and also tracks every customer opportunity in the sales data that we have. Like this, a virtual team of agents can automate and transform the entire business workflows from customer management all the way to project management. So this is our AI-driven expert platform in action.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1570.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1570)

This is impressive, right? But we didn't get here overnight. We have been laying the foundation for over the years, and let me talk about how we got here in that two-act story I was talking about.  So Act One is the Age of Predictive AI. We run over 400 ML models, predictive AI ML models, in production today, and they are deeply embedded into our product experiences. This traditional predictive AI is a complete workhorse. If you see the numbers, we basically pass vast volumes of data to predict about 60 billion predictions every single day across all these models. About 2 million transactions we categorize using these models, and then 25 million plus customer interactions via NLP. These are the models that power our product experiences.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1620.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1620)

 So here are some of the models that are widely used across our business units and products. For instance, our FinTech Group uses the fraud detection model to detect frauds in the payment transactions. QuickBooks uses classification to categorize and classify the expenses and the transactions in the businesses. Similarly, other business units like TurboTax also use these models, and these are just a few examples, but there's a lot going on there in predictive AI.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1670.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1670)

The reason I tell you that is because with usage of all this, the success led to a major challenge for our developer teams, and what I call it as a good  problem. So we have hundreds of models and we have thousands of technologists. You basically get chaos. Our product teams were suffering with experimentation that was unorganized with no central place for tracking those experiments, and this meant there was an issue with constant issue with the reproducibility and the way teams collaborated with each other and within the teams, how they collaborated to track the experiments. So it was slow, and for the leadership we were flying blind. The leadership had no idea how much time was spent into experimenting to get a new ML model out in the market. We needed a system of record to capture all this information and experimentation.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1730.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1730)

So this is where  MLflow came in. In Q1 of 2023, we did multiple vendor evaluations for general experiment tracking solutions, and then open source MLflow came out as, you know, it was determined as an optimal fit for Intuit's requirement. And then we adopted that MLflow as our central experiment tracking platform. And it has been transformative since then. I mean, if you look at some of the gains that we gained, teams reported that their productivity has improved significantly. Also, the model development times went down for some of the teams from 3 to 4 months to less than 2 weeks, which was pretty impressive. So basically we were democratizing AI not just for data scientists, but also for developers. So here we had solved for velocity.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1810.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1810)

And you know, we were basically done back then, right. And then what next? So there comes  the Generative AI revolution, which basically brought in more challenges for us because it was not just about how our entire developer community can access those large language models, but it was also about how they can access that through our proprietary GenAIce platform at Intuit. So this introduced a lot more components and tools that we supported.

One is LLM evaluations. So it's basically an LLM leaderboard where we basically benchmark the LLMs based on Intuit's data and then criteria. Then we have an evaluation service to measure the quality of LLM applications and an evaluation service to measure the performance and quality of AI agents. So basically we are not focusing on F1 scores anymore. We were tracking a lot more data and metrics for LLM applications and agents.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1890.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1890)

So this plot basically brought in an explosion of experimentation and metrics that  we were capturing. If you look at it, Intuit is basically, all the products are making over 16 million LLM invocations per day to all these experiences. And to support such a massive volume of invocation, we basically needed a robust system for evaluation because we don't want to put something out there in our customer's hand without knowing it is safe, it is accurate, and it is producing desired results.

So it created a lot of experimentation for evaluations and runs for evaluation and metrics. If you see the charts are going vertical towards the end of 2024 and in 2025. There are about more than 1,000 new experiments created in just last three quarters, and within those experiments, we have more than 43,000 runs in just Q3 of 2025 alone. So we were basically facing a hyperscale of the agentic AI.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/1970.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=1970)

And to solve that,  explosion of experiments and runs for evaluation, we had to standardize the paved path for how we actually did evaluation. So that is what is depicted here in the short architecture diagram here. So a developer writes agent code in their IDEs and notebooks. And then once they are ready, they test it with the evaluations in their local setup on the IDE, or they can run it in the CI/CD pipelines, or they can do a large-scale evaluation in a batch pipeline.

So once they run the evaluation, the result of that is stored in a central store which has two main components. One is an observability tool which stores the detailed logs and traces of the interaction in the evaluation. And the second component is MLflow, which is the metric store, which stores the summary and scores of how the agent performed in the evaluation. And this core is basically powering our quality dashboard, giving every developer and leader a single view of how the agent is performing.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/2070.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=2070)

So with that, we had to also think about rearchitecting our  MLflow setup because now we were looking at a different scale than we had before. And for that, we thought we'll go with managed MLflow platform that provides massive scalability and availability, and then we don't have to think much about how to maintain the system, which we did before when we hosted it at Intuit with our own infrastructure. So we didn't want to spend a lot of time in migration, so we engaged with the AWS ProServe team who helped us make this move faster.

That was a very key partnership that we had with AWS ProServe, and that basically speeded up the process for us to get our platform onto managed MLflow on AWS so that our teams can focus on what they do best in waiting for the customers with AI agents.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/2140.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=2140)

 So what did this essentially enable for Intuit, the scaled version of the MLflow platform? For the Intuit AI agents that I talked about before, they wanted to have confidence in quality, accuracy, and safety. Managed MLflow is what provides that quality, accuracy, and safety. This is what gives us confidence to ship our AI agents. Again, you can't ship what you can't measure. So this managed MLflow platform is basically a measurement and evaluation platform for us that gives the confidence to take these agents into production.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/2200.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=2200)

 So what's next in the journey? You've seen the charts. It's not going flat anywhere. It's all in the vertical direction going up. So we have to keep up with that growth, and we want to be more dynamic. We want to make sure that we are truly serverless in architecture that way we don't have to spend time in managing the platform to scale manually. So we are actively exploring the managed MLflow serverless solution for our MLOps platform. It will basically help us to scale instantly and in a more cost-effective manner. So that will help our developer organization to focus on their development activities.

To conclude, our journey at Intuit has been one of constant evolution, from solving for velocity with predictive AI to enabling scale for generative AI. By migrating to managed MLflow, we have turned our experimentation engine into a strategic advantage that powers innovation for our millions of customers. With that, thank you all for attending. Back to you, Rahul.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/20d4191816ecb2c2/2330.jpg)](https://www.youtube.com/watch?v=xgdmK3bMarg&t=2330)

### Key Takeaways: Simplification, Productivity, and Seamless Integration

Thanks Rohan for sharing your story. I think we can all agree the multi-part journey that Intuit went through is very indicative of the value propositions and everything else that I was talking about earlier in terms of the journey that customers typically take from going from a self-hosted instance as they were before to a fully managed capability and then where serverless is heading as well. So I wanted to leave everyone with three key takeaways  from today's talk.

First and foremost, with the launch of serverless MLflow, it's that infrastructure simplification. No longer, as we recall, do you have to worry about is it a small, medium, or large size. Do I need to stop and start the server? Do I need to upgrade? What does that look like, and how do I need to provision this for my users to start working with it? All of that went away, right? That quick demo that we were able to demonstrate, all of that and all of those capabilities being integrated by default.

What that led to was point number two. As a data scientist, you're able to get to the job to be done. So whether it be SageMaker pipeline or model customization, having that integration by default allowed me as a data scientist not to have to worry about, hey, what is MLflow, what is the tracking server, what is going on behind the scenes. I'm able to get your job to be done as quickly as possible.

And then last but not least, that's number three. Everything is built on AWS infrastructure. So in terms of being able to scale that out, being able to integrate with the SageMaker security, the studio provisioning, making sure that it also works seamlessly with other aspects of your build is what that last point is about. Hopefully this was helpful. Thank you for attending today. I really appreciate it. If there are any questions, happy to take that outside, but thank you everybody.


----

; This article is entirely auto-generated using Amazon Bedrock.
