---
title: 'AWS re:Invent 2025 - Amazon Aurora HA and DR design patterns for global resilience (DAT442)'
published: true
description: 'In this video, Tim and Marc explain how to build resilient applications using Amazon Aurora MySQL, Aurora PostgreSQL, and Aurora DSQL. They cover five key questions about database resilience: node failures, performance scaling, connection management, regional failures, and maintenance. For Aurora MySQL/PostgreSQL, they demonstrate six-copy storage across three availability zones with zero RPO, read replicas for high availability with 30-second RTO, Aurora Global Database supporting up to ten secondary regions with one-second lag, and blue-green deployments for zero-downtime upgrades. Marc then presents Aurora DSQL''s active-active architecture with no single point of failure, automatic horizontal scaling, optimistic concurrency control, and multi-region clusters using witness regions for quorum-based failover without data loss.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/0.jpg'
series: ''
canonical_url: null
id: 3092977
date: '2025-12-08T19:03:42Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Amazon Aurora HA and DR design patterns for global resilience (DAT442)**

> In this video, Tim and Marc explain how to build resilient applications using Amazon Aurora MySQL, Aurora PostgreSQL, and Aurora DSQL. They cover five key questions about database resilience: node failures, performance scaling, connection management, regional failures, and maintenance. For Aurora MySQL/PostgreSQL, they demonstrate six-copy storage across three availability zones with zero RPO, read replicas for high availability with 30-second RTO, Aurora Global Database supporting up to ten secondary regions with one-second lag, and blue-green deployments for zero-downtime upgrades. Marc then presents Aurora DSQL's active-active architecture with no single point of failure, automatic horizontal scaling, optimistic concurrency control, and multi-region clusters using witness regions for quorum-based failover without data loss.

{% youtube https://www.youtube.com/watch?v=TNURufOhu_A %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/0.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=0)

### Introduction to Database Resilience: Defining Availability and Disaster Recovery

 First, I'll apologize for a bit of AV. We might have some challenges partway through, but thank you for your patience. Hi folks, welcome to DAT442. If you're not here to learn about Aurora and resilience, you're in the wrong place. My name's Tim. Shortly, I'll be joined by Marc, and we're going to talk today about Aurora and how to bring resilience to your applications using Aurora.

So who here, show of hands please, runs a database in production? Most people. Who here, because you're still at re:Invent and you're not at work right now, who here feels a little bit uneasy about running a database in production? Some, lots of hands actually. So you've got a pager in your pocket for that kind of thing. I'm happy that you're here. We're going to learn today. There's a whole bunch of choices we can make about making any system more resilient, and Aurora is no different.

Alright, so what we're going to do first is define resilience so we know we're talking about the same thing. Resilience is, by technical definition, the ability of a workload to recover from disruption, or for infrastructure to recover from disruption, to dynamically acquire resources to meet some demand, and to mitigate disruptions caused by misconfiguration or something like that. So it's not just about when something crashes, it's not just about when a server catches fire. It's got performance and a whole bunch of other things in it too.

And it really has two pillars underneath it: availability and disaster recovery. Availability is the proportion of time that a workload is available for use. So this is usually a historical time measure. We might say it's been five nines availability for the past year or something like that. That's when we talk about availability. The other one is disaster recovery or DR. Normally when we talk about DR, these are the techniques you use to recover your workload when something goes wrong, maybe there was a volcano or something like that, or some human threat.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/120.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=120)

Here we use Recovery Time Objective, RTO, as one of the measures that we talk about. So that's the maximum amount of time  between one single event and the resumption of service. That's your return to operations time. And then there's the Recovery Point Objective, the RPO, which is the maximum amount of data, normally measured as an amount of time, that we can afford to lose, maybe due to asynchronous replication or something like that. And you might want your RPO to be zero. That's a valid value as well. Okay, so that's the level set.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/150.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=150)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/170.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=170)

### Aurora PostgreSQL and MySQL Overview: Addressing Database Node Failures

Now, just a bit of an agenda. Next we're going to talk about Aurora PostgreSQL, which I'll call APG, and Aurora MySQL,  AMS. I have to say that word about ten thousand times, so apologies for the acronyms there. Now they're both very similar, so a lot of the features that they support are similar, so I will talk about them together with respect to resiliency. And then about halfway through, Marc will pick up and he will talk about the same set of questions that we're going to ask ourselves about DR and HA.  He's going to answer them again, but from the point of view of Aurora DSQL. So keep your bearings. First part, Aurora MySQL and PostgreSQL. Second part, DSQL.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/180.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=180)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/210.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=210)

 Okay, so first, if you don't know anything about Aurora, APG and AMS, what are these things? They're relational database services that combine the speed and availability of a high-end commercial database with the simplicity and cost effectiveness of the cloud and open source. So APG and AMS are fully compatible with PostgreSQL and MySQL open source engines, and that allows your existing applications and tools to run without modification. So today we're going to get into some questions, like five questions we're going to ask ourselves,  but if you want to ask your own questions, there's a chalk talk, 424, on Thursday as well that you can come along to and hit me up.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/220.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=220)

 Alright, so question one we're going to ask ourselves about HA and DR: What if my database node fails? So if you're in self-managed, this is self-managed, little tag up in the corner there we can see, so if you're just starting out with self-managed, you might have a system that looks like this. You've got one node. It's got your database engine. It's got your storage inside the same node, and there's only one of them. So that means that all of your storage and execution work happens in there.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/250.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=250)

You might back that up with some third-party tool. You might send your backups to S3.  When you do the backup, we might need to do some kind of quiesce thing there, so that's going to slow down your performance, maybe give you some bumps in availability. That's not great. Because that's really difficult, we probably don't do that backup very often. Maybe we do it once a day. So that means that your RPO is something like twenty-four hours. That's not so great.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/270.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/280.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=280)

So then if that node goes down, it means that your storage and your engine has gone down,  so that's not great for your availability and it's maybe not great for your durability either. So your RTO could be many hours. It's not a great situation.  So let's look inside Aurora and look at the durability story first, the RPO part. So you can start with a single database cluster over there on the left-hand side there in one Availability Zone, single instance, and we're going to use this empty box at the bottom here. We'll fill it in as we go through today.

### Aurora Storage Architecture: Six-Copy Durability and Continuous Backup to S3

This is Aurora storage. Grover is its code name. This is where all the magic happens with Aurora for AMS and APG.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/310.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/320.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=320)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/330.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=330)

Aurora storage is a multi-tenant storage fleet. We've got storage nodes, the big pink boxes, spread across  three availability zones. They're multi-tenant, so they've got some other customers' data inside there as well, represented by the little colorful boxes. And so what we're doing is  for every write that you make from your engine, we're going to make six copies of that write across three availability zones. You're only going to pay for one of those copies. What that means is that we can  lose a whole availability zone worth of storage nodes and one more of them, and we still don't have any durability problems. So it's a really strong durability story out of the box with Aurora. You did nothing to configure this, and you're getting six copies across three availability zones.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/350.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=350)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/360.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=360)

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/370.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=370)

Your RPO is zero because there's no asynchronous replication going on here. So just by using Amazon Aurora MySQL and  PostgreSQL out of the box, you get this. So we'll dig a little bit deeper in this area. What is this data that we're storing in here? So a traditional database  writes a log record of what it's doing, then it writes the pages, and then it might come back and checkpoint them. It has to do this in case it gets interrupted. What Aurora's doing is it's writing  just those log records. It's not writing pages, never writes pages to disk, it only ever writes log records.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/390.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=390)

The storage understands how to apply those log records and turn them into database pages again, which we need when we need to save some space or we need to satisfy some reads. So if you think about it, what that means is Aurora has this  whole timeline of what's happened inside the database. Every single commit that's happened into the database, we have it there in sequence. So that's how we can implement some of these features we'll talk about.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/410.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=410)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/420.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=420)

The first one of those features is backup and restore. So on the left there you've got your production cluster, your database is writing down, so I've got some  colored lines there in those boxes. They are the different database pages and their log record lineage. So what we're doing is the storage engine,  not the head node, is continuously backing up all that data to Amazon S3. So the first key point there is your head node's not doing it, so it can't possibly impact performance of your head node. There can't be any quiescing or anything, this is continuous. It's happening all the time.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/450.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=450)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/470.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=470)

So what this allows you to do then is to do a point in time restore from anywhere from the earliest restorable time, which can be up to 35 days, configurable in your retention window, up to the latest restorable time, which is about five minutes ago for a busy database. Any point in time there we can choose.  So let's do one of those restores. Let's choose time equals seven because I'm unimaginative to do this restore. What's going to happen is those storage servers are going to pull that storage data back from Amazon S3. We're going to apply our internal log records to catch you up to exactly that point in time.  And then we're going to start up an engine, and the engine's going to do its normal transaction recovery logic, so it's logical level, to handle interrupted transactions and things like that.

### High Availability Through Multi-AZ Standby Replicas

So again to answer the question, the second part of your job's really easy because all of this comes out of the box. You didn't have to do anything to support point in time restore. Aurora's doing it for you underneath. That's data, but what about database instance availability? So in your most basic configuration, it can look like this. You've got one instance. If it fails, then we can make another one, but it'll take 10 or 15 minutes.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/530.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=530)

So what you can do is you can bring up another instance here, we'll call it a standby. Aurora will keep it in sync as well, so remember they're sharing the same storage and every transaction that's happening, we're telling the other instance about it as well. So now you've got a replica, so whenever something needs to fail over, we'll make them the same size. Whenever something needs to fail over, then the other one could take over really quickly within just a few seconds, and because they're the same size, they can shoulder the same amount of load,  right? This one becomes a primary.

So this gives us an RPO of zero because they're shared storage. They're not replicating storage. We're not worried about asynchronous commits or anything like that. When you fail over, your RTO is typically around 30 seconds. So by doing this, this is one of those patterns to get what we call a Multi-AZ availability pattern to really increase your availability.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/580.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=580)

### Performance Resilience: Scaling Reads with Up to 15 Replicas and Serverless Instances

What about performance is the next question. Performance can fluctuate. Your application can push harder on you, then it can push softer on you and you might be wasting money instead, so we want to be resilient to these performance level changes too. With open source, you've broadly got two ways to deal with this. You can scale up an instance, which probably means buying some hardware and doing some migrations, and that's a bit hard. Or we can do some scale out, which means in the application layer, you would have to be aware and divide your database  into pieces and the application has to know which piece to go and talk to. That's really difficult for consistency, you have to deal with it as well. And noisy neighbors is tricky to deal with too in this situation.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/590.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=590)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/600.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=600)

 So back to Amazon Aurora MySQL and Amazon Aurora PostgreSQL, what can we do about this? Well, we can add that replica that we had before. We don't just need to add it for  availability purposes, we can add it for performance purposes too, because we can just send reads to that other replica. It's going to see stale data.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/610.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=610)

Up to that asynchronous lag,  100 milliseconds or something maybe like that, but it's completely offloaded. It's not going to have any impact on performance on the writer. Now if you've got another workload, maybe like some kind of analytics job or something, then you can run that on another replica. Then all the resources on that replica are isolated for that application, with the buffer pool being the most important one for performance. So this is really important for resiliency against performance changes when you can separate out your noisy neighbors.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/640.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/650.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=650)

So you can add up to about 15 of these replicas,  so you can be quite fine-grained here. Each of them, if you're really big, could be 48 X largest, that is 1.5 terabytes of memory. That's a big system, 15 of them, right? Now,  I haven't talked about anything to do with storage scaling yet, and that's because you don't have to worry about it. So the storage is scaling in capacity automatically and it's also scaling in performance automatically. There's no performance provisioning in Amazon Aurora MySQL or Amazon Aurora PostgreSQL at all.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/670.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=670)

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/680.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=680)

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/690.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=690)

OK, now instance scaling. So we have this instance off to the side there, it's an R9G,  latest Graviton 4. It's a 16 X large, pretty big box, not the biggest one we make. So we can add the  same sized ones, good for failover like we talked about. We can add different sized ones, 8 XLs, 4 XLs. We can add this DB.serverless thing too, which we'll talk about some more.  That's an elastic instance that grows and shrinks all the way from zero all the way up to 256 Aurora capacity units.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/700.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=700)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/720.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=720)

 When we do have a lot of instances like this, we have to decide which order we want to fail over to them. There's no good me trying to fail over to one of those 8 XLs because it'll be too small to deal with the load that I have. So I need to define these failover tiers, we call them. The tier with the lowest number, tier 0 in this case, that's where we'll fail over to  first. That serverless instance, we'll talk about a little bit more, can expand and contract so you're resilient to going up when you have a spike in workload. And then you're not going to get stuck there paying a bunch of money, it's going to scale back down again.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/740.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=740)

### Connection Management: Endpoints and AWS Advanced Wrapper Drivers for Fast Failover

OK, so with all these instances, that's all good, but my application has to connect to them somehow, right, and that can be a bit of a pain  to know which one is the writer. I need to talk to that one if I need to send transactions that have writes in them, like updates. How do I do it? And I don't want to have so many of these connections either. I don't want to have lots of idle connections, that's not great for performance, not great for cost. So I don't know which connection's going to go where, and it's going to have high overhead even if I can work it out. So that's probably not a great situation for self-managed.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/780.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=780)

So I can use two techniques here, pooling and limits. So we can limit connection count per database instance. The big one should get more connections than the small one maybe, per type. I can avoid reconnections. This is all pretty standard, there's no special Aurora here. If you've done databases for a while, this should look pretty familiar.  Some of this stuff is provided for you by the driver frameworks that you already use, if you're using Spring maybe and JDBC and some other things. But they tend to fall down with clustered databases like what we're using here, because they don't understand what's going on.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/800.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=800)

So the first thing we're going to talk about is endpoints. With Aurora, we have two endpoints, generally speaking.  You have a writer endpoint which will always point to the node that is the writer right now. And when that changes, we will point it somewhere else. So you can always just send your writes to the writer. Then we have a reader endpoint, which points to the pool of, remember, because there could be 15 of them, the pool of instances that can accept reads using DNS round robin. So because it's pooled, that lets you just do some coarse load balancing there as well.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/830.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=830)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/850.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=850)

So when we need to fail over, then we just repoint the writer endpoint. That can happen really quickly just with a DNS  lookup and a reconnection. Your application already has to handle this reconnection anyway for other errors. So this way we can turn over in basically what the DNS time is, 35 seconds or something like that. For further flexibility, you can create a custom endpoint. If you want to just group together a few endpoints and call them the analytics endpoint or something, you can do that too,  it's up to you.

Right, so further to this though, we have this thing called AWS Advanced Wrapper Drivers. So this is when we talk about availability, not just performance of that connection kind of question. So each of your applications, pieces of code has to have specific reconnection code inside it to deal with these errors. It has to recognize that this is a retryable error, it has to deal with that. It has to do this fast enough because this is availability time, it's a bit of a problem, right?

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/890.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=890)

So Aurora gives us two extra tools here. We've got rapid drivers and we've got RDS Proxy. So the rapid drivers, they've got intimate knowledge of what's going on inside a  cluster. They know which ones are the readers and the writers at this point in time. And so they can handle failover much more quickly. They're built by wrapping around your existing platform driver like JDBC in this case, but we have them for ODBC, Node.js, Python, a bunch of other ones. So if you've already qualified your platform driver, we're not going to change it.

We're just going to wrap it with some plugins. One of those plugins is called Enhanced Failover. Another one's for read-write splitting and blue-green deployment, which we'll talk about, and a few more. So if you're not using these drivers, we really encourage you to go and do that. They're great for a whole bunch of reasons. What this will do is reduce your failover time down to about six seconds as well.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/930.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/940.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=940)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/960.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=960)

### Multi-Region Resilience with Aurora Global Database

If you're interested,  there's a QR code for the GitHub project there where all of these are open source.  We're not doing anything kind of secret squirrel here that you can't do yourself. We're just doing it for you. So that was all in one region, but what about multi-regions? What about if a whole region fails?  Well, that's a problem, maybe if my business needs to continue when that region's down, or even if that region didn't fail but if it's cut off from the network, which is maybe even a more likely failure scenario.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/980.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=980)

What we've got here, what we'd like to have is a copy in another region, and we'd like it to be as up to date as possible. We'd like it to not cost us very much money, and we'd like to be able to use that copy for other things. If you're self-managing, this is pretty hard. So with Aurora, we have Aurora Global Database for Amazon Aurora MySQL and Amazon Aurora PostgreSQL.  This is where you can add up to ten, but I can only draw one here because my slide's not big enough, ten secondary regions.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/990.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1000.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1000)

 On the left, Region A, we have the same picture we've been talking about for 15 minutes now, some replicas and some storage. When we add another region into this thing, we internally in Aurora spin up a replication server, a replication agent.  We handle this asynchronous physical replication between them. Notice how your head nodes are not involved in this at all. This is not logical replication. You're not having to pay for the performance tax of binary log in MySQL and things like that. We're doing it all underneath.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1020.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1030.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1030)

 That means your application doesn't even know that this is happening, so you can have up to these ten global secondaries without even knowing about it.  We write some of those log records, those colorful boxes I'm so good at drawing, and they also get replicated over to the other side on the storage in Region B. You notice how there's no head nodes in Region B at the moment in my picture. There's only storage. So in this particular configuration right now, this is good for keeping your data available and durable, but it's not really good for availability. We haven't got our workload able to run over there.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1070.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1070)

Your RPO, the lag, because this is asynchronous between Region A and Region B, is typically about one second. It depends on your choice of region pairs. I can't do anything about the speed of light, I'm afraid, so if they're far away, it's going to be a little bit slower.  And then if you wanted to, what we could do is we could add some other instances over to the secondary region as well. Now I've drawn a symmetrical configuration here, but it does not need to be symmetric at all. You can have as many different instances there if you'd like. And they're read-only nodes. They're replicas just like we said before, so they can do low latency local reads.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1090.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1090)

 If you don't test it, then you don't know it's going to work. So what you've got to do first, we have this global endpoint thing. The same question we had before about how do you know which one is the writer that you've got to point at. Well, if we zoom that out to the next level up, the global level, how do you know which region is the primary region at that point in time? Well, that's where this thing called a global endpoint comes in.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1110.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1120.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1120)

 If there's some issue in Region A, we've got that global endpoint name there, and you're going to trigger a failover first. We'll talk about that some more.  That means the writer is going to move over to Region B. This is a managed operation. You see that allow data loss flag down in the bottom corner there. That's to try and remind us all that this is asynchronous replication, so anything that was inside that asynchronous replication window has a chance of being lost when we do a forced failover.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1140.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1140)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1160.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1160)

 When that's happened, the writer is moved over to the other region. We've got a new primary region, now Region B, and the global endpoint is going to be pointed back at that using Route 53 data planes, so that happens as fast as we can make it happen. This gives you really high write availability across multiple regions.  So what if something hasn't failed in global database? We just want to do what's called a switchover. So this is what you do when Region A and Region B are both healthy, not when one of them's gone bad.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1170.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1170)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1190.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1190)

 What we want to do is we want to tell Region A you should stop being the primary, and Region B you should start being the primary at this point in time. Now this is pretty interesting the way we built this to allow it to happen so quickly, so I want to go into it a little bit.  We have this log record stream, so we insert this special log record into that stream, a marker log record, and that flows through just like we described before, and the other side sees it. So when the other side sees it, it says at this instant in log time, not wall clock time, I should become the primary right now. And Region A knows that it should stop being the primary at that time. So there's a nice clean handoff we can get, zero RPO, really low RTO.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1210.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1210)

I just think it was pretty interesting, and I wanted to tell you how it worked. 

### Maintenance and Upgrades: Automated Patching, Blue-Green Deployments, and Organizations Rollout Policy

So we also know that this will give us an RTO of about 30 seconds and an RPO of zero. The final question we're going to ask ourselves is what about maintenance? For example, version upgrades. Remember we're fully open source compatible, so we have a version. We might have PostgreSQL 16, and we need to get to PostgreSQL 17. Well, how do we do that? We might just have a patch or an OS patch that we need to apply. How do we do these things? They're all upgrades, as we call them.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1270.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1270)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1280.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1280)

If you're self-managing, then you'd have to take a downtime window. You'd have to go and talk to your customers and work out when they would like to have some downtime. You would need to take some snapshots so that you can do some rollback. That might be difficult if you don't have the cool storage like we do. You'd have to coordinate with everybody. You'd have to do compatibility and testing, and lots of it, between your application, between the old database versions, and all these other things. Something people often overlook is performance. When you do an upgrade, probably even if you just do any old restart, your performance is probably  going to drop because you've just restarted your engine and lost your buffer pool. So performance testing has got to be part of your upgrade compatibility as well if you're self-managing. 

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1290.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1290)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1290.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1290)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1310.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1310)

Whether you're in Amazon Aurora PostgreSQL or Amazon Aurora MySQL,  because we're fully open source compatible, we can't ignore this. We must maintain a strong security posture.  So we provide a fully managed and automated minor version and patch upgrade experience. We can see here in the console we've got some maintenance actions there. There's an upgrade available for this database cluster. We do rolling operating system upgrades now that we didn't used to do, and that means we have increased availability. 

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1320.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1330.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1330)

You define a maintenance window. If you don't want to define it, we'll define it for you. That says that this system will receive any automated maintenance within this window of time.  You'll get maintenance notifications here and in the AWS Health Dashboard, and this is all able to be automated if you like. It's opt-in. We of course strongly recommend that you opt in because  you don't have to think about it anymore and we'll deal with it.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1340.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1340)

Now, a new thing for us here is AWS Organizations upgrade rollout policy. It's a bit of a mouthful, but what that means is  we had a good story for a while about how to automate the lifecycle of one database cluster. But I hope that most of you probably don't have one database cluster. You've probably got tons of them. You've got a fleet of them, and you want to manage them as development and QA and production, or some other mechanism that you've got. But we never really gave you a way to deal with that before. This is where Organizations upgrade rollout policy comes in.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1370.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1380.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1380)

So on the left we have a GUI screenshot, and on the right we have the JSON version of the same thing. What this means is that you set a policy based on tags.  Here we have a tag called env, and we say that any resource inside my AWS organization, that might be an account or a database cluster,  that has an env tag value of prod will be upgraded last. You can see some other ones there. If it has an env value of dev, it'll have to be upgraded first. And anybody who doesn't have a tag that I recognize, they'll get upgraded second. That's the default at the bottom there.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1400.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1400)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1410.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1410)

So how does this happen  when we actually roll it out? We built that policy, and now we have to assign it to the resources in our organization. Maybe it's an account or all of the things in the organization.  Then what will happen is you can see this. You can see the upgrade order is determined to be second for that resource there. That's really helpful. So first we'll notify you, then we'll wait for the maintenance window of the resources in the first wave, and then we'll upgrade them. Then we'll let them sit there and cook for a little while until the maintenance window for the resources in the second wave comes along, and then we'll upgrade them. We'll let them sit for a while in the final wave.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1440.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1440)

We're not actually blocking from the first one to the second one, but  we're waiting. If you've found some issue in that time, you'll know how long that window is. It's different for different resources. Then you'll be able to remove the tag or turn off auto minor version upgrades at that point in time to stop the progression of upgrades.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1480.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1490.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1490)

For in-place upgrades, if we're doing major upgrades, they're incompatible by definition with open source. We can upgrade in place. We can do this first by creating a clone of the database cluster, which happens really fast and doesn't cost anything. We don't charge you for the storage other than for when you make changes to the storage. Then we can practice on the clone. We can upgrade the clone. We can test the clone with an application.  Then when we're happy with that, we say great, okay, throw the clone away and upgrade in production. That's one way to do an in-place major version upgrade. Pretty straightforward. 

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1500.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1500)

A more flexible way, if we can do it, is to use what we call blue-green. So here I've got my production situation. I've got a couple of replicas, an endpoint, and I'm going to use blue-green  to do a major upgrade. I don't have to do it just for a major upgrade. I can do schema change, static parameter change, anything like that.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1510.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1510)

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1520.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1530.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1530)

Blue-green means I'm going to call my old environment  blue. I'm going to make a new environment called green, and I'm going to keep them in sync with logical replication. You're not doing any of this. I'm doing this. We do the upgrade, still in sync with logical replication, and you test it.  When you're happy, you trigger the blue-green switchover. In the switchover, I promote the green one to be the production.  The old resource is still there. All the resource names are renamed so that you don't have to go through any of your scripts afterwards. After the blue-green has happened, it's as if it never happened. It's very flexible, and this switchover can complete in as little as a minute.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1550.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1550)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1570.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1570)

In case you missed it, this same thing  with blue-green, we just announced for global database as well. So here we have our blue configuration and our green configuration across two regions in this case. Everything I just described to you for blue-green is exactly the same, except across multiple regions. So far we've learned how Aurora PostgreSQL and Aurora MySQL out of the box give you protection, just that storage at the bottom there, against  one whole availability zone unavailability plus one. Then we saw how we can add some replicas in here to increase our availability and allow failover to happen, and we can use AWS advanced rapid drivers to smooth that out.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1590.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1600.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1600)

Then we learned how global database can be used to provide durability and availability across up to ten secondary regions,  supporting low latency local region reads and simple cluster endpoint management with that global endpoint. And finally, we learned some of the patterns to follow  to keep your database cluster up to date with managed and automated cluster-wide upgrades, fast clones, and blue-green. So now I'm going to hand over to my buddy Mark when he comes, and maybe change some slides I think, to tackle those same five questions again, but from the point of view of DSQL. Thank you.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1630.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1630)

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1640.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1640)

### Aurora DSQL Architecture: Active-Active Design with No Single Point of Failure

Hi, everyone. My name is Mark. Sorry about the unplanned downtime there. For the rest of this talk, we're going to be looking at building resilient  applications using Aurora DSQL, which is the newest engine in the Aurora family. Aurora's been around for about ten years. This is only six months old. And DSQL comes from  our customers asking us to give them the best of relational. So this is the ability to run complex queries, joins, and evolve your schema. The best of distributed database architectures like DynamoDB where there isn't a single point of failure, where we scale out rather than up. And if that's not all, also the best of serverless services like Lambda, where you're only paying for what you use.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1670.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1670)

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1690.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1690)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1700.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1700)

We're going to go through all five questions again, starting with what if my database node fails.  But before we do that, let's take a crash course into the DSQL architecture. On screen, we have self-managed Postgres that you've downloaded from the internet and you're running on an EC2 instance. Our application is connected in. We're exchanging some credentials. And the server is going to fork itself, creating a dedicated Unix process for our connection known as a backend process.  We can connect a few times, creating a bunch more processes. And each of these sessions can run queries that are going to parse, plan, and execute your SQL  against local storage. Of course, we can run updates too, sending those to the write-ahead log, which is being used to keep storage up to date.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1710.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1720.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1720)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1730.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1730)

Keep in mind these three components because they're going to  make a comeback right now. Here's DSQL. Our applications are connected in, and it's connecting to a component that we call the query processor. This is just like that backend session. We  get one of these per connection. And just like we saw in Postgres, we can send SQL select statements that are going to be turned into read queries, sent over the network to the DSQL storage engine.  Meanwhile, writes work a little bit differently. If you run an insert statement, putting a new row in the system, that row is going to be buffered in memory on the query processor. If we run an update statement, the query processor is going to take that update statement and turn it into a read against local storage to fetch the current version of the row. It processes that statement to produce a new version of the row, and then again, keeps it in memory. And you can keep doing this as you build up your transaction: insert, update, delete, until we call commit.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1760.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1760)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1780.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1780)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1800.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1800)

When you call commit,  that entire bundle of everything you've asked the database to do is going to be sent over the network to a service called the Adjudicator, which is responsible for concurrency control. So this is the part of the DSQL architecture where we're checking for if there are any conflicts happening with other transactions running in the system. For transactions that commit,  they're sent to a service called the Journal, which is a backbone service of key AWS services like S3. And the Journal is responsible for durability. It will replicate our new transaction to at least two availability zones. And finally, just like what we saw in traditional Postgres, we're going to be using that journal to keep our storage up to date.  If we open another connection, we have another query processor.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1810.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1810)

That query processor can do reads and writes, but there's nothing in this architecture that says that the query processor needs to be running on the same host or even in the same availability  zone. DSQL is an active-active service, which means that any connection can read or write any data at any point in time.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1840.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1840)

So let's talk about availability. What happens if my database node fails? We're connected to that flashing QP in Availability Zone 2, and we're doing some fancy joins. It turns out our storage node that we're talking to fails. It explodes. Oh no, what do we do? And the answer is you as a customer don't have to do anything because the QP is going to detect that failure. It's going to start to shift  traffic over to a healthy replica. Now this replica may be in zone, it may be out of zone. I showed that here on the slide. And we're going to continue with only a small increase in latency for any in-flight operations that had to be retried.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1860.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1870.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1870)

Meanwhile, DSQL is continuously backing up your data and will detect the loss of this node.  DSQL is going to go and take a recent snapshot of your data and create a replacement node in the same zone. That node will come online, it will connect to the journal, it will catch itself up. And then a Query Processor is going to start to  shift traffic back over to its local replica, which is going to give us the best performance because the intra-AZ latency is simply better.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1900.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1900)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1910.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1910)

But what about rights? What about that adjudicator sitting over there in Availability Zone 1? Well, the reason we have a single adjudicator is because that's going to allow us to do this conflict resolution protocol really quickly without coordinating over the network. But we don't want a single point of failure. And so what DSQL does is it's going to keep standby adjudicators in the other zones. Notice the arrow from the  journal. These standbys are simply following along with everything that the leader adjudicator is doing. And if anything fails, then within just a few milliseconds,  these standby adjudicators are going to run a leadership protocol. One of them is going to become the leader. Hey, it turns out it's Availability Zone 2. And the QP is going to retry any in-flight commits against the leader. There's nothing for your application to do. It just works out of the box.

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1950.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1950)

Now, there's a lot more going on with the DSQL architecture than I can show you on the slide. But the good news is that you don't have to know about how any of this works. DSQL is an architecture that's been designed for high availability. There is no single point of failure in DSQL. If you're only spending a few pennies per month on DSQL or running entirely on the free tier, you're getting active-active out of the box.  But of course, DSQL is designed to scale too. And so let's dig into how we do that.

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1960.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1960)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1980.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1980)

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/1990.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=1990)

### DSQL Scalability and Connection Management: Optimistic Concurrency Control and Automatic Sharding

First, let's talk a little bit more about that concurrency  control protocol. DSQL uses a protocol called optimistic concurrency control, which makes the assumption that transactions seldom conflict with each other. But what happens when they do? Here we have two clients that are trying to update the same row, where ID equals 1, to a different value. It doesn't actually matter what the value is, but they're going to prepare their transactions and they're going to race to the adjudicator.  It turns out that the transaction on the top gets there first. The adjudicator is going to record that transaction in the journal, and our other client is  going to be given a PostgreSQL serialization failure error, and it's going to be told to retry.

And so when you're building applications on DSQL, this is something that you need to take care of in your application. When you call commit, you need to take care of availability issues, you need to take care of optimistic concurrency control issues, and then just simply retry. This is just a few lines of code. It's a while true loop. And what we recommend that you do is you build little helper functions that you can use throughout your application. Or if you're using some kind of framework, then put that into the lowest layer possible.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2030.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2030)

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2050.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2050)

Now, when you design your applications, you want to avoid creating unnecessary  conflicts that we don't need to retry. And if you do that, then we're going to be able to commit these two transactions at the same time because notice that they're updating different rows. And as you do this more and more and more, you may start to increase the load on the adjudicator, the load on the journal. And so what happens then?  Well, before you exhaust the adjudicator or the journal's capacity, DSQL is going to notice this increase in utilization and start to shard itself internally.

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2070.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2080.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2080)

And so if we're updating some records, the QP will automatically know about this internal sharding strategy and direct your commits to the right stack. But of course, we can run transactions that sometimes  touch rows for multiple shards. DSQL will handle this automatically for you. We have an optimized implementation of two-phase commit that will ensure your transactions are atomically committed even across multiple  shards. Something similar is happening on the storage side, which we won't have time to get into today. But I do want to talk about rescaling.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2090.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2090)

So imagine we're opening a bunch of connections.  These connections are all running their selects, they're slamming our single storage host, load is going up, latency might go up. But again, well before this becomes a problem, DSQL is going to detect this. And using the same trick that we saw in the availability slide, DSQL will use a recent copy of your data to bring a new node online.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2110.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2110)

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2120.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2120)

That node will catch itself up from the journal,  and the query processes will start to automatically load balance across this replica. We can keep scaling this out without limit, and DSQL is doing this automatically on your behalf.  A key reason we're able to do this is because in DSQL, replicas are always strongly consistent.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2130.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2130)

So in order to understand this, let's  make a new DSQL cluster. It's completely empty and doesn't have a single row in it. We've made a table called my table, and we're just about to insert our first row into that table. Here we go. It's in the journal and has been committed to at least two availability zones. That means the journal says it got your data, and we're going to send that commit acknowledgment all the way back to our client.

Now our expectation is we can start a new transaction, run select star from my table, and we should see our row, right? Because otherwise, we'll have an eventually consistent system. Eventually consistent systems are very difficult to reason about and very difficult to write correct code against. That is the reason for our small technical interruption, because we had old slides.

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2180.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2180)

What we do in DSQL is use a time-based synchronization protocol, and we combined two techniques here. The first technique is we use the EC2  time sync service, which gives us a microsecond accurate timestamp using GPS clocks. The second technique is that we use a library that AWS developed called Clockbound, which is going to allow for any errors in that measurement. When we combine these techniques, what we get is a value that is linearizable. That is, it is a timestamp that is guaranteed to be after any commit timestamp that the service has previously given out.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2210.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2210)

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2230.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2240.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2240)

Now that we have this transaction start time,  we include it in our API request to storage. The job of storage is to identify that it's running behind, to wait to see the update come off the journal, and then return our rows. So how do I manage connections?  Here we have six connections to DSQL, and how do we get them? Well, between your application and the query  processor is a service known as the session routing layer. Its job is to make sure that connections are always available, that you can get connections quickly, even in the event that many hundreds of thousands of connections flap due to a networking event.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2260.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2260)

This is a very deep and complex service, but at a high level, what's happening is that  there is a massive warm pool of these query processes. Again, each of these boxes represents a single process. Our team is maintaining a large fleet of EC2 instances, and we're taking on the responsibility of capacity management. On these instances, we're packing them full of query processors that are ready, they're running, and they're just waiting for you to connect. When you ask for a connection, DSQL will simply pop one of these query processors out of the pool, and your application can start to run transactions.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2290.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2290)

 Now, let's look at failure of connections. We've got six connections open, and one of them has failed. Ideally, what's happened here from an RTO point of view is that we've simply experienced an error on one sixth of our connections. The other five connections are totally healthy, right? This is a system with no single point of failure. What your application needs to do to handle this is to detect that failure and retry. This is where that retry handler from the optimistic control section is going to come in handy. Add a retry on connection errors, and you'll get another connection, and you're back to 100% availability.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2330.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2330)

Now, unlike failures of storage and failures of the adjudicator,  we can't have your application automatically reconnect, right? This is something that you need to do. In order to take advantage of DSQL, you need to build the ability to detect failures and retry on new connections into your application. We were actually quite worried that customers wouldn't realize this because, you know, see page 43 in the manual. So what we did instead is we said that connections have a one hour maximum lifetime in DSQL.

If you simply use the best practice of a client-side connection pooling library, this is the Java C3PO library, and configure it to do health checks and configure it to have a maximum connection age, then right out of the box, your application is going to be following our best practices. Now, while we do recommend using client-side connection pooling, in DSQL, there's no need to use server-side connection pooling. You don't need PG Bouncer, and you don't need RDS Proxy. That's because the query processor layer is a separate service and it's horizontally scalable. We've been able to build the concept of connection pooling right into the service.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2400.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2400)

If you want to run PG Bouncer, feel free to do so, but that's just going to be something that you have to manage, patch, and monitor for availability.  It's not going to provide you any benefits. Now, throughout this talk, I've been showing your application talking to query processors in the same zone.

And the reason we do that is to lower latency, right? Postgres transactions that are interactive, you're going back and forth between your clients and the application all the time. Each of these trips is going to take some time. And so by talking to a Query Processor in the same zone and to a storage host in the same zone, we're going to give you the best performance.

Now, in the event that your application is somehow available, and yet DSQL is completely down in a region, don't worry about that. DSQL from the top to the bottom is able to automatically fail over to a healthy zone. And so if an entire zone becomes unavailable, just reconnect and you'll get a healthy Query Processor. If you'd like to learn more about this part of the service, please join me tomorrow at 1 p.m. for session 439, where I'm going to be doing a deep dive into how we built this.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2470.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2470)

### DSQL Multi-Region Clusters and Zero-Maintenance Operations

Let's talk about regional failures. DSQL allows you to set up what we call a multi-region cluster. And the way you do this is you create a cluster in region A, a cluster in region B, and then you designate a third region as the witness region.  Now, you'll notice the witness region doesn't have any storage boxes. It doesn't have any Query Processors in it, it doesn't have your application in it. The job of the witness region is simply to have a copy of the journal for reasons that we'll get to in a moment.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2490.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2490)

Once you do this, DSQL is going to pair these journals together using a quorum-based protocol.  And then our application can continue to be active-active even across regions. So here you can see we have our application in region A, Availability Zone 2, our application in region B, Availability Zone 2. And they're both able to do writes.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2510.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2510)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2530.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2530)

So what happens if region B fails? Well, imagine for a moment that we did not have  the witness region. It would be very difficult to disambiguate between region B having failed completely versus region A and region B simply not being able to talk to each other. This is where the witness region is going to come in handy, because it's going to vote alongside region A to kick B out of the quorum.  And DSQL is going to be able to detect this automatically.

It's going to reconfigure the journal so that only A and C talk to each other. And if you were somehow able to connect to the cluster in region B, it would be completely unavailable for reads and writes. This happens automatically again, without any intervention. It happens very quickly. And by doing this, you can continue to use your cluster in region A. There is never any data loss. And if you continue to write data into region A, DSQL upholds its guarantee that transactions are replicated to at least one other AWS region before acknowledging the commit.

Meanwhile, later, region B is going to come back online. DSQL will detect that. It will catch it up to date. And then it will undo that configuration option, and we'll be back to two zones.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2580.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2580)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2610.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2610)

So how do you take advantage of  this? The pattern we recommend is the same one that we've been using throughout this talk, one of active redundancy. And so you should deploy your application alongside the cluster in region A and alongside the cluster in region B. There's a couple of reasons for this. One reason is that you're going to get the best performance, right? Because again, you have this low latency connection to your cluster. But the second reason is this is going to allow you to continuously validate that your application works in both regions. 

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2620.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2620)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2630.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2630)

On top of this setup, you should create some kind of global endpoint. You can do this with something like a Route 53 DNS record. And then you'd automatically direct your customers' traffic to the closest  available healthy region. So what happens if something goes wrong, region B goes offline. You're not going to lose any data, and DNS is going to start to shift your customers over to the healthy region.  And you know that this region is going to be healthy. Why? Because you've been exercising it all of the time.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2640.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2640)

So this is my favorite,  favorite slide I had to make here throughout re:Invent, because there's not a lot on it. And the reason for that is with DSQL, there's simply no maintenance for you to worry about as a customer in the same way that there's no maintenance to worry about as an S3 customer. DSQL is a fully managed service. Our active architecture is allowing us to do much more maintenance on your behalf.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2680.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2680)

So think about those Query Processors, right? They're only running for an hour. After an hour, either you hang up or we hang up, we throw them away, we make a new one. And that new one is going to have the latest operating system, the latest security patches, the latest performance improvements, the latest features. And so there's just really nothing for you to do as a  customer.

Okay, so what are the takeaways for today? Aurora has three engines. We have Aurora for Postgres, for MySQL, and our newest engine DSQL. Out of the box, all three of these engines are going to give you three Availability Zone durability. All three of these engines are going to give you the ability to scale your workloads. With Aurora PostgreSQL and Aurora MySQL, you're going to scale primarily by going up for writes and out for reads. With DSQL, we offer hands-free, automatic scaling, but you need to make sure your application schema are designed to work best with the DSQL architecture.

If you've outgrown a single region, consider going multi-region. With Aurora PostgreSQL and Aurora MySQL, you can test your failover practices without losing any data. With DSQL, you can take advantage of active-active across regions. And finally, all three engines offer easy maintenance with automated security updates and minor version upgrades.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e7196ad620a65c10/2750.jpg)](https://www.youtube.com/watch?v=TNURufOhu_A&t=2750)

Thank you so much for attending our talk. Tim and I will be hanging around afterwards for questions, so please do come and talk to us. And thank you. Have a great re:Invent.  Please complete the session survey in the mobile app.


----

; This article is entirely auto-generated using Amazon Bedrock.
