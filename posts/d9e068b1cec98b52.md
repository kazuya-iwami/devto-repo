---
title: 'AWS re:Invent 2025 - Automate insights and drive innovation with cloud and AI solutions (IND384)'
published: true
description: 'In this video, Aparna Galasso from AWS discusses how retail and CPG companies are elevating IT to strategic partnership roles. Chris from MondelÄ“z shares how his team built a self-service automation platform using infrastructure as code, Spacelift, and GitOps principles, scaling to 300+ engineers managing hundreds of AWS accounts with read-only access and microsegmentation. JC from Grainger presents Project Nightingale, an agentic AI solution using Amazon Bedrock and AgentCore that generates executive-quality FinOps summaries, reducing analysis time from one engineering day to minutes while serving 50+ domain leaders. Both demonstrate how automation and generative AI enable IT teams to move at technology''s pace while providing C-suite visibility into cloud costs and optimization opportunities.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/50.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Automate insights and drive innovation with cloud and AI solutions (IND384)**

> In this video, Aparna Galasso from AWS discusses how retail and CPG companies are elevating IT to strategic partnership roles. Chris from MondelÄ“z shares how his team built a self-service automation platform using infrastructure as code, Spacelift, and GitOps principles, scaling to 300+ engineers managing hundreds of AWS accounts with read-only access and microsegmentation. JC from Grainger presents Project Nightingale, an agentic AI solution using Amazon Bedrock and AgentCore that generates executive-quality FinOps summaries, reducing analysis time from one engineering day to minutes while serving 50+ domain leaders. Both demonstrate how automation and generative AI enable IT teams to move at technology's pace while providing C-suite visibility into cloud costs and optimization opportunities.

{% youtube https://www.youtube.com/watch?v=5GxPoCeENmw %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### The Strategic Imperative: Elevating IT from Service Provider to Business Partner

Good afternoon everyone and thank you for being here today. My name is Aparna Galasso. I lead the retail and CPG business development teams for North America at Amazon Web Services, and today I'm really excited to be here with two customer stories of how our customers in retail and CPG are innovating using cloud and AI to drive competitive advantage within their companies. I'll set the stage with a couple of minutes here on why this matters, and then I'll pass on to my speakers Chris and JC.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/50.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=50)

Let's start with a conversation about the most common technology that we're all talking about today: generative AI. As we've seen over the past few years, generative AI has moved from  art it possible to proof of concept to we have lots of use cases in production to the point where we're really starting to think about business value with generative AI. The problem is just as all of this has been happening, companies are still feeling stuck because now we're talking about agents and multi-agent orchestration. There never seems to be a moment in which you're able to get ahead of the curve with generative AI or with any other technology for that matter.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/80.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=80)

That's where I, for any of you in the audience who were there last year, I use this  example and it still stands true a year later. How many of you remember reading Alice in Wonderland? So for those of you who don't remember or those of you who need a reminder, Alice is running and running and running in Wonderland in the Red Queen's race, and she's running as fast as she can, but she realizes she cannot get ahead. She's stuck in exactly the same place. This is where our CPG companies are. They feel that no matter what they do to get ahead of the generative AI curve, they're stuck in exactly the same place.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/120.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=120)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/150.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=150)

Now we're in a pivotal moment with CPGs in general. We're at this point where there's  unprecedented change in the marketplace. Consumer expectations are higher than ever, and consumers are expecting you to move at the pace at which their minds are changing. However, margins have gotten tighter because the cost of serving these consumers has increased, and at the same time, you also don't see that the pace of change of technology is decelerating. It's actually accelerating. Everything is moving faster, and companies are forced to have an ability  to keep an agile approach to their future development.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/160.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=160)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/170.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=170)

Now the problem is this: the C-suite in these companies is used to thinking about consumers, and they may know where to head strategically,  but they are not very well aware of how to use algorithms or how to keep ahead with the pace of change of technology to serve their consumers.  So really where this puts us is a very simple place. We're at this pivot point. You could think of it as a pendulum swinging, and the pendulum has swung from a place where IT was more of a reactive service provider following what business would set as a strategic direction to being there on the front end helping set the business direction with IT.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/210.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=210)

Now, arguably this pendulum analogy fails because I don't think the pendulum is ever going to swing back. The pendulum is probably going to move into a future new state where you think about CPGs not only being in the business of selling products, but being in the place of building new digital-first business models. Now what this means is if you think and you look  forward, you know, companies all talk about transformation and what does transformation really mean? Transformation in the future is going to require IT and business leaders to show up together and to tackle business decisions together. You're going to have IT that is making sense of the AI capabilities that are out there and translating that into business opportunities. You will also have IT part of the situation on the back end, taking those business opportunities and converting them into technical roadmaps.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/260.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=260)

So essentially think about it this way: the point here really is that IT is no longer sort of invited to have a seat at the table. Really, if you look at the future and how business decisions are going to be made, you need IT to be an essential voice at that table. And so today we're going to have two stories,  different paths, same outcome, elevating IT into that position of strategic partnership at CPG companies. We're first going to have Chris. He's going to talk about how he's been building an automation platform at MondelÄ“z that has allowed him and his team to be positioned as agile innovation partners to the business. Then we will have JC and he will talk about how he's gone more on a FinOps route to use agentic AI to elevate decision making around AI and other cost plays to the executive leaders in the company. And with that I'll pass on to Chris.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/310.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/320.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=320)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/330.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=330)

### MondelÄ“z International: A Company Built on Heritage and Transformation

Thank you. Thanks Aparna for that great introduction.  As you said, we've been focused a lot on trying to make sure that our foundation is sound. We need to be agile. We have a very aggressive strategy  of many acquisitions and divestitures in our company. We really need to be agile and therefore our foundations need to be sound and we need to be able to enable the business to move quickly. 

My agenda that I want to share with you today is around a little bit about MondelÄ“z, our cloud journey so far, and then the platform that we've built and how we've scaled that for hundreds of accounts and thousands of pipelines and applications, how we manage our costs, and then a little bit about what's next for us.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/360.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=360)

You may not have heard about the MondelÄ“z brand, but hopefully you've had many of our products. Oreo is a big favorite of mine, obviously Cadbury mini eggs. I'm really thankful we only get them at certain times during the year because they would be bad if we didn't for me. So just a little bit about MondelÄ“z, we do about thirty-six billion dollars in revenue in last year, and growing as I said before. 

I'll start with a little bit of history of MondelÄ“z. We're technically a young company. We were born from the Kraft food split in 2012, but we have a very long history. You recognize a lot of those brands are not just from 2012. A lot of our company is taking on that heritage of all of those companies and brands that have joined together to make MondelÄ“z what it is today.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/390.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=390)

### Building Cloud Engineering Capability from the Ground Up

Our cloud journey, like a lot of CPG companies, started around 2018. We were doing very much a lift and shift from the data centers at the time, and more in a managed service provider model where we didn't have maybe all of the expertise at the time, like most of us didn't at that time, and we were relying on getting everything to the cloud. The journey lasted a couple of years and actually late 2020, one of our largest regional SAP systems, we actually failed to get that cut over and it's been in the data center ever since. 

I like to think about it as a lot of the hardest things that we had to migrate, unfortunately we still had leftover in 2021 when we started our cloud engineering team. That's actually when I joined MondelÄ“z and I think I had a really great opportunity because we didn't have a cloud engineering capability within MondelÄ“z at the time. But I was empowered and able to build a team from scratch, which was really an awesome opportunity.

Between 2021 and 2024, we gradually grew the team from just me in 2021. The first year we hired our first sixty-seven engineers, and by 2024, we have a global team of over three hundred engineers that are working on our platform. This culminated in 2024 with our strategic partnership that we signed with AWS, and then later in 2025 when we announced that we were going to do all of our SAP RISE environment on AWS as well.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/510.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=510)

When we started this cloud engineering capability, we got to do a green field environment, and these were some of our guiding principles.  A lot of the engineers and I that started had already gone through this journey once, learning our way through it, and we really tried to be very thoughtful about what were those principles that we wanted to have that would be different and what we're trying to achieve. We really had this concept of making sure that we were going to have a self-service platform for developers that was enabling and empowered developers to do the right thing and to do it quickly and easily.

Secure by default, automation of absolutely everything, everything infrastructure is code, making sure we had microsegmentation. I remember talking to our SA who's here in the crowd on our first day saying, I know we have no accounts today, but I want to know what happens when we hit one thousand. It was a little bit funny then, but really it's satisfying to see it come to life today. A lot of these principles were really what we tried to take forward into the design that we had.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/580.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=580)

What we have today is a controlled, easy, self-service platform, and we started to adopt as we were building it some of the buzzwords in the industry around platform engineering, golden path, enabling constraints.  Really what our mantra was is we want to put the power of the cloud in the hands of all capable developers in a secure and centralized way so that we can.

### Infrastructure as Code: Automating the Golden Path

Make changes rapidly and not have things age and get out of date and need to be constantly replaced. This is an evergreen approach to making sure that we have the guardrails and the controls in place to enable people to do the right thing. The biggest principle here, and the thing I'm most proud of that we've done as a team, is making sure that all of this infrastructure is code. When a request comes in for a new product or a new application, we deploy all of these things that you see here: the GitHub code repository, all of the branches, static branches for the code that correspond with all of the AWS environments that the application needs.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/670.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=670)

If you need a production and a non-production environment, you have two branches in the code repository that are mapped there and automatically connected with the CI/CD pipelines to watch those repos for pull requests and get those deployed to the AWS applications. This is a simple example to show you another version of what we had on that previous slide.  With one request, all that you see here is automatically provisioned. This includes the GitHub repository with all of our controls, all of our branch protection rules, where we can enforce our GitOps standards that we want to follow.

We use Spacelift, which is our Terraform deployment CI/CD tool, and we automate the creation of individual stacks that are tracking the static branches. When a developer pushes a change in Terraform into one of these branches, it gets picked up and it does a Terraform plan against the AWS environment. The developer can see what those changes will be and then either reject them or confirm them and push them to production. On the top, we also automatically generate standard roles, and you see them across the top. There are actually more than this because there's an individual role per environment, but for the sake of simplicity, we have DevOps owner, DevOps contributor, and read-only roles.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/750.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=750)

These roles have different capabilities across the different platforms automatically.  This shows you how that works. You see the roles across the top, all of them are automatically created and available within twenty to thirty minutes from the time that the request goes through in our identity governance system. All of the requests for membership automatically get routed to the business owner of that product. There's no central team that has to decide whether or not someone gets access as a developer; that goes to the business owner of the application to manage themselves.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/810.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=810)

The best thing about the environment that we've built is that the most powerful role that we're provisioning is all just read-only access in AWS. You don't have the ability to go in and manually edit, click, create, or change things. This is how we're making sure that we are owning and controlling the path to production.  On the left, you'll see some Terraform code, which is obviously too small to read, but another view on the right shows all of the things that get provisioned automatically when you make this request: the GitHub repository, the branches, all of the standard rules, the CI/CD pipelines, and then all of the standard components in the AWS account that gets vended.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/860.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=860)

We're using Account Factory for Terraform, and then we're able to seed that account when it gets created with all of the things that each account will need to be able to manage its own environment. This includes delegated DNS for the account, our standard backup strategy, tags for AWS Backup, all of our security tooling, and all of the automation to get all of those logs securely into our SOCK.  In the middle, you'll see that same code snippet, but just to give you a flavor, this is how we provision network. There's nothing special or a separate ticket that needs to be created. If your application in this example is a public-facing application, the system knows that because in the request, you can see it says that it's requesting regional public subnets.

We have standard regions that we deploy to and all you need to tell it is what the IP CIDR block range is and the size that you need per region. Then this will provision automatically through the provisioning process all of the VPC and network that you need to connect both to our AWS core network.

This core network then connects back into our transit gateway into the MondelÄ“z network itself. This is something I think is pretty easy once you see it, but it was a very long and hard process that a lot of really smart engineers were able to put together. This just gives you a flavor of some of that automation that we tried to strive for.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/930.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=930)

### Enforcing GitOps with Policy-Driven Development

I've said this a couple of times,  but really we're trying to make sure that we're enforcing GitOps. This is the slide that I think tells that story the best. Again, it's all been provisioned. You have two environments in this example called non-prod and production, and those static branches are the ones that have the branch protections on them.

A DevOps owner or a DevOps contributor who has access to this GitHub repo can go and create a pull request, make their changes, and initiate the pull request. Once that pull request is seen by our Spacelift stack, it triggers a plan and gives immediate feedback to the developer showing the resources that you are trying to create in this environment. The developer can then approve, apply, or reject it.

Once the merge is complete into the static branch, it officially runs and the developer gets one last chance to look at that plan, make sure that it's not going to do anything problematic, and then confirm that. That's how we get all of our resources in each AWS account. The important thing on this slide is that little circle on the bottom that says OPA policy enforcement. This is key for us because it allows us in every pipeline that we've automatically provisioned to attach our policy to all of those stacks.

This is very powerful because we're able to keep updating our security standards and our guard rails and controls as we go to make sure that as plans and applies come through, they meet our latest and greatest standards. For example, we have a lot of private Terraform modules that we develop that enforce our own standards. At this layer, we can say you can deploy as many S3 buckets as you want, but you have to do it with our Spacelift module, with our Terraform module that enforces all of the MondelÄ“z standards.

This is really where the concept of enabling constraints comes in. We're constraining the developers, telling them you can only do it this way, but it has to be an enabler. It has to be something that they want. It has to be a pull, not a push. It has to be something where developers think, "I don't have to worry about the 150 settings that I could set on an S3 bucket. I can actually just say S3 bucket name, and that's it, because I know it has sane defaults that are going to meet all of our security policies and all of our best practices."

Because we've deployed all of this with infrastructure as code, we realized that we weren't enforcing TLS 1.3 in our load balancers a year and a half in. It was just an oversight. We didn't do it. So we went and pushed a policy that just checked to make sure that that's there in every stack that we have. This gives us one single place to make a single change in line of code and impact thousands of CI/CD stacks and hundreds of AWS accounts all at once.

That's really the heart of what we're building. Another example is Open Tofu. When the Terraform licensing changed, we were able to relatively quickly migrate. I don't want to do a disservice to the people who had to make sure that this change was tested and rolled through, but we can say now all of the stacks in our environment are on Open Tofu. This is something that would be extremely hard to do if you had hundreds and thousands of different CI/CD stacks because you would have to rely on each team to go and make that change. Instead, we said we want to be on Open Tofu now, so that's what we're going to do. There's real power in that.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1160.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1160)

### Enabling Constraints: Forced Collaboration Through Standardization

A little bit more about some of this and some of the key concepts behind it:  When Amazon Q started doing pull request reviews where you could have Amazon Q do your PR review, again, one line of code and all of the different repositories, we could go hook that up and enable that across the environment very quickly. On the Spacelift side, our OPA agents allow us to write our policies and enforce the various policies to make sure that not only are you following best practice, but you're doing it in a way that forces collaboration.

If our standard module is so restricted that a development team finds that it's not usable for them, there's one of two things that's going on.

Either they don't quite understand what best practice is, and it gives you a chance to say, "No, you can't deploy that S3 bucket without encryption at rest." So request denied. Or it's a new use case that we've not thought of before. Again, one place, one source code, even a pull request, right? We can review it. We can update that module so that the next developer, the next team, and in our environment, it might be another third-party developer coming in, but they all benefit. So it's this forced collaboration which is made possible by the enforcements, guardrails, and controls that we have.

On the AWS side, on the accounts, I'm really proud to say that we have read-only access primarily. So instead of wondering whether an account is in a certain state, it's been really neat that I've been able to answer questions for security by just going to our GitHub organization and searching. I don't need to check the account. I know that what's in the Git repo is going to represent, at least if it's in the main branch, what we have in our environment. So I think that's very powerful there.

This line between what we can do in our CI/CD pipelines from a control perspective versus what we do in the AWS account itselfâ€”we try to do belt and suspenders. So we still have Wiz that's trying to scan everything. If it finds something that's bad, we can go and fix it, and it allows us to figure out how we can write more policies so that it never even gets there, as opposed to finding it and trying to remediate it later.

### Cost Transparency Through Microsegmentation and Future Roadmap

Of course, it's always about cost. This is just a random snapshot from our AWS Kudos dashboard. If you're not familiar, the Kudos dashboard is something AWS will give you for free. What I really like about it is this whole concept of microsegmentation. Not only can you not inadvertently affect someone else's application, but you also know all of the costs that are in a particular AWS account. You don't have to worry about whether all the tags are up to date or allocated the right way. In my former experience when we had more monolithic mega accounts, tagging was always an issue. I never trusted what the actual cost was for a particular application because I was always worried that someone fat-fingered something, had a typo, and it didn't show up in the report, or maybe a developer had forgotten to do it.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1310.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1310)

There's a lot of power in being able to say to any application or product owner, "You want to know what your cost is? Do you want to know it by the second, by the day, by the month? Do you want to know your trend, your budget? What do you want to know?" That microsegmentation has really helped us with that other principle around cost transparency and being really efficient and being able to tell what our costs are per environment. 

So for what's next, we're actually at MondelÄ“z on an organizational journey to products and platforms, which is really helping us. My team and I talked about how we created a platform where there weren't a whole lot of customers. We built something that no one was begging for, but this has been really awesome for us because now we're creating customers in our product teams who don't know it yet, but they really want this. When they're asked what their TCO is and they ask us how much their cloud costs, I say, "Here's your dashboard, go look at it yourself." So I think we're really trying to enable and leverage what we've built so far to take that next step to drive even more transparency in our company.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1400.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1400)

We're also in the middle of wrapping up a lot of our legacy cloud migration. We were left with a lot of on-premises data center stuff that was the hardest thing to tackle. We're on the cusp of shutting down two out of four data centers early in 2026, with the last one being our big SAP environment that never got moved, which has a separate timeline. But we're really excited to close down those data centers. Of course, we're not done then. We've done some lift and shift, some cleanup, and some opportunistic changes, but what we need to do then is go back and refactor, re-engineer, and make serverless where we can make serverless and really continue to evolve from that perspective.  And then, of course, we're looking to leverage our AWS strategic partnership.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1530.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1530)

As a CPG company, we sell a lot of products to Amazon.com, so we're looking at how we can make that better and leverage digital media and advertising. I'm really excited for that next step as well. That was it for me, so thank you for allowing me to share our story.  Now I'm going to turn it over to JC to share his experience with Grainger. Thanks, JC.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1550.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1550)

### Grainger's FinOps Challenge: Democratizing Cloud Cost Intelligence at Scale

Thanks, Chris. Thank you, Chris. Thank you, Aparna. I'm excited to be here and talk to you a little bit about our FinOps journey.  Our roadmap today is going to follow a problem to solution journey that I think a lot of teams of specialists can relate to. First, I'll provide a brief introduction to Grainger, and then dive into our strategic challenge: how did teams of specialists close the knowledge gap at enterprise scale?

Then we'll dive into an initial proof of concept where we inevitably hit an impossible math problem, and we'll talk about why we chose generative AI as a solution. It's important to note that it wasn't without its challenges, so we'll step through the journey on the path to production. Then I'll introduce Project Nightingale, and lastly, we'll talk about the results. I'll give a brief demo and we'll discuss where we think this is going next.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1620.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1620)

Grainger is a leading broadline distributor and the largest MRO company in North America.  To give you a sense of our scale, in 2024, we generated over $17.2 billion in revenue and served over 4.5 million customers with 30 million plus products available globally. We employ over 26,000 team members and we've been fulfilling our core motto of keeping the world working since 1927. What differentiates Grainger is the fact that we combine assortment, technology, and high touch solutions via complex technology and digital infrastructure. That digital infrastructure is where our FinOps story begins.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1680.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1680)

 The strategic challenge is something I think anybody in security, platform, SRE, or DevOps will relate to. Critical insights remain trapped in complex systems. You have valid and thorough reporting, but your stakeholders are busy and they don't have expert knowledge to interpret the data. No one's coming to your dashboards. Attempting manual communication with these stakeholders through email, Slack, one-on-one meetings, and workshops simply cannot keep up with the pace of enterprise growth.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1750.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1750)

For us in the context of FinOps, the reality was that as our federated cloud adoption continued to accelerate, we faced a pivotal moment where we had to pivot from centralized cost optimization and cloud cost management to democratizing FinOps with enterprise-wide adoption of FinOps practices and principles.  So what did we do first? We identified a target demographic that had the highest return of value. For us, that was product and technical domain leaders. These leaders hold large swaths of cloud utilization and control the levers of prioritization and roadmaps. FinOps also serves multiple other personas including product, indirect procurement, engineering teams, senior executives, and finance. But this particular demographic was critical to influencing our overall cloud efficiency and effectiveness.

So how do we meet this demographic? We landed on executive summaries,

a format that they were familiar with. Delivered to their inbox, the place where they were already working. We couldn't expect them to come to us. We couldn't expect them to change their ways of working and visit our dashboards. So no new tools, no behavior change required on their part. These executive summaries were generated manually by our team of FinOps specialists. A few of them are in the room today. And essentially they consist of everything an executive leader needs to know. They can walk away with a full understanding of their cloud usage, its growth, and optimization opportunities within five minutes.

But we quickly ran into a problem. These summaries took on average about one full engineering day to write. We started off with an initial group of just six domain leaders, but we had an eventual audience of initially thirty, growing to fifty recipients. So we were talking about up to 4,800 engineering hours a year to serve just one persona. That simply didn't scale. The good news though is that leaders engaged with the summaries and took action. We actually saw people starting to take action now that they were informed about their cloud usage.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/1900.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=1900)

### Project Nightingale: Choosing Generative AI to Overcome the Impossible Math Problem

Meeting this impossible math problem,  we knew we had to look elsewhere for the solution. We had validated the theory that executive summaries delivered to the inboxes of leaders would bring about engagement and action, so we had to find a way to scale it enterprise-wide. Initially, manual summaries were quite time intensive, but they had quality that only FinOps specialists could provide. We evaluated essentially three core approaches: manual, programmatic, and ultimately generative AI.

Why not programmatic? At the end of the day, programmatic solutions could meet the scale that we were facing, but the problem is they lack the nuance, the tailored customized metrics, trends, and analysis that our FinOps specialists were able to create on their own. So we had to shelve the idea of a programmatic solution and seek out alternatives. That's what ultimately led us to generative AI, and it changed what was possible. We found that with the right constraints, templated prompts, organizational data, and served through the right tools, generative AI can meet that specialist quality of our FinOps analysts but at an enterprise scale.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2000.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2000)

 Let's talk about some of the initial challenges though, because it wasn't all straightforward. That's one thing to evaluate and assess a generative AI proof of concept on your laptop. Most of us could get a little proof of concept working in a weekend. That's an entirely different prospect to take that to production. For us, when we started out as a cloud team we were familiar with traditional Infrastructure as Code approaches using Terraform and Boto3 to interact with AWS resources. But they simply weren't keeping up with the pace of generative AI innovation.

We found Terraform to be really clunky and often the AWS provider didn't have the configurations we needed for our agents, so we pivoted and we found Strand's agents. With Strand's agents we were able to define agents inline in our source code and quickly accelerate on our goals. The second core problem though was that unlike most generative AI solutions, we weren't building a chatbot. We didn't have variable questions and variable responses. We needed consistent deterministic outcomes from what are essentially probabilistic models.

Our solution was to develop a headless MCP client with no human interaction required whatsoever, and the output is the product itself. Thirdly, enterprise constraints presented a challenge. We couldn't just go pick up any vendor tool or solution off the shelf. Within the enterprise we had to adhere to security, procurement requirements, and legal requirements.

We stumbled upon AgentCore, which was still in preview, but we quickly learned that it would provide the enterprise requirements as well as the configuration and ease of authentication, observability, and monitoring that our team needed within a very short runway.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2170.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2170)

Productionizing was not exactly simple. We were developing something that was semi-novel, not being a chatbot, and so we didn't really have a lot to reference. AgentCore was also in preview and lacked documentation. Often we had to dive into the source code of AgentCore itself to find answers. 

That path to production eventually led us to creating Project Nightingale. Leveraging AgentCore and Amazon Bedrock with Claude models, we were able to construct analyst-quality FinOps cloud cost intelligence at enterprise scale. This essentially consists of two key agents: a FinOps Analyst and an Executive Summary Writer that specializes in synthesizing financial analysis into executive-friendly narratives. This multi-agent architecture unlocked the capability to meet that enterprise scale.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2250.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2250)

The agents alone were not just the answer. We also built our own MCP server with custom MCP tools. Underneath this is a series of different data sets including organizational data, recommendations from Cost Optimization Hub, and billing data from our cloud cost management tool. 

One of our requirements was that we needed consistent output with our executive summaries. We couldn't have hallucinations or errors, or we would quickly lose the trust of our stakeholders. The data had to be reliable, the format had to be consistent, and we had to be able to have confidence that what we were sending to our executive leaders was reflected in our tooling.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2280.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2280)

What is somewhat unique about our solution is the sequential nature of the workflow.  Instead of the variability you often find with chatbots, we have a structured sequential workflow, essentially an agentic pipeline. With the input being a templated prompt, it is also somewhat unique because it is not a chatbot, and so we are able to control the input, which is a prompt template, as well as the interaction between the agents. That ultimately gave us the consistency and accuracy we needed to have confidence in sending these to executive leaders.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2340.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2340)

### Multi-Agent Architecture: Delivering Analyst-Quality Executive Summaries

We follow this workflow from left to right. Initially, the domain analysis prompt enters into the FinOps Agent. This prompt has a set of criteria for the analysis as well as context about the domain the agent is performing the analysis on. The agent has at its disposal a couple of important MCP tools. 

The first is our billing data. It is important to point out that initially we tried to wire up the billing API for CloudZero directly, just wrapping the OpenAPI spec in an MCP server. But the volume of billing data quickly overflowed the context window and tool use failed. So we had to pivot and write our own custom MCP tool for the billing data. What this provided for us was even more consistency and even more confidence around no hallucinations.

The CloudZero MCP tool returns structured data to the agent and simplifies the task that it has to perform with the analysis. As I mentioned, optimization hub data from Cost Optimization Hub is also fetched from an MCP tool, and these are combined with organizational data to generate that cloud cost analysis and the cost intelligence by the FinOps Agent. Once that is complete, the analysis is passed to the Executive Summary Writer Agent.

That agent has a couple of important tools at its disposal. First, as I mentioned, it synthesizes that analysis into a distilled, executive-friendly narrative. Our customers and stakeholders don't have the expertise of a FinOps analyst, so we have to bring the data to them in a format that they'll understand and engage with. That's the key role of the Summary Agent.

The two tools this Summary Agent usesâ€”first and foremostâ€”is an HTML template tool. We evaluated a couple of options, and we knew ultimately these executive summaries had to be consistent. They had to follow the same format across the board. You couldn't have one director getting format A and another director getting a completely different format, or every time they're seeing these summaries, there's something new or different. We wanted the consistency so that as they began to adopt and learn from these summaries, they had the reliability and they knew what to look for every month they received them.

So the tool for HTML rendering is essentially a Jinja template underneath the hood. It takes in a JSON object from the Summary Agent and renders that as HTML. The agent then passes it to the fourth tool, which is an SES service. It's important to note our cloud native products team built a fantastic SES wrapper that really simplified this tool. It was virtually plug and play to send emails to every director at Grainger.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2520.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2520)

We talked a little bit about the workflow. Let's talk about the architecture.  Nightingale consists of really two core runtimes. We've separated the agentic layer from the MCP layer, and these two runtimes interact with the MCP protocol. On the left are two agents. On the right-hand side are the four MCP tools. What's nice about this partitioning is we can continue to refine and add new tools to our MCP server layer without a single code change to the agentic layer.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2570.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2570)

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2580.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2590.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2590)

Authentication is handled out of the box by AgentCore. In this case, we're leveraging IAM authentication for its simplicity. But we evaluated OAuth as well, and ultimately for us in this specific use case, IAM authentication was ideal.  The tools themselves are backed by data layers using Redshift.  Exports from Cost Optimization Hub and external APIs and services.  This is the output, right? As I mentioned, the output is the actual product, not the input.

Let's step through this executive summary a little bit. The idea here is that a leader can walk away in five minutes with everything they need to know about their cloud cost and usage. They weren't coming to our tools, so we brought the data to them. At the top is the headline metricsâ€”what is the trend for the last ninety days and what is essentially the total spend for that time period, followed by key points.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2640.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2640)

With this, we really wanted to keep the most important information up at the top so that if they only had two minutes, they would have sufficient information to walk away with at least the bare minimum they needed to know about their cloud footprint.  Followed by a deep dive, which goes in a bit further into service and account-driven cost increases or decreases. We also highlight potential anomalies. What's important to note here is this is agentic inference, right? Most people reading this would not know that this was generated by an agent unless we put the little disclaimer at the bottom.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2690.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2690)

We found with this that in many ways it met the same specialist quality as our FinOps team. And as I said, as opposed to a matter of a full day, it can now create this summary in minutes. Lastly, we have recommendations and outlook, and this is kind of key.  Currently we're in the informed phase of the FinOps life cycle as we begin to democratize and broaden adoption across the organization. But we really want to get to this place where we're operating, right? We're optimizing and operating.

Within FinOps capabilities, and that's where the recommendations come in. These provide real actionable insights for our people leaders aggregated at their level and tied to a dollar amount. They can pass this along to their tech leads, and these tech leads can take action by identifying the root account where these resources live. Similar to MondelÄ“z, we're able to attribute account level ownership because each account belongs to a specific team.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2760.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2760)

### Scaling Beyond FinOps: The Future of Specialist-Quality Insights Across the Enterprise

Lastly, the outlook contains essentially a forward-looking perspective, assuming the recommendations are acted upon and current trends continue.  Where do we see this going? There's a lot of interesting possibilities here. I think there's nothing unique to FinOps when it comes to the scaling problem. We've spoken with DevOps, SRE teams, and other platform engineering teams who are in a similar boat. They have critical insights trapped behind dashboards, and the customers and stakeholders simply aren't coming to them.

Within FinOps itself, we have multiple personas that we serve. It's not just domain and product leaders. We serve product owners, finance, and procurement executives. Each one of those personas has a different way of working and ultimately a different signal channel where we need to bring the data to them. As we look to extend this tool, I believe it involves a number of things.

First and foremost, we didn't use AgentCore Gateway when we started because at the time they didn't support native MCP tool use. Today, now that it's generally available, they do. Adding AgentCore to Nightingale really unlocks the scalability of adding more MCP tools and servers at the enterprise level. We can start pulling right-sizing recommendations for Kubernetes and pushing pull requests to GitHub repos for each specific Kubernetes deployment, or we might be creating Jira stories and Jira projects for our product teams. It could be pushing data to Power BI dashboards for our finance team, all controlled through the MCP layer and routed via AgentCore Gateway.

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/2930.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=2930)

At the agent layer, as we continue to expand, we're looking at adding more agents with varying expertise. We found that it's best to narrow the focus of an agent to one specific use case or one specific field. Efficiency scores, for example. As we start meeting our engineering team demographic where they work in developer portals, we could have an agent specialize in building out efficiency scores for each team. This would essentially be followed by an analysis from the executive summary agent and linked back to our cloud cost and usage tooling. 

Finally, to tie it back together, with Project Nightingale, we were able to solve the scalability challenge and began to democratize FinOps by using FinOps quality analysis and meeting an audience of fifty. Through generative AI solutions, Project Nightingale has unlocked a path for us where we now have a force multiplier that we didn't have before. We're a team of seven, and there's no possible way we can meet every persona with this level of quality at this scale in a feasible amount of time.

As I mentioned, this is only one single channel among many, but now we have the foundation to extend beyond just products and domain leaders. Lastly, thousands of specialist hours are now saved. I'm not saying generative AI is replacing specialists. This unlocks our FinOps team to now go focus on the hard problems that humans are good at, whether that's deep dives into architecture, helping platform teams re-envision a more efficient and effective platform, consulting product owners about PCO.

Our team of specialists now has more time to be served to better purposes. So, if you're an SRE, DevOps leader, or platform owner, don't just wait for your customers to come to you. Seek out solutions leveraging generative AI and bring the data to them.

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/3060.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=3060)

Thank you, Chris. Thank you, JC for sharing stories from both of your companies. We heard a little bit  about how you can use automation to create a way for the IT department to move at the rate of change of technology and be best at supporting the business. We heard a little bit about how you can use agents in FinOps to create better visibility of costs and revenue implications of IT decisions all the way up to the C-suite.

These are two examples. What I'll leave you with is a framework to think about how you can be on your own transformation journeys. Just as consumers don't move sequentially through their path to purchase and jump around through it, where you might be in this framework might differ based on your idea or your solution or where you are simply in your maturity with AI. I'll leave you with this for you to think about what you might be able to innovate in next, and then we'll leave the extra time that we have for you to come and talk to the speakers afterwards on the side.

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d9e068b1cec98b52/3120.jpg)](https://www.youtube.com/watch?v=5GxPoCeENmw&t=3120)

Thank you, and  before my head of marketing kills me, please fill out the survey on the app. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
