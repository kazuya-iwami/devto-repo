---
title: 'AWS re:Invent 2025 - Binge-worthy: Netflix''s journey to Amazon Aurora at scale (DAT322)'
published: true
description: 'In this video, Ammar, a Staff Software Engineer at Netflix, explains how his data platform team migrated over 100 databases from a third-party PostgreSQL-compatible distributed database to Amazon Aurora PostgreSQL. He details their three-phase approach: pre-flight checks using AWS DMS schema conversion tools to identify compatibility issues across the entire fleet, schema and data copying with custom verification tooling to catch edge cases like VARCHAR truncation and data corruption bugs, and live validation using both batch processing in their data warehouse and real-time Flink jobs. The team achieved zero downtime for reads and minimal write downtime by leveraging proxy-layer control for cutover automation. They completed over 90% of migrations with no rollbacks, discovered and fixed 10 data corruption bugs upfront, and observed improved latencies in many cases due to Aurora''s architecture.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Binge-worthy: Netflix's journey to Amazon Aurora at scale (DAT322)**

> In this video, Ammar, a Staff Software Engineer at Netflix, explains how his data platform team migrated over 100 databases from a third-party PostgreSQL-compatible distributed database to Amazon Aurora PostgreSQL. He details their three-phase approach: pre-flight checks using AWS DMS schema conversion tools to identify compatibility issues across the entire fleet, schema and data copying with custom verification tooling to catch edge cases like VARCHAR truncation and data corruption bugs, and live validation using both batch processing in their data warehouse and real-time Flink jobs. The team achieved zero downtime for reads and minimal write downtime by leveraging proxy-layer control for cutover automation. They completed over 90% of migrations with no rollbacks, discovered and fixed 10 data corruption bugs upfront, and observed improved latencies in many cases due to Aurora's architecture.

{% youtube https://www.youtube.com/watch?v=cxXG5fZ5wik %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/0.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=0)

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/30.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=30)

### The Challenge: Migrating Dozens of Databases from Third-Party PostgreSQL to Amazon Aurora

 Thank you. Hi, everyone. My name is Ammar. I'm a Staff Software Engineer at Netflix on the data platform team, and today we're going to talk about how we moved dozens of databases that were on a third-party PostgreSQL-compatible distributed database to Amazon Aurora PostgreSQL. We'll start by talking about the problem, what we actually did, and why.  Then we'll spend some time talking about how we did it. I'll leave you with where we are right now and what learnings we have so far. There's a ton of content and not as much time, so I'm going to move through this quickly. If you have questions at the end, I can explain things in more detail.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/50.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=50)

 I'm on a platform team. We work with a whole bunch of application teams that run the business. They work on projects like "Stranger Things" season five production and a whole bunch of other things. Many of these teams use our databases, and particularly many of them ran on this third-party distributed PostgreSQL-compatible data store. It's self-managed, which really means our team manages it or managed it, and that comes with its trade-offs. Being self-managed means we do a lot of work on managing it, upgrading it, and handling instance replacements. It's PostgreSQL-compatible, which means it's usually PostgreSQL, but until it's not. We found that those trade-offs, plus the fact that it can get quite expensive running such a powerful database across the entire fleet, made us reconsider. In most cases, you don't need that level of power. In the past few years, especially Aurora PostgreSQL has come a long way in terms of feature set, reliability, and cost.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/130.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=130)

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/140.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=140)

 We made the decision as the central platform team to move all these databases over to Aurora PostgreSQL. It's important to note that this is the platform team making a decision for all application teams. The more work we can do, the better it is, because we're asking teams to move.  Additionally, it's better for the company if we take as much of the work and execute it ourselves rather than making the application teams do it. In general, database migrations are a solved problem. You copy the schema and the data, you validate it to make sure it's all there, and then in theory all you have to do is point the application to the new database. AWS has tooling to help with this. There's something called the Database Migration Service that we used as a building block. It can do schema conversion and copy, it can migrate data, and it can even validate the data. Then in theory, all you have to do is point the application.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/160.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=160)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/180.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=180)

However, when it comes to the real world, things start to get more tricky.  In our case, we had over a hundred applications with different data sizes. Each application, at least from the database perspective, is very different. The data shape is very different, and the data access patterns are different. If you do this naively, you can run into different issues per database, and you don't want to do that. You want to catch issues early as much as you can.  These applications are written in different languages. We can't just build something for Java and ignore all the other use cases. You have to make sure you build something that works in all languages instead of building something individually in each language. You want to find a way to support all these use cases together.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/200.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=200)

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/220.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=220)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/230.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=230)

 Additionally, these different applications have different downtime requirements.  Some are okay with being down for four hours on a weekend. Others can't support more than two minutes of downtime or even thirty seconds of downtime. So it gets quite tricky.  These applications also don't exist in a vacuum. For every application database, there are often streaming connectors and analytics connectors. If you did this naively, we'd migrate the database and then say, "Hey app team, you're on your own to migrate your Iceberg exports or your Flink jobs and things." We have a lot of this at Netflix. You have a whole data platform of all these pieces that you can plug into your database. So that's where it gets tricky.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/270.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=270)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/300.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=300)

### Pre-Flight Checks: Testing the Migration Across the Fleet Before Involving Application Teams

Now that we've talked enough about what we're trying to solve and why, let's talk about how we solved it. We're going to go over this in three sections. First, we'll talk about what we did before even involving application teams, what sorts of pre-work we can do before talking to these teams so we can minimize back and forth.  We'll talk about how we copy the schema and the data. Spoiler alert, we leaned heavily on DMS for this with some additional tooling on our end. Then we'll talk about how we validated the data to make sure it's all there and did a live-ish cutover, live to the point where reads continue to work throughout the entire process. Writes are what we took down for a few minutes.  So let's jump right into the pre-flight checks. This is something we did before even involving application teams, and we did this across the entire fleet.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/260.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=260)

We used AWS DMS tooling, specifically their schema conversion tool.  You can run it against a database and generate a report indicating whether a particular source and target combination will work. Our target was Aurora PostgreSQL. AWS did not initially support our source database because it was third-party software, so we worked with them to build in support so we could run this migration. The tool generated a report highlighting issues such as indexes that would not work in Aurora PostgreSQL or hidden columns that would become visible columns. We took these findings and worked with the application teams to ensure their schema would function correctly on both the source and target systems.

The second piece involved handling the data. Application teams execute SQL statements against their third-party databases. We sampled those statements and ran them against PostgreSQL without involving the application teams, essentially to verify whether they would actually work. This gave us confidence that the schema would work and that the schema changes would be compatible. We also examined their source cluster upfront and provisioned the target clusters based on the traffic patterns we observed. This allowed us to properly size the PostgreSQL database instances on the target end. We copied over all ownership metadata and authorization information to ensure their applications could communicate with the database.

We provisioned not only the final clusters but also a temporary cluster. We copied the schema and data to that temporary cluster, validated it, and handed it off to the application teams. There is only so much you can accomplish by examining the schema and data alone. You actually need to run your application to see how the database performs and whether all the SQL actually works. We discovered numerous issues, such as implicit casts that worked in the old database but not in the new one. These represented three key ways we tested the migration across the entire fleet before even speaking with application teams.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/450.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=450)

### Schema and Data Copy with Built-In Verification: Leveraging AWS DMS and Custom Validation Tooling

The next step was the actual schema and data copy.  For schema copy, we built tooling around AWS DMS. Schema copy involves any database object such as tables, views, and sequences. Because we were switching database engines, some of these had to be converted, and it was not always a one-to-one mapping. This conversion was performed by AWS DMS, which copied things over for us. However, we built in verification because, especially given that AWS did not technically support this source, we found edge cases where things were being mangled during copying. For example, we had VARCHAR(3) that was converted to VARCHAR(1). What is the impact of that? These were country codes, and you cannot really have a country code that is just a single character. That would not work. We found these sorts of issues by writing our own schema verification tooling to double-check the work we were doing, since we were accountable to our application teams.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/510.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=510)

The next step was the actual data copy.  We again built tooling around DMS. DMS performs a full load, which is a point-in-time copy of all data from the source to the target. It also performs CDC, which is change data capture. As the application writes to the source database, it is being replicated to the target. Keep in mind this is a live migration, so we are not shutting down the application and then copying the data. We are doing this live, with the application running, the source database running, and the target database running. We copy the data and then start replicating it. Similar to earlier, we built in tooling on our end, specifically monitoring tooling. We found that in some cases the task would fail partway through because of a transient issue, and we would not realize it until it was time to cut over. Then we would discover that database replication was not happening and we would need to abort. This monitoring helped us find issues earlier instead of during a cutover.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/560.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=560)

Just because the data is all there does not mean you are done.  You actually have to validate that the data is all there. At this point we have a source and a target with data being replicated from the source to the target, but effectively we have two different databases with very similar data sets, though perhaps not exactly the same. That is what we needed to find out. We learned this the hard way and found issues upfront. DMS does support validation, but there were two problems. First, it was not built for this custom third-party source database. Second, we wanted to perform validation without putting pressure on the source and target, because these are live databases, at least the source initially. We also wanted to ensure this works well for both large and small data sets in a reasonable amount of time.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/610.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=610)

To accomplish this in a reasonable amount of time, we used a lot of existing infrastructure and took those source  and target databases and attached them to a data feed. Think Kafka, for exampleâ€”though not Kafka in this caseâ€”but we connected to a data feed so that we dumped the data and also performed CDC from there, sending it to our data warehouse. Now that it's in a data warehouse, we don't have to worry about putting pressure on the source database because we can write a giant distributed SQL join across two tables to identify what data is different and ensure that everything matches.

Since this data is changing over time, we cut off at a certain pointâ€”let's say five minutes in the pastâ€”and ignore any records that have come within those five minutes. Everything else, we validate the entire dataset. We can do this faster and out of band as well because we're taking a cutoff at a certain point anyway. However, that's only part of the problem.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/670.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=670)

The second part is validating data that's still coming through and ensuring the most recent data is still correct.  This is where live validation comes in. We take those same two data streams, which are processing data coming into the source from the user and to the target from the source through replication, and pipe them into a validator. In this case, it's a Flink job, but a validator that takes the two streamsâ€”source and targetâ€”and compares them. For each record we see in one stream, we make sure we see the same equivalent record on the other stream.

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/720.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=720)

This is much faster than validating the entire dataset in one fell swoop. This lets us do the cutover much faster because we can be confident that the data before is valid and the data that is going through is valid. The last detail is that this is the full picture of our validation.  We have the data streams going and both batch validation and online validation happening at the same time.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/730.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=730)

### Zero-Downtime Cutover and Results: Automating the Migration Process with Proxy-Based Switching

Now that we have validated our data, let's talk about the cutover.  We realized we can achieve zero downtime for reads and minimal downtime for writes. To do this, there are two pieces. First, we need to take the application out of the loop as much as possible. We don't want to be sitting on a call with the app team coordinating things back and forthâ€”it's a mess because they have to shut off the app. If we can move them out of the loop faster, we can do a lot more things on our end.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/770.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=770)

The second part is automation. It's one thing to do one migration by hand, but it's another thing to do a hundred plus migrations. So let's go over both of these. For the cutover process,  before I get into it, one thing we tend to do at Netflix is run proxies in front of data stores. We do this in many cases for authentication and authorization, but depending on the data store, it might have other functionality as well.

We realized that to get the application out of the loop, if we handle things at the proxy layer, we can take the connection from the proxy to Postgres and re-point it to Aurora without the application being involved. To do that, we have to make sure the application will work with both the source and the target. At this point, we have to ensure that their previous schema checks and SQL checks are passing, everything there is good, they've done all their testing with a temp cluster, and then we make sure they're talking to the proxy.

This means that if they're talking to the proxy, we control the proxy, and we can control the cutover time and shut off writes and do all that without the application team being involved. We also make sure they're using the correct credentialsâ€”credentials that work on both the source and target. This lets us do cutover without the application being involved.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/840.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=840)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/850.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=850)

Now let's go over the actual cutover process.  This involves a lot of steps, so I'm going to move through it quickly. Remember at this point we have source and target replications happening from the source to the target.  The first step is determining how long the replication delay is. We don't want to attempt cutover if the replication delay is an hour because then you're sitting and waiting for an hour for replication to catch up.

What we do is write a record into the source and wait for it to show up in the target. It's that simple. We use an internal metadata table that we create so we don't mess with the user's data. We set an arbitrary cutoff of about a minute, but in reality, we saw in most cases it was fifteen to sixteen seconds of replication delay. We measure this so we know it's accurate.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/890.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=890)

Once we know that replication is happening, we have to run our validation.  We have batch and online validation. At this point, remember the application is still running, everything is up, nothing is down. We just run our batch validation to make sure that the data we copied over is accurate. Assuming it's accurate, we have to make sure that the user is still actually talking to the proxy.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/910.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=910)

 We found cases where users were not talking to the proxy or they had some application that was running on a batch schedule. We ran some checks to make sure no one is talking directly, but we also removed direct access and made it so they had to talk to their proxy. This made it more difficult for users to mess things up later on by having a split brain scenario where they're talking to the wrong cluster.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/940.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=940)

And then the last step is that databases don't exist by themselves.  We take all that streaming and batch infrastructure and copy it over from the source to the target, because at this point we know that the data is all there and it's correct, with replication happening. So it's safe to do that. All you're doing is introducing another few seconds. If the replication delay is 15 seconds, you're adding 15 seconds of delay to their downstream pipelines. That's fine. No one's going to care or notice.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/970.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=970)

This gets us to the critical section.  At this point we shut off writes by modifying permissions for the user on the source database. Read is still working so the application is still up, but it's slightly degraded because writes aren't working anymore. We did coordinate with app teams on a downtime window for this. Applications are again up and writes are blocked.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/990.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1000.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1000)

I talked about sequences briefly earlier.  Sequences are basically stateful counters in the database and they don't get copied over our replication.  So you have to take the sequences from the source and then make sure the ones in the target match up to what they're supposed to be. It's a small detail.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1030.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1030)

We measured replication delay earlier, and now we have to wait for replication to catch up.  It's the same process as before: we push a record into the source and wait for it to show up at the target. At this point we have blocked writes, so we know that the record we pushed in is the last record. When it comes through, we know we're done with replication and then we look at our validation. This time we look at only online validation because we previously verified batch validation and also online at the time.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1040.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1040)

At this point we're just asking: is the data still correct? Is it still not mangled? Do we have confidence that we can cut over safely?  Assuming that's true, we're essentially done. We take the proxy and update it to point to the target, which in this case is Aurora PostgreSQL. We also terminate connections on the source database just to make sure there are no lingering connections. We kill all connections and now the application's talking to the target. We never turned off write to the target, so it just starts working. That was the cut-over process.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1070.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1070)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1090.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1090)

That was a lot. I'll leave you with how far we are, because we're not done, and the learnings we have from this adventure.  We started writing the automation in December last year, so almost a full year. But we ran our first migration in April. It was kind of rough, with some manual code running on my laptop and some running in Jupyter Notebooks, but we made it work and then we built pieces as we went.  At this point we're over 90 percent complete. There's a handful of cases that require some special handling, such as large data sets that we'll be finishing off in the next few weeks. I'm pretty impressed with this progress on our team. We did not think we'd be this far right now.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1110.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1110)

Additionally, we've seen in some cases, not all, that latencies drop.  This is partly because this is no longer a distributed store. We're not doing Raft consensus for reads and writes, and with Aurora PostgreSQL, you're generally reading from memory. So there are reasons for that. But this is a side benefit: latencies are generally lower in a lot of cases, and you can see it's pretty obvious from this graph when the cut-over happened. There's a very clear demarcation.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1140.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1140)

This is critical for us as the data platform team: we found a number of data corruption bugs and we fixed them all as far as we can tell.  I mentioned earlier that we ran our attempt cluster creation across the entire fleet. That included validation and found about 10 data corruption bugs. Some were related to null handling of columns, some were encoding getting mangled, some were generated columns not being copied over correctly, and some were data truncation where columns were just being dropped. We found all these upfront, or almost all. There were a couple we found later on, including one weird one with statements in a transaction being ordered differently depending on when they came into replication. We found that when we were doing test cut-overs because that was CDC related. Our initial process was less CDC and more just full load.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1200.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1200)

I'm pretty happy with this because we found all these bugs early on without going back and forth with the app teams.  We worked with the DMS folks because sometimes it was edge cases on their end and sometimes it was engine differences that we had to work around. But we got all these sorted and validation passed. After all that, we had no rollbacks, which I'm pretty proud of. We had a couple of fix-forward use cases. It wasn't all sunshine and rainbows. Some of them were just under-scale clusters and some were metadata that didn't get copied over correctly. We fixed all of those, but everything happened and no one had to roll back.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4ba97e2c3c9d0fb9/1220.jpg)](https://www.youtube.com/watch?v=cxXG5fZ5wik&t=1220)

The last thing I'll leave you with is that the initial fleet-wide dry run we did was amazing because it saved months of time.  Not even exaggerating, by doing this all up front across the fleet and working with the DMS folks, we built resiliency into our tooling and that all worked really well. We built on top of a lot of existing tooling, including DMS and some of our own tooling as well, but we got a lot of value out of building verification and making sure things actually worked correctly. The last piece is that we focused on the lowest common denominator and built things that would work across applications and across technologies. But we did spend some cycles building specific tooling for Java Flyway because it turned out a lot of folks at Netflix use that. We built some specific tooling just for that, even though it's not really applicable to all use cases. That's all I have. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
