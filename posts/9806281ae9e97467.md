---
title: 'AWS re:Invent 2025 - Optimizing for high performance with Amazon ElastiCache Serverless (DAT437)'
published: true
description: 'In this video, AWS engineers Elad Bernstein and Yaron Sananes explain optimizations for building high-performance applications using ElastiCache Serverless for Valkey. They discuss achieving sub-millisecond latency for millions of requests per second through techniques like persistent connections, connection pooling, pipelining, and reading from replicas. The session covers mitigating hotspots via key duplication and splitting, cross-AZ latency measurements showing under 700 microseconds, and how the serverless proxy architecture enables seamless scaling from zero to 5 million requests per second in 12 minutes. They demonstrate Valkey''s IO threads contribution that achieves over 1 million requests per second per instance, and explain how ElastiCache Serverless automatically scales up 8x instantly while scaling out horizontally, doubling cluster capacity every 2 minutes without disrupting connections.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Optimizing for high performance with Amazon ElastiCache Serverless (DAT437)**

> In this video, AWS engineers Elad Bernstein and Yaron Sananes explain optimizations for building high-performance applications using ElastiCache Serverless for Valkey. They discuss achieving sub-millisecond latency for millions of requests per second through techniques like persistent connections, connection pooling, pipelining, and reading from replicas. The session covers mitigating hotspots via key duplication and splitting, cross-AZ latency measurements showing under 700 microseconds, and how the serverless proxy architecture enables seamless scaling from zero to 5 million requests per second in 12 minutes. They demonstrate Valkey's IO threads contribution that achieves over 1 million requests per second per instance, and explain how ElastiCache Serverless automatically scales up 8x instantly while scaling out horizontally, doubling cluster capacity every 2 minutes without disrupting connections.

{% youtube https://www.youtube.com/watch?v=fXB6wftmigU %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/0.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=0)

[![Thumbnail 10](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/10.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=10)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/20.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=20)

### Building a Massive Multiplayer Game: Why ElastiCache Serverless for Valkey?

 Good morning. Welcome to Reinvent.  If you do put on the headphones and I'm speaking too loud or too softly, you have a volume button on the right side.  Let's say we're building a game. All of us are starting a new gaming startup, and we're going to create a massive online multiplayer game that, if we're successful, is going to support hundreds of thousands of users. So we're going to need massive scale, probably millions of requests per second. And we're going to have to maintain very low latency for our users to have a consistently responsive game, otherwise they probably won't play our game.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/70.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=70)

We're going to use ElastiCache Serverless for Valkey because it provides predictably low latency and supports the massive scale that we need.  My name is Elad Bernstein. This is Yaron Sananes. We both work for AWS. ElastiCache Serverless does take away many concerns that you have as a developer or DevOps engineer. For instance, you don't have to worry about security patching. You don't have to worry about version upgrades. But most importantly, you don't have to worry about sizing your cluster and you don't have to scale it when your application load increases.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/130.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=130)

This session is going to talk about optimizations that are relevant for very high performance applications. These optimizations are going to be relevant both for ElastiCache Serverless and also for self-designed clusters, or even if you're self-managing your own fleet of open source Valkey.  In case you're not familiar with Valkey, it's an open source in-memory database. It was forked out of Redis when they changed their license, and Valkey has been adopted by the Linux Foundation. It's maintained by a very active community, including some industry leaders and a few cloud providers, including AWS.

Let's go back to our game. We're just in our labs now. We're taking a few clients and connecting them to a Node.js server. We'll probably just store all the state of our game in memory, which would include things like where our users are, where the monsters are, and maybe their health. And we're going to use some disk if you want to store some more persistent resources, such as our game maps or castle maps, things that don't change that often.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/200.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=200)

 Our users on the proof of concept are probably going to have terrific latency. In the lab, what would we expect for latency? Usually when networking is not a concern, you would see latencies of around 100 microseconds round trip. Most of this latency would come from the operating system, the network stack, and the kernel communicating with user mode. So it's not really network latency here; it's mostly just the operating system.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/260.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=260)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/280.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=280)

When we go and access memory for our state management, reading state and changing state, we're probably going to have access times of a few tens of nanoseconds. That means we can actually go to memory many, many times before it actually impacts significant latency. We could do it hundreds of times on a single user request, maybe even thousands of times without it impacting latency.  This is very different when we talk about disk latencies. Even if you have the best SSD, it's going to be a little less than one millisecond probably, but it's going to vary.  Why is it going to vary? Because when you read an object from disk, you're not just doing a single access. Sometimes you're doing multiple accesses because disks store objects on different blocks. So disk latency is usually very variable, which we probably don't want in our application.

We don't want this variability. For our game proof of concept, we can probably just load all the maps when we start, keep them in memory, and then use them there. But when we take this to production, it's going to be a little bit different. Our users are on the internet, we're using multiple instances of our application because we want scale and high availability, and we'll probably use a bunch of availability zones to get that. Our resources instead of this are going to be naturally placed in some database.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/380.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=380)

When we read the database, latency is going to be very variable, somewhere probably typically between 1 millisecond to 100 milliseconds. That's because traditional databases, whether relational or document databases, usually use a combination of both memory and disk to deliver queries, and it's very hard to control where your queries are serviced from. It's hard to say that everything must come from memory. That's where Valkey becomes very handy.  Valkey uses exclusively memory to serve your queries and provides you with predictable latency, which is what we want for our game.

By the way, this applies to any application, not just games. We chose the game because games are fun, but any application that needs millions and millions of requests per second with strict latency requirements will benefit from Valkey because of the low variability. Since Valkey uses memory, most of the latency you'll see isn't even coming from Valkey usually. The most dominant factor with Valkey latency is probably going to be the network. If you're on the same availability zone as your Valkey node that you're accessing, you'll probably have latencies far below 1 millisecond, sub-millisecond latency.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/470.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=470)

### Understanding Network Latency: From Microseconds to Milliseconds Across AWS Infrastructure

Now it may be that your application needs to go from another availability zone and access a Valkey node on a different availability zone, and then latency increases slightly. I'll dive deeper into these latencies in a minute. But what if we could manage our application such that we're reading from a replica node that's on the same availability zone? That way we can maintain, at least for our reads, low latency for all of our application  instances, lowering our P50 and our average latencies for the application.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/490.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=490)

Let's dive deeper into latencies.  We're going to talk about milliseconds, microseconds, and nanoseconds. That's the world we live in. Let's start off with your users. When you have users coming over the internet connecting to your application, you'll typically see latencies somewhere between 20 milliseconds up to 100 milliseconds, maybe even 150. It really depends on where your users are. They could be very close to the data center with an excellent internet connection, or they could be very far away, even on another continent. So that's the variability we'll see.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/530.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=530)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/550.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=550)

When we continue into the data center, if you're communicating between two servers in the same data center, latencies are going to be very low, very similar to the lab we discussed before.  Network latency is going to be nanoseconds usually, and it's going to be around  75 to 100 microseconds round trip, usually because of operating systems. You can optimize that using tools like DPDK that optimize the kernel to use lower latency, but usually that's what you'll see, and these are very low latencies anyway.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/600.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=600)

Now when we go from one data center to another on the same availability zone, we can expect latencies of up to 1 millisecond. In case you're not familiar with this, availability zones usually have one or more data centers. They're not just a single data center, and these data centers can be miles and miles apart. So latency is slightly higher. And when you go over the availability zone to another availability zone,  those data centers are even further apart, and then latency can increase.

When regions are far apart, latency can be in the milliseconds range. The most dominant factor here is distance. We have to carry this signal over fiber optics from point A to point B and back. We're bound by the speed of light. That's the thing. If they're close, it's going to be good latency. If they're farther apart, it's going to be higher.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/640.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=640)

Let me show you some real-life examples.  What we're looking at are actual latencies from US-EAST-1 region. I took this a few days ago when I prepared the slides, and these are cross-AZ latencies, latencies from AZ A to AZ B, AZ 1 to AZ 2, AZ 2 to AZ 1 to AZ 3, just a few measurements. You can see that most of these latencies are far below 700 microseconds. These are the cross-AZ ones. So we said they could be milliseconds. In this case, in this particular region, they're far lower.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/690.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=690)

Let's zoom in now.  What happens inside each AZ? These are latencies coming from comparing inside each AZ. So there's a latency from two points inside AZ 1 and other latency from two machines in AZ B. These latencies are much lower, as you can probably spot, and most of them are around or below 100 microseconds. So we have excellent latency in this case. There is some variance here. It depends on which region you use and which AZs, where they are, and the distance between them.

What you could do is check out something called the Network Manager. It's available on your AWS console, of course, the APIs, and you can actually take a look at all the latencies between different regions, different AZs in those regions, and then you can look at your application and consider the factors around latency. What latency can we expect between AZs? What latency can we expect between regions for any use case that you have?

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/770.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=770)

### Optimizing Connection Management: Persistent Connections, Pipelining, and Connection Pooling

Moving on.  You folks have tasked me with creating our first microservice for the game. I didn't do an amazing job, and I created this microservice for getting user profiles. Now this is not a good practice. See if you can spot my mistake here. Let's go over what we're doing here. This is a very simplified HTTP server. Whenever there's a new request, the first thing that it does is go and create a new Redis client called Connect. Then it does a get for the profile that we actually need.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/820.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=820)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/850.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=850)

The problem here is that whenever we have a request, we're doing all these things  just to get a single key. We're getting a new connection, which means a DNS query, a TCP handshake, TLS initialization, cluster discovery, doing cluster slots, understanding where everything is placed on the cluster. And then finally, what we actually wanted, which was getting the key and doing the get command. Most of you already know this, but the good practice is to use persistent  connections. That means taking this connection initialization part and taking it outside.

Now, persistent connections are extremely important, and you can actually do them even if your compute is Lambda. Some people don't realize it, but Lambdas don't actually get torn down after each call. They persist. If you have more calls and they remain hot, persistent connections are definitely useful both for EC2, ECS, Kubernetes, or other use cases. Another important point about connections is to understand what happens when we do this good practice.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/930.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=930)

Pipelining is another important factor that can influence performance, especially when you have a high-performance application. Pipelining means sending out multiple requests on the same TCP connection without waiting for responses. This is exactly what happens in our web server. When user one  sends a request and the server reads it, the server sends out a GET command to Valkey. This is synchronous with an await, which means execution waits, but the server is not blocked. It can continue servicing more requests in the meantime.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/960.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=960)

Another request is read from user two and gets sent as well, and user three's request gets sent as well.  Now we have a pipeline, even though I haven't written the word pipeline anywhere in this code. It's an implicit pipeline. Pipelining is an excellent way to increase throughput with Valkey or open-source Redis because if you send things in batches, the server has to do less work to service them, with less I/O. They can potentially read all the requests in a single I/O operation.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1020.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1030.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1030)

The problem arises when you have very high performance and our game is going to have millions of requests with very high concurrency for each web server application. What happens if this is not three requests at the same time but a thousand? That's when things get dicey. Let's zoom into the connection.  We had request number one from user one, request from user two, and request from user three. The REST protocol for both Valkey and open-source Redis is ordered.  Responses must arrive in the same order as the requests we send, making the system very susceptible to head-of-line blocking.

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1040.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1040)

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1050.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1060.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1070.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1070)

What happens if user one  requests a very large profile? We have to read all the response for user one before we read the response for user two and  three.  That response for user one is literally holding the head of the line for us. With small numbers this is not significant, but with large numbers it is. There's another problem: what happens if you run into packet loss?  We're using TCP, so we should worry about packet loss. TCP guarantees delivery, but the only problem is that retransmissions in TCP take at least 200 microseconds to happen. That's hard-coded in the Linux kernel and not something you can change without rebuilding it yourself.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1110.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1110)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1140.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1140)

If we have large requests or just a request that's not large, and there are a thousand requests waiting behind it,  and there's packet loss, all these requests and all these users are going to have a major impact. Packet loss is a fact of life when you have a distributed system. We cannot avoid it, but we definitely want to reduce the impact when it happens. The way to do that is to use more connections.  We can use more connections from the same application, thus limiting the concurrency on each of these connections. This helps us minimize the blast radius whenever there's a large response or something like that.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1170.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1170)

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1190.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1190)

We can even think about things like placing slower request types on a dedicated connection. That's another idea you can consider. Another very easy way to reduce concurrency on each connection  is to scale up the application. The more application instances we have, the less work each of them needs to do concurrently. Of course, that comes with a cost. Let's talk about connection pooling. Connection pooling means using multiple connections from the same application instance.  Some Valkey and Redis clients have connection pools built in. But many of these clients, especially the asynchronous ones, don't. They actually use a single connection and multiplex requests in that connection.

If we actually need high concurrency, such as millions of requests for an application with high concurrency on a single connection, we can build connection pools ourselves. For our game, we can build something like this. Notice what I'm doing here: instead of creating one persistent connection, I've looped and created a bunch of them. Whenever I want to use a connection, I need to pick one. In my case, I've done a modulo over the user ID to load balance between these connections, but you can do randomization as well. You can do round robin or modulo over something else that makes sense for your application.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1270.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1270)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1290.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1290)

### Solving Hot Spot Problems: Key Duplication and Object Splitting Strategies

 Everything is fine. Our game runs perfectly. Our cluster is very healthy with a lot of total resources. We have enough total memory, enough total CPU, and enough total network bandwidth. Everything is great, and then suddenly it's not.  Even though we have enough resources, we have an alarm. Our latency is spiking and our throughput is reducing. We scramble to figure out what happened. In many cases, we run into hot spots. Hot spots mean that a lot of your traffic is hitting one particular node in the cluster, usually because you have hot keysâ€”keys that are very popular and need to be read many times. With Valkey and Redis, it doesn't matter if it's serverless or not serverless. Each shard owns each key, and each key is owned by a single shard.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1340.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1340)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1360.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1360)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1390.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1390)

 When we dive into the metrics to understand the problem, let's say we go through the metrics and reach the network bandwidth case. We see all of the shards have moderate network bandwidth, but one of them is significantly higher. That's probably a good indication of a hot spot.  When we debugged our situation, we found that we have a castle map in the game that's 100 kilobytes in size, not very huge. It's very popularâ€”users love it, which is a good thing. We have to read it many times. But the problem is that we didn't choose Serverless for this particular example. We chose an instance type for our nodes which has 937 megabits per second bandwidth.  If you calculate and divide it by 100 kilobytes, you can only do around 1,000 requests per second for this object. That's extremely low for Valkey. Just 1,000 requests per second.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1420.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1420)

The most obvious thing we can do in these cases is probably to scale up our nodes. We take the nodes and replace them with larger instance types.  Let's say twice as large, so we get twice the bandwidth, and we can do 2,000 requests per second instead of 1,000. But of course this comes with a cost. Also, with Serverless, you don't have to worry about this particular scale-up because Serverless automatically scales your instances up in place. We're going to dive deeper into that in a minute. But even with Serverless, what if you don't want 2x? What if you want 4x? What if you actually want 10x or 50x or 100x? We're not going to find instance types with that much network bandwidth. It just doesn't exist.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1470.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1470)

So what can we do? One nice mitigation that we can try is we can duplicate our object.  We can take our castle map or all of our castle maps and create copies of them. Now because each copy is a different key, it gets placed in a different shard. Whenever we read a castle map in the application, we can choose which copy to get. Now we're spreading the load and load balancing the reading of these objects between different shards. Think about it. With Serverless, it automatically scales and can reach hundreds of nodes. That means if you do 100 copies of your object, you'll get 100 times your throughput.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1530.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1530)

Another thing we can do is read from replicas. If we can read from replicas, we have more nodes participating in our load balancing. Let me show you a very naive example of code for this.  We have our castle cache. When we read, we need to choose one of our copies. We're selecting a copy here using modulo over the application worker ID. It helps with data consistency, but you can randomize, you can do round robin, you can do whatever business logic makes sense for your applications.

Do you see the caveat here? What happens when we want to update the castle map? We have to go and update all the copies. If there are 10 copies or 100 copies, we have to do 100 writes. This works great for situations when you have a lot of reads but not as many writes. That's a very common use case. This is super helpful. But if you have to do a lot of reads and writes at similar frequencies, this won't work for you.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1610.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1610)

So we have another mitigation that we can try. It requires a little more work on our application, but we can actually split and break apart  our objects into smaller ones. For our game, we can take each castle and break it down into rooms or areas. Now when we want to read them, we can actually pipeline multiple reads, get all the parts for our castle, and then assemble them on the application side. The load is spread over more shards, mitigating the hot spot.

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1660.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1660)

If we can do further optimizations for the application, we can write logic that we don't need to get all the rooms for a particular user. We can just get a subset of things. Of course, you can use your imagination  outside of this game example to see how you can break apart your objects in your application. Right now, let me turn over to Jeon, who is going to tell you how we designed ElastiCache Serverless and how we're doing all this automatic scaling.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1690.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1690)

### Inside ElastiCache Serverless: Proxy Architecture and Heat Management

Thank you very much. Before I dive deep into the service  and the way that we build all the technology behind the scenes, Elad mentioned multiple challenges that we need to take care of when we build an application, specifically if we build them for high scale. I want to tell you that with ElastiCache Serverless, you don't need to worry about capacity planning or right sizing your cluster. You can still achieve millions of requests per second with sub-millisecond response time just using ElastiCache Serverless.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1750.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1750)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1770.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1770)

When you create an endpoint to ElastiCache Serverless, we automatically distribute all the infrastructure across multiple availability zones. We do that first for high resiliency and high availability, but we also do that to achieve very good performance and to reduce latency. I will show you in a moment how it's happening. Once you create your endpoint  to ElastiCache Serverless, you connect from your VPC using a VPC endpoint to the ElastiCache Serverless VPC. At the beginning, you reach the NLB, and the NLB is responsible for balancing the traffic across the Serverless proxy. 

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1800.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1800)

Now I'm going to talk about the proxy in the next few slides and explain why we built the proxy. Overall, the main job of the proxy is to route your request to the correct cache node. We also locate your local proxy that sits in the same availability zone so you can reach minimum latency while you connect to ElastiCache Serverless. As I mentioned,  the proxy is responsible for routing the request to the correct cache node.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1810.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1810)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1840.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1840)

The request goes to the correct cache node, and we do this very effectively.  The reason we can do this effectively is because we are using a multiplexing technology and multiplexing protocol. We use a single TCP channel to connect to the cache node while multiplexing multiple different client connections over that channel. In this way, we reduce the number of connections and also reduce the number of system calls for every network transaction. 

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1850.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1850)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1870.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1880.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1880)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1900.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1900)

I want to move back to the cache node itself. We are running in a multi-tenant environment on a physical host.  We constantly monitor this environment. We have a service called heat management, and its job is to monitor the cache load and all the physical loads across our fleet. Because we are running in a multi-tenant environment, we share resources like CPU,  memory, and networking.  The cache nodes run inside VMs within the physical host, and as you can see, each cache node has a different size. The reason for this is that each cache load has a different workload to handle. Our job as a service is to make sure you have enough extra capacity to run your workload. 

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1910.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1910)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1950.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1960.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1960)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/1980.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=1980)

I mentioned before that we have the heat management process that runs and consolidates all the physical host status across our fleet.  Because we constantly monitor and ensure you have enough resources to run your workloads, we sometimes find physical hosts that reach certain thresholds. For example, here we reach a threshold for memory and network. We need to think about how to evict some of the cache load from this physical host to a more available physical host. We use the algorithm of the power of two random choices.  We pick the two hottest cache nodes and choose to move only the second one.  The reason we choose to move only the second one is because we do not want to interrupt the busiest one that is probably under heavy load and undergoing scaling operations. We want to let it succeed, but we still want to move the one that will release enough resources once we move it to a more available cache node. 

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2000.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2000)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2020.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2020)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2040.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2040)

I explained earlier why it is so important to have a persistent connection. There is a cost when you create a new connection, and with the proxy we promise that while all the changes happen behind the scenes, we store your connection and keep it persistent to the ElastiCache service. Of course, the proxy is also responsible  for moving the connection to the newly created cache node, the one we move to a different physical host. In this way, we keep all the fleet very balanced with all the resources required. Now there are situations where you cannot  wait for heat management to free enough resources for your workload, and you need something more drastic. You need to handle a burst of workload that kicks in because of an event or something happening in your business. For that, we built a very nice technology in ElastiCache Serverless to support this. 

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2070.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2080.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2080)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2090.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2090)

### Scaling to Five Million Requests Per Second: Multi-Tenant Platform and Read from Replica

We are using a platform technology that does not have a fixed memory or CPU footprint and can be rescaled up and down very instantly. We use technology based on a multi-tenant environment so we can keep the cost very minimal. This technology allows us to scale up very quickly  and use more cores and memory that we have on the physical host. We can  support eight times more throughput on demand once it kicks in to the ElastiCache Serverless endpoints.  Now because we start with scaling up, we have time to start scaling horizontally, scaling out. The way we do that involves three main stages. We have the detection, which happens very quickly.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2140.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2140)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2150.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2160.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2160)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2170.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2170)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2190.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2190)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2200.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2200)

We also have the ability to project incoming and upcoming workloads because we are constantly monitoring your workloads and can predict what the workload will be in the next coming minutes. This allows us to pre-scale your cluster behind the scenes and be ready for the workloads that may arrive. The second stage is provisioning. To provision a new node is very costly, but we use a warm pool.  A warm pool is a list of cache nodes that are predefined and pre-installed, waiting to be attached very quickly to the cluster. Once we use the warm pool,  we attach them to the cluster. The next and final phase required is to move the data from the original shards to  the new shards that we just attached. For that, we monitor the data within the slots and determine which  slots are considered hot and which are cold. Based on that, we can decide which slots to move to which targets. We can do this in parallel within the slots, allowing us to move them very quickly, and we can do it in parallel between different targets so we can assign multiple targets to our cluster.  We can transfer the data in parallel so the scale-out happens very quickly, allowing us to double the cluster throughput every two minutes. 

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2210.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2210)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2250.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2250)

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2260.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2260)

I promised that I would speak to the proxy and explain why we built it.  First, we decided to build a proxy to have a single logical entry point to the serverless cluster. The proxy's main job is to route requests from the client application to the cache nodes. It encapsulates all the underlying cluster topology and everything related to failover, disconnection, and scaling, all happening behind the scenes. Your application can consist of a single connection to the proxy, while the proxy maintains thousands of connections behind the scenes to the cluster.  You don't need to worry about this because you can scale your throughput very instantly using this proxy technology. With all the scaling I mentioned, you can  jump from zero to five million requests per second in only twelve minutes.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2280.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2280)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2330.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2330)

Now, if your application is latency sensitive and you don't have a strong consistency requirement, you can use the read from replica feature.  We also have this on ElastiCache Serverless. Every time you create a replication group, we create one primary and two replicas at the beginning. If you want to connect to the replica, the only thing you need to do is flag the connection to indicate that you want to read the data from the replica, and the proxy will do it for you. Another thing the proxy always ensures is to read from the local availability zone. So regardless of whether it's primary or replica, the proxy will make sure to read the data locally so you can achieve microsecond response times. 

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2340.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2350.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2350)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2370.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2370)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2380.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2380)

Now let's look at how to build the code so we can see how simple it is to work with ElastiCache Serverless.  Here in my code, I'm connecting to my ElastiCache Serverless instance. I'm using TLS enabled because when we're running serverless, we support only TLS connections.  Here I'm flagging on that specific connection, telling the proxy that I want this connection to also read the data from the replica so I can scale the throughput and achieve sub-millisecond response times. I have three very simple blocks of code.  The first one populates a few keys to my cache server list. The second one is a for loop that fetches the data from my cache node.  The third one is similar, just with pipeline, where I'm doing a batch of multiple keys that run all together. As simple as that is, we are going through the proxy. The proxy will make sure to spread the request to the correct child behind the scenes. It will also ensure that if there is any disconnection, scaling happening in parallel, or failover, nothing will disrupt your operations.

The proxy takes care of all of that. Most importantly, the proxy promises to read the data from my local availability zone. The beautiful thing here is that my code stays clean. I don't need to worry about this or put any special code to handle it. Everything is managed by the proxy.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2440.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2440)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2470.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2470)

### Valkey's Evolution: From Single-Threaded to IO Threads and Best Practices Recap

Now I want to go back to Valkey. I talked about Valkey, and all the examples and technology that we build here are based on Valkey. I want to show you how we leverage Valkey technology to improve the horizontal scaling that we offer today for our customers.  Let me go back in time to see how Valkey started. Valkey was designed as a single-threaded process, and the main reason for that was simplicity.  There is no risk of race conditions, no synchronization is needed, and you can still scale out and use a shared-nothing architecture to improve cache concurrency.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2500.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2500)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2530.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2540.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2550.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2550)

With this technology, we can achieve around 1.9 million requests per second, and 50% of the time we're busy on I/O, on read and write operations.  To simulate that, we have the main thread. Every time another workload comes in, it waits in the pipeline for the main thread to finish processing the workloads in the queue. What happens is that we have a kind of head-of-line blocking. Later, Valkey introduced IO threads, and the main idea behind IO threads was to offload all the I/O requests to the IO threads themselves.  This simple idea almost doubles the performance to close to 400,000 requests per second.  This time we are now more busy on command processing.  50% of the time the process itself was busy at that stage.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2580.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2580)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2600.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2600)

Last year, specifically, AWS and my team contributed to Valkey the new IO threads architecture. The idea is very simple. We have the main thread that orchestrates all the jobs that span the IO threads.  Now we ensure that there is no race condition and the number of IO threads can be adjusted according to the workloads that are running on the host itself. Despite the dynamic nature of the IO threads, the main thread is also responsible for affinity and always makes sure that data is assigned to the specific thread so we can enjoy cache locality and improve performance. With this ability we can achieve even more than 1 million requests per second for a single instance. That was a huge win for Valkey. 

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2620.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2620)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2640.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2640)

So to simulate that, now we have the main thread running, and every time we need to spawn another thread when a workload is coming in, we just spawn it.  We can process the workloads in parallel and remove some of the threads to release some of the compute once the workload is relaxed. Now let's see how everything plays together.  We talked about the connection pool, we talked about the proxy itself, and we talked about the physical nodes that we're running on a multitenant environment. Here in my example, I have a cache node running with two cores and two IO threads because we're running a very steady state of 300 requests per second, which is the throughput.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2660.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2670.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2670)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2690.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2700.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2700)

But now let's assume that we have some events that kick in.  Our customers are expecting us to grow our instances very instantly.  So immediately we can scale our throughput by 88 times because we assign more cores. We are spawning more IO threads to the cache node, and in that way we can scale up in parallel. Meanwhile, we are just assigning more cache nodes to the existing cluster because we are using the connection pool.  The proxy automatically detects that, and all of that happens in parallel because it's encapsulated from our application.  We started to move the data from the old shard to the new shard, and in that way we can scale the throughput and double the cluster capacity every 2 minutes.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2720.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2720)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2730.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2730)

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2750.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2750)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2760.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2760)

This achieves a very seamless, serverless experience. I have a pre-recorded demo here  that we started with by adding more workloads to the cache serverless nodes.  As you can see, I started with 250 cache requests per second, and I'm going to add more and more clients and workloads to my cache serverless nodes. While you see that the throughput is growing, my P50 latency is staying  below the milliseconds response time, which means we are still having microseconds response time. Even when we push more workloads  to the cache node, as you can see, I've already achieved more than 1 million requests per second, and the P99 is going to jump to 3 milliseconds. This is because we started to move things and are using more hardware resources, so it might impact the P99 latency. However, the P50 will stay below microseconds, and the P99 will saturate while we finish with all the scaling process.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2800.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2810.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2810)

The demo will run until we reach 3 million requests per second, but we don't need to wait until that point.  I want to recap and talk about a few best practices that we learned today.  We talked about connection pools. Establishing a new connection is an order of magnitude more expensive compared to simple GET and SET commands. Using a connection pool with persistent, long-lived connections is very important. We talked about reading from replica to scale read throughput, achieve high availability and resiliency. Specifically for serverless, it will benefit you to achieve microsecond response time.

We talked about hotspots and have two mitigations for that. Elad mentioned key duplication, where you can duplicate the keys and read and use all the hardware resources that you have on your cluster. We also talked about key splitting, where you can split keys specifically across your shards to read even more throughput on that specific key. Related to that, as a key principle, we say limit the size of your objects because if you have a very large object, it will consume processing time and CPU. The payload itself can also lead to a situation where you exceed the network baseline.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2900.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2900)

Because we are talking about caching,  one of the most important principles is how you can save your memory with more freshness, keeping data close to the original without stale data. Specifically with serverless, it can also help you reduce the cost. Time to live expiry is one of the ways to do that. You explicitly assign it to a key or a set of keys, and you can use relative time or absolute time. However, one of the most important things is that if you're going to use it, try to use random jitter when you delete them. Once they're all going to be deleted, we don't want them to be deleted at the same time because that would make your system very busy with that. We want to spread it across a time window.

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/9806281ae9e97467/2970.jpg)](https://www.youtube.com/watch?v=fXB6wftmigU&t=2970)

That was all for today. I hope you enjoyed the session. Thank you very much. Elad and I will stay around, and you can talk with us and ask questions. I hope you enjoyed that. Please fill out a survey and tell us how you enjoyed the session. Thank you. 


----

; This article is entirely auto-generated using Amazon Bedrock.
