---
title: 'AWS re:Invent 2025 - Slickdeals'' AI-powered deal discovery with Amazon Sagemaker (SMB206)'
published: true
description: 'In this video, Deepti Venuturumilli from AWS and Mike Lively, SVP of Engineering at Slickdeals, discuss how Slickdeals migrated from rigid on-premises infrastructure to AWS cloud, achieving transformative results. Slickdeals, a community-driven deal discovery platform with 12 million monthly active users, faced scalability challenges and lacked real-time data capabilities. They migrated to Databricks, moved applications to EKS, and built in-house personalization using Siamese models and XGBoost for deal ranking. By implementing real-time instrumentation through Amazon Gateway, Lambda, and Kafka, they reduced deal scoring latency from 3 hours to 30 seconds and test cycles from 6 weeks to 1 week. The modernization resulted in a 7% increase in merchant outclicks and revenue, demonstrating how cloud migration enabled faster innovation and strengthened their competitive differentiation.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Slickdeals' AI-powered deal discovery with Amazon Sagemaker (SMB206)**

> In this video, Deepti Venuturumilli from AWS and Mike Lively, SVP of Engineering at Slickdeals, discuss how Slickdeals migrated from rigid on-premises infrastructure to AWS cloud, achieving transformative results. Slickdeals, a community-driven deal discovery platform with 12 million monthly active users, faced scalability challenges and lacked real-time data capabilities. They migrated to Databricks, moved applications to EKS, and built in-house personalization using Siamese models and XGBoost for deal ranking. By implementing real-time instrumentation through Amazon Gateway, Lambda, and Kafka, they reduced deal scoring latency from 3 hours to 30 seconds and test cycles from 6 weeks to 1 week. The modernization resulted in a 7% increase in merchant outclicks and revenue, demonstrating how cloud migration enabled faster innovation and strengthened their competitive differentiation.

{% youtube https://www.youtube.com/watch?v=573R_d_eFAk %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/0.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=0)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/20.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=20)

### Introduction: AWS Solutions Architecture and the Evolution of AI Technologies

 Good afternoon everyone. I'm Deepti Venuturumilli. I'm a Senior Solutions Architect at AWS. I've been here for four years, and today I'm honored to be joined by Mike Lively, who is from Slickdeals. He's our customer, and I have been supporting this customer for a couple of years now.  We'll share our story on how they have migrated, modernized, and innovated on AWS.

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/30.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=30)

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/50.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=50)

 All right. Artificial intelligence, as many of us know right now, is more of a way to describe any system that can replicate tasks that require human intelligence. Now there are different subsets of it, and we have been evolving from the traditional artificial intelligence to machine learning.  Again, machine learning itself uses large amounts of data, and you're trying to get some kind of probabilistic outcome so that you can have certain kinds of decision logic built out and have a high degree of certainty for any of the work that you want to do.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/80.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=80)

Then the other detailed subsets are one of the Classification AI, which is trying to identify certain patterns for some objects or some specific items. Then you have Predictive AI  that is actually predicting future trends based on the statistical patterns using historical data. Now finally we have Generative AI, which is a subset of deep learning that can create new content by using very large models and content such as conversations, stories, images, and videos. You all know you have all been using them, and these Generative AI is powered by large language models which are pre-trained on vast corpora of data.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/120.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=120)

Now let's move on to Amazon Sagemaker AI. Amazon Sagemaker AI is a comprehensive  machine learning service that enables business analysts, ML Ops engineers, and data scientists to build, train, and deploy models on AWS regardless of their ML expertise. You have different tools to accomplish the same, and these are some of the use cases that you would see for machine learning and AI ML. And now I actually hand it over to Mike to share their story.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/160.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=160)

### Slickdeals' Journey: From Rigid On-Premises Infrastructure to Cloud Innovation

Thank you so much, Deepti. All right, so I'm Mike Lively. I'm the SVP of Engineering at Slickdeals, and  we'll talk a little bit about Sagemaker, but really the story I wanted to tell is about how we have moved away from a relatively rigid on-premises infrastructure over the last couple of years onto a cloud-based infrastructure where we're able to move a lot more quickly, innovate against our differentiator, and just do a lot of more interesting things than what we've been able to do in the past.

So a little bit about Slickdeals first. At our core we're really a community-driven deal discovery site. The community of our deal enthusiasts, that really is our core differentiator and the thing that sets us apart from a lot of others in the space. Our goal is, can we connect those users to the best deal online that is interesting to them today. We have 12 million monthly active users, and we've saved those users $10 billion over the last 25 years that we've been in business.

So what we've been focusing on over the last couple of years is around how can we improve personalization. Like I said, getting the right deal in front of the right people at the right time. We've also been working on a real-time deal scoring platform so we can understand a little bit better what are the deals that are highly engaging with our users so that we can either try to find more of them or in some cases even create similar deals. And then all of this is built on top of our core asset, which really is our deal sharing platform, where we take those community insights and turn that into intelligence information for our other users that are looking for those great deals.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/240.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=240)

So  I want to talk a little bit about where Slickdeals sits for those that aren't familiar with it. If you think about the graph on the left as a two by two, on the bottom of it we're talking about utilitarian shopping platforms. At the top we're talking more about entertainment. On the left-hand side we're talking about just general shopping, and on the right-hand side we're talking about a deal focus.

So if you focus on the lower left-hand side, your general e-commerce sites like Amazon, Walmart, and Best Buy, they're really just focused on helping you find the product that you're looking for and finding it quickly and making the purchase. Then what's kind of emerged over the last several years is this concept of social commerce, where you have creators on a platform like TikTok where they're putting together custom commentary and just interesting insights over top of products that you might be interested in.

Then we have the coupons and cashback sites, which is you already know what it is that you're going to buy. You're just looking to get the best deal on it, a lot more deal focused than the other two. And then we sit in that upper right-hand quadrant where we're trying to combine both the deal focus that you get from coupons and cashback, but we're also building a platform that actually helps you discover what are the best deals right now.

At our foundation, we're very much around deal discovery. With that platform, we've been able to offer our advertiser solutions that are generating over $1.45 billion in annual partner sales in addition to the savings that we're getting the end users.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/330.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=330)

So let's talk a little bit about the business challenges that we've had.  With that desire to get the best deals in front of our users, as we grew, we were seeing a lot of scalability concerns with our data. We were coming from an on-premises platform and an on-premises data platform in particular, and one of our most important signals, impressions, we didn't have the capacity either from a compute or storage perspective to leverage those. Then we also had a relatively rigid on-premises infrastructure set up like many did 20 to 25 years ago where it was just very difficult for us to innovate, test new platforms, test new features without having to spend a lot of time on infrastructure.

Then we also lacked real-time interaction data. We were very much focused on an ETL-based analytics platform where we were oftentimes looking at data about three hours after it was actually generated. All of this really presented us with an inability to effectively run our own personalization platform, and as a result we had outsourced that entire part of our business to a third party.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/390.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=390)

 With all of that, I think our biggest blocker though was for us to be able to understand whether a deal was good or not in time. The typical deal life cycle is commonly 24 hours. A great deal will hit the internet, and the better the deal is, the more likely it's not going to be there for long. However, we wouldn't learn from our downstream merchant partners whether or not a user transacted on a deal until two or three days later. So oftentimes the deal would already be done or expired before we ever understood is this a great deal or not.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/420.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=420)

### Building a Real-Time Personalization Platform with Deal Scoring Framework

 In order to help fix those problems, we decided we're going to do a few different things. First, we needed to get a much stronger data foundation in place where we would be able to pull a lot more data in, especially those impressions, so that we would have more signal. As a part of that, we also wanted to scale the entertainment and social value of Slickdeals. Could we get more users commenting and voting on the deals to create a richer set of engagement that we would then be able to use to understand what deals were resonating with our audience?

Finally, we knew we needed to bring personalization and intelligent recommendations in-house. While it was good to have a third party partner that was able to help us with that, no third party is ever going to understand our consumers and our data better than we will. So how could we position ourselves to make sure that we are able to take that over ourselves and leverage our own insights into the business as the driver for personalization?

And then finally, predictive analytics. Since we couldn't understand if a user was converting, could we take those leading engagement signals like votes, comments, views, and clicks, and could we turn that into information on whether or not a deal is likely to convert well for our users? A part of that was building the deal scoring mechanism that I was talking about for enhancing our understanding of consumer engagement, so leveraging information from the consumer in context of a deal would better be able to predict what was going to hit.

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/510.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=510)

So what we did to implement that, first we migrated our data platform.  I don't think I mentioned it, but we were on a SQL Server on-premises warehouse. We decided let's move into Databricks and leverage a little bit more of a pipeline-based system in order to get in and process our data. Then we also looked to migrate our applications. We went from an on-premises VM-based LAMP stack to an EKS orchestration to power all of our user-facing applications, as well as to build new ones like our personalization and search.

And then we also looked to enrich our data collection. In order to support that real-time or to leverage rather that real-time pipeline, we wanted to build a much better instrumentation platform that was able to process a large amount of scale in terms of number of events and do it very quickly. And then finally bringing that personalization and search in-house so that we could have something that was a little bit more purpose-built and that leveraged our data and our understanding of our business better.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/560.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=560)

 So this is kind of what the personalization aspect of our site looks like today. It's really a very common two-stage personalization where you have a retrieval stage and a ranking stage. We also have a filtering stage there to help kind of narrow things down a little bit better. On the retrieval side of things, we're using a Siamese model to help us understand, based on the text embeddings, when a user clicks on a deal, what are some of the other deals that are interesting and similar to what they've been clicking on so that they can see something a little bit more relevant than what they may have seen previously.

Once we have that information, then we're able to send it through some heuristics-based filters, and this allows us to weed out deals that for one reason or another we shouldn't display. One very common thing that we do there is

based on impressions. If you've seen a deal a lot, you probably don't need to see it again, so that's a phase for us to be able to remove that deal from the picture. And then finally we rank those resulting deals. We are using an XGBoost model that is looking at various different features on the deal, whether it's the category of the deal or price points, and it's using that to rank the deals so that we wind up with a ranked list of personalized deals for you that would be interesting to you that we can use throughout the site.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/640.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=640)

 The two main platforms that we put together and framework is our deal scoring framework. So with our deal scoring framework there's really two different scores that we're looking at. The first one is what we call our engagement score, and this is what we are using in lieu of having latent revenue data to really understand what is the total engagement with the deal. So we have several different types of engagement that we track, like I said, votes, comments, clicks, things like that, and then we develop weights based on what's the value of that engagement to us as an organization, to our merchants, as well as to our users.

Using those weights and those inputs, then we have a scoring function that winds up giving us an engagement score that we can then use to compare one deal against another. And then on the right hand side we're also developing now a content score which doesn't take engagement into the equation. Instead it's looking at the context of a deal so that when a deal is first posted to our site we can get a pretty good understanding of how well that deal is going to perform based on things like the category, the discount of that deal, the brand of the deal, information in the description and the title of the deal, as well as even imagery.

So with those two scores together, we have both an understanding of what is the actual value as well as what do we think the predictive value is going to be. We use those across the site to help power things like operational tooling. We're beginning to use these scores to help our deal editors understand when should they be promoting a deal versus not. We'll be using this to also help with our deal quality assurance as well as deal sourcing, and on the personalization side we'll be folding this in to help with the ranking, and then we're also using it already today to help understand what's the performance of deals as we go.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/740.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=740)

 So in terms of the architecture of this system, on top I've outlined kind of how we have built that instrumentation platform. So really it's just a simple pixel-based platform where we do have both client and server side tracking going through Amazon Gateway processed by Amazon Lambda, put into Kafka, and that's where we then make it available to Databricks. This information is then used not just to power search and personalization but also the deal scoring framework that I had mentioned before.

For both personalization and search and deal scoring, those are going to be powered and in the case of search and personalization are powered by EKS. And on the search and personalization side, we're using a combination of Elasticsearch and then on the search side specifically we are leveraging SageMaker to help with some of the things like query classification and quite a few other things as well. That is an area where we're planning to expand even further. There's a lot of opportunity for us. We're learning a lot of great things around Nova Foundation models and things like that that we'll be able to apply back into the models that we're trying to generate to help us understand our deal scores.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/810.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=810)

### Demonstrating Real-Time Impact and Key Outcomes from Cloud Modernization

So I wanted to walk you through real quick how we are leveraging this real-time instrumentation to impact our personalization.  So this is the front page of Slickdeals, and on the top you'll see our JFY ribbon, which is really the primary place on the web where we're showing personalization. On the right hand side, that's just a Databricks window. You'll see the events coming in as a part of a structured stream.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/830.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=830)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/840.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=840)

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/850.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=850)

 So I'm going to be coming in and just, I'm going to focus primarily on home improvement deals.  You can see now as I, as I'm just control clicking, so it's opening in the background tab, but you can see the views are starting to show up relatively quickly, within 30 seconds on the right hand side.  So now I'm going to vote on a deal. This is pretty good, $299 for a 5 tool, but this is way better, same price, but I'm getting 8 tools instead at 62% off.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/870.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=870)

So let's go ahead and give that a vote, and you'll start to see that fall in as well on the right hand side. It'll take a little bit, like I said, around 30 seconds or so, less time because this is sped up.  And then the next thing I'm going to start doing is just out clicking on the deals. This is when we send you to the merchant site, and this is really the last piece of information we gather in real time.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/880.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=880)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/890.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=890)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/900.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=900)

 Once you get off of our site, it takes us a while to understand what do you do with the merchant because we're very reliant on them sending the information over to us.  So I'm going to finish up all of these different out clicks.  And now we're back onto the front page. We're going to go up, we'll do a quick refresh and you'll see we've already changed the for you, just for you, carousel to a significantly larger number of home improvement.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/910.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=910)

So let's talk a little bit about the outcomes.  Probably the most impressive thing that we've been able to accomplish with the changes that we made, both to our data platform as well as our application stack, is our ability to innovate. This is one of the key reasons why we wanted to move to the cloud platform. Previously, for us to run personalization tests and understand where the changes that we were making were doing well, and this was actually very true for non-personalization tests that we were running as well, it would be a six-week turnaround time. With a lot of the work that we did around the personalization framework and architecture, as well as our data setup, we've reduced that time. Now we're running and getting learnings off of tests within a week.

In terms of deal scoring latency, we went from that ETL-based system where we got data and intelligence on engagement after three hours. We now can see what's going on within about 30 seconds. That gives our deal operations staff a much better way to move deals through our ecosystem more quickly. Most importantly, from a business perspective, we were able to see a 7% increase in our merchant outclicks as well as revenue from the personalization changes that we made.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/980.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=980)

So what are some takeaways that I would have for all of you from the learnings that we've had with this? The first  one is really focus on your ability and the speed at which you can scale. One of the things that was most important for us in this transformation was getting out of the world where we had to focus a lot on infrastructure and instead get us to a spot where we could reduce the focus. You still have to have some, but reduce the focus so that we can look more at innovation, and especially in the areas where we were differentiated. That was the next thing, our differentiator. Find ways that you can leverage the thing that is unique to you and your business and find ways to accentuate that and have that be a pervasive part about every project that you run.

And then data foundations. A lot of what we wanted to do, we would not be able to do unless we came up with a much better platform for our data where we were able to respond to data in real time, not just analyze the data, but actually make operational decisions off of it quickly. And then the final piece is feedback loops. Can you make sure that you're leveraging short test and learning cycles so that you can understand what is working well and you can do more of that, and what is not working well and you can go and move on to do something else.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/1050.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=1050)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/0a27b92fe802801f/1060.jpg)](https://www.youtube.com/watch?v=573R_d_eFAk&t=1060)

So if I were to kind of summarize up the learnings, it would really be that modernization for us really did unlock innovation and strengthen our ability to differentiate  ourselves. Cool. So that's all I had. Thank you all for coming. I do have a few of the technical folks from  my team. If there is anybody that has technical questions about what we have done, I don't have time to explain it here, but thank you all for being here, and I hope you enjoy the last few hours of re:Invent. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
