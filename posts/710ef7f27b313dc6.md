---
title: 'AWS re:Invent 2025 - Deep Dive into the AWS Nitro System (CMP316)'
published: true
description: 'In this video, Ali Saidi and Filippo Sironi explain AWS''s Nitro System, which moves traditional hypervisor functions to purpose-built Nitro chips for improved security and performance. They detail how Nitro cards offload VPC data plane, EBS storage, and networking functions, enabling instances with up to 600 Gbps bandwidth using SRD protocol and ENA Express. The Nitro Hypervisor implements secret hiding to prevent access to customer data, demonstrating resistance to attacks like L1TF Reloaded. Security features include the Nitro Security Chip for hardware root of trust, encryption of DRAM and PCIe links in Graviton4 servers, and confidential computing options through Nitro Enclaves and EC2 instance attestation. The system''s composability enabled AWS to launch over 800 instance types since 2017, including Mac instances via Thunderbolt connectivity.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/0.jpg'
series: ''
canonical_url: null
id: 3089007
date: '2025-12-06T13:45:51Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Deep Dive into the AWS Nitro System (CMP316)**

> In this video, Ali Saidi and Filippo Sironi explain AWS's Nitro System, which moves traditional hypervisor functions to purpose-built Nitro chips for improved security and performance. They detail how Nitro cards offload VPC data plane, EBS storage, and networking functions, enabling instances with up to 600 Gbps bandwidth using SRD protocol and ENA Express. The Nitro Hypervisor implements secret hiding to prevent access to customer data, demonstrating resistance to attacks like L1TF Reloaded. Security features include the Nitro Security Chip for hardware root of trust, encryption of DRAM and PCIe links in Graviton4 servers, and confidential computing options through Nitro Enclaves and EC2 instance attestation. The system's composability enabled AWS to launch over 800 instance types since 2017, including Mac instances via Thunderbolt connectivity.

{% youtube https://www.youtube.com/watch?v=cD1mNQ9YbeA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/0.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=0)

### Introduction to AWS Custom Silicon Strategy: Nitro, Graviton, and Trainium

 Hello and welcome to a deep dive into the Nitro System. My name is Ali Saidi. I'm a Distinguished Engineer at AWS, and with me today is Filippo Sironi, who's a Senior Principal Engineer. Today we are going to talk about the Nitro System and why we've done things in a certain way and what benefit you get from that.

[![Thumbnail 30](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/30.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=30)

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/50.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=50)

 So far at AWS, we've built chips spanning multiple areas including data center IO, core compute, and machine learning infrastructure. With the Nitro System, we have moved functionality away from a traditional hypervisor to our purpose-built Nitro chips. This started back in 2013 when we were working with a startup called Annapurna Labs, which we would later acquire a few years later.  This talk is mostly about the Nitro System, but I want to talk about a couple of other areas where we're investing in custom silicon.

The second area is with Graviton, where we build host CPUs that deliver the best price performance for a wide range of cloud workloads. We have a range of customers running on Graviton, people like Epic Games, who are the makers of Fortnite, Stripe, the payment processing company, DataDog, and even SAP running commercial databases. Some of these customers are running the majority of their compute on Graviton today.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/80.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=80)

 The third place where we've invested in custom silicon is our machine learning accelerators with Trainium, and we announced Trainium 3 instance availability yesterday. We built purpose-built deep learning accelerators from the ground up to deliver the best price performance for machine learning inference and for training. So why do we build our own hardware? It's not necessarily easy to build hardware, but the answer is kind of simple.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/110.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=110)

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/120.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=120)

 We're building our silicon where we can provide  improved price performance for our customers. And there's a couple of ways we do this. One is through specialization. By building our own chips, we get to specialize the hardware for AWS use cases. That lets us tailor our designs to the AWS operating environment, and we can tailor the performance and the design of them to our specific needs and not burden them with many features. You might ask why are people burdening chips with any features, and the reason is it's expensive to build silicon. So when companies build it, they want to apply it to a bunch of different markets. By focusing just on our use cases, we get to optimize. I'll show you today how we tailored the Nitro System to AWS infrastructure to improve both the security and the performance of our servers.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/170.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=170)

 The second is speed. We get better speed of execution when we own the end-to-end development process from defining the product all the way through deploying it in our data centers. We get to bring technology to customers faster and shrink the time between concept and delivery by hardware and software co-design.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/190.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=190)

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/220.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=220)

 The third is innovation. When we're building our chips, we're building our servers, we're putting them in our data centers, we get to innovate across traditional silos. That can be really powerful and let us optimize in a way that you can't with those silos. And lastly is security. Nitro provides us a mechanism to enhance the security of our servers through a harder root of trust,  verification of firmware on the server, and limiting interactions with each of our servers through a narrow set of authenticated and auditable APIs.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/230.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=230)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/240.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=240)

### Rethinking Virtualization: The Origins and Architecture of the Nitro System

 So Nitro is really a fundamental rethink about how virtualization in the cloud should be done.  At its lowest level, the Nitro System has our Nitro cards. These offer networking, storage, and security functionality. In the host processor, you run our Nitro hypervisor, and on top of that, your virtual machines and your applications. I'll talk to you today about how this separation has provided better security and also let us bring features to you rapidly. But first, let me rewind a little bit and talk about how we got here.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/270.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=270)

 It all started with a simple question. After building EC2 for almost a decade, if we applied our learnings, how would we change our server platforms? We got lots of suggestions. There were performance features like improving throughput, simplifying the hypervisor, reducing latency and jitter, and being able to have bare metal instances that looked and felt like our virtual machines. And then there were a number of security features like having transparent encryption, building a harder root of trust, removing operator access from our systems, and having a narrow set of auditable and authenticated APIs.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/300.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=300)

 Nitro is a combination of hardware and software. The chips we're building in AWS, and we've built six generations of these chips now.

We introduced Nitro back in 2017 with a C5N instance, but we really started back in 2013 when we started with enhanced networking, moving functionality away from the hypervisor onto special purpose cards. Over time we've expanded the IO that we offer through our Nitro Cards to add EBS and our local storage, and then we built a hypervisor where we got to remove a bunch of functionality. Since 2017, all the EC2 instances that we've launched have been based on the Nitro System.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/350.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=350)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/360.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=360)

So what is it tangibly? On the left here, I've got one of our Nitro Chips, and that's integrated into  one of our Nitro Cards that you see on the right. And this is what one of our servers looked like before the Nitro System.  We had customer instances, and they were running on top of the hypervisor Xen. Now Xen's great, but it did a lot. It did CPU scheduling, device simulation, network limiters, security group enforcement, packet encapsulation, and quite a bit more. It even has a full-blown Linux user space in this privileged Dom0. And all that functionality used resources on the host CPU.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/390.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=390)

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/400.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=400)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/410.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=410)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/430.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=430)

### Nitro Networking Cards: From VPC Offload to 600 Gigabits with SRD and ENA Express

So we started offloading those capabilities onto our Nitro Cards.  And the first place we did that is networking. So let's dive into  networking. We have a family of Nitro Cards across now six generations of chips, but from the networking point of view, they all do a very similar thing.  They provide the VPC data plane offload for our instances. This is things like ENI attachments, enforcing security groups, creating flow logs, routing, encapsulating packets, providing you with DHCP and DNS, and all this used to run on our hypervisor  and is now offloaded into dedicated hardware on our Nitro Cards.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/450.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=450)

Most of our Nitro Cards since the third generation also enable transparent AES encryption of networking packets in transit to other instances without any performance overhead.  And the VPC card presents the ENA device that you see in your instances today. The Elastic Network Adapter is really cool in that it's been very extensible. When we first launched ENA we had 10 gigabits of networking. Since then we went from 25 to 100 to 200 to 300, and today we have instances with over 600 gigabits of networking bandwidth.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/490.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=490)

And it's really powerful that with that same ENA device model, you can have that range of performance. You can stop an instance on one machine, move it to another, and have an order of magnitude more networking bandwidth if that's what your application needs.  This card also provides our Elastic Fabric Adapter. EFA is a network with features geared to HPC and machine learning workloads. These workloads are special. Most workloads either need low latency or they need high bandwidth, but ML workloads and HPC workloads tend to need both at the same time. They want high bandwidth and low latency.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/540.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=540)

And to build our Elastic Fabric Adapter, we had to build our own protocol to transport data across our network. We call this a Scalable Reliable Datagram or SRD. SRD lets us use multiple paths through our network simultaneously to find you low latency and high bandwidth paths. Let's talk a little bit more about that.  Traditional network protocols take a single path through our network, and if there's congestion, they're relatively slow to react to failures.

Our networks look like this. They're a Clos network, and between any two servers, you can see there's many different paths that your packets can take. And with the knowledge of our networks, we can detect congestion and we can route around it at data center scale times, not at internet scale times. And this is exactly what we do with our SRD and our Nitro Cards. SRD provides a foundation for using many paths through our network simultaneously, allowing us to distribute bandwidth across those paths and react quickly to any congestion, any link issues, and significantly reduce tail latencies.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/600.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=600)

These tail latencies are particularly important in distributed applications where the P50 is not what you're looking for, but you're looking for P99 tail latencies. And let's  look at how SRD is used.

The first way we used SRD was with the Elastic Fabric Adapter. Here I'm showing scaling of an HPC workload where, as you increase cores, you'd expect to also scale linearly. You'd like to be on the red line. Before EFA, we were on that purple line. We scaled pretty well to about 400 cores, and then everything tailed off and actually we started scaling negatively. When we added EFA, you can see that scaling trend continues. We've now had four generations of EFA, and we launched the most recent one with our P5B200 and GB200 instances.

Now, another place that expects in-order packets are TCP flows. TCP uses packet ordering to detect congestion. And so through our network, flows get hashed to a single path, and that single connection, all the packets flow through it. Now, sometimes large flows, even in an overprovisioned network, can end up hashing to the same path, and there they can result in network congestion. That's what I'm showing here in the yellow line. This can result in packet delays and packet drops, which causes TCP to back off and results in some retransmits, reducing total throughput, and everything continues happily. TCP congestion control is generally good, but it was built for internet scale time.

Other than congestion, occasionally you can have a failure in the network, like a link failure. And this is what you see with this red circle and with this X here. In this case, TCP doesn't respond well at all. You have to wait for a timeout, re-establish connection, and try again. So, why not just send this across multiple paths? Well, because TCP has that expectation of in-order delivery. So with our Nitro cards, we also built something we call ENA Express.

While TCP doesn't handle out-of-order segments, we've offloaded the functionality of this to our Nitro cards. It allows you to send traffic across multiple paths at once, either TCP or UDP traffic over the SRD protocol, and the Nitro card takes care of assembling the packets into the right order before delivering it to your application. And the benefits can be pretty amazing. The single flow bandwidth goes from 5 gigabits up to 25 gigabits, and we've seen tail latencies reduced by 85%. It's really easy to enable ENA Express. It's a simple configuration. It's available in the same availability zone, and it's transparent to TCP and UDP.

So when EC2 started, 1 gigabit networks were pretty common, and it was this way for almost a decade. Today we've seen a shift to 25 gigabit to 100 gigabit to 200 gigabit networks. The increasing base of bandwidth has been incredible. And if you look at our instances this year, we announced CMR AI instances that have up to 100 gigabits of bandwidth in our core sets of platforms. And in our network optimized sets of platforms, we have the C and R HGN instances that have two 300 gigabit network interfaces in a single instance, so 600 gigabits of network bandwidth are available to an instance if you need it.

### Nitro Storage Solutions: EBS Acceleration and Custom FTL for Local SSDs

Obviously, I was talking about core instances there. If you look at our machine learning instances, we've gone from 400 gigabits with the P5 instance back in 2020, up to 6.4 terabits in a single instance today with our P6 instances. Okay, so we talked about networking. Let's take a minute and talk about storage.

NVME is a great standard protocol with lots of standardized drivers across different operating systems. And our Nitro card exposes an NVME interface on one side and translates those commands to the EBS data plane on the other side. This lets us use those Nitro cards to encrypt your data in transit with a hardware engine on the Nitro cards, and it means you don't need to make a trade-off between performance or encryption. You get encryption just transparently if you enable encrypted volumes. And you might imagine here too, EBS uses SRD, that protocol I just talked about, to send packets during multiple paths to where your EBS volumes are located.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/910.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=910)

This reduces tail latencies.  And just like with networking at the EC2 instance level, you can see the amazing increase in performance that we've offered over the years. If you go back a decade, we had 2 gigabits of EBS bandwidth. Today we offer up to 150 gigabits of EBS bandwidth and up to 720,000 IOPS in a single instance. This lets you run things like demanding databases while also having the durability and enterprise features of EBS.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/960.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=960)

Now, EBS is great. It's the answer for most customers, I think. You get durability, you get snapshots, but there are some customers who also want local storage. So let's talk a little bit about our Nitro SSDs. And to do that, I need to tell you a little bit about  how an SSD works. There are a couple of components in an SSD. The first is the NAND. This is where the bits are stored, but NAND has a lot of peculiarities. You can only write it after you erase it, and the chunks that you tend to erase are about 1 megabyte. So even if you want to update just a single byte, you actually need to copy the data to a new location with an update. And so this ends up needing something that looks a lot like a write-logging database.

NAND also has a lifetime. If you write one block a bunch, you'll wear it out. So you need to spread the writes around all that flash. And to manage this complexity, there's something called a flash translation layer. It's usually a chip on an SSD. It maps the logical addresses that an operating system knows about to the physical blocks where the NAND is. It performs garbage collection. It performs wear leveling. And ultimately you end up doing something like a write-logging database.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1030.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1030)

Now there's lots of flash manufacturers that  produce their own SSDs with their own FTLs. Just like with databases, actually, each of those FTL implementations behaves a bit differently, even though they all offer the same external API. They all do a good job in the average case, but our experience over many years is they have some unpredictable behaviors. Garbage collection decides to kick in just at the worst possible time. And these unexpected behaviors make it hard for us to engineer consistent performance.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1060.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1070.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1080.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1080)

So how can we get the performance we need?  Well, you could probably guess by this point, we integrated the FTL into our Nitro cards, and so with this, we have up to 60% lower latencies.  We have improved reliability, and through our Nitro cards we can encrypt all your data with an ephemeral key that never leaves the server.  So this is what the server now looks like after we moved a bunch of functionality away from the host CPU onto our Nitro cards. You can see that Dom0, that privileged domain in Xen, is significantly offloaded, and most of the functions are now handled by the Nitro System.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1120.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1120)

### The Nitro Hypervisor: Secret-Free Design and Protection Against Transient Execution Attacks

Now I'm going to turn it over to Filippo. He's going to talk to you about the Nitro Hypervisor and some of the other features we offer in the Nitro System. Thank you, Ali. So let's now look at the Nitro Hypervisor. After having offloaded I/O to dedicated hardware and purpose-built hardware,  what we are left with is the bare minimum that a hypervisor needs to do. This means focusing on CPU, memory, and device assignment. With device assignment, I mean providing a path for the virtual CPUs to have direct access to the dedicated hardware that we just added to our systems.

On top of that, we want to make sure that we strip down the hypervisor, removing every other feature that's not necessary. For example, when the hypervisor is not involved with networking or storage, we can get rid of the network stack. We can get rid of the storage stack, so no more file system support. We can also get rid of additional services like SSH server to make sure that access to the hypervisor, which is the closest component to customer data, is completely restricted.

This results in a very small, lightweight, and secure hypervisor, a hypervisor that is quiescent, and it's only going to execute code whenever the customer instance asks for hypervisor services. Think about instruction emulation, for example. And when you put this all together, you have a hypervisor that provides very minimal performance overhead such that our virtual machines can have close to bare metal performance.

If you take a look at our most recent series of virtual machines and bare metal machines, it's very difficult to find differences in terms of performance between our metal and our virtualized instances.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1220.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1220)

 Beyond stripping down the hypervisor to the bare minimum, we also wanted to go one step further. The hypervisor runs at a layer of the CPU where it normally has access to everything. It's the most, is the highest priority component that normally has access to all the CPU and all the memory in our system. However, this violates one of the main security principles, which is the principle of least access. So we wanted to make sure with the Nitro Hypervisor that the hypervisor was not going to have access to any customer data unless it really needed to.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1290.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1290)

When you look at the commodity hypervisor, it owns the entire address space of the server. When you start a virtual machine, both the CPU context, which means all the CPU registers, as well as the memory, is part of the hypervisor space, which means that the hypervisor has full access.  This holds for all the virtual machines that we start.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1300.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1300)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1310.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1310)

When we look at the Nitro Hypervisor, the address space of the Nitro Hypervisor is minimal.  We are talking about hundreds of megabytes. They are the bare minimum to run our systems. Whenever we start a virtual machine,  both the CPU context and the memory for that virtual machine are outside of the space of the hypervisor. This gives us great properties because whenever there is a bug in the hypervisor or, for example, a security problem, we have better properties that allow us to reason about whether customer data would be at risk or not.

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1340.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1340)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1350.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1350)

This is true for all the virtual machines that we start on our server. Each one gets  its own address space. And whenever the hypervisor needs to perform services on behalf of a virtual machine, for example, instruction emulation,  what happens is the hypervisor maps the bare minimum amount of memory that's necessary to perform its operation, and as soon as that's done, the hypervisor goes back to having its previous address space that does not map any of the customer data.

We call this type of memory management secret hiding. Since we started our journey implementing secret hiding by removing memory and context from the hypervisor address space, this has also been called the design of a secret-free hypervisor, or in some cases people are referring to the inability of the hypervisor to access customer data as confidential computing.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1400.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1400)

 A few months ago, a research group from a university in Europe published a very interesting piece of research talking about a new security vulnerability called L1TF Reloaded. L1TF Reloaded is a transient execution attack that relies on L1TF and what researchers call a Spectre gadget. So it's one attack that derives from Spectre and Meltdown that came in 2018. Through the clever combination of L1TF and Spectre gadgets, the research team was able to show that when running on a commodity Linux and QEMU hypervisor, as well as other cloud providers, it was possible to exfiltrate the private keys of a web server from one instance to another.

On the other hand, when the same attack was mounted against two instances running on the Nitro Hypervisor, the researchers were not able to extract any of the private keys, and this is thanks to the design of a secret-free hypervisor. We recently published a blog post with a deep dive into memory management of the Nitro Hypervisor. I invite you to scan the QR code on the screen and take another look at the blog post.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1480.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1490.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1490)

### Nitro Security Chip and System Composability: Enabling 1,000+ Instance Types

 Let's continue our journey on the security aspect of the Nitro System.  The next component in the Nitro System that has to do with security is the Nitro Security Chip. The Nitro Security Chip is a piece of silicon that is baked on our motherboard and helps the primary Nitro Card or the Nitro Controller establishing a hardware root of trust.

If you look at motherboards, they have a series of non-volatile assets, specifically flashes. These flashes contain BIOS UEFI images or the binary image of the BMC, the BMC being the component that manages the fan speed and collects several information about the system like the temperature, voltages, and so on to guarantee that the system is operating correctly. Through the Nitro Security Chip, we are able to consistently and continuously monitor the content of the flashes on our systems.

We do this for two purposes. First, we want to be able to update them out of band to make sure that we always have the latest software running on our fleet. On top of that, we are able to validate that the software and the firmware that we are running on our systems is exactly what we intend to run. Beyond that, the Nitro Security Chip keeps monitoring the auxiliary buses on our systems, things like the I2C bus or the SPI bus to access flashes.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1580.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1580)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1610.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1610)

 We have removed the hypervisor overhead. We have moved the management of security. We basically stripped down Dom0 to the bare minimum, potentially removing it, and we designed a new hypervisor, a hypervisor that's quiescent and lightweight. And that gives more resources to customers, which translates into higher performance. On top of that, we have a very flexible system in the Nitro  system because the Nitro system is extremely composable. We can increase the number of Nitro Cards in our servers to provide more local storage, to provide higher performance for remote storage, or higher network performance.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1650.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1660.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1660)

If you look at our pace of launches in terms of instance types between 2006 and 2017, we launched 70 instance types. However, after the full Nitro system has been developed and put in production in 2017, we have since launched more than 800 instance types to the point where we are at 1,000 plus instance types today, and we did stop counting.  Let's continue our journey into the Nitro system  and keep focusing on the security aspects.

### Confidential Computing in AWS: Shared Responsibility and Protection from Cloud Providers

Security is the number one priority for AWS, but security is also a shared responsibility. There is security of the cloud and security in the cloud that we need to take care of. AWS is responsible for security of the cloud, and this starts from securing our facilities and making sure that our hardware is in top shape, and the firmware and the software that runs on our hardware is exactly what we intend to run. On the other hand, the customer is responsible for the security in the cloud. This means that the customer is responsible for updating the operating system that runs in their instances, making sure that their applications are using all the very latest libraries, for example, cryptographic libraries that are not subject to timing attacks. On top of that, customers are responsible for using all the security features that we make available like security groups and configuring them correctly to prevent instances that are not supposed to talk with one another from communicating.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1740.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1740)

 Let's continue our journey on the security front and talk about confidential computing. Confidential computing is something that popped up a couple of years ago, and it tries to specify how to protect customer code and customer data. There's been a bit of confusion, and I want to address a few topics on this front. At AWS, we really believe that confidential computing is all about making sure that customer data and code is protected, but we need to understand from whom this needs to be protected.

We have two dimensions here that we need to consider. The first dimension is protecting customer code and data from the cloud provider itself and from one customer to the next. When it comes to the cloud provider, the Nitro system provides several improvements.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1820.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1820)

We have achieved several improvements because we removed operator access to the Nitro system and to the Nitro Hypervisor, ensuring that customer data is safe and customer code is protected as well. Now let's recap how the Nitro system achieves that.  We have encryption for all the communication channels where customer data may be in transit. We use Secure Boot and Measured Boot extensively to ensure that our hardware is running the latest firmware and software, and most importantly, the firmware and software that we intend to run. We perform cryptographic validation that this is the case from the very early stages of the boot process.

On top of that, we have periodic deployments of newer versions of firmware and software to patch our systems with the latest security fixes and bug fixes. This includes all the firmware and software that we run, including the Nitro Hypervisor, which we can live update with no interruption of customer workloads. As I mentioned before, we eliminated operator access to our servers. There is no shell, no SSH to the Nitro system and to the Nitro Hypervisor.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1920.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1920)

Beyond that, every piece of software and firmware that we build is built by multiple teams that are globally distributed across all the continents, and it is code reviewed by multiple people. It is extensively tested, and only when all our testing is passing is this code production signed to make sure that the EC2 deployment service can deploy it following our policy across all the regions that AWS provides. 

Now let's take a look at the second dimension of confidential computing. We want to provide options to customers to make sure that the customer code and data is also protected within the customer organization. There are several levels of trustworthiness of software that we usually deploy, that anybody deploys in systems. There are highly trusted pieces of code that, for example, operate on signing keys, and there are less trusted pieces of code that provide public endpoints. We want to make sure that these are decoupled so that a bug in a piece of code that deals with a public endpoint does not result in leaking a signing key, for example.

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/1970.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=1970)

### Advanced Security Features: Nitro Enclaves, UEFI Secure Boot, and EC2 Instance Attestation

 One of the solutions that we offer on this front is Nitro Enclaves. With Nitro Enclaves, the parent instance is able to donate resources in terms of CPU and memory, and these resources are used by the hypervisor to start a sidecar instance. These sidecar instances, or enclaves, have all the security properties of an instance. They benefit from all the secret hiding that I mentioned that the Nitro Hypervisor implements, for example.

However, it's also a much more enclosed environment because a Nitro Enclave does not have persistent storage and does not have a network interface, for example. It only has a thin pipe between itself and the parent instance so that the parent instance can provide commands or perform RPC calls, for example. Nitro Enclaves are integrated with AWS, and when all the software in the Nitro Enclave is attested, an attestation document can be used to unlock content that only the enclave needs to access. One of the most common use cases for Nitro Enclaves is to store signing keys, for example.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2060.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2060)

Beyond Nitro Enclaves,  we are also providing features like Secure Boot. AWS provides an immutable way to boot software on an EC2 instance. However, there is a lot of use of UEFI Secure Boot on premise, and customers have been asking us to provide the very same features for EC2 instances. Through UEFI Secure Boot, it is possible for the guest firmware to cryptographically validate that the bootloader and the kernel and the rest of the operating system that's going to be loaded is signed with a key that the customer trusts.

This allows the customer to protect against the type of malware that may persist across reboots and ensures that an EC2 instance will only run the software that the customer intends to run. NitroTPM is the next step on this journey.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2130.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2130)

 NitroTPM is a TPM 2.0 implementation that conforms to the standard and allows cryptographic measurement of software, extending this measurement into the TPM so that these measurements can be used to, for example, unlock the local storage on an EC2 instance using solutions like LUKS or BitLocker. Now, Secure Boot and NitroTPM have been available for a long while. However, the software stack to make use of them has historically been complicated.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2200.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2200)

Having end-to-end attestation of a workload is not a simple step, and that is why customers love Nitro Enclaves. Nitro Enclaves provide an easy solution to validate that the entire software that's been running the workload is exactly what it's supposed to be. Customers asked us to provide and port this ease of use to instances as well for use cases where  Nitro Enclaves are not enough. That's why we released EC2 instance attestation.

EC2 instance attestation is built on top of UEFI Secure Boot and NitroTPM and makes use of what we are calling Attestable AMIs. Attestable AMIs are AMIs that can be built over and over, and rebuilding those AMIs following a recipe will always result in the same bits that we can measure. Again, we can use those measurements to provide an attestation document, an attestation document that, like with Nitro Enclaves, can be used to unlock secrets.

What can EC2 instance attestation be used for? One of the use cases that we have in mind is confidential inferencing. Think about the case of a cloud provider wanting to make a closed-source machine learning model available to customers. The model provider may not want to disclose the model weights because those are the IP of the model provider, and the cloud provider wants to make sure that the customer data, meaning the prompts and the inferred content, will not be available to the model provider.

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2300.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2300)

Now, EC2 instance attestation allows us to build systems that achieve these guarantees. EC2 instance attestation is available on a  large range of Nitro instances, including instances with Inferentia accelerators as well as NVIDIA GPUs. You can start building UEFI and Nitro-enabled Attestable AMIs using Amazon Linux 2023 or NixOS, and this is working for both Graviton, AMD, and Intel instances.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2330.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2330)

A few years back, we published  a white paper discussing the security design of the AWS Nitro System, where we go in more depth into the security aspects of the design of the Nitro Cards, the Nitro Hypervisor, and the Nitro Security Chip. In addition, the NCC Group, which is an independent company, performed a review of the Nitro System, including all the APIs that the control plane or any operator can use to work with the Nitro System. They concluded that there is no mechanism for a cloud operator to gain access to the underlying host or to the customer data that is stored in the instance memory, instance storage, or volumes. I invite you to scan the QR code and have a look at the white paper in depth.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2390.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2390)

### Graviton 4 Architecture and Instance Lifecycle: Cryptographic Chain of Trust from Manufacturing to Runtime

Now, let's continue our journey  into the Nitro System and let's look at a Graviton4 server. This is a server that can run many virtual machines or one or two bare metal instances.

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2420.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2420)

When we run two bare metal instances, the life cycle of the two CPUs is decoupled even though they share a single Nitro system. When looking at the server, all the high-speed connections and the DRAM are encrypted. DRAM encryption is something that we started doing with our sixth generation of instance families, leading with Graviton 2, and since then all these instance platforms have used DRAM encryption.  With our eighth generation of Nitro instances, again leading with Graviton 4 this time, we also started encrypting the links that connect the CPU to the Nitro Cards using PCIe encryption.

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2480.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2480)

But with Graviton 4, we went one step further. Graviton 4 is the first Graviton solution where we are supporting multiple sockets, so up to two CPUs in a server communicating with one another, and the coherency link between the two CPUs is also encrypted.  Now, for the cloud to be secure, we have to have certainty of everything that's running, and we have to make sure that at any time we have cryptographic certainty of all the software and firmware that we are running on our servers. That's what we call attestation. However, doing this at the scale of AWS is a challenging problem.

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2510.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2520.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2520)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2530.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2530)

 Let's take a look at how an EC2 server boots, or any server boots for that matter. When the silicon is lit up,  it loads code from the ROM. The ROM will instruct the CPU on how to load firmware,  and then the firmware will hand over control to the bootloader, which usually resides on the boot volume. The bootloader is further concerned with loading the operating system and from there loading the customer application. Now, at every time when we hand over control from one piece of firmware or software to the next, there is a chance that something may go wrong, so we have to make sure that every single step of the boot process is validated.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2570.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2570)

But where do we start? We need to start from the very beginning, from the manufacturing plants, and we have to follow  our hardware from manufacturing through the assembly lines, transits from the assembly lines to our data centers, installation in our data centers, and power on in our data centers. This is about creating an unbroken chain of custody and verification of every piece of hardware, firmware, and software that runs on our servers.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2610.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2620.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2620)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2640.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2640)

Let's dive specifically into how this works for our most recent Graviton 4 servers, and it all starts with a Nitro Chip.  Every Nitro Chip comes  with a private and public key, and this couple of private and public keys allows us to establish a chain of trust at every step of the boot process. We use this to create a new pair of private and public keys, destroying the previous pair  and making sure that the next piece of software that we load is exactly what we intend it to be.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2660.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2670.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2670)

With Graviton though, we push this one step further. We do not just look at the Nitro system itself. What happens is that  the idea of having a private and a public key extends to the CPUs in our systems, and  this allows us to make sure that only two CPUs that are Graviton 4 CPUs can talk with one another, and we can establish a cryptographic channel between the two of them whenever we are running coherency between the two CPUs. Also, we can establish a secure link between the Nitro system, meaning all the Nitro Cards that we have in our systems, and the CPUs, again relying on this cryptographic foundation.

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2700.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2700)

 Now let's put everything together and

look at the life cycle of an instance. Whenever a customer calls for an instance, the run instance call will go through the EC2 control plane. All the necessary resources will be made available by the control plane, but the last mile of the control plane will use the Nitro API, which is an authenticated, authorized, encrypted, and most importantly, logged API. That's going to instruct the Nitro system on what to do.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2740.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2740)

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2750.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2750)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2760.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2760)

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2770.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2780.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2780)

The first step  is for the control plane to tell the Nitro controller to allocate an empty shell which only contains CPU and memory.  The Nitro controller will do so by making RPC to the Nitro hypervisor so that the Nitro hypervisor can perform the allocation. The next step is for the EC2 control plane  to instruct the Nitro controller to set up the Nitro cards so that we can expose EBS through devices and volumes.  The Nitro controller will later on instruct the hypervisor to create a channel between the virtual CPUs  and the dedicated hardware to make sure that the hypervisor will not be in the picture whenever there is IO happening.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2800.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2800)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2820.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2820)

The last step is for the EC2 control plane to tell the Nitro controller to start the virtual machine. At this point, the Nitro controller will instruct the hypervisor.  The hypervisor will start executing the guest firmware, and from the guest firmware, the boot loader and guest kernel and guest operating system will be loaded, potentially using secure boot and Nitro attestation and so on.  On a Graviton-based server, whenever we are running in non-coherent mode, this is happening. This could happen at any time on any of the two hosts, and most critically, this also works whenever the hypervisor is not in the picture.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2840.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2840)

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/710ef7f27b313dc6/2870.jpg)](https://www.youtube.com/watch?v=cD1mNQ9YbeA&t=2870)

This relies on the Nitro  controller and the Nitro security chip holding the CPU in a reset state so that the control plane and the Nitro controller and the Nitro cards can set up the system so that devices exposing the root volume become available to the CPU. This allows the CPU to start booting the bare metal instance as soon as it is released from reset. The very same solution is what enabled  AWS to also bring Mac into the cloud.

If you are familiar with our offering, we have multiple iterations of Mac instances, and in this case we connect the Nitro system to Apple Mac Mini using a Thunderbolt connector. This allows us to provide the elasticity, reliability, and security of EC2 to Mac users, including EBS volumes, VPC, and every other feature that AWS provides. This is a great option for customers who are building applications for macOS and iOS and need to perform continuous builds, testing, and signing of their applications.

With this, we are concluding our talk. We are happy to take questions outside of the room. Thank you for your attendance.


----

; This article is entirely auto-generated using Amazon Bedrock.
