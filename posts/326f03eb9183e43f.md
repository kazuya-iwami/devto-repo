---
title: 'AWS re:Invent 2025 - Customize Amazon Nova models for enhanced tool calling (AIM380)'
published: true
description: 'In this video, AWS specialists Harsh Asnani and Anupam demonstrate customizing Amazon Nova models for enhanced tool calling using a legal assistant agent use case. They cover the full workflow from synthetic data generation using Meta''s Synthetic Data Toolkit with Nova Premier as the teacher model, through Supervised Fine-Tuning with LoRA adapters achieving 75% accuracy, to Reinforcement Fine-Tuning reaching 90% overall score. The session includes detailed code walkthroughs showing SageMaker training jobs, Lambda-based reward functions for RFT, and deployment to Bedrock using on-demand inference. They also introduce Nova 2.0 launches including Nova Lite 2, Nova Forge with intermediate checkpoints, and various fine-tuning techniques like Parameter-Efficient Fine-Tuning, DPO, PPO, and Reinforcement Fine-Tuning available for reasoning models.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/0.jpg'
series: ''
canonical_url: null
id: 3093233
date: '2025-12-08T21:35:22Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Customize Amazon Nova models for enhanced tool calling (AIM380)**

> In this video, AWS specialists Harsh Asnani and Anupam demonstrate customizing Amazon Nova models for enhanced tool calling using a legal assistant agent use case. They cover the full workflow from synthetic data generation using Meta's Synthetic Data Toolkit with Nova Premier as the teacher model, through Supervised Fine-Tuning with LoRA adapters achieving 75% accuracy, to Reinforcement Fine-Tuning reaching 90% overall score. The session includes detailed code walkthroughs showing SageMaker training jobs, Lambda-based reward functions for RFT, and deployment to Bedrock using on-demand inference. They also introduce Nova 2.0 launches including Nova Lite 2, Nova Forge with intermediate checkpoints, and various fine-tuning techniques like Parameter-Efficient Fine-Tuning, DPO, PPO, and Reinforcement Fine-Tuning available for reasoning models.

{% youtube https://www.youtube.com/watch?v=nhlA06hMisc %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/0.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=0)

### Introduction to Model Customization and the Spectrum of Fine-Tuning Techniques

 Hello, everyone. Thank you for joining us in this session, AIM380, Customize Amazon Nova Models for Enhanced Tool Calling. We appreciate you spending the time with us. There are some information sheets in front of you that tell you what kind of fine-tuning techniques and models we are going to cover in this session.

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/20.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=20)

 My name is Harsh Asnani. I'm a Worldwide Specialist Solutions Architect for Generative AI at AWS. With me joins Anupam. Hi, this is Anupam. I am a Senior Solutions Architect in the Nova team, so happy to be here to introduce the customization features that we have launched. Perfect. So a quick overview of the agenda. We'll go into a little bit of concepts of what model customization is, what key considerations are for model customization, especially for tool calling, what techniques are available, and what we used. We'll quickly also go over the recent re:Invent launches about Nova, and from there we'll dive deep into the code walkthrough, taking you through some code that we've developed in this session to show you how you can enhance tool calling capabilities using Amazon Nova models.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/80.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=80)

Okay, why model customization, right? So before we dive into Nova's customization capabilities, let's get into the spectrum of customization techniques available,  not just in Nova, but generally on foundation models. So it all starts with choosing if you have to modify the weights or not of the model, and you can see from the left to the right, the level of customization and effort increases when you try to change the weights of the model. On the left is a light touch technique. You could try something like prompt engineering where you tell the model the way you want the model to respond in a particular format, but essentially the model still retains the concepts that it has been trained on. So if you have new concepts that are specific to your organization's data or the styles that you are used to, you can probably provide that in a prompt engineering text and a couple of examples and basically just take an off-the-shelf model and see how it performs for you.

You can also use RAG, which is short for Retrieval Augmented Generation, and what that lets you do is also without modifying the weights of your model, it lets you provide organization-specific data at the time of inference. So again, if the model hasn't learned some things after it was trained, but you have it in your organization's corpus, it will pick it up and respond accordingly. There are times where you will see with these approaches that you will see saturation. You'll probably see styles of responses not matching up exactly to what you need, which is where you go to the right side. These techniques will modify the weights of the model. They're divided into post-training and pre-training. All the way to the right is pre-training, and with pre-training, what you do is, basically the use cases, if there are concepts again in your organization that haven't been learned in a model, you can use pre-training techniques. But if you only want to modify the model for the style of responses in a particular format, then you could use something like post-training, which also modifies the weights of the model but is not as intense and doesn't require as many resources or expertise to train the model.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/230.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=230)

### Amazon Nova Model Launches and Customization Capabilities: From Nova Lite 2 to Nova Forge

 So now that we've understood the progression of the customization techniques, let's get into how you can do this with AWS, especially for Nova models. We have two offerings: Amazon SageMaker fully managed training jobs and Amazon SageMaker HyperPod. What SageMaker training jobs let you do is define your training workload through declarative syntax and Python SDKs, where you provide your custom training scripts, your data, and it just takes care of everything about spinning Docker containers, bringing them down, observability, scalability, things like that. But if you're coming more from a background of Amazon EKS and you like more control on getting into your EC2 instances, SSH into it, and you have a requirement for maximization of control, you can use SageMaker HyperPod as well.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/280.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=280)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/300.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=300)

 With that, I'll hand it over to Anupam to go over some launches. Yeah, thanks, Harsh. Yeah, so we'll basically go over some of the launches that happened this year in the keynotes.  Primarily, first is the addition of different models that we added. So you can see the first generation models that were released last year, Micro, Lite, and Pro.

Hopefully you guys played with it and are using it. The second generation is something that we launched the day before yesterday, which was Nova Lite 2. That is again a new generation model built from scratch with newer training data, improved multilingual capabilities, more agentic use cases, and more coding abilities, so it shines in those cases. There are public reports available so you can read the technical report which has very clear benchmarks and comparisons with other models.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/370.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=370)

And then we also released two of the early access models, or private preview as we call it, which is the Nova 2 Pro, which is like a big brother to Lite. It's a bigger model but better in terms of knowledge and intelligence. And then Omni, which is an image and text model, that means you can pass in any modality and it answers back either in text or image, for example, kind of a mixture of a Nova Pro with Canvas. Apart from the base model launches, we also  released something called Nova Forge, which is basically an open training concept where you get not just the base model or the recipes for customization, but you also get intermediate checkpoints of Nova which you can use to get started with your customization.

Until now, you could only use the final checkpoint, which was the production model that everybody would have access to, but this allows you to get a mid-trained, previously trained, only Continued Pre-Training trained checkpoint so that you can start from various other checkpoints as well. Apart from checkpoints, we also added Nova data, which means that you can mix Nova data with your data when you are training. So there have been scientific evidences that show that when you do a mix-in between your data and the data of the foundation model that it has seen when it was training, it increases the learning capability of the model.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/430.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=430)

So yeah, those are some of the key value propositions and benefits of Nova Forge. You get access to multiple different checkpoints  rather than just the last checkpoint. You can blend your proprietary data with Amazon Nova curated data. You can also use reinforcement learning by bringing your own environment, which means we work with customers who have built their own environment where they do reward function calculation and use that to improve the model, and they didn't want to move the whole setup to AWS because they have invested and created that whole environment. So this lets you just hook up that external environment which is calculating the reward function while training the model in the VPC.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/490.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=490)

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/500.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=500)

And then yes, it comes with the push button recipes. So you get pre-baked recipes with the most optimal values for multiple hyperparameters that you can use. And then lastly, you also get a Responsible AI toolkit, so that if you have a security use case or a content moderation use case, you can utilize and modify or customize the Responsible AI layer of Nova as well.  So yeah, that's pretty much the launches. This  table gives you a higher level overview of the 1.0 offerings and then 2.0 offerings in terms of different techniques.

Like Harsh mentioned, they are post-training. So if you see Parameter-Efficient Fine-Tuning, which is a type of fine-tuning where you are only changing the adapter of the model, as well as Full Fine-Tuning where you're changing actual weights of the model, both are available in 1.0 and 2.0. DPO is another kind of post-training where you are getting that thumbs up, thumbs down feedback and you want to teach the model what good looks like versus what bad looks like so that it can steer towards your preferences or human preferences. That is also available in 1.0, and similarly PPO, which is another class very similar to DPO but another class of reinforcement learning from human feedback technique.

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/570.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=570)

And then we have Reinforcement Fine-Tuning, which is the state of the art reinforcement learning from human feedback as of today, which is available in 2.0. And the reason why it is only in 2.0 is because reinforcement learning from human feedback works, or Reinforcement Fine-Tuning works, much better on a reasoning model, and Nova Lite 2 is a reasoning model. So that's why that combination. With that, we will jump into the use case and just give you a brief idea of what we are going to  do today. Rather than spending too much time on the presentation, we'll move to code now.

### Security Agreement Legal Assistant Agent: Use Case Architecture and Training Workflows

So the use case architecture that we're going to pick today is a Security Agreement Legal Assistant Agent. So this agent would be responsible for looking through multiple documents, and then we will give it a question and it has to pick the right tool to call to get access to the right information, right parameters to pick, and then go from there. So in different modules that we will cover, firstly we will teach you how Synthetic Data Generator works or how you can generate your own data.

We'll show you how you can generate your own data, because we see a lot of customers who don't have the data to begin with when it comes to customization, and that is a P0 or prerequisite. We'll give you a sneak peek at the synthetic data generator we have been using for this particular demo. Then we'll do a base model evaluation just to get a sense of where we are with just the base model without any customization.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/630.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/640.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/650.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=650)

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/660.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=660)

We'll move on to Supervised Fine-Tuning. That's  the first thing we would use for fine-tuning with prompt-response pairs. We'll do an evaluation on that post-SFT checkpoint  to see how much we improved from the base model. Then we'll move on to Reinforcement Fine-Tuning, which would be sitting on top of the SFT checkpoint.  So it is an iterative fine-tuning approach. And then we'll do a post-RFT evaluation to see how that goes. Finally, we'll make a deployment to Bedrock to make inference. We'll cover all these modules in today's code talk. We have  only one hour, but too many modules to cover, so hopefully we are good there.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/670.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=670)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/710.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=710)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/720.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=720)

Just to give you an idea of what SFT is, if you are not familiar with it,  it all starts with an adapter trainer if you go with the LoRA-based technique or parameter-efficient approach. That is what we will be showcasing today. It's the most lightweight SFT you can do with limited resources. It's quick, you don't really have a lot to spend, and you get decent results in the first go, or at least get a good signal if the model is capable of learning for your use case or not. Adapter training basically gets the data from your database, which would be S3 in this case. It gets the base model from the recipe, and then it basically does a forward-backward pass to learn from the data that you're providing along with the base model,  and gets a trainer. Then you get the base model and you do a fusion where the adapter is fused with the base model to get the final model.  So that is what SFT does essentially. SFT adapter-based training essentially involves just a light adapter that you're training and then doing a fusion.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/790.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=790)

RFT is a little more elaborate. It has a training VPC and a customer VPC. In training, there are two main modules: there is a rollout and there's a trainer. In your customer account, there is the Lambda function, which is the reward function. Again, you get access to the prompt data, which is your dataset that you want to train the model with using the RFT technique, and you get the base model. You load both things from the rollout, and the trainer also loads the base model. Then it calls the Lambda function to calculate the predictions. What the rollout does is think of it like a system that generates any number of answers. The rollout is like N number of generations or N number of predictions and passes all those predictions to the Lambda to calculate the score based on your evaluation metric. It gets the responses back, which are basically rewards. So it gets the rewards back  and then it feeds those rewards back to the trainer so the trainer can learn from those answers or from that feedback and further improve. Then it does the forward-backward pass on the trainer to get the learnings, and that's where it goes back to the rollout to update the weight policy. This cycle continues where more rollouts are generated, the Lambda function calculates the rewards and passes them back to the trainer. The trainer updates the weights, new rollouts are generated, and that cycle continues for a number of steps that you define in your recipe. That's how the model starts learning, and we'll see today how it happens in a real example.

### Synthetic Data Generation: Creating Training Data from EDGAR Filings and SEC Regulations

Okay, with that, I will move on to the code sample. Let me switch. Okay, so time to dive right into the code. This is a code talk. There's a bunch of code we've developed to demonstrate the use case. We're planning to get through all of it in an hour, but it can get dense. We have planned to show you the essential code blocks and hide away some of, or glance over, some of the utility helper functions. We will take questions at the end of each notebook, maybe a couple. We'll do a quick time check, and if there are more questions, we can just probably take them out in the hallway. This is not a demo and we want to keep this session educational and interactive, so please let us know if you have any questions at the end of the notebooks.

First up is the data prep step. What's interesting here is that instead of taking an existing dataset, we have generated data to fine-tune the foundational models for an example use case, essentially just pulling it out of thin air. The idea is that if we can generate this data from silos anywhere on the internet, data that is structured, unstructured, or a combination, and create agentic applications that are hyper-personalized, you can do the same with data existing in your organization's silos.

Coming back to the use case that we have here, there are three data sources that we use. Imagine that the end user is an attorney or a paralegal or someone who's trying to ensure securities transactions like private stock sales and public offerings comply with SEC regulations.

I do want to point out that you can use Nova models for a variety of use cases, but especially for legal and healthcare use cases, please use it at your discretion, and especially with other regulated and sensitive use cases as well. But here, we have three data sources. We have the EDGAR filings that are the actual agreements that were filed with SEC themselves. So apparently an attorney filled the form out, you know, pushed it out for review to SEC. Then there are the SEC formal language regulations that tell us the legal language that should be in those agreements. And finally, there are judicial interpretations from federal judges and courts of those filed agreements.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1000.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1010.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1010)

So the model can use these interconnected datasets to enable cross-document analysis and validate the agreements against, you know, the text that's required, the text that's provided, and how the text was interpreted. We will glance over some of these functions. We will also publish these in a GitHub repo soon, so you'll  have access to everything that we covered here and more. But these are just basically crawling some data from open source websites. These can be easily  modified to, you know, download the kind of forms that you want to download, for example.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1020.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1020)

Then comes the actual synthetic  data generator. Once we do have the raw data sitting with us, we can start generating data in a format that is ready to be consumed by Nova, the Nova models that we'll use. The format will depend on the model that you use, and you can, we recommend using Nova, but you can try this with any other models available on Bedrock or SageMaker. Important to note is prompt engineering that should be provided while generating the data, so the model knows what format to generate that in.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1070.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1070)

A quick animation shows you how that is happening. I will give it about half a minute for it to play.  So, you see here on the left that we have a data generation kit, and essentially what you see here that's happening is chunking the data. Maybe we can actually go to the still. It's chunking the data into a predefined set of character limits. That is done because generally, you know, data in silos exists in like gigabytes or terabytes, and this will blow up the context size of your model, and it will not be able to generate any data. So we chunk it, and the chunk size depends on the context window of the model that you choose. Again, M is the parameter.

Another parameter is N. What N does is it defines the number of overlapping characters between those chunks so that the model knows if there is semantic continuity, if, you know, sentences cut midway. The model is instructed in its prompt to say that you will receive chunks of data, this is the context, try to maintain semantic similarity. Each chunk is actually then fed into the prompt, and that makes a single invocation into a teacher model. For our use case, we use Nova Premier. So if you have 100 chunks, you'll have 100 invocations that finally generate synthetic data.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1170.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1170)

We have it in a format that's consumable by our student model, which is Nova Lite, and we fine-tune it, which we'll see in further notebooks to be consumed by end users. Quick glance over how the format looks  for Nova, all Nova family or SFT particularly. You have a system prompt where you define what you would want the model to do or the agent application to do. Here, for example, we would say something like, you're a helpful legal assistant. Please help me ensure that my agreements are to the point. We can look at an example of how that is after generation.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1200.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1200)

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1210.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1210)

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1220.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1220)

  It is a bit different for Reinforcement Fine Tuning, but the concepts are the same. You have a system prompt, you have a user prompt that actually feeds in the query,  and you have an answer that is generated by the model. These are all JSON objects, so we explicitly instruct the model to generate it in that format, but as you all know, our foundation models are non-deterministic in some form. So we do provide a validation loop to ensure that these can be consumed by our underlying model.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1250.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1250)

Ultimately, our goal is  for this model to select the right tools, given an input query. So we measure what good looks like by measuring some metrics about the tools that it selected. So given a query, did it select the right tool? If it needed multiple tools, did it select it in the right sequence? The tools are essentially Python functions, so they take parameters. Did it provide the tool the right parameters that it needs to perform the query? So these are some metrics that we are tracking. These are customizable. You can provide your own metrics as well, and we'll look at that in code in a second.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1310.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1310)

But for now, let's look at the prompt that we provide to the synthetic data generator. You'll see some annotations here. You'll see double hashes, guidelines, and caps. Each foundation model has prompting best practices. So for Amazon Nova, this is what it looks like, and we've incorporated it here. We tell the model what it needs to do in the prompt through these annotations. We tell it what tools it has  access to and what each tool needs as a parameter.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1320.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1330.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1330)

We also tell the model to classify each query into eight predetermined  categories. So those then finally map to what tools it will use. There's a quick table that I can go  over that shows how that works. Basically, if it's a regulation definition, this is the tool that should be used. If it's a regulatory compliance analysis, two tools should be used in this sequence.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1350.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1360.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1360)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1370.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1370)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1390.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1390)

And finally, we tell them all that we want  strictly JSON schema that adheres to this structure. We also provide some example definitions, essentially doing a few-shot prompting in  this of how the data should look like. And finally, at the end, we insert the chunk that was generated by the Synthetic Data Toolkit. The  toolkit that we used in this session is an open source toolkit by Meta. It's called Synthetic Data Toolkit. Again, you can use any open-source framework for this, but essentially what we've done in this code is  provided the capability to use Bedrock and SageMaker models with the toolkit to generate this data.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1400.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1400)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1410.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1410)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1420.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1420)

From there,  we turn the kit on, we try to do a system check. We use the Amazon Nova model hosted in Bedrock. We provide an input prompt that just says, hi,  and we can see the model is ready to take invocation requests. After which,  we just basically file parse the data and then start generating prompts.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1430.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1430)

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1440.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1440)

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1470.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1470)

Once that is done, maybe we can  get a quick sneak peek of how that looks like. We combine it into a JSONL, JSON Lines file because that's how Amazon Nova expects us to provide it for Supervised Fine-Tuning.  We also split it between three sets. For example, Amazon Nova generated this query and what an appropriate answer should look like. So we further feed it to the  student model and it learns how to do this on actual user queries.

We'll pause here before going to the Supervised Fine-Tuning notebook to see how we do the actual fine-tuning and take any questions. Sorry, the GitHub is public or not? Is that your question? It's not yet public, but it would be public by today. So yeah, it would be available in Nova Samples GitHub. Any other questions?

### Supervised Fine-Tuning with LoRA: Training the Nova Lite Model and Evaluating Performance

Mike, can you apply a similar process to Nova Sonic, or is that down the line? You mean customizing Nova Sonic? No, that's not available as of today. Maybe in the future, if there's a use case, we can talk about where you're thinking of customization. Okay. If there are no further questions, maybe we can move to the Supervised Fine-Tuning.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1540.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1540)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1560.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1560)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1580.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1580)

Perfect. So, as Anupam told you about Supervised Fine-Tuning, the magic  with LoRA and SFT is that when you use an adapter trainer, which is a parameter-efficient fine-tuning technique, it essentially takes 1 to 2% of the model weights and tries to fine-tune those instead of the whole model, which reduces your infrastructure, training time, costs, everything. We've seen that  if you give it pertinent data, it gives you up to 90% accuracy performance as compared to full supervised fine-tuning. So, basically, the adapter is the 1 to 2% of the weights that LoRA extracts, trains it, adds it to the base model, and finally gives you a fine-tuned  Nova Lite model.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1590.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1590)

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1600.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1600)

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1610.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1620.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1620)

Since we've covered a bunch of that, let's get into model fine-tuning code.  The first thing we do is set up a SageMaker session so that we can interact with the SageMaker clients, submit jobs from our local notebook, and provide the dataset that we need to.  We select the particular instance that we will need. We also do distributed training here, so we select 4 instances so that  training is faster. We select the image URI, which is a Docker image hosted in ECR. These are specific to the model that you  use. So if you use Nova 2.0, that'll have a particular image URI.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1630.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1630)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1650.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1650)

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1670.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1670)

From there, we use recipes, and recipes, as Anupam  let us know earlier, are just abstractions of configurable parameters. What that looks like is, let's say you want to define your training hyperparameters, the number of steps you would like to run, your  learning rate scheduler, the kind of learning rate adapter you would like to use, if you would like to use LoRA or not, and then, you know, LoRA-specific fine-tuning hyperparameters as well. Once that is done, we select  what we call in the SageMaker terminology, an estimator. We use a PyTorch estimator because Nova essentially trains this using the PyTorch Lightning framework. We provide it, again, whatever variables we defined earlier, and we give it the recipe, and finally, we give it the image URI.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1690.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1690)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1700.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1700)

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1710.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1720.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1720)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1730.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1730)

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1740.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1740)

 The data that we generated in the previous notebook and start training. So this is a completed notebook,  just to show you how it looks in the console.  From here, you can look at some metrics about the infrastructure utilization and look at logs.  You see one log stream each for each  instance that it trained on,  which exited each with a status 200 code, which means training was successful.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1750.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1750)

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1760.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1760)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1780.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1780)

Okay, so training  takes anywhere between 20 to 40 minutes. The typical ML workflow is after you're done with training, you want to look at training validation curves. So we download  the model tarball of the fine-tuned data and extract it, which has a manifest file and a train loss file that tells us how the model converged. As you can see from  step, we gave it 100 steps, but from step 0 to 100, it converged from 70 to 10, which is pretty good performance on the train set only.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1800.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1800)

Now one thing to keep in mind is this evaluation metric is actually  different from our evaluation metrics that we want to test the model on.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1820.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1820)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1840.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1840)

We want to test the model on specific examples because we actually want to see what tools it selected, the sequence that it selected them in, and whether it was JSON parsable, things like that. So the training curve is just telling you if it matched the same output that was in the training data, which is  essentially what the assistant responds with. It's a text-to-text match. If it was a match, the training loss converged. 

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1910.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1910)

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1920.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1920)

From there, we are measuring the accuracy of tool selection by a couple of metrics here. Some are traditional classical accuracy metrics. You see precision, recall, and F1 on tool, which is how many times the tool selection was done properly, how many times you saw false positives, and what the AUROC curve looks like and what F1 looks like. We also have an overall score that is weighted based on the parameters that were provided to the tool, the tools that it selected, and the sequence that it selected them in. These are customizable. One thing that's different between the evaluation that happens in the training loop versus what we do in the eval loop is that you have this test split that was sent to the base model, and then it was sent to the fine-tuned model. The way we evaluate these metrics  is we have predefined functions that are loaded in Lambda. So on each invocation of the base model, as you see on the left,  the answer goes to a Lambda function. The Lambda evaluates it for customizable functions and sends those metrics back to the base model. It does that for the fine-tuned model as well, and at the end, we just compare the both.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1940.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1950.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1960.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1960)

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1970.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1970)

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/1990.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=1990)

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2010.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2010)

Base model eval and fine-tuned model eval  look very similar to a training job. So you select instance types, counts, the image URI, which is different than the training loop.  You configure recipes again that point to the Lambda ARN of the Lambda that we have just configured. We define the estimator just like we did.  We provide it with a test split, start the evaluation job.  It takes about 20 minutes. From there, we want to download the tarball again and look at what the results are visually.  These are metrics on the base model itself. It wasn't fine-tuned. We'll have similar metrics for the fine-tuned model, going significantly up, and then we compare. So you can see overall score has jumped from 44% to 75% on the Supervised Fine-Tuning loop. 

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2030.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2030)

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2040.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2040)

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2050.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2050)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2070.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2070)

### Reinforcement Fine-Tuning Implementation: Reward Functions and Iterative Model Improvement

This is where we end the SFT cycle. We take this SFT fine-tuned model and then further try to increase its accuracy by doing Reinforcement Fine-Tuning. Now that we have a customized adapter,  let's move to the RFT, which is a cool part of it. In RFT, as I explained, it's basically doing a rollout, which is going to generate a number of  responses, and then a trainer, which will take the feedback that is coming from the reward function, which is a Lambda, and use that feedback from that reward function  to update its weights, improve itself, and then do that cycle multiple times. Again, the multiple times can be defined in the steps, which is a recipe parameter. The number of rollouts that you want can be defined in the recipe parameter. So those things are hyperparameters that are configurable. 

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2080.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2080)

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2100.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2100)

First, just the basics: setting up the IAM role, doing the imports, retrieving the two main things from the previous notebook, which was  the checkpoint after SFT because that would be needed to run RFT on top of it, and the results that were shown that are captured in a data frame. For data prep, this is the format for RFT: system, user, and reference answer.  Reference answer is basically anything. You can pass in any random thing which you care about, which the model should learn.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2120.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2120)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2140.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2140)

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2150.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2150)

The reference answer does not need to be the ground truth only. It can be thinking notes or whatever you have, because Reinforcement Fine-Tuning is, the essence of it is that it will just learn from whatever you're passing with it. It will not memorize the ground truth only, which is what Supervised Fine-Tuning does.  And then we will create the datasets in that format and store it in S3, and then create our Lambda function, which is the reward function. That is the critical part in getting a good RFT, getting a good reward function. If your reward function is biased or it can be easily hacked, then RFT will go for hacking the reward function. And by hacking, I mean it  will try to find loopholes in your reward function to maximize the reward, and then your model will just not learn that well. So it's essential  to see how we created the reward function.

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2160.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2160)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2180.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2180)

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2190.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2190)

So firstly, we have the Lambda handler and Lambda grader. In the Lambda grader,  you get input as this, which is, so in this case, you see we passed the user prompt, assistant response that came from one of the predictions, and then the reference answer, which is sort of our ground truth. So we pass what type of information is needed, what tool you should be calling, what parameters you should be using for that tool,  and any reasoning why you picked that tool. And then it returns back an aggregated reward score. Now this is a score that,  because RFT works on a numerical reward function, so at the end you have to return a numerical value which it tries to optimize on.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2210.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2210)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2220.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2220)

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2230.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2230)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2240.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2240)

So the aggregate reward function is right now the weighted average, as Hersh mentioned, on all the metrics that we care about, like is the right tool being called, are the right parameters being used, is the sequence of  the tool correct, is the JSON schema parsable. And we kind of aggregate that through this weighted average. So yeah, these are the metrics,  default values zero if some error happens or something, but higher values if things are right. And then we capture all the metrics here and the aggregate  score at the end as well, and we pass the result back. So that's pretty much what the reward function is doing.  The main thing is you should make it gradient in your reward function. It should not be binary, like plus minus or true false. It should have a gradient like this is good, this is average, this is close to good, so that it uses that information to optimize close to good to closer to the best answer.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2260.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2260)

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2270.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2270)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2290.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2290)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2310.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2310)

And next, we  create the Lambda, push it to, deploy the Lambda on AWS, and then kick off the RFT. So this is the crux of everything.  So here we are passing the checkpoint which was trained. We are passing our recipe. We're passing the image URI which is publicly available. We are using P5.48xlarge, four instances are minimum that are needed to run RFT. We'll be using Nova Lite 2 as the base model.  And then we kick off the RFT job. So again, the PyTorch estimator call, very similar to SFT, defining your training, train data, validation data, starting the dot fit command, which is basically what will kick off the training. And you can see the logs, so you can also print the logs in notebook or you can go to the console,  but you can see the logs, start seeing the steps that it is training on and all the different metrics that it is capturing.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2320.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2320)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2330.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2330)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2340.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2350.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2350)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2360.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2360)

And this is a screenshot  of how the console looks like, so you can see it starts training, downloads the image, and then begins the training. Next, we capture  the training results because we need to see how the metrics look like after RFT. So in this, we get the results, which is the  stepwise training metrics to see how the training is doing. And this is a screenshot of  a previous run that I did. Instead of 55 steps, I did 150 steps just to see how much I can stretch it. And then I looked at the train reward function, train reward metric,  which is the key metric that we should be looking for RFT, like how is our reward improving. We want to see a gradual increase in reward from a low value to going as high as close to one, and that indicates that the model is learning as the cycles are going through and the reward is improving.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2380.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2380)

So in this, we see  a decent lift from 0.81 or 0.82 to 0.8586 in a gradual increase. It even reaches 0.9 in some cases. And that is closely correlated to the evaluation metrics. So what we were seeing on SFT, which was around 74, 75, it's now reaching around 88, 89 after RFT. So that gives confidence that reward functions are increasing, and we also see the entropy is

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2410.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2420.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2430.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2430)

reducing  so it's getting more deterministic as well. Then for evaluation, we again do two evaluations, one on the base model RFT and then second  on post-SFT post-RFT eval. So again, the recipe looks like this for RFT eval. You can define your inference parameters.  One cool thing is that it also supports log probabilities. So you can actually get probabilities of tokens that are being predicted, and you can use that to calibrate your answers, or if you have a classifier, you can use that to calibrate what your precision recall boundary is.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2450.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2450)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2470.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2480.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2480)

And then in your RL environment, here I'm passing that lambda function that would  be used for evaluation as well, which is the same as the reward function. So I basically kick off the PyTorch job. I overwrite those parameters because my reward function right now is just a random string here, so I'm overriding it with the actual one of my lambda function, and I'm passing the final checkpoint  that I got. So this is post-SFT eval, which is similar to what I showed as well. And then this is post-RFT eval. 

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2490.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2490)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2500.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2510.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2510)

So next, what we do is we again unpack the post-eval results, and we kind of find it comes in a dictionary, it's a JSON file.  So we get these custom metrics from there that we were calculating in the reward function as well, and create a dictionary so that we can compare in like a  matplotlib using the data frame that we loaded from the last call. So this is how we see it. The base model was 0.44 on this  task, fine-tune made it 74, RFT made it around 90. So that progression tells us that it is improving if the RFT is introduced at a later cycle than SFT.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2550.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2550)

We also tried to switch this up, like doing RFT only on the base model instead of doing SFT first, and we didn't see that good results on that. And that indicated that SFT is actually helping with RFT because SFT is getting the basics right, getting the formatting right, JSON schema right, all that, and then RFT is improving the quality of the parameters that are being predicted. This is more results on different valid, like different metrics. This is only on overall  score, that is what we care about, but these are the submetrics and where we see it failing the most, or we can still continue doing the failure analysis on it, improving our RFT data to see further improvements if we want.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2610.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2610)

### Deploying the Fine-Tuned Model to Amazon Bedrock for On-Demand Inference

But that gives a good indication of how you should be doing it, like starting with your base model eval, SFT and then post-SFT RFT. But the good part is you can keep continuing these iterative loops, so you can keep building on top of a previous checkpoint and saving that checkpoint, evaluating that, and then building on top of it. The last notebook that I wanted to show is the deployment. Now that we have the final checkpoint, the final checkpoint goes as an S3 URI. So you don't get the model itself, you just get an S3 URI. If I can find an example here, let me show. 

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2620.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2620)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2630.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2630)

So this is the checkpoint S3 bucket that gets loaded, but I can find, let's see,  yeah, like for example, this is an example of how a checkpoint would look like.  This is a bucket which you will not have access to. It's a service-side bucket, but that checkpoint can be accessed through the IAM policy that you will be using. So this checkpoint is now our task is to take this final S3 checkpoint after RFT and somehow move it to Bedrock so that we can make inference calls. And that is done through two Bedrock functions.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2660.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2670.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2670)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2690.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2700.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2700)

So basically, what we will do is first we will export the model from S3 checkpoint to Bedrock.  So with that, we will use the create custom model API which will move the model to Bedrock. We will monitor the status of it. Once  it is active, that means the model is deployed on Bedrock. And then we will make the second call, which is using create custom model deployment. Now this will use on-demand inference and basically create an inference endpoint for you of your custom model to start hitting on Bedrock. And this is again  because we use LoRA across the thing, like we did SFT on parameter efficient, we did RFT also on parameter efficient.  We can use on-demand inference because everything was on adapter only, so it can reuse the same hardware. And then we finally

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2710.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2710)

see the model in Bedrock, which is now  hosted and can be made to hit publicly through your AWS account.

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2720.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2720)

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2730.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2730)

So how do we change code when we are making the inference call? It's basically one line of change.  This is a Converse API which all you might be familiar with. Here, I'm passing the model ID as base model, which is Nova Lite 2, and  I just get some random results like explanation, which is useless to me because I was asking it to be structured output.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2740.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2740)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2760.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2760)

Then I  did the same call, but this time I passed the deployed model ID which is the ARN of the on-demand inference that I got from the previous call. Here, I get much structured answer with the right tools being called, like the statute retrieval and then compliance checker. That indicates to me that the model has  learned and it's improving its accuracy from the base model.

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/326f03eb9183e43f/2780.jpg)](https://www.youtube.com/watch?v=nhlA06hMisc&t=2780)

### Closing Remarks and Q&A Session

That's pretty much it. What we wanted to cover end to end from data prep to SFT to RFT and then deployment. We're open for questions if anybody has any. We'd love to talk about the use cases that you have as well since we have some time, if you wanted to discuss those.  Awesome. Cool.

If there are no further questions, you can scan and give free ratings or whatever it's called, CA, so you can give it. Thanks for attending the talk. We'll be hanging out here. If you have any questions, thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
