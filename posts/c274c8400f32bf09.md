---
title: 'AWS re:Invent 2025 - Reimagining AWS operations with autonomous AI agents (DEV207)'
published: true
description: 'In this video, Geethika Guruge from Mantel Group shares three production AI agent implementations for autonomous cloud operations. The first agent automated compliance enforcement during a 500+ server migration by reading Confluence documentation and creating pull requests in Terraform code. The second monitored CloudWatch logs during EKS modernization, reducing incident resolution time by over 90% by automatically triaging configuration and security issues. The third handled low-value EKS support requests via Slack, reducing support costs by 20% and enabling platform team scalability. Key lessons include the challenge of fine-tuning system prompts across multiple specialist agents using a swarm pattern with orchestrator, the importance of agent observability, and starting with high-volume, low-value workflows in well-documented processes like compliance and troubleshooting to build business trust.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Reimagining AWS operations with autonomous AI agents (DEV207)**

> In this video, Geethika Guruge from Mantel Group shares three production AI agent implementations for autonomous cloud operations. The first agent automated compliance enforcement during a 500+ server migration by reading Confluence documentation and creating pull requests in Terraform code. The second monitored CloudWatch logs during EKS modernization, reducing incident resolution time by over 90% by automatically triaging configuration and security issues. The third handled low-value EKS support requests via Slack, reducing support costs by 20% and enabling platform team scalability. Key lessons include the challenge of fine-tuning system prompts across multiple specialist agents using a swarm pattern with orchestrator, the importance of agent observability, and starting with high-volume, low-value workflows in well-documented processes like compliance and troubleshooting to build business trust.

{% youtube https://www.youtube.com/watch?v=U7Nkuyt6X0M %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/0.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=0)

### Introduction: The Case for Autonomous Cloud Operations

 Hello, good afternoon, and thanks for making it all the way to Mandalay Bay. I know it's Wednesday afternoon. Is it Wednesday? Yes, right. Anyway, that's what re:Invent is like. So before we start, has anyone built AI agents in production? Anything in production yet? Good, good. So maybe you can leave now. But anyway, let's start.

So I'm Geethika Guruge. I work for a place called Mantel Group, which is an AWS partner, a consulting partner based in Australia and New Zealand. I'm based in New Zealand. I'm going to talk about a few stories about an agent, or multiple agents, we built for a few of our customers and the efficiencies we got. Most importantly, I'll share some of the lessons learned.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/60.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=60)

 So this is our agenda. I'll just take a few minutes to explain why autonomous cloud operations matter, the value proposition of that, and then those use cases I mentioned. Of course, we'll look at the architectural view because no presentation is complete without the architecture diagram, right? And then, as I said, most importantly, the challenges and lessons learned and what you can do when you go home.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/90.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=90)

 So if you look at cloud operations today and for the last seven, ten, twelve years, we've been migrating things into the cloud. Every time we do that, and also when we modernize things on the cloud, that means more services, more environments, which results in more logs, more support tickets, and things like that. Also, increasingly, compliance and security teams are getting very agitated, and there's quite a bit of pressure on security and compliance. As we all know, manual fixes take time, and more than the fix, it's the triage that takes time. Once you've triaged, once you understand the root cause, the fix is not all that hard, but identifying the root cause takes time.

So tomorrow, the future we see is there will be autonomous agents. Operations will be autonomous, and AI will understand the cloud, the code, and the policies. Policies in this case could be compliance, could be security, and things like that. Also, most importantly, there is an aspect of human in the loop if needed. I think no organization at this point is comfortable letting agents loose, so that human in the loop becomes very important. Humans will be approving, supervising, and more importantly, innovating, because now that the tedious task of manual fixes and triage is taken away from them, they can do more innovative, fun tasks.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/180.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=180)

### Three Real-World Use Cases: From Compliance Automation to EKS Support

 So I'll take you through these three challenges we had, or three use cases. The first one was the compliance thing. We were doing a huge migration, more than 500 servers, and this was an insurance client. They had a very tight compliance regime and a few others. They had a number of compliances they had to meet in the new environment. Every server we migrated had to be ticked off for those compliances, and those compliances were documented, reasonably well documented, in a wiki page in Confluence. So the compliance criteria is on Confluence, as I said, and there was another criteria as well. Anyway, when we do migrations, what we follow is we do all the infrastructure as code, and in this case it was Terraform.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/230.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=230)

 So our solution was, once we migrate a server or build an environment, we would trigger an agent. That agent is exposed via an API, so we have our internal tooling for migrations, but at the end of that workflow, we would call this agent. The agent would go and read the compliance documents from that wiki, from the Confluence, and then it would assess the server or the environment against that compliance document. Any gaps it finds, it will create a, first, it will create a Jira ticket for traceability, but most importantly, it will go into the code repository and raise a pull request, or take a branch, do the change as it sees fit, and raise a pull request. So that's when an engineer would come in and look at the pull request, and more often than not there will be some revisions and some changes happening, but that initial phase is done.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/290.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=290)

 And then the benefits, of course, the automated enforcement of compliance at scale. As I said, this is more than 500 servers, a number of environments, and if you imagine a human having to go through each and every server, read that document every time we migrate a server or build an environment, that would be a tedious task. So the agent is doing that boring task for us right now, which of course made our migrations faster.

More importantly, it provided traceability because we create Jira tickets and everything goes through a pull request process, so everything is recorded and that traceability is there.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/330.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=330)

So the next one is a modernization project. As we migrate,  it's not just lift and shift. We do lots of modernization, and what we found here is whenever we modernize the application, in this case it was an Amazon EKS environment with a working application in Java like a monolith, we were containerizing it and putting it into EKS. Most of the errors we got, and traditionally most of the errors we get in this kind of project, are configuration issues and security issues and things like that. The biggest problem is when we do these kind of modernizations, the SMEs are already bottlenecked with other tasks, but then we have to go to the SME to understand what's going wrong with the application.

But if you think about it, if you take a step back, this is already a working application, so it cannot be too much of a task to fix this. More often than not, the issue is some misconfiguration or some security permission, maybe IAM permission, things like that. But to triage this manually takes too much time, and SMEs are bottlenecked, so that's the other aspect.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/400.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=400)

So what we did was, again, we deployed this agent.  It would monitor CloudWatch logs, and in this case all logs were going to CloudWatch, but it can monitor any logs. Whenever the test cases run, it would identify when an error comes up. The agent gets triggered, identifies the root cause, and as a last instance it will create a Jira ticket and raise a pull request. That initial triage and the first pull request is created, and then a human can come in and do the rest.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/440.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=440)

This actually increased our incident resolution time by over 90%.  Because now most of these minor issues are handled by the agent, and most of these pull requests didn't have any further commits. It was just rubber stamping them. There was less reliance on SMEs, and that was the biggest aspect because now we are not reliant on SMEs. They can do their day job and we can go to them whenever there's a real problem. Of course, that's a continuous improvement task because we integrated this with the CI/CD pipeline. So now, not only for the migration, once we've already modernized the application and they are running it in production, this agent is still running. Whenever they do another check-in, any error, any test errors, this agent kicks in and raises a pull request.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/490.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=490)

And the last item I'm going to talk about  is automating low-value requests in EKS. So once this customer went live, as I said, this was a brand new EKS environment for them and they had to spin up a platforms team. They already had their DevOps team, and DevOps is not platform engineering, but that's a separate conversation. Anyway, so they built this brand new platforms team, but what they realized was there was a high number of support tickets because now the application developers needed quite a lot of support from their platform team for some low-value tasks. When I say low-value, this could be the creation of a new namespace or even increasing some quotas in a namespace and things like that.

Of course, the application teams need to rely on the platforms team to get these tasks done, but it is not the best use of the platform team's time. And then of course there were long wait times and reduced user satisfaction because the platform team was taking too long to get these minor things done.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/550.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=550)

So what we did here was, now this agent  is triggered by Slack and it goes to the agent. Basically, a Slack message triggers the agent. The agent can access logs and also through MCP servers into Confluence, any documentation it needs and any compliance things it needs to follow. Then it would do simple fixes and raise a request. Here, one interesting point is we used the AWS EKS server to look into the EKS cluster and understand what's going on, but we used it in read-only mode because we didn't want to go and directly apply the change there. We wanted the changes to go via infrastructure as code, so it just looks at the cluster, validates it against the request, and then applies the change in the repository.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/610.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=610)

Again, it reduced support costs by 20% because now the support team doesn't  have to do all these tasks. They only have to look at the pull request, maybe change it a little bit, approve it, and push it. Of course, this also enables faster resolution because now up until that pull request stage, it is done within minutes, under five minutes. Initially we ran this in a Lambda before Amazon Bedrock Agents was generally available, and we set that Lambda's timeout because obviously Lambda times out in 15 minutes, so we had it under 15 minutes. But in reality, most of the things were done under five minutes.

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/670.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=670)

Again, this is scalable because now that more and more applications are getting into the EKS cluster, more and more application teams are dependent on the platform team. But the platform team doesn't have to scale when the number of applications scales because the agent scales, and the platform team is only approving the pull requests. So if you look at the architecture here, you have two paths in. One is through Slack, but it doesn't have to be Slack because as you can see, it's an API  gateway, so you call an API basically. The other path is through CloudWatch, and again this doesn't have to be CloudWatch. It can be any log or any observability platform. It's a matter of triggering an alert basically with all the error information.

If you come through the CloudWatch or the API Gateway path, it comes into a Lambda. In this Lambda, what it does is it will create an agent called runtime and call that runtime. We've used multiple agents here, so we are using a swarm pattern but with an orchestrator. What happens is the orchestrator receives the request from Lambda and it decides which agent to call. We have multiple specialist agents: Confluence, Jira, PR, and AWS agents. The PR agent is the one which is responsible for creating a branch, doing the pull request, and all that. The AWS agent has all the tools to go into AWS accounts and look at the AWS accounts, as I said, like EKS MCP server. That is the high-level architecture.

Then the agent core memory, because with that what happens is every time a request comes and a root cause is identified, that part is now in the memory. So next time when some similar request comes, the agent learns or the agent knows what worked well before. Based on that, it can do a better job. Then of course the agent observability, that is whatever happens here. There are OpenTelemetry logs that go into CloudWatch. There are third-party tools, but we found that simple CloudWatch is good enough. It is not great in viewing those logs, but if you are happy to spend some time on the logs, you can follow through.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/790.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=790)

### Lessons Learned: Fine-Tuning System Prompts and Building Trust in AI Agents

I think this is the most important slide in this entire presentation, so the challenges and the lessons learned. The biggest challenge we had  was fine-tuning the system prompts. As I said, we had multiple agents. Each agent was given its specialty or given its persona with that system prompt. It is the prompt which says, okay, you are a specialist of this, you can do this and this, and these are the tools available to you. What we did was we came up with the initial system prompt, ran the system, and obviously we didn't get the desired outcome, so we iterated on the prompt. There's no real art, or it's a fine line between art and science, that's fine-tuning system prompts.

A few challenges or a few interesting aspects we found there was once when we had a pull request coming on to some security permission issue, it was a test case. Every time we run this, we would manually remove some permission so it would generate an error, and then it gets triggered and fixes the error. But what happened was after the third or fourth time, the agent directly applied the change on the master. When you go through the logs, it says, I can see five branches had been created for the same issue and hadn't been merged to master. I see there's a problem in merging the pull requests. I'm going to apply to master.

These kinds of things you have to guard against around the, obviously you can guard against in the repository itself, but then again the prompt, you say never ever merge to master because otherwise the agent can get too smart or try to get too smart. Things like that. Also, another angle we had was when we implemented these bug fixes or the error-fixing agent.

We only wanted that specific error to be fixed because obviously these are existing code repositories with huge tech debt. However, the pull request agent's response was trying to fix everything in the code base. When we tried to tell the agent to only focus on this particular issue, it got confused because the orchestrator agent was asking the pull request agent to fix something. So we had to fine-tune all these things.

What we ended up doing was creating two paths in the orchestrator agent based on where the request comes from. Based on the request pattern, it would identify whether this is a surgical pull request path or an improvement task, and that flag goes into the pull request agent. At that point, the pull request agent knows whether to do a surgical fix just to fix this bug, or whether it has the keys to paradise and can improve the entire report. Fine-tuning system prompts is an art, and probably a science in some time when people know more, but that was interesting.

Of course, federated permission was important because we use multiple MCP servers. Most of these things had to go through security. For example, GitLab or GitHub have security, and then Confluence and Jira require credentials. We also had to understand whether the end user who's triggering this actually has enough permission to perform this action. In our case, with these three use cases, it wasn't a big deal for us because it is ending at the pull request stage anyway. We created system users for these MCP servers, so every time you create a pull request or create a Jira, it would go as that agent's API.

Earning trust from the business was interestingly not as hard as we thought, but I think that was because we chose some high-value tasks, specifically high-value, low-risk tasks. They were happy to trust us on that, and once we built that, the business trust gradually became important. Monitoring and observability, as I said, using agent observability or any other tooling is crucial because especially while fine-tuning the prompts, you need that to understand what's going on.

Even after that, you would need it to understand what are your costs and what are the right and wrong responses. I think yesterday there were evals within the agent as well, so that way you can figure out how many of your prompts or how many of your requests were actually successfully evaluated and got a successful result. Monitoring and observability, as in any other IT system, is very important and has to be a first-class citizen.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/1090.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=1090)

The last one, so once you go, if you want to build an agent, these are the lessons learned and this is what we've done.  You can identify high-volume, low-value workflows because that way, since it's high volume, it's high impact. If you automate that, the business will get behind you, and sometimes it can be low value because then business sees it as low risk. Even if you fail, it is not a big deal.

Then focus on compliance, troubleshooting, and support because those are already well-documented processes if you think about it. In any organization, these things are already well-documented, very streamlined workflows. It's very easy to start there because you already know what to do and you can start with a very good system because you already know the workflow. Then gather feedback and expand with more complex workflows because by iterating, you would know what's going on and you can feed that feedback loop and then improve on that. So I think that's all.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/c274c8400f32bf09/1160.jpg)](https://www.youtube.com/watch?v=U7Nkuyt6X0M&t=1160)

At the end, I'd like to say AI agents will not replace you,  will not replace humans. They will amplify the impact. That's how I see these autonomous agents. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
