---
title: 'AWS re:Invent 2025 -How Toyota Built an AI Platform that Revolutionizes the Dealer Experience-IND320'
published: true
description: 'In this video, AWS and Toyota present their generative AI journey, focusing on dealership applications. Bryan Landes introduces AWS''s Gen AI stack and Agent Core for deploying agents at scale. Stephen Ellis explains Toyota Motor North America''s Enterprise AI team structure and their build-configure-buy approach, highlighting a RAG-based product information assistant serving 7,000+ monthly dealer interactions. Stephen Short provides technical details of version one''s architecture using ECS, EKS, OpenSearch, and Bedrock, including their ETL pipeline, compliance workflows, and disclaimer handling through stream splitting. He then unveils version two''s migration to an agentic platform using Agent Core Runtime, Gateway, Memory, and Identity services, demonstrating innovative use of Agent Core Memory as a distributed cache for API responses, targeting Q1 2026 launch.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 -How Toyota Built an AI Platform that Revolutionizes the Dealer Experience-IND320**

> In this video, AWS and Toyota present their generative AI journey, focusing on dealership applications. Bryan Landes introduces AWS's Gen AI stack and Agent Core for deploying agents at scale. Stephen Ellis explains Toyota Motor North America's Enterprise AI team structure and their build-configure-buy approach, highlighting a RAG-based product information assistant serving 7,000+ monthly dealer interactions. Stephen Short provides technical details of version one's architecture using ECS, EKS, OpenSearch, and Bedrock, including their ETL pipeline, compliance workflows, and disclaimer handling through stream splitting. He then unveils version two's migration to an agentic platform using Agent Core Runtime, Gateway, Memory, and Identity services, demonstrating innovative use of Agent Core Memory as a distributed cache for API responses, targeting Q1 2026 launch.

{% youtube https://www.youtube.com/watch?v=lp_tXtr_aL4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/0.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=0)

### Introduction: AWS and Toyota's Collaborative Journey in Generative AI

 Hi folks, my name is Bryan Landes. I'm a Global Solutions Architect. First off, welcome. I hope you're having a great day. I hope every one of you makes like $5 million in the casinos. If you don't gamble, I hope you're having great food. And if you're anything like me and you hate Vegas, we're almost out of here.

I want to introduce myself again. My name is Bryan. I've actually been supporting Toyota for about 7.5 years, and we wanted to talk a little bit about some generative AI use cases and agentic platforms, specifically in the dealership. I'm here with Stephen and Stephen. This is a little bit different, and there's definitely a lot of other talks, I think BMW and Rivian and a couple of others, but we kind of wanted to go a little bit different this time around. We're actually having two Toyota entities on stage at once, right? So we have Toyota Motor North America and then also Toyota Connected North America. They will introduce themselves in a second, but again, I just want to thank you for being here.

[![Thumbnail 70](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/70.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=70)

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/80.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=80)

By the way, we do know how hard it is to get around re:Invent. We really do appreciate your time. If you ever see us in these orange badges running around, we're trying to give you the best of show. We know how things get a little  crazy. So again, thank you for being here.  So yeah, this is basically the flow. This is what we're going to be doing. So I'm going to come up here. I'm going to be about 100 level. I'm just going to kind of quickly level set for a second. I'm going to actually talk a little bit about the story of AWS and Toyota. That's something that we don't normally talk about, but I just kind of wanted to give you a sneak peek. I'm going to pass it off to Stephen Ellis. He's actually going to tell you about TMNA or Toyota Motor North America's Gen AI enterprise approach, and then we're going to nerd down with Stephen Short. It's going to be great.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/110.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=110)

### AWS Generative AI Stack and Agent Core: Democratizing AI Deployment

So yeah, just to kind of level set for a couple of seconds, basically just to kind of again level set,  we look at our generative AI stack, basically what you see here, right? If we start from the infrastructure layer, we have multiple GPUs, P4s, P5s, whatever have you. And you also have SageMaker that is actually doing the undifferentiated heavy lifting to basically help build out models or whatever it may have within your actual stack itself. If we actually go a little bit higher up, we use Amazon Bedrock, which is our way of actually helping provide LLMs at scale for our users. And if we go a little bit higher, we still have Nova and also we have frameworks, which is actually our open source framework to help build out agents and deploy them at scale again. We have Kiro, Amazon Q, but I think more importantly about this slide is Toyota is using all this in various different forms and fashions, right? So we try to democratize our AI tooling, and this is kind of how they approach it. But again, this is from AWS. This is how we kind of look at this whole thing.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/170.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=170)

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/210.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=210)

 I think what's kind of important too is that we all may have definitely by now, right? I think ChatGPT launched three years ago, you know, generative AI chat assistants were kind of the big thing, but what we are truly seeing, and I feel like, you know, even with Swami's keynote today, is that we are seeing Gen AI actually being deployed throughout the enterprises in different specific lines of businesses, and it's actually becoming much more agentic all throughout. And in fact, when Stephen Short gets up here in a little bit, he'll talk to you a little bit about how he's actually been working with Agent Core to be able to deploy these agents at scale within Toyota Connected and also  Toyota Motor North America.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/250.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=250)

And that's kind of why we built Agent Core. I don't know how many of you all may or may not be using Agent Core, but basically this is our service to have composable infrastructure units so that you can basically deploy agents at scale. So instead of spending 20 months writing identity scripts or trying to figure out observability with DataDog for six hours, you know, basically we have actually built out these composable points that you can use with your agents and be able to deploy them at scale, right? Because a lot of time we were all trying to kind of figure out like do we keep making chat assistants, but actually how do we actually work with these LLMs and agents, and so that's why this is actually made and  that's what we're seeing used at scale.

Very quickly, just kind of show you a little bit more about Agent Core. I'm not sure if a lot of folks may or may not have used Agent Core. I feel like it's still relatively, you know, a day one service, but basically this is a great small example, right, where you have an application that might be hitting API calls to Agent Core runtime, which is an isolated VM so that basically you can have your own secure environment. And then you can use any type of framework, right? It's actually framework agnostic and also LLM agnostic, right? But in these particular examples it's frameworks, LlamaIndex, LangGraph, and also CrewAI, right? And so basically you would use these frameworks and then you would be able to deploy these frameworks and your agents with again the actual Agent Core runtime. Also there's Agent Core gateway which Stephen Short will talk about in a couple of minutes about how he's implemented with that. And then also there's some actual tools that come out of the box such as Agent Core browser and also your code interpreter.

I don't know if anyone's actually working a lot with agents as well. Identity is kind of a very hard challenge right off the bat with authentication and authorization. So we actually take this away from you and help you with this so you can actually have true identities with your agents without going too crazy or letting them go off on their own, right?

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/350.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=350)

Last but not least is memory. That's a huge thing, context windows and everything, right? Again, we take that away and we put it on our backbone. And then also last but not least is the observability layer. You want to actually know what these agents are doing. You actually want to see if it's actually bringing value to your lines of business, right? And that's another key component to why we built out this infrastructure. And again, when Stephen Short comes up here, he'll give you a solid example. 

### Enterprise-Wide Agent Deployment Through Platform Engineering

Again, I'm going to talk very high level again. That's why I said I'm at 100 level right now, but basically I do want to mention that literally we are seeing agents across the stack, specifically with automotive and manufacturing being deployed through almost every single line of business. We are going to talk and focus on the digital customer experience within this particular talk, but I am seeing software defined vehicles being actually agents deployed. I am seeing in-vehicle assistants being deployed within the head unit. We are seeing agents actually being tested through smart manufacturing and quality control, and we also are seeing product engineering with your CAD as well to help generate and build out faster vehicles and prototyping, especially if you're a PM, right?

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/400.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=400)

Sometimes us nerds, like we nerd down too much, but your PMs, they use something like Lovable or some of these other stacks so they can quickly integrate fast, right? And basically we're seeing productivity  increase across the board. So I do get asked this a lot, how do you actually do this, right? So we're talking about all these cool services. How do we actually do this for an enterprise? And basically, platform engineering is the key, right?

Platform engineering, if you are not familiar with that term, I highly recommend you look it up. It's kind of taken the world by storm outside of Agentic AI, but basically the idea is that you have one centralized platform and your platform will actually enable DevOps at scale. So I actually am a platform engineering leader, and I actually do work with a lot of other customers that's not just automotive manufacturing. In fact, my colleague Neil actually spoke with Cisco on Monday about this very particular use case, but again, ideally one centralized platform. I will brag a little bit. Toyota has like four or five. It depends on your organization, how you're set up, but nevertheless you focus on your developers and then you start building out more features and capabilities to your builders, which I kind of use it as an umbrella term, and then data scientists, and then don't forget about your business, right?

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/480.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=480)

And here you can see very quickly, it doesn't matter where you deploy these things, it could be in a VPC or if you have a corporate data center. Remember, internal development platforms solve your cloud problems now. That's why so it turns out when you actually start using internal  development platform throughout your organization and you start using agentic frameworks and integration into the IDP, you actually have an ability to actually deploy agents at scale within your own confined environments and scale and identity and guardrails so your security team is not freaking out, right?

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/510.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=510)

But basically I'm using Agent Core as icons and also MCP servers and also I just put strands there for fun. But ideally what you're trying to really do from a centralized cloud team is democratize agents so that anyone can deploy these and actually scale them up at scale.  This is what I'm seeing the most for sure. Like I just said, Cisco kind of talked about their whole experience on Monday. I worked with Spotify, same thing, and this is kind of the way we're going.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/540.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=540)

### Seven Years of AWS-Toyota Partnership: Building Together on the Bleeding Edge

So just remember it doesn't matter if you're using Kubernetes, Terraform, CDK, CloudFormation, but ideally what you would do is use something like Agent Core, and you would let these internal development platforms have self-service templates so you can deploy them and say, legal or HR and anything like that, right, and help scale these things up. So story of AWS and Toyota,  this is kind of personal for me. I've been at AWS for about seven and a half years. Before I actually started really helping out Toyota, I actually floated around our South Central Enterprise strategic and global accounts.

So I was actually working with agriculture, so like Tyson Foods. If anyone's ever been out to Arkansas to have fun with that, also media and entertainment, gaming. But one day I was very lucky I got to be on Toyota. I joined in 2018 and their footprint was very, very small to keep it light and my team alone, we have actually spent every single day and we're helping them with their mobility journey and it's not just North America, right? It's also Toyota Motor Corporation in Japan. It's also Toyota Connected in Japan, Woven by Toyota, Toyota Racing Development, which is the NASCAR team, they're actually using SageMaker to predict races.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/600.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=600)

And basically I just wanted to talk about like if you're a customer out here, force your account teams to do this because literally I wake up in the morning, I'm calling Toyota when I go to bed I'm calling Toyota. 

That's also one big thing too. They push on our services every single day, and we're actually discovering new types of workloads all the time. In fact, when I walked into the internal development platform, I called up Kishore, which is right there, and basically said, hey, are you doing this? Sure enough, we're on the bleeding edge and building together. I do only have four use cases up here, but there's almost about like forty-seven. It's crazy, right? A big thing for us is that we're always learning, we're always building, and that's why I'm going to show you something better. Steven, if you don't mind, let's go places.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/680.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=680)

### TMNA's Enterprise AI Team: A Build-Configure-Buy Approach to Gen AI Governance

Right, thank you, Brian. As Brian alluded to, Toyota and AWS have worked together for a very long time, and one of the things that's kind of led to the inception of our journey with generative AI was the formation of our Enterprise AI team. The Enterprise AI team was brought together to be a center of excellence for generative AI for TMNA overall, but it also has a kind of unique factor that we don't see in a lot of enterprise COEs where almost the entirety of the team are engineers. What we do is we focus on either building AI accelerators  which are baked into tools that we're using in existing platforms and then bridging the gap between the outcome we're trying to get and the efficiency we're trying to create, and at the same time researching into ways that we can generate new use cases and create what we call AI teammates. Toyota has a policy to keep our teammates at the center of all of our work, and so because of that, everything is based on augmenting our current teammates and capabilities and going to reach full capacity for whatever the use case is.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/740.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=740)

One of the things that Stephen Short is going to show is how we're looking to enhance our capability for delivering product information at scale so that our advocates in our brand engagement center, our end users on the websites, our dealers in the shop can get all of the latest and greatest data as fast as possible. But we don't have to create unique custom pipelines for every single use case and iteration and then continue the normal sprawl that you see in IT.  To do that, our group has a kind of unique structure inside of our IT organization where we are kind of a diagonal across. Brian Cassar, the head of Global AI, likes to call it our diagonal. We're not a horizontal, but we start with exploration. This is novel use cases starting with brand new technology that no one's ever done before, and then through experimentation and education where we work with our IT teams, bringing that into business as usual.

This is where if someone says, you know, I want to use Gen AI for document retrieval, that was one where they want to take all the contracts and combine them together to start looking at trends across the entire SOW scope. We had a couple of analysts that were doing that. There's three hundred thousand contracts. They were getting through about thirty thousand a year. So one of our first implementations two years ago was to bring that all into one platform, use generative AI to go through and do the analysis, pull out all the key components, and then turn that into analytics data for them to look at all of it at scale. Time reduction was somewhere in the order of like fifteen to seventeen hours per user, which then also allowed us to start seeing things like creep in, labor increases, or like we see contracts that had expiration clauses that we were missing and so we're paying way more than we should have been paying because we weren't doing good decent compliance work. We're still evaluating the savings which continue to grow every year as we do more and more business with different partners.

We also take those learnings and enable other teams to replicate, so one of the things we're looking to do is democratize all of the different use cases across the board. I kind of break them down to like three groups. You're taking data and doing analysis, you're taking content and generating new content, or you're distilling disparate content down into some unified source that can be distributed to different groups and different stakeholders depending on the audience. Pretty much all of our use cases fall into that bucket, and so once we find the capability and are able to develop it for Gen AI, we then want to spread it out across the different groups because time to market is kind of the only key metric that we're looking at because this space moves so fast. If you don't do it quickly, you might as well not do it at all.

Once we have those identified, then we engage with business users for adoption. This is one of the things that's probably most important in change management, getting people to understand that there's no perfect version. It's whatever version you can get done and then you improve upon it based on learnings, and sometimes that is a little bit interesting in a manufacturing perspective because everyone kind of wants to de-risk the project and plan it out perfectly before we do it. Doing that in Gen AI just means you're behind every single day. And then lastly, we work with our business units in legal and cyber and then also for policy.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/920.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=920)

Everyone's using all generative AI responsibly, and we're not drifting off or letting additional viewpoints that are not Toyota-specific come into our products and then turn us into a company that may not be exactly following Toyota's way. To do that, we've  taken a build-configure-buy approach to manage all of that at scale. Because we started with engineers and research scientists, we were able to build a lot of these capabilities from day one. As soon as ChatGPT was available as an API, we started building systems from the get-go. The reason we started building was because those didn't exist as products.

Once we've built them and defined requirements that we know work for our use cases, we then look for a product that can be configured so that it can be worked into an existing platform. And then finally, if those existing products and features turn into a SaaS platform or are delivered through one of our trusted partners, we look to buy that based on knowing that we're going to get a certain outcome from that spend, which then allows us to get support, get maintenance, and get upgrades without us needing to invest all of that money into places that we're not the best at. Toyota is a manufacturer of cars. We're not a UI/UX shop. We're not really a software development shop, and so it doesn't make sense for us to try to recreate the wheel when someone is already delivering it at scale.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/990.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=990)

To make that lifecycle work, we engage with business on ideas. Basically, the idea is  I want to use generative AI for a specific purpose. We engage with them from the product management perspective, and we have a couple of engineers that usually engage with these units to shape that into a submission for our AI and ML governance board. It basically means we would look to evaluate to see if the solution, vendor, or the technology is going to be compliant with existing standards. In some cases, we find that there is not a standard for what we're trying to do, and so we then help shape a new standard to make sure that it can be adopted by groups across the board.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1050.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1050)

Once it goes through the governance board, we then engage to build prototypes, set up productionization plans, and then we also help with supportization when necessary. There are a lot of teams that already have their full stack, and so once we build a prototype or we validate the solution, we hand it off and then support them by enabling new technology. What that leads us to is we're now looking at how do we come up with a unified enterprise strategy for agent success.  What we have decided to focus on is using this stack to guide most of our choices and decisions across the board when it comes to the future forward-looking enterprise.

### Unified Enterprise Strategy: Deploying AI Agents Across Toyota's Dealership Network

Toyota has a lot of existing AI and ML investments. We have an Office of Data Science that is going to continue to build models and tools. Those tools flow up through the business rules and workflows that are enabling existing processes. Once those processes have been solidified and we understand what our KPIs are, we usually run them through an orchestration layer that says, okay, here's how we're going to manage this at scale, whether it's a single team member, a small team, or an entire organization.

Then, utilizing interoperability anchors, as we're calling them, MCP servers, the app-to-app protocols, frameworks like Strands Agents, we're then helping each team build agents and bots for their specific use case but with a common context that allows them to be shareable across the organization and also maintain precision on the outcome. As models change, if we see drift, if we need an update for new use cases, or a use case develops a new capability that we hadn't seen before, we can actually bring that back in and update very quickly and iterate.

One of the reasons that we're looking at it at that layer is because we want to infuse this into our existing platforms so that the end users that are actually doing this on a day-to-day basis feel comfortable with the space that they're engaging with these agents, and they don't have to relearn an entirely new UX process to then get these capabilities. That will really hinder our ability to get this out at scale because, as most people understand, this space changes so quickly. Every three months there's something different, and so once we have an anchor inside of a space where a team is using a familiar interface, it's a lot easier for us to upgrade in the background than to try to redesign those products every single quarter at this point because the technology moves that fast.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1170.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1170)

For the project we're looking at today, we were addressed with an interesting problem.  The experience for the dealers before generative AI came into the mix was they were dealing with customers who were highly researched. Most people would go on YouTube, they would look up a vehicle, they would go through videos and gather a lot of information about our products, sometimes the history of it. I remember when the Supra came out, you could see forums where everyone was talking about the differences between the Mark IV and then the new one. Then they go into a dealer and they ask the salesperson, hey, why should I buy this new Supra? And the dealer's response is, because I'm trying to sell it to you. And so then they go on Google on their phones on the side to try to figure out why this customer is very interested in the specific model or the engine that's going into this vehicle versus a different trim.

To solve that, we took all of our product data about the vehicles that comes from our manufacturing, our sales, and our marketing, and put that together inside of a pipeline that allowed it to be delivered as a RAG implementation. In addition to governance rules and guidance from legal as to how we're supposed to manage the brand voice, we enabled that to be delivered to the dealerships for them to have that conversation in concert with the user, and then also to the dot coms for them to do product research. Currently, that first version of the system is deployed to the entire dealer body for the Toyota dealers, and we're looking at about 7,000 plus interactions per month that is going through the system where people are having these sales and discovery conversations on a regular basis.

Actually, the day we submitted this deck, Bryan sent me a video of himself in a dealership with the product up talking to a dealer as we were prepping for showing this. So to go a little bit deeper into how the system works, I'm going to hand off to my partner in crime, Stephen Short.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1280.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1280)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1300.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1300)

### Version One Architecture: Custom RAG Solution for Toyota Vehicle Information

Hello everyone, good afternoon. My name is Stephen Short. I'm a senior engineer at Toyota Connected, and I'm the lead engineer on this project. Toyota Connected is an independent Toyota company that works as a software development shop and innovation hub.  Today I'm going to provide a technical dive on what we've built out for version one of our system, which, as Stephen said, is a RAG-based implementation, as well as where we are heading for version two, which is an agentic platform that we are building  within Amazon Bedrock Agent Corp.

To continue with the business context, what we've built here is an assistant, a platform that serves an assistant that allows for users to ask natural language queries to learn about Toyota vehicle information. You can think of this as information such as vehicle specifications, pricing details, trim options, and accessory options. We built this in-house as a custom RAG solution in order to meet our business needs. Some of the key capabilities of our platform are, one, we integrate with Toyota official backed data sources. We did not want to rely on the world knowledge of LLMs in order to represent our vehicles. Two, we provide accurate citation-backed responses. And three, we support all Toyota North America vehicles from model years 2023 through 2026.

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1360.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1360)

In order for us to get this to production, it  was really a team effort. We worked with many of our business units within TMNA, such as Enterprise AI, our legal teams, and our cybersecurity teams, in order to get this out to market. One of the key requirements that was provided to us from our legal teams is that any answer that the assistant sends needs to include contextually relevant disclaimers. The raw data from the sources that we are pulling the data from includes disclaimer codes as raw text, as you can see on the left here. The left is a segment of a large JSON object, which is our primary data source, and within it, it has a disclaimer code inline that we have to parse and then map it to an unaltered legal disclaimer text that must be shown whenever contextually relevant. There were many other requirements that we had to attend to as well, such as verbiage of how we talk about fuel economy or Toyota Safety Sense features.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1420.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1420)

And now I'm going to walk you through the architecture diagram for version one of our system.  To begin with, the front end will make a request to the Enterprise AI account. It will be routed through Route 53, which has a WAF attached to it, and then we will utilize Lambda Edge authentication in order to authenticate and authorize the incoming request utilizing Entra ID. Once this is authorized, then it will route to a piece of code that is deployed within ECS that we refer to as the intent router. The intent router's purpose is to identify which vehicle that the user is asking about so that we know which data to look up before we perform any execution or inference on the intent router.

We immediately route it to a solution that was built in-house by our cybersecurity team called PromptGuard. PromptGuard is a solution that is intended to help identify and block malicious activities such as prompt injection. Once this check clears, then the intent router performs the vehicle identification utilizing an external LLM. It will create a WebSocket connection with the front end. It will initialize a conversation and track that history utilizing DynamoDB.

After this is done, it will send the request over to Toyota Connected's main account. This will go over the internet, go through Cloudflare, which has a WAF attached to it, and hit our API gateway. From there it will route immediately to the Toyota Connected Shared Services account. This is an account that is maintained and supported by our cloud engineering team at Toyota Connected who helped us take our solution and help build this up and scale it up to meet our traffic numbers.

We have a RAG application code living within an EKS cluster inside of this account. We are also utilizing DataDog for our observability platform, and we will forward all logs to this and export those to DataDog. When we are performing the actual inference, the RAG application will call the data services from the account, the main account. It will first hit Amazon SageMaker. It will generate embeddings for not only the most recent query, but the previous five turns of the conversation and use a weighted average algorithm in order to provide more preference towards the most recent conversation and then keep those within the contextual window.

We'll take those embeddings and then we will perform a semantic search against our vector database which is hosted inside of OpenSearch Serverless. We will then retrieve using the semantic search 30 documents per vehicle that is asked about. We will use these documents as our primary source of truth, and then we will utilize Amazon Bedrock. We are utilizing the Anthropic models hosted there in order to drive the inference. In addition to our system prompt as well as the data that we retrieved, we are then able to begin generating an answer to meet our business requirements. We also do some post-processing on the streaming output, which I will dive into shortly.

Once the inference is actually completed, we will then push the message to an SQS which is picked up from a Lambda, which will take these logs and export them to MongoDB which we utilize to satisfy our compliance reporting requirements. From there we will begin buffering the output stream and post it back to TMNA on a webhook. This will then add it to the DynamoDB conversation history and post it back to the front-end client so that it is streamed. It is worth pointing out that from the Toyota Connected's perspective, the RAG application is completely stateless. All conversation management is handled by the Enterprise AI account.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1680.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1680)

### ETL Pipeline and Compliance: From JSON Data to Natural Language with Legal Guardrails

 In order to support this use case, we had to figure out a way to take this raw JSON data and then translate it into a usable format. The approach that we took was we landed on utilizing natural language summarizations for these JSON objects. You can see an example of a single chunk from a large JSON object that represents one feature of one of our vehicles. Inside of there, there are internal mappings for trim codes as well as MSRP information, descriptions, titles, and other various fields.

We take that and then we utilize Bedrock to generate natural language translations of this object that we then utilize in order to perform the semantic search. In addition to the processed fields, we also include the raw data that was, the processed field was generated from. This raw data is important for us to actually perform the citations workflows that I was mentioning previously.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1750.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1750)

And now I'm going to walk you all through  our ETL pipeline of how we ingest this data. We have a data account within Toyota Connected which utilizes AWS Step Functions to orchestrate a series of Glue scripts to perform the actual extract and transform portions of the ETL. The first script is the extract script. It will pull all of the supported vehicle data from our Toyota API servers and then push those into S3.

The second script is the transform script. This is the heaviest portion of our pipeline. In order to maximize throughput, we were able to parallelize this, and we are able to transform up to around 30 vehicles concurrently. This script will first chunk the JSON data and then second, it will do the translation that I mentioned before and generate natural language summarizations

using Amazon Bedrock. Afterwards, due to the nondeterministic nature of large language model output, it is up to us to ensure that these summarizations are accurate. Hence, we added data quality checks that verify the output of these summarizations and ensure the accuracy of critical pieces of information such as pricing details and trim availabilities. From there, another script will take these outputs and generate embeddings of the natural language summarization, then tie them together with the raw data and publish those into Amazon S3. From there, the extract and transform is complete, and now we need to move on to the load. This is facilitated via publishing an event to Amazon EventBridge.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1860.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1860)

 This then triggers a Lambda function on each of our development, staging, and production accounts. This Lambda will first grab necessary parameters from AWS Systems Manager that it utilizes to configure an OpenSearch ingest pipeline. From there, it will proceed to create a new index in our OpenSearch collection. This index is created based off of the timestamp of the start of the ETL pipeline. Then it will proceed to create the OpenSearch ingest pipeline itself, which is then configured to read from the output of the transform step on our data account S3 bucket. It will take that output and then ingest it into the newly created index.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/1930.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=1930)

Once this is completed, we stop the pipeline, and then the data is almost ready to be used. However, as I mentioned before, there's nondeterminism involved in every large language model call. In order to ensure the quality of our system and that the newly ingested data will meet our performance expectations,  we have created an evaluations engine. We have created and orchestrated a series of scripts utilizing GitLab runners. Our counterparts at Toyota Motor North America provided us with a golden set of data for one of our vehicles. This is a series of question and answer pairs that has been proven from one of the subject matter experts over at Toyota Motor North America.

We took this output and we utilized it as our golden data set in order to generate a synthetic test set. This synthetic test set can be generated for any of our vehicles. We will take that output and then use that output to invoke against our deployed RAG application. From there, we will take the output of the evaluation calls and actually perform, utilizing a council of large language models approach, to understand and gauge against a series of metrics that we have defined to measure the performance of our system and the quality of data. Once this is done, then we have an understanding of how well the new data set has performed, and we can switch our index alias, which is how our RAG application knows which data set to utilize, and point it to the newly created index. This allows for us to swap to a new data version without any downtime.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2020.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2020)

Beyond evaluations, it is also  important for us to ensure that we are meeting the compliance guidelines that were provided to us from our partners, from our legal teams at Toyota Motor North America. They provided us with a set of guidelines of how the assistant should answer questions and how it should generally behave. The way that we do this is we take incoming question and answer pairs and then we utilize Amazon Bedrock to first categorize the incoming question to understand what the user was asking about, and then second, measure how well the response measured up and complied with these guidelines. We will then ingest this compliance status to MongoDB, which we utilize as the primary backend database for our compliance reporting engine.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2080.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2080)

You can see a snapshot of the front end that we've created to support this, and we share access to this with our friends from legal in order for them to understand how the system is performing in production.  On the topic of compliance, one thing I mentioned before was how we have to handle disclaimers. In order for us to provide disclaimers that are contextually relevant to the output, we have to ensure that our disclaimers text is treated as immutable data. In addition to the disclaimers text, we also support showing vehicle images, which also includes the image URLs and the image metadata.

The image URLs and the image metadata must not be altered by the LLM. The way that we approached solving this problem is we took a stream splitting approach. We provided instructions and provided many examples to our system prompt to utilize and leverage in-context learning to split the stream into three distinct segments.

The first is the main output stream. This is what gets streamed and shown directly on the front ends. The second is a list of disclaimer codes that the LLM will utilize as citations and then populate the disclaimer codes that it believes should be surfaced as part of its inference. And then the third, the images follow the same principle. Any images that need to be shown to the end user as part of this answer are published. We collect image IDs, and then after this is done, we will take the list of codes and then the list of image IDs and then utilize a mapping that we have available to our service to send this up and post it onto the stream without ever being touched by the LLM.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2200.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2200)

An example of how we do this is shown in the code on the right. We are essentially going through the output stream of the invoke model with a response stream, and we are looking for specific delimiters for either the disclaimers case or the images case and then switching the state of the output accordingly. 

### Version Two Evolution: Building an Extensible Agentic Platform with Amazon Bedrock Agent Core

As we built out version one, we saw that there were many opportunities for how we wanted to expand our services. First, we wanted to onboard additional datasets and additional clients. There are many use cases that we could pivot into. And we also wanted to reduce the reliance on our ETL pipelines, as you can see there's actually a lot going on for us to do this ETL, and this was compounded by the fact that every year when Toyota is rolling out a new model year, the frequency of changes in this upstream data source is very rapid.

Every time that the data changes, we would need to do the entire ingestion and then rerun the evaluation strategy. This data staleness issue was a real problem for us and something that we saw as an opportunity for us to improve on. In addition to this, the business started to have appetite for actions that were beyond the capabilities of what our version one assistant was able to perform, such as allowing users to ask and look for local availability from their local dealership for Toyota vehicles.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2280.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2280)

As part of these opportunities,  we started to perform research into agentic platforms. Our early experimentations utilizing the Claude SDK and creating our own agents and MCP servers revealed to us a key finding. We actually can utilize, due to the advancements of both MCP and LLMs, the ability to connect our agents directly to the data and then remove the support. We're able to drop our ETL pipeline and traditional RAG system entirely, and then that allows us to avoid the data staleness issues while giving flexibility to support actions and advanced reasoning.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2320.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2320)

 However, while this was easy for us to put together proofs of concepts and demo this, actually planning a system and agentic platform and getting this to production is difficult. Agents bring a lot of complexities, as well as MCP servers, creating sane authentication and authorization strategies, auto-scaling configurations to support the expected traffic loads, and then guaranteeing context and session isolation is a difficult task.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2360.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2360)

And so to reiterate some of the platform goals of what we are looking to build for version two of the system,  we are building an extensible agentic platform. We are going to be enabling actions, performing enhanced authentication and session isolation to address the complexities of a multi-agent system with MCP servers. And we are going to be doing this to support increased traffic numbers as well as having lower infrastructure overhead for our team to maintain.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2400.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2400)

Our platform target for version two is we are looking to build a series of Claude agents and MCP servers leveraging Amazon Bedrock agent core services. Our early experiments in Agent Core have been great.  The services are actually built out to address many of the concerns that I had of getting our system from demo to production.

Agent Core Runtime is a Firecracker VM-based solution that provides isolation by default. It's scalable as a serverless technology, and there's low infrastructure overhead for us to build these. Agent Core Identity will tackle the complexities of identity and inbound and outbound authentication within a multi-agent and multi-MCP system. Agent Core Memory allows us to simplify the conversation management pattern as well as provide some interesting novel use cases that we can tie into our agents.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2470.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2470)

For MCP servers, there are two options here. You can utilize Agent Core Gateway, which is a fantastic service. If you need additional customization beyond that, such as response caching, what I found is that we can actually deploy the MCP servers into Agent Core Runtime and couple them with memory, which I'll dive into  shortly.

Now I'm going to walk you through our target infrastructure diagram for version two of our system. As you can see on the left, much of the stack from the Enterprise AI side is similar. There are two key pieces that I want to point out, however. First is that we are looking to replace the intent router with an orchestrator. This orchestrator will integrate with an agent registry that Toyota Connected will be building. This agent registry, you can think of it as a mapping of authenticated clients to which agents are available to each calling client.

Utilizing this registry, when a front-end request comes in and hits the orchestrator, the orchestrator will know what agents are available for the client, and then it will route it to the appropriate agent. We are also looking to remove the external LLM calls that we utilize inside of the intent router in version one and utilize Amazon Bedrock calls for that orchestrator in version two.

On the right in the Toyota Connected infrastructure, you can see I have a couple of agents: the Product Expert Agent and the Product Support Agent. Both of these are going to be Strands agents that are deployed within Agent Core Runtime. These agents are going to be coupled with MCP servers that provide tools that are necessary for them in order to do their job.

We have a couple of options for MCP servers. As I mentioned, we believe that Agent Core Gateway is going to be a perfect fit for our Product Support MCP server. To back up a little bit to explain the differences between the agents, the Product Expert Agent, you can think of it as basically identifying the version one capabilities of our platform. That's the purpose of that agent. The Product Support Agent is a separate agent that is intended to help service customer inquiries about their vehicle.

We believe the Product Support MCP server will be a good fit for Agent Core Gateway. The Product Expert MCP server, however, it is on us to ensure that we are responsible consumers of the Toyota APIs that drive the data for this agent. As such, we must implement response caching. It's a hard requirement. We have to make sure that we do it. I found via some experiments last week actually that we can utilize Agent Core Memory in order to achieve response caching, and I'm going to provide an example of how we can utilize memory as a distributed cache. It looks like it will work quite well actually.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2670.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2670)

In addition to these Agent Core services, we are looking at utilizing the other Agent Core services such as Agent Core Observability. It supports OpenTelemetry by default, so we will be passing these along and forwarding the logs to our Datadog instance. We will be utilizing Agent Core Identity to integrate with our chosen identity provider, and we will  continue to support our compliance analysis workflows.

In order for us to perform response caching, one of our experiments that we've been doing is, I'll walk you through it. We have our Product Expert Agent which will be making a tool call to the corresponding MCP server. From there we will first check to ensure whether or not this exists in the cache. We are utilizing Agent Core Memory event metadata to accomplish this. If there is a cache hit, it will pull the information from the event metadata and then pass it up to the calling agent.

On a cache miss, we will be going out and making the call to the external APIs and then caching them within the response cache. On the right you can see some code that I've created that is a decorator which we can apply to any tool call within our MCP server. This is a decorator that essentially takes the function signature as well as the parameters that are passed in the invocation, concatenates them together, and then creates a hashing key utilizing SHA-256. Once we have this caching key, we can then make a lookup within Agent Core Memory.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2750.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2750)

 I wanted to touch on this because this is an important piece of information. Utilizing the Agent Core Bedrock SDK for Agent Core Memory, it exposes a high-level client. This high-level client has a list of events function on it. However, the high-level client does not have the capability to filter based off of event metadata. However, the low-level client does, and you can access the low-level client directly utilizing the code that I have on the bottom. This code will invoke the GMDP client list events function, which is identical with additional functionality to support this metadata filtering option.

On the top you can see how we utilize the metadata filter and construct that to query on event metadata only if the cache key matches. This allows us to achieve our goal of utilizing Agent Core Memory as a response cache. It's also worth pointing out that in order for Agent Core Memory to be shared across different sessions of the actual MCP server, it is important to note that you have to configure a couple of key pieces of information. I believe it's the actor ID and the session ID or the agent ID. I can provide that afterwards. Anyways, once you have that and you statically code those lookup variables, then this memory can actually act as a shared cache across any agents.

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/d7b270b2ab904686/2850.jpg)](https://www.youtube.com/watch?v=lp_tXtr_aL4&t=2850)

 To walk away with some of the key findings that we've had from version two of our product APIs, Agent Core services are great. They're going to enable us to build an agentic platform that doesn't bring aboard all of the infrastructure overhead complexities that we were anticipating when pivoting into an agentic future. Also, Agent Core, utilizing the combination of agents and MCP, will allow us to avoid the data staleness issues that we have today in version one. We are targeting our launch in Q1 of 2026, and thank you everybody for attending.


----

; This article is entirely auto-generated using Amazon Bedrock.
