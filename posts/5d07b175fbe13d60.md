---
title: 'AWS re:Invent 2025 - A complete guide to Amazon EVS: Unlock AWS scale for VMware workloads (MAM201)'
published: true
description: 'In this video, AWS introduces Amazon Elastic VMware Service (EVS), which enables customers to run VMware Cloud Foundation workloads directly in AWS VPCs on bare metal i4i.metal EC2 instances. The presenters explain the "Seven Rs" migration strategies and emphasize that EVS allows lift-and-shift migrations while maintaining existing people, processes, and technology. Key features include full administrative access, bring-your-own-subscription licensing, deployment via Cloud Builder in approximately three hours, and integration with AWS services like FSX for NetApp ONTAP and VPC Route Server for BGP routing. NYU Langone Health shares their success story, reducing migration time from 320 days to weeks by leveraging EVS for business continuity while maintaining operational control and security requirements. The service supports four to sixteen hosts per environment across twelve AWS regions, with pricing based on EC2 instance costs, VPC Route Server endpoints, and control plane fees.'
tags: ''
series: ''
canonical_url: null
id: 3085173
date: '2025-12-05T03:35:48Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - A complete guide to Amazon EVS: Unlock AWS scale for VMware workloads (MAM201)**

> In this video, AWS introduces Amazon Elastic VMware Service (EVS), which enables customers to run VMware Cloud Foundation workloads directly in AWS VPCs on bare metal i4i.metal EC2 instances. The presenters explain the "Seven Rs" migration strategies and emphasize that EVS allows lift-and-shift migrations while maintaining existing people, processes, and technology. Key features include full administrative access, bring-your-own-subscription licensing, deployment via Cloud Builder in approximately three hours, and integration with AWS services like FSX for NetApp ONTAP and VPC Route Server for BGP routing. NYU Langone Health shares their success story, reducing migration time from 320 days to weeks by leveraging EVS for business continuity while maintaining operational control and security requirements. The service supports four to sixteen hosts per environment across twelve AWS regions, with pricing based on EC2 instance costs, VPC Route Server endpoints, and control plane fees.

{% youtube https://www.youtube.com/watch?v=d0TLechcV74 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/0.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=0)

### The Moving Metaphor: Understanding Cloud Migration Complexity

 All right, to get started, who hates raising their hands in sessions? All right, cool. Now that we got that out of the way, how many of you have moved at least 5 times in your life? All right, how many have moved 10 times in your life? Oh, all right, so get a load of this. You're going toâ€”this is the one thing you're going to like. Next week you're going to be like, "Man, the one thing I remember from re:Invent is this right here." Ready? All right, the average person between age 18 and 45 in the United States has moved 11 times. I kid you not. You're going to wake up a week from now and you're going to be like, "This bald bearded guy told me that people moved 11 times in their life."

Why do people stop moving so much after age 45? Yeah, it's painful. Your lifestyle changes. You don't have time for this. Moving can be a real pain. One more move for sure. So when we started our planning for this session, Aarthi and I were like, "You know, we really like these sessions to be fun. We'd like to figure out some way to actually make this engaging." So what do we do? We reach out to our product marketing person. I wanted to share with you some behind the scenes of how this actually works.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/100.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=100)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/110.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=110)

So Aarthi emails Bianca, our amazing teammate who works in product marketing, and she's like, "Hey, can we get a moving truck up next to the stage? We've brought trucks on stage in the past. Could we do that?" Bianca's response was like, "For real?" And we're like, "Yeah, you know, don't worry, we won't drive it. We'll just sit there. Maybe we can get some moving boxes." And you know, she was not reassured on this. All right, so we couldn't get moving trucks. We couldn't get boxes.  What do we do? Next best thing. We used AI. About 10 prompts later, I was able to successfully generate an image that I thought was great.  A huge thank you to my wife for telling me what a boho aesthetic is. I would not have known that. And my children for saying that this high-key slays. So I think we have a pretty good image here.

So the question you're all asking is, why in the world am I looking at an AI-generated image of a couch? The thing I want to point out here is when you're moving, the way that we move this couch versus the way that we move the books in the background versus the way we move the lamps or the plantsâ€”they're all different. It's not one size fits all for how we move all of it. The new house you're moving into may not have amazing AI-generated bookshelves in the background to put those AI-generated books on. They may have a different layout. The house may have a different number of rooms, but it's all different when you're moving from point A to point B and trying to figure that out.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/190.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=190)

Typically it's pretty easy to actually move the couch. I say that not having moved the couch recently, but it is pretty easy to move that. It's a big fixed item. You can just lift it and shift it into the new location. Whereas the books, a lot of the knickknacks, the smaller itemsâ€”figuring out where all of those go in the new houseâ€”that part can take a little bit of time. So as we talk about moving, it's the same thing that really happens when we're talking about moving our applications and moving our workloads from an on-premises  environment into the cloud. One thing that we really think about is that there is not a single path. There is no single way to move everything from on-premises into the cloud that makes sense for everybody.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/220.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=220)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/230.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=230)

All of you may be at a completely different place in your cloud journey. You may have completely different types of workloads that need to move in completely different ways. Yes, picking up a VM and moving it from point A to point Bâ€”that part can work pretty well. It's pretty straightforward. But you may have other applications that are better suited to run on Spot instances within EC2  or even different sizes or different capabilities within those EC2 instances. You may have cases where you can move into a serverless architecture.  You could leverage Lambda. You could leverage EKS. You could use RDS and move it into managed services.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/240.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=240)

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/260.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=260)

The thing here is that we talk about customers not being one size fits all. There is noâ€”we say this customer is going to lift and shift. That's really not the case. This customer may use lift and shift for certain applications, or they may be using different methods as they go along.  When we talk about these methods, if any of you have ever seen the AWS articles about the Seven Rs, there are different methods for actually relocating. A lot of the methods I showed here fit into the Seven Rs.  You can re-platform. You can refactor. You can re-host. You can relocate. There's a lot of different ways that you can go about moving these applications from point A to point B.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/270.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=270)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/290.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=290)

### Introducing Amazon Elastic VMware Service: VMware in Your AWS VPC

And even the point B may be different in many cases. Point B may still be on-premises. You may be able to use those workloads or run those workloads within something like an AWS Outpost, which can still run in your on-premises environment.  So let's talk about one way to do that, and that's lifting and shifting from an on-premises VMware environment  into a cloud-based VMware environment. So earlier this year we launched the Amazon Elastic VMware Service. The Elastic VMware Service is the ability to take and run VMware-based workloads, the VCF stack, directly in your Amazon AWS VPC.

Just like how you would run an EC2 instance within your VPC and connected to the environment, you can now run the VMware stack directly on AWS bare metal infrastructure sitting within that environment. This is the same VMware stack that you would run in your on-premises environment. It is not something special or a modified version. We literally take the commercial off-the-shelf capabilities and software and run that within AWS VPCs so that you are able to move from point A to point B without having to change the people skills that you have, the processes that your teams operate, or the technology that they use, whether that be third-party solutions or scripts.

From a basic feature perspective, we run the standard VCF stack. We take Cloud Builder, which is part of the VCF stack, and gather a bunch of information from you up front. We ask you to tell us about the underlay networks that you want to run and the number of hosts that you want in the environment. We collect this information from you up front through the AWS console, take that information, bootstrap the bare metal instances, and push the configuration into Cloud Builder to let Cloud Builder deploy the VMware stack. In VCF, it is basically just like how you would take those old school CDs that used to have preconfigured configuration for VMware and pop it in and the server brings itself up. It is very similar to that in the cloud world where you give us a bunch of information ahead of time. We take that, bootstrap the environment, pass the information to Cloud Builder, it runs and configures the environment, and we hand you back the credentials so you can log in and you are running VMware on EC2 instances within your VPC.

You can interface with this through both the AWS console and the API. If you were to go into your AWS console and look for the EVS service, you would actually find that service and could go in and deploy an environment and get started today. We are running on i4i.metal EC2 instances. This is bare metal, not nested virtualization or any kind of virtualization on virtualization or special hardware sitting off in the corner of our data centers. This is standard EC2 bare metal infrastructure with the i4i.metal instances. Today we run VCF 5.2.1, and you have access to SDDC Manager, vCenter, and all the normal VMware tools that you would run on-premises. You can access those in EVS.

A really simple way of thinking about this is that it is almost like using AWS bare metal infrastructure like a co-location environment, but you are able to dynamically scale up and down those servers with the click of a button as opposed to ordering more hardware. You have full administrative rights. We do not run this as a managed service. This is completely controlled by you, the customer, just like if you were doing it in an on-premises environment. You have full admin access and can do just about anything in this environment that you could do in an on-premises environment.

From a consumption perspective, you have the ability to deploy instances using Savings Plans. If you wanted to commit at a one-year interval or consume on-demand or even three-year intervals, you have flexibility and choice as far as how you consume those instances. We operate in a bring-your-own-subscription model. Going forward across all cloud providers running VMware, it is a bring-your-own-subscription model. You can take the existing VCF subscriptions that you have on-premises, bring those with you, and apply and use those in the EVS environment.

You have the choice to run these yourself or engage a partner. We work with a lot of partners who have built managed services where they can come in and operate this for you in AWS just like you would do in an on-premises environment. From a scale perspective, we launched EVS back in August and had six regions available. We are now up to twelve regions and continuing to launch more regions later this year as well as going into next year. You will continue to see us work on getting EVS launched into all AWS regions over the next few months.

This customer-managed versus partner-managed distinction is a big deal for a lot of customers who are moving to the cloud and have had on-premises managed service provider partners that they were using who are no longer part of Broadcom's managed service provider program. In that case, you are able to move into EVS and leverage third-party partners who are still in that program. They have built up a lot of expertise with running in EC2 environments inside of AWS and running in VMware environments, so there are a lot of partners that you can leverage to do that or you have the choice to do it yourself.

The big thing here is that there are a lot of offerings out there that force you into a managed model. They force you to say that you do not have full administrative access and you are operating in a delegated model. In our case, we have built this as a customer-managed solution first, so by default you have access to everything, and then you can bring in a partner if you would like them to run and operate the environment on your behalf.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/610.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=610)

### From Isolated Infrastructure to a Vibrant Cloud Ecosystem

Quickly mentioning some of those partners, it's not just the managed service provider partners  that we have here shown at the top, but also on the ISP side. A lot of the tools that you use in your on-premises data centers will operate just the same way in Amazon EVS. Think of things like NetApp or Pure Storage or Beam. A common thing we used to hear from customers is, "When I moved to the cloud, it's not just about moving the workloads. I have to now suddenly reevaluate all these solutions that I'm usingâ€”my backup provider, my DR provider, my security toolsâ€”all these things that I've built and integrated into my environment. I now have to reevaluate how I'm going to do those in the cloud."

[![Thumbnail 660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/660.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=660)

When you lift and shift with Amazon EVS, pick up those workloads and move them from VMware to VMware, you can keep the people, the processes, and the technology exactly the same as what you had on premises because you're still using a VMware environment. Going back to that moving metaphor that we talked about in the beginning,  imagine you have a house and your house is kind of out in the country a little bit and you have a whole bunch of land, so you have a whole bunch of space available where if you wanted to build things you could. You could build a bakery, you could build a store, you could build whatever you wanted. You have tons of space, but ultimately you have to build it yourself.

Now think about moving into a destination where you're part of a vibrant village, a community that has all of these things already in there. You already have a baker, you already have a grocer, there's a hardware shop. All of these things exist in this village, and now it's walkable. You can just walk over and interface with these things without having to go and build it yourself. That's really the way you think about scaling and operating in a VMware environment in AWS. You're taking the same house, the same floor plan, everything that you had before, and you're able to move that into a community that now has all of these other capabilities.

So you can take this workload that you were running in VMware and now it's sitting in your VPC. You have RDS that you're running in your VPC. You have S3. You have EFS. You have all these other cloud capabilities that are now sitting co-resident in the same environment with your VMware-based workloads, and we see a lot of customers do this. They'll take a database, for example, that's running on a VM inside of VMware, and they may move that over to RDS, but the application itself is still running in VMware on a VM.

If the application is really hard to transform or modernize or change, we are totally fine with being able to have you run that workload in VMware. In fact, if you have workloads that you plan on keeping in VMware, we want to be the best place in the world for you to run those VMware-based workloads. If when you move that into the cloud you want to expand and extend that application to be able to interface with other AWS workloads, we want you to have that capability as well.

### Three Key Building Blocks: Compute, Storage, and Networking

I'm going to hand it over to Aarti, and she's going to talk a little bit about the building blocks of Amazon EVS. Thank you, Andy. While our product marketing person was not able to get us the couch or the truck, we got a good Monday session that I requested for. The primary reason is what I'll be doing in the next few minutes is talking about the three key building blocks that make up the service. This is an introductory level session, and then we have some great deeper dives throughout the week that you can go in and plug into as you want to learn more about the storage or networking, and so on.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/810.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=810)

He touched upon this. I want to spend a couple of minutes here.  When Amazon EVS deploys the VCF stack, we get inputs from you. If you've deployed a Cloud Builder on VCF stack on premises, you would see that the Cloud Builder requires certain inputs, and that's all the service is doing. We collect all these inputs from you and take care of the deployment. It is a fully self-managed service minus the initial deployment. We wanted to remove that deployment from you and take care of it on behalf of you.

So when the stack is deployed, you get all these components deployed from SDDC Manager to the ESXI hypervisor to storage to networking, but we also take care and as you'll see when we talk about the building blocks, the underlying connectivity and storage options for you depending on your use case and your application needs when we deploy the stack. We deploy it in a single consolidated domain, but of course there's flexibility for you as you deploy more hosts and you can bring up additional clusters. You can do multiple workload domains.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/880.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=880)

The Amazon EVS service deploys the environment  for you in your AWS account in a VPC that you choose. So obviously from a security standpoint, the AWS shared responsibility model is still applicable. Where it varies is we as AWS are responsible for the underlying AWS components that you see here, so this means from a region availability standpoint or an availability zone standpoint we take care of that. We also take care of capacity for you.

So you place in a capacity request reservation saying you need 10 hosts by a specific date, and we enable that capacity into your account. But anything upward of the hypervisor, the ESXi stack and above, is your responsibility. This means you're responsible for patching your environment and responsible for updates.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/930.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/940.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=940)

There are three key building blocks that I've been mentioning. The first one is obviously compute. We started with i4i.metal  as our first instance type. This is a bare metal instance where we deploy the environment on. We have a good history with i4i.metal. For some of you who are familiar with VMware Cloud on AWS, the managed service, we utilized i4i.metal. We've received good feedback from customers across various use cases of their applications. These instances also go through full certification with Broadcom. So today if you go into the Broadcom compatibility guide, you will see Amazon i4i.metal listed as a certified instance to run VCF 5.2.1.  We do go through the entire service certification, storage and network certification with Broadcom.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1010.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1010)

Our goal was to launch with one instance type, and our goal is eventually to get every metal instance that's available on the AWS portfolio available for you so you can choose based on your needs. For example, if you have a high memory workload, you need more memory versus compute. That's our goal and it's on our roadmap. Regarding scaling, if you think about how you scale this environment that we deploy, think of the EVS stack that we deploy as the environment as a top level construct.  Within the environment, we take care of that first cluster you see here. It's a four host minimum. This four host minimum comes from the VCF requirement that we have, and every environment can have up to 16 hosts. We've tested up to 16 hosts and are satisfied with the performance, and we are slowly expanding this number of hosts soon as well.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1060.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1060)

You could have up to 16 hosts, and you could have this as multiple clusters as well within that environment. If you have a requirement to scale beyond 16 hosts today, you could obviously create multiple environments, and these environments can communicate to each other using a transit gateway. The next building block is storage.  The i4i.metal instances come with 30 tebibytes of local NVMe storage, which is what we use for the primary storage, which is vSAN in this case. We are using the ESA, the express storage architecture, which gives better performance compared to the OSA architecture. vSAN forms the primary storage that is available.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1100.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1100)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1130.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1130)

We've also had feedback from customers saying they want to make sure this is consistent with their on-premises experience. If you're doing storage policy-based management on premises, you want to control what your RAID levels are or what your failure tolerance is. All that's still possible. You could log in to your vCenter console once the environment is deployed for you, and you can configure these as you would do on premises.  One of the feedback we've received when we started running private preview and then moved into public preview is scalability on storage. Not a lot of workloads need that compute but are more storage intensive. So we've partnered with our internal service, Amazon FSX for NetApp ONTAP, as an additional storage option.  The primary storage is still vSAN, but you could create additional data stores using NetApp ONTAP.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1170.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1170)

Andy talked about how you have full administrative access to this EVS environment, which means if you're using ISV solutions on premises like Veeam or Pure Storage Cloud, for example, you have the ability to integrate with these storage vendors as well, depending on your application needs. If you take a look at how this fits with FSX for NetApp ONTAP, the primary storage is still our vSAN data store.  You could create an additional file system and mount it to your ESXi host, and that could be either using NFS or iSCSI depending on the protocol, and it becomes your additional storage. For customers who do not want to just increase the number of hosts primarily because of storage reasons, you could have your minimum number of hosts and then work with either ISV vendors or FSX for NetApp ONTAP to increase the storage.

### Network Architecture and Pricing Model for Amazon EVS

How does this look like? If you take a look at this diagram here, customer VPC is your VPC. Let me walk you through this diagram.

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1210.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1210)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1230.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1230)

The Amazon EVS environment gets deployed  inside this VPC. You can create a file share in FSX for NetApp ONTAP within your AWS account. As long as you enable the right security group permissions, you have access to these file systems using NFS. The same architecture is available for iSCSI as well, where you create the LUNs  and configure security group permissions to enable traffic in both directions.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1240.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1240)

The third building block is networking.  We have done significant work in this area. When we deploy the environment, we take care of both storage and networking. Traditionally, AWS has always been a flat Layer 3 network with a VPC, CIDR block, and subnets. As we built Amazon EVS, we wanted to bring the VLAN subnet concept into the VPC. We have taken care of that for you. We manage the overlay networks and underlay networks as part of the deployment, so you do not have to worry about it. All you need to provide us is input on what your appliance IP address spacing should be.

A key part of this is leveraging existing AWS features. As you think about your Amazon EVS environment, you probably want to connect on-premises to your VMware environment or maintain a hybrid environment. We use the existing AWS Route Server functionality, which acts as a BGP propagation mechanism. With many underlay IPs, overlay IPs, VM IP spaces, and appliance IPs, the Route Server enables BGP communication. It acts as a BGP endpoint between your NSX Edge and all your route tables, propagating your route table so you do not have to worry about failovers or manually adding static routes into your route table.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1290.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1290)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1350.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1350)

Looking at the full external connectivity,  you can have multiple Amazon EVS environments within a region as part of your VPC. You can also have other workloads running within the same VPC. All of this connects into a transit gateway, and from the transit gateway, you can have either a Direct Connect or a VPN connection back to your on-premises network. This enables full end-to-end connectivity. 

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1380.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1380)

Let me quickly touch upon pricing.  We operate in a bring your own license or bring your own subscription model. For AWS, there are three cost dimensions. The first is the actual instance cost, which is the EC2 instance you are running. There is a four-host minimum, and all your existing savings plans and reserved instance concepts still apply. This is a standard EC2 instance with nothing out of the ordinary. The second cost is for the VPC Route Server endpoints. You require two VPC Route Server endpoints per environment, so there is an associated cost for this. Finally, there is the service control plane fee itself, which covers the deployment and the underlying connectivity between your appliances and the VPC space. There is an hourly fee billed for this environment. These are the three dimensions you will incur as part of an Amazon EVS pricing model.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1450.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1450)

### NYU Langone Health's Journey: From Months to Weeks in Cloud Migration

With that, I will pause. It is always good to hear from customers. We have been working with Jignesh from NYU Langone for the last several months.  I would love to invite Jignesh to share their journey with you. Thank you, Aarti. Hello, everyone. I am Jignesh Shah, Manager of Cloud Engineering at NYU Langone Health. I would like to start with a number: 320. That is the average number of days it takes for an enterprise application to migrate to the cloud. I will be talking about how NYU was able to bring that number down.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1490.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1490)

A little bit about who we are:  we are a large academic healthcare system based in New York City. Our mission is to deliver world-class patient care, advance medical education, and drive innovation in research. We consider the needs of our patients, families, and our society. This includes patient care, research, and the NYU Grossman School of Medicine.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1530.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1530)

The medical school is free if you can manage to get in. The three pillarsâ€”patient care, research, and the medical schoolâ€”are really powered by world-class technology that we provide to support them with the help of partners like AWS. 

Some key facts and statistics: we have been number one for quite some time for the best outcomes and are constantly ranked at the top for several subspecialties, whether it's neurosurgery, orthopedic, or pediatric. We have about five large campuses in the five boroughs, along with close to 400 outpatient centers. We see about two million outpatient visits every day, and in terms of number of employees, we have close to about 53,000 in total. So quite a large academic medical center.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/1590.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=1590)

Our adoption of Amazon EVS came about a few months ago. I was watching Monday Night Football with my sonâ€”I think it was Giants versus Eaglesâ€”and I got a phone call from my boss saying we have to go from taking months to just weeks to migrate our applications. My first thought was to tell my boss that he was crazy, that it's impossible. But the next morning, we huddled. I called my team, which includes smart cloud architects, security analysts, and SRE engineers. I challenged them to find a solution to accelerate our journey to the cloud. 

Being a VMware shop on-premises, it was a natural transition to go to something like VMware Cloud Foundation in one of the hyperscalers. So we started contacting different hyperscalers and came up with comprehensive success criteriaâ€”really detailed success criteria all the way from security to chaos engineering. We had everything in it. A few months later, we had a breakthrough with Amazon EVS.

Our business case was to accelerate migration to the cloud. How can we go from months to just a few weeks? We also wanted to leverage the cloud for business continuity. Being a healthcare system, our uptime is absolutely crucial, so business continuity was one of the biggest use cases. Some of the business requirements we had were to maintain control. Like Arti and Andy mentioned, we get administrative control over the vSphere clusters running in Amazon EVS. We can bring our operational cadenceâ€”how we patch, how we monitor, our observability.

Security was another key requirement. We were able to extend what we do on-premises to the cloud and make it more of a hybrid environment. We also wanted a flexible licensing model. One of our requirements was to bring your own license, so we were able to bring our existing VMware Cloud Foundation license into Amazon EVS. Multilayer security was important because we are able to run in our own VPC. Not only could we use our existing security guardrails like SCPs and RCPs, but we could also extend our NSX security to the cloud. It was really transformative when we saw that was possible.

Minimizing staff turnover was one of the requirements that our management had. The requirements our management had were that we did not want people leaving us, we wanted to leverage existing skill sets and then slowly upscale as needed. That was one requirement, and Amazon EVS made that happen. Obviously, reducing capital expenditure was another key requirement. We wanted to move from a capital expenditure model to an operational expenditure model, so we would not have to spend money buying hardware and servers or leasing data centers. Amazon EVS becomes an OpEx model that is billed hourly, which allows us to manage costs effectively.

Why did we choose Amazon EVS? As I mentioned, we are able to run this in our own VPC. Some of the other competing solutions tied us into a vendor-managed VPC, which did not give us enough visibility or control over what was happening behind the scenes and what we could do. We are big into provisioning everything as code, so we wanted to leverage our existing CI/CD pipelines for anything that we deploy in the cloud, and Amazon EVS had that offering.

We talked about bringing our own VMware Cloud Foundation license, starting with a pilot light deployment. A great feature of Amazon EVS is that we can start with a small pilot light cluster and then add nodes as needed. You can start with as small as a four-node cluster and then add and scale up as you migrate applications or put disaster recovery workloads into it on demand. We could scale out the number of nodes in the cluster in the event of a disaster.

Growing our ISP partners was important because we are a heterogeneous environment on-premises. It was important for us to maintain the partner ecosystem that we use on-premises, whether it is pure storage or FSX on NetApp. We wanted to keep our partner ecosystem intact and have as little changes as possible when we move the applications. Automated security provisioning was another benefit. We are able to continue using some of our security tools that monitor communications and traffic and take action when they see an anomaly or threat detection. So we were able to leverage and continue automating security.

The last benefit is one flat rate per node. We are paying per node, not per VM or per EC2 instance. We are able to run as many VMs as we wish on a single Amazon EVS node, and that was really useful to our use case. Although this was a successful journey, we did run into some bumps in the road. With the help of AWS support, the account team, and our OEM partners, we were able to quickly overcome some of those hurdles.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2030.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2030)



One challenge was operationalizing Amazon EVS. We had to figure out a way and build a custom module to integrate Amazon EVS availability and performance with our service management back on-premises. Things like monitoring FSX NetApp performance or integrating Amazon EVS with our ticketing system required a learning curve, and it took some time to really operationalize Amazon EVS. Another issue we had was figuring out network routing between the Amazon EVS overlay and the VPC underlay. Because we have a sophisticated network topology, it took us time to work through this.

We had to work through routing and put policies in place to get the overlay VMs working with the underlay services such as EC2, as well as the on-premises applications. Data ingress cost was another area where we encountered a surprise. AWS typically says data in is free and data out will cost you money, but in this case we did see some ingress cost. However, we were able to work with our AWS account team and bring that cost under control.

CloudFormation presented a learning curve for the NYU team since we are not a CloudFormation shop. We had to learn how to use CloudFormation to provision EVS. Looking ahead, today EVS supports a single Availability Zone, so we are looking forward to multi-AZ support. They already have multi-region support available. We are also looking forward to them inviting and certifying more ISV partners as this ecosystem grows, and we are looking towards Terraform support for EVS.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2200.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2200)



[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2220.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2220)

### Live Demonstration: Deploying Amazon EVS Through the AWS Console

I appreciate you sharing your journey, Jignesh. We will jump into a quick demo where I will walk you through the actual deployment experience.  This is the landing page in the AWS console. When you go into get started, there are a few things here before deployment. We want to make sure we collect a lot of information from you. Business level support is the minimum required as part of having EVS. We also want to make sure that as you think about how to get started, there are prerequisites, so we provide you all this information in terms of what prerequisites are required. This helps you with planning because most customers we have talked to have separate on-premises teams and cloud teams. There is a separate network team that handles IP address spacing, so it is important to understand what these prerequisites are.

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2260.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2260)



[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2280.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2280)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2300.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2300)

EVS also interacts with compute, network, and storage. It spans across other AWS services, so we have provided links to other services that you will interact with. When you go into environments and create an environment,  you will have to give a name for your environment. This is where we collect some basic information. We are starting with VCF version 5.2.1, and then as part of your agreement with Broadcom, you would have a site ID. You give in a site ID and provide the solution key and the vSAN license key. It is important that we do checks for this solution key and license key.  What this means is you want to make sure that if you are deploying a four-node environment, you have enough cores to support what you have in this key.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2330.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2330)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2340.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2350.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2350)

Once you have given the solution key and license key, the next page is where you deploy the host itself.  These are the EC2 instances, which is the compute that I was talking about. The initial cluster, as Jignesh mentioned, is a four-host minimum, and this is all deployed in a single Availability Zone.  You do have other options for choosing placement group settings, but you can go ahead and create all four hosts here. Then we go into the actual network configuration.  We launched this feature very recently for customers that are familiar. HCX enables you with migration of your VMs from on-premises into EVS, and you could do it over private connectivity either using a Direct Connect or using internet connectivity. Both these options are available for you.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2400.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2400)

This is where you tell us which VPC you want the EVS environment to be in. You select your VPC, and then you select what we call a service access subnet. EVS deploys into your account, so we will have service access into your particular subnet that you choose and what security group that you select.  This is a subnet that we will use, and this model is very typical to other AWS services like the Network Load Balancer or RDS, for example.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2410.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2420.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2420)

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2440.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2450.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2450)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2470.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2470)

Next comes the actual VCF connectivity  options. We deploy these stacks, which means we also take care of deploying these management appliances for you. What we do, and this is part of the prerequisite from an IP  planning perspective, is ask you to provide us with what CIDR blocks are required so we can use that as part of the Cloud Builder deployment. Once you have all those IP addresses, as you can see here, I've prefilled some of these things. This is where you select what route server you would need to create, and you'll need to tell us which route server peering to use.  You can select the route server peerings here and then proceed to the next step.  The next page covers standard DNS hostnames. You can either use Route 53 or your on-premises DNS. We ask for DNS hostnames for your appliances. Once you have your DNS hostnames, the last page is just tags.  You'll have the ability to review all these settings. It's important to note that some of those IP addresses for those appliances cannot be edited later, so it's important to ensure you're filling in the right information as you go through this process.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2520.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2520)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2530.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2530)

Once you provide us with this information, we collect all of it, and our control plane service takes care of all the bootstrapping and deployment for you. The typical deployment time today is approximately three hours, and this involves building the VCF stack and getting the hosts up and running. At the end when we hand off the environment to you, it is fully functional. You can immediately log into your vCenter, and your vSphere credentials are managed in KMS. I've recorded this so I can show you, but let me show you an environment that I've created as a demonstration.  This is an environment that I've created, so you would see here that the details of the environment are displayed.  This shows what you provided us: the hosts and the keys. We go ahead and deploy these hosts for you. You'll see this is a standard EC2 instance with an instance ID associated with it. You'll also see this is where you provided us your network configurations from your VPC to the service access subnet and route server, and then you'll see the VLANs that we've created for you.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2560.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2560)

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2570.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2570)

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2580.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2580)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2600.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2610.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2610)

These are the ten VLANs that Amazon creates in your account, and these are created in a special subnet that you see here.  The DNS hostnames are also available for anybody who's done a deployment on-premises.  These VLANs probably look very familiar to you if you've worked with on-premises deployments. This is the same thing that you would deploy in an on-premises environment. The real difference here is that rather than racking and stacking, cabling, and having your network team configure everything, you're providing just the logical information, the IP subnets, those types of things, and clicking deploy.  But this is nothing special from a cloud perspective. This is really the same stuff you would use in an on-premises deployment. Once this is available in your account, you should be able to log into your SDDC Manager or NSX Manager  and continue your day two operations like you would normally do. 

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2650.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2650)

### When Speed Matters: Key Takeaways and the Power of Layer 2 Extension

That wraps up our demo. We do have a lineup of sessions this week. As I mentioned earlier, this is an introductory level session, but you can see there are very specific storage-based sessions or networking-based sessions that are related to EVS and migrations throughout the week. Feel free to take a look at some of these sessions. Some of the chalk talks are not going to be recorded, so I highly recommend doing some of these chalk talks. These are smaller sessions where you can dive into as much detail as you want depending on your needs. We do have a getting started guide and the deployment prerequisite checklist that I mentioned earlier.  Take a look at it. Andy and I do this session every year. If you have any feedback, we take it very seriously, so please make sure to give us some feedback. It should be available on the app. We're going to be hanging out after this session and we have some time, so we're happy to take some questions and answers as well. Jignesh is also going to be here, so if you want to talk to any of us, come chat with us. I really appreciate all of you showing up on a Monday morning to this session. I know it's a long week and there are a lot of other fun announcements. The only AI we could include in our session is Andy's couch, but there are a lot of good announcements and keynotes coming up.

A couple of big takeaways or things I want you to think about as you're leaving: one of the questions we get asked a lot as we're talking with folks is when would I want to use this. As Jack Nash mentioned, a key thing for when you want to consider EVS is when speed matters.

Aarti and I have conversations, I think probably at least once a week if not more, where customers say my data center contract is coming up for renewal, and I need to be completely out of my data center in six months, or my server support is running out in three months, and I need to be out of it very quickly. So a big case for using Amazon EVS is when speed matters, when you need to move from point A to point B very quickly. This is definitely the fastest way to get from point A to point B while maintaining all the people, process, and technology aspects that remain the same.

Some of the real secret sauce that allows this to happen is that you do not have to change the IP addresses of the VMs as you move them. We didn't cover this tremendously in the networking section, but I would encourage you to go to the deep dive sessions if you can. Some of the real magic of this is the ability to actually extend those Layer 2 networks from your on-premises environments into the cloud. This means you do not have to change the IP addresses of the VMs as you move them. When we talk with customers, this is one of the biggest things that they bring up: if I don't have to change the IP addresses, it reduces my testing burden, it allows my app teams to do less work, and I can move much faster.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5d07b175fbe13d60/2800.jpg)](https://www.youtube.com/watch?v=d0TLechcV74&t=2800)

We were trying to provide this at a higher level. This is what it is and how it works at a very high level. If you can attend some of those deep dive sessions, they'll go into a lot more detail about the magic behind this and being able to literally use VMware HCX technology to move the VM from on-premises into the cloud without changing IP addresses with very minimal downtime and being able to do that very quickly. So I encourage you to check that out. I'll just echo what Aarti says: thank you so much. We would really appreciate it if you could provide us feedback. We hope you learned something, and we'll hang out down  here and we're happy to answer any questions that you have. Thank you all. Have a great rest of your week.


----

; This article is entirely auto-generated using Amazon Bedrock.
