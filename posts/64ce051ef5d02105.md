---
title: 'AWS re:Invent 2025 - 3x faster data migration with AWS Data Transfer Terminal (MAM224)'
published: true
description: 'In this video, AWS Solutions Architects Hemanth and Daniel introduce AWS Data Transfer Terminal, a service enabling secure, high-speed data transfers up to 200 Gbps by connecting directly to AWS''s 9-million-kilometer backbone network. They explain optimization across four dimensions: software (AWS S3 sync, AWS Common Runtime), hardware (CPU, memory, transceivers), storage medium (NVMe, SSD, HDD with RAID), and network (ECMP support). The service is ideal for 20 terabytes to 1 petabyte migrations, offering faster and more cost-effective transfers than Direct Connect or Site-to-Site VPN. Daniel demonstrates migration patterns using Data Transfer Terminal combined with AWS DataSync for change data capture, highlighting use cases for files, databases, and legacy VMs. Real-world examples include Rivian''s 3x faster data ingestion.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/0.jpg'
series: ''
canonical_url: null
id: 3093089
date: '2025-12-08T20:04:33Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - 3x faster data migration with AWS Data Transfer Terminal (MAM224)**

> In this video, AWS Solutions Architects Hemanth and Daniel introduce AWS Data Transfer Terminal, a service enabling secure, high-speed data transfers up to 200 Gbps by connecting directly to AWS's 9-million-kilometer backbone network. They explain optimization across four dimensions: software (AWS S3 sync, AWS Common Runtime), hardware (CPU, memory, transceivers), storage medium (NVMe, SSD, HDD with RAID), and network (ECMP support). The service is ideal for 20 terabytes to 1 petabyte migrations, offering faster and more cost-effective transfers than Direct Connect or Site-to-Site VPN. Daniel demonstrates migration patterns using Data Transfer Terminal combined with AWS DataSync for change data capture, highlighting use cases for files, databases, and legacy VMs. Real-world examples include Rivian's 3x faster data ingestion.

{% youtube https://www.youtube.com/watch?v=EZKgN72E-ys %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/0.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=0)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/20.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=20)

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/40.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=40)

### Introducing AWS Data Transfer Terminal: Direct Access to the AWS Backbone Network

 Hey everyone, welcome to our session, 3X Faster Data Migrations with AWS Data Transfer Terminal. My name is Hemanth, and with me is Daniel. We are solutions architects with AWS, specializing in helping our customers move to the cloud efficiently and maximizing their value. In today's session,  we'll be talking about the AWS Data Transfer Terminal service and giving you some tips on how to optimize high-speed transfers and some example patterns that we've seen customers adopt with the Data Transfer Terminal. Then Daniel will walk you through the migrations with AWS Data Transfer Terminal, the use cases and scenarios,  and we'll finally end the session with some resources for you to take back. With that, let's get started.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/50.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=50)

One thing that brings all of our  infrastructure together is our backbone. This is the AWS backbone network. It connects all of our regions, availability zones, and points of presence. The backbone mostly runs on 400 gigabits per second, and it runs on fiber that we own and on hardware that is built by AWS. The backbone network provides global connectivity across 200 plus territories and countries.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/110.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=110)

Quick question, does anybody know the length of AWS's fiber? If you don't, Matt Garman actually hinted to this in his keynote yesterday. It's actually 9 million kilometers. That is giving you the high-speed, low latency global coverage. Now, customers such as yourselves can leverage your data and innovate with it only as fast as you can get it  onto AWS, right? I've encountered scenarios where I thought, okay, if I can get access to a higher speed just for a day, just for a couple hours, I can just get this data transferred and move on.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/140.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=140)

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/150.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=150)

So we asked ourselves, how can we make our backbone network work for you and give you the on-demand flexibility, complete control of the process, and the ability to upload at an accelerated speed?  This is where we introduced the AWS Data Transfer Terminal service. The Data Transfer Terminal service will provide you with a secure physical  location that you can reserve on demand through your AWS accounts. And when you do that, you also specify a team of transfer specialists that go on site and perform your data transfers, right?

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/170.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/180.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=180)

You bring your own storage medium, put your data on any device that you have, bring it over, connect it to the AWS network, and upload  at speeds of up to 200 gigabits per second. And as I said, it is an onsite experience. Once your transfer teams get to the  location, they will need to check in with the security, show their government IDs, and the security will then also write down all the serial numbers of the devices that you bring. We want to make sure what you bring is what you take back as well.

[![Thumbnail 220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/220.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=220)

And as you can see on the top right corner, that is how a Data Transfer Terminal location is going to look. Once your teams are checked in, the security will escort them into this room where they will have access to two fiber optic connections, each with up to 100 gigabit speed. Prior to Data Transfer Terminal, customers were transferring the data using Internet  or any sort of general connectivity, right?

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/240.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=240)

There are some obvious challenges here in terms of the intermediate hubs or the limited bandwidth or throughput or potential bottlenecks, which are not suitable for high volume transfers. In the second instance, customers  have access to Direct Connect, which provides consistent and guaranteed throughput. But if the data center is in a remote location or if the data transfer needs are only for a limited time, they may not want to set up a permanent Direct Connect connection with that, right?

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/260.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=260)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/280.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=280)

So with Data Transfer Terminal, it essentially overcomes these challenges  where the customers can directly plug into our backbone network and transfer at higher speeds while eliminating all the intermediate hops. These are the locations where Data Transfer Terminal is currently available, with some planned and potential sites  for the future. Now, customers have asked us how they can optimize their bandwidth and essentially optimize their high-speed data transfers.

Because some customers need an affordable way to copy smaller amounts of data on a regular basis, while others will want to transfer a large amount of data one time, right?

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/310.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=310)

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/320.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=320)

### Optimizing High-Speed Transfers: Four Dimensions and Customer Adoption Patterns

So this is where we've essentially broken this down into four dimensions of high-speed transfers.  The first at the software level, it is recommended to use AWS S3 copy or AWS S3 sync commands which are part of our command line interface.  And for best transfer rates, it is also recommended to leverage optimizations by using AWS Common Runtime, which is a library implemented using C, which is usually more performant than the default Python interpreter. And for customers with even higher or more demanding transfer needs, they should explore implementing S3 transfer agents using software development kits, which are available in multiple languages, and they should also look at implementing parallelization and batching to improve their overall transfer efficiency. While encryption, while essential for security, can introduce overhead and lower the overall transfer efficiencies, especially on systems without hardware encryption in it.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/380.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=380)

At the second layer, the hardware,  the hardware components directly impact the transfer performance with CPU, memory on your devices, transceivers, switches, network interface cards, and storage servers all impacting directly. With storage servers, customers are able to enable large-scale transfers up to petabyte scale by allowing hot swapping of disks so that you're not impacting any operations that are currently ongoing.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/410.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=410)

The next dimension, the storage  medium itself, affects the overall transfer performance with NVMe disks giving the highest possible bandwidth, followed by solid state drives, followed by hard drives. So in this scenario, if customers are using any low-performing disks, it is recommended that they set up a RAID to essentially club multiple low-performing disks together and achieve higher efficiency transfers.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/440.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=440)

And finally, network itself matters. The performance of the network itself matters.  Although you're being provided with multiple 100 Gbps connections, you should account for the network overhead, things such as the TCP IP overhead, the storage performance, as well as the characteristics of the data set that is being transferred. Data Transfer Terminal, however, supports ECMP, equal cost multipath, where you can combine the two fibers that are available to you to achieve speeds of higher than 100 Gbps.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/470.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=470)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/480.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=480)

 Now let's take a look at some of the patterns that we've seen our customers adopt. The first pattern is very simple. A customer with a laptop and a hard disk with some data on it  connects it to Data Transfer Terminal and copies it onto their AWS endpoints. The limiting factor in this is the performance of the storage medium, the hard disk that they're using, the bandwidth that is available through the hard disk.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/500.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=500)

Now, the next pattern is an extension of the previous one where Data Transfer Terminal, each fiber in Data  Transfer Terminal is capable of giving out five IP addresses through DHCP. So in this scenario, instead of a single machine, we are essentially doing a parallel or creating a cluster of up to five machines per fiber to achieve or to essentially multiply the bandwidth.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/520.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=520)

The third pattern here is  we've seen customers actually bring a storage server and copy the data onto AWS using Data Transfer Terminal. In this scenario, they're able to transfer data in hundreds of terabytes all the way up to petabytes and also achieve or essentially utilize the bandwidth that is available to them. With that, I will ask Daniel to walk us through the migrations with Data Transfer Terminal scenario. Thank you.

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/550.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/560.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/570.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=570)

### Migration Considerations: When Data Transfer Terminal Fits Your Use Case

 So when we're talking about migrations and you wanted to know if Data Transfer Terminal is going to help your migration, there's really four key  considerations to take into account. The first one is the amount of data. Is it in gigabytes? Is it in terabytes, or is it in petabytes?  We find that most of our customers who are using Data Transfer Terminal for migrations are using between about 20 terabytes up to about one petabyte. Now we find that less than 20 terabytes can usually be handled by any existing bandwidth that already exists at the data center, and larger than one petabyte, that data is usually stored on a large storage array, which will be improbable, if not to say risky, to bring to a Data Transfer Terminal location. But with that said, we do have some AWS partners who offer very storage dense in very compact form factors, up to one petabyte. And so that can be an option for Data Transfer Terminal.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/610.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=610)

 The next thing to consider is the location of the data. Now, Hamonth already showed a location of where Data Transfer Terminal is located and some future planned sites. So, is your data stored in a location that's convenient to one of those locations? And if it's not, I would encourage you to reach out to your AWS account team and see if we can bring Data Transfer Terminal to a location that's going to be more convenient for you.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/640.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=640)

The next thing to consider is what type of data is it.  Data Transfer Terminal excels with files, logs, telemetry data, even things like compliance data bound for S3 or Glacier. Now, if you've modernized your applications and they're running on Kubernetes, we say send the blueprints, not the house. But after you redeploy your containers in AWS, you're still going to have a back-end database. You could use Data Transfer Terminal to send the database to AWS and then use another service like AWS Database Migration Service or DataSync to do any change data capture or CDC updates.

Now, if you have live VMs, we are going to recommend that you look at AWS Application Migration Service. But perhaps you have some legacy operating systems kicking around in the back of your data center that may not be supported by Application Migration Service. In that case, you can send a copy or a snapshot of the servers up to S3 and then use VM import to convert them to an EC2 instance.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/710.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=710)

Now, the next factor to consider is time. How much time do you have to complete your migration?  With some data like log data, it may go stale in 24 hours, and you need to get data into AWS immediately. Or there may be a compelling event like a large or costly contract renewal, and you need to get your data out of your data center into AWS before that renewal hits, and you may not have sufficient bandwidth. Data Transfer Terminal can come to the rescue there.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/760.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=760)

### Comparing Transfer Methods: Time, Cost Analysis, and Real-World Implementation

So when we look at everything together, the sweet spot is really going to be data that is in the hundreds of terabytes. It's going to be within a reasonable distance to a Data Transfer Terminal facility, like 3 to 4 hours driving distance. It's going to have an imminent or urgent time to value, and it's going to be a flat file or the database situation like we talked about.  Now on the left-hand side here, we have increasing amount of data from 1 terabyte all the way up to 1 petabyte, and across the top, we have the various AWS services that you can use to get data into AWS.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/770.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=770)

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/790.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=790)

 And when we look at the time to copy the data into AWS for Data Transfer Terminal, we can see that we're averaging between 1 minute to 18 hours to get even larger amounts of data into AWS. So this can be a very quick solution for us.  Now, on the opposite end of the spectrum, we have slower services like Site-to-Site VPN and even the newly released high bandwidth Site-to-Site VPN. And we can see that this transfer time increases dramatically to almost 4.5 months. That's a lot of time, and that's a lot of opportunity for your data to change, in which case you would have to bring the changes to AWS, which will increase the time, which will increase the chances for more changes, and I think you can see where I'm going with this.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/820.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/830.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=830)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/840.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=840)

 Now, when we looked at Direct Connect, even at 10 gigabits and 100 gigabits, they're still improving performance. But let's take a look at how cost comes into effect here.  Now with VPN we have a very cost-effective solution. Even at the longer lengths, it's still going to be a cost-effective solution for us if we can accept this time frame.  Now, with Direct Connect, there is a premium to be paid for private connectivity at low latency. These costs look really good, but as all of you, I'm sure know, AWS is a pay as you go service.

So these costs are assuming that as soon as you get your service, you put in your order for Direct Connect, as soon as you get it, you start your copy job, and as soon as that job is done, you disconnect your service. These are theoretical costs. In reality, you're going to get a notification that your service goes live. Then you're going to tell your network team that they need to go and implement a change request to enable routing. And then you're going to tell your firewall team that they need to put in a change request to allow the traffic through. Maybe you're going to do a change request to do some test data. And then finally, you're going to do your migration, and then you're going to do another change request to tear it down.

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/900.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=900)

 When we factor in real world times into this, we start seeing costs like this. And one other note, Site-to-Site VPN is an overlay network, meaning that it requires an underlying circuit for connectivity.

In this case, a Direct Internet Access circuit, or DIA, is needed. In order to support a 1.25 gigabyte circuit, you're looking at an average cost of about $1,000 per month, and to support a 5 gigabyte VPN, you're looking at about $2,500 a month.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/930.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=930)

Now, when we show data transfer costs,  you can see that there is a direct advantage here for time and cost when compared to some of the other methods. Now, I will pause and say there's always more crayons in the box, and it's always good to have multiple options. So in some cases, existing bandwidth may be a solution, but if your time frame or your costs are going to exceed what you're comfortable with, this Data Transfer Terminal may be an option to supplement.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/960.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=960)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/970.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=970)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/980.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=980)

Now, for those who are visual learners  like myself, this is what it's going to look like in practice. So we start with our corporate data center. We have our AWS account.  Our data is currently located in our corporate data center, and we need to get it into AWS. So we start with Data Transfer Terminal and we do a one-time copy of all of our data.  And then for any live things like databases, in which case there may be some changes during the time frame of the migration, we use AWS DataSync for any CDC data like we talked about before.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/1000.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=1000)

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/1010.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=1010)

Now, once the data is in AWS, it's available to any on-premises applications that are still running on-premises,  as well as any of the applications, VMs we've migrated, or those containers that we've now redeployed in AWS. And now, we can go one step  further and make that data available to our AI/ML workloads for analytics and to gain insights from the data and to make data-driven decisions.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/1030.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=1030)

Now, these are some resources that we'd like to leave you with. Feel free to get your phones out. The first is the service page to  Data Transfer Terminal. The next is a blog post from Rivian about how they increased their data ingestion three times faster using Data Transfer Terminal. The next is a blog post on how to optimize. So there is some hardware, as Hemanth showed, involved with using Data Transfer Terminal, and this blog post here on the lower left has some ways and tips and tricks on how to optimize the performance.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/64ce051ef5d02105/1060.jpg)](https://www.youtube.com/watch?v=EZKgN72E-ys&t=1060)

And finally, we'd like to thank you for coming and we encourage you to  get out your phones and offer us feedback. Let us know what we did well and where we can improve. Thank you. Thanks, everyone.


----

; This article is entirely auto-generated using Amazon Bedrock.
