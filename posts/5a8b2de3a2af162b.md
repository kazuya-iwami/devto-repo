---
title: 'AWS re:Invent 2025 - Scaling instantly to 1000 GPUs for Serverless AI inference (AIM2201)'
published: true
description: 'In this video, Modal CEO Erik explains how Modal provides serverless AI infrastructure for companies like Suno and Lovable. He describes how traditional tools like Kubernetes struggle with GPU-intensive AI workloads, leading Modal to build custom container runtimes and file systems. The platform enables developers to deploy Python functions to cloud GPUs in under a second using simple decorators, with no YAML or Docker configuration. Modal manages tens of thousands of GPUs across AWS and other clouds, offering usage-based pricing that scales to zero during idle periods. Customers can access any model, run inference, training, batch jobs, and code sandboxes, with some processing thousands of years of audio in days.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Scaling instantly to 1000 GPUs for Serverless AI inference (AIM2201)**

> In this video, Modal CEO Erik explains how Modal provides serverless AI infrastructure for companies like Suno and Lovable. He describes how traditional tools like Kubernetes struggle with GPU-intensive AI workloads, leading Modal to build custom container runtimes and file systems. The platform enables developers to deploy Python functions to cloud GPUs in under a second using simple decorators, with no YAML or Docker configuration. Modal manages tens of thousands of GPUs across AWS and other clouds, offering usage-based pricing that scales to zero during idle periods. Customers can access any model, run inference, training, batch jobs, and code sandboxes, with some processing thousands of years of audio in days.

{% youtube https://www.youtube.com/watch?v=qb9hZx3ZTb0 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/0.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=0)

### Modal: Building AI Infrastructure for the Modern Era

 Hey, it's great to see you. I'm the CEO of a company called Modal. We build AI infrastructure. Serverless inference is one of the many things we offer. We power some really cool customers like Suno, Lovable, and many others, doing things like inference, training, code sandboxes, and a lot of other stuff. So let's walk through what Modal does and how it works.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/40.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=40)

We're based in New York. We've been doing this for about four years. I started the company around 2021.  Modal is an infrastructure platform that makes it easy for developers to build and scale AI applications. When you're building these AI applications, you need infrastructure. You need a place to run all these applications, right? As it turns out, when you're building these things, working with large-scale models, working with GPUs, a lot of existing infrastructure doesn't really cut it.

If you're trying to deploy things using Kubernetes, EC2, Docker, it's kind of a nightmare, and it really drains developer productivity. Then you have a lot of these challenges around managing predictable demand, and particularly with inference, you need to be able to scale up and down very, very fast and manage the utilizations. GPUs are expensive and GPU capacity is limited. So you have all these issues around infrastructure management, and that's what Modal solves.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/100.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=100)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/110.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=110)

We run tens of thousands of GPUs all over the world. We manage that for you. We're a big customer of AWS, very happy with that.  We basically started the company with this thesis that today's infrastructure is not cut out for AI applications.  In particular, when you go deep on it, you look at Kubernetes in the past, and sorry to pick on Kubernetes, but it's a lot of traditional infrastructure as a whole. It's built for a kind of different era of compute. It's sort of CRUD, you know, kind of steady demand, CPU-based applications. It's pretty cheap.

But then you go to AI land and now you're running very expensive GPUs, and it poses a lot of challenges for how we deploy and scale applications. Some of those challenges are just the high cost. Some of them are scaling and working with these large models. Some of them are kind of new things, like how do you programmatically execute code that you don't necessarily trust? It may be code that comes from an LLM, so you need sandbox primitives. And then of course, there's training, how do you do all of the storage of all of this underneath, big batch jobs, all these things.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/160.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=160)

 So we basically built our own stack. We started at the ground level. We built our own container runtime, we built our own file system, and then we built a bunch of storage primitives. The reason we did that was that it enables us to have this amazing developer experience that lets us iterate extremely fast and offer a really amazing developer experience. You're trying to find something that sparks joy when you're working with Modal, and I'll show you some code in a second. You can take code from your laptop and run it in the cloud in less than a second on GPUs, state-of-the-art B200s.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/200.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=200)

 What's cool about Modal is we power so many different use cases and so many different parts of the AI journey, right? We focus on machine learning developers primarily, but we do inference, training, we even have notebooks. You can do batch jobs, you can run code execution, we're building many more things. So Modal is sort of an end-to-end platform for a lot of different things in AI.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/230.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=230)

We have a lot of different customers, some may be recognizable.  Maybe you've heard of Meta, but we also have some really cool new AI companies. I mentioned Lovable, they're using us for all the code execution. We have some companies like Cognition and Decagon using us for LLMs. Suno, AI-generated music, they run all their inference on Modal. So we're at some pretty large scale, and it's all over the place.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/260.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=260)

 We're also actually doing a lot of biotech too. We have a lot of customers using us for scanning millions of compounds and doing advanced protein folding on GPU or sequence alignment and things like that. We have people using us for weather forecasting. We have people using us for curing cancer. We have people using us for audio transcriptions, text-to-speech, speech-to-text, and a lot of applications in audio. But really, any modality. LLMs, of course, are a cornerstone to modern AI.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/290.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=290)

 How does it work? We're not an AI API. We don't necessarily have a set of models. You can run any model on Modal. Some people run their own proprietary models, some people deploy an open-source model. So it's not like you're limited to a set of models, you can really run anything here.

### How Modal Works: From Python SDK to Scalable GPU Infrastructure

We take any code, we take your code, we take a container, and we make it run on Modal, and we manage all the scaling. The general way people do this is to write a little bit of Python that basically takes functions and turns them into serverless functions.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/320.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=320)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/340.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=340)

 So you can think of Modal, here's a quote from, and I actually think this is a really cool example. It's a robotics example, so they obviously have very low latency demands. We're basically using Modal to control a robotic arm. Incredibly cool application.  One way to think about Modal is that it's two parts. One is a Python SDK where you can write a little bit of code that defines the functions you want to run and scale in the cloud. Underneath is the infrastructure substrate that basically scales that and manages all the capacity, so you don't have to think about scaling.

If you need 1,000 GPUs, we can give you 1,000 GPUs within sometimes seconds. We have these crazy large-scale, very bursty jobs, sometimes big batch jobs or very unpredictable inference. We can scale up and down extremely well. Let's talk about the first step, the Python SDK.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/380.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/390.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=390)

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/400.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=400)

 Here's a little bit of an example. Hopefully you're familiar with Python and how it works.  Here's an example of how you can deploy a function in Modal. You start with a little bit of Python, you add a decorator. Sorry, you import Modal first, of course.  The key thing is really here, you put this decorator on any function, and that turns this function into a serverless function that runs in Modal.

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/420.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/430.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=430)

We run a lot of stuff on AWS, but also other clouds, many neo clouds and the big hyperscalers. In particular here, as you can see,  we run this on an L40S, which is one of NVIDIA's latest GPUs, very good for inference, especially for smaller models.  Then we load the model just using Hugging Face Transformers. That's it. Zero infrastructure needed.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/440.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=440)

When you get started with Modal, you just write a little bit of code, you run Modal run or Modal deploy, that turns it into an app, it runs in the cloud, and that's it.  Underneath, well, here's the beauty of this. Put in the words of one of our favorite customers, Suno. They run AI-generated music in the cloud. What they love is that they're a small team, they can focus on what they're really good at, which is to build models and build consumer applications, and we take care of all the infrastructure underneath.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/470.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=470)

With a few lines of Python, they get all this done, and it's all in code.  Zero YAML, zero configuration. No one likes to write YAML or Dockerfiles, actually. You write a few lines of Python and it just works. We can also take an existing container and just move it into Modal, but the main way people use Modal is just through the Python SDK.

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/490.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=490)

 Underneath, how does it work? We built our own container runtime, we built our own file system, we built our own container image builder, we built our orchestrator, built our own scheduler, all of this stuff. We make cloud development just work, which is a very hard problem. Hats off to AWS for doing something similar. They built EC2 and all these things, and we use some of those. These are very hard engineering problems, and we built this underlying infrastructure stack because we felt the infrastructure for AI just wasn't there.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/520.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/530.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=530)

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/540.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=540)

 Some of the cool stuff we can do is we can snapshot CPU memory, we can also snapshot GPU memory, we can restore that. We have incredibly fast container cold starts.  We can start GPU models and start running inference calls in less than one second, even with large models of tens of gigabytes. Part of that is all the deep investments we made in the infrastructure below.  We also manage all the scale.

As I said, we manage tens of thousands of GPUs all over the world, a lot of them in AWS. It's all usage-based, so you only pay for the time the code is actually running, which is very nice with inference because you have very unpredictable demand. Especially with inference, when you're scaling up and down, you don't want to go out and make a reservation of a rectangle. You want to just pay for what you're using. Maybe your app goes viral and you need 1,000 GPUs tomorrow, maybe while you're sleeping. We handle that. We scale it up and manage that scale for you.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/590.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=590)

When there's no traffic, we scale down to zero and it costs you nothing. You can also deploy CPU functions. This is a lot of the work underneath. As you can see, we're on a lot of different AWS regions.  This is one example of A10Gs, how we manage that. This is a lot of under the hood work. But the point is, we take care of all of that complexity for you. You don't have to think about it. Under the hood, all this complexity about getting GPU capacity and managing that and scaling up and down, don't think about it, we got it.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/610.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=610)

We also have a very fancy internal  observability dashboard. You can see this is one screenshot, but you can see latencies, GPU temperature, and number of containers. This is something a lot of our developers really like, the machine learning engineers. They love seeing and having full control, taking stack traces, looking at the logs, and really understanding what are my GPUs doing at any point in time and how do I maximize them.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/650.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=650)

So when you deploy the model, you can just log into your dashboard and see all these things for all your apps that are running and see exactly how much you're paying at any point in time. Part of how we think about the future is  we take all this demand and just manage all that complexity for you. By doing that, we can run it way more efficiently than anyone else can because traditionally, at least in the last few years, the only way to get GPU access has been to go make reservations. I don't think that makes sense because then everyone has to make a reservation for the peak. But by taking all these workloads and combining them, we can manage that utilization way more efficiently.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/700.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=700)

### Developer Experience and Getting Started with Modal

A lot of people actually, even though our $1 per GPU hour sometimes is higher, the net cost might actually be lower moving to Modal because you're only paying when the GPUs are actually running. We can do that because we invested a lot in driving GPU efficiency, GPU utilization, because we have this multi-tenancy.  You see some amazing developer love. I really think that the developer experience is what sets Modal apart. People really comment on how you can just write a few lines of code, get it out in production, and you don't have to worry.

It's really time to market that I think people come to us initially for. They want to have their very complex models, maybe they're training or doing fine-tuning. They want to scale up big batch jobs or run code sandboxes, and all you have to do is pip install Modal and get started. People really love this developer experience. The one thing if you talk to our customers is we just handle all of that underneath, all the infrastructure, and you just have to write a few lines of Python.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/740.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=740)

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/770.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=770)

 Here's another example. We help Substack scale out to hundreds of GPUs, like big batch jobs. We have many other customers that scale out to thousands of GPUs. We had a customer the other day who processed 3,000 years of audio in a few days doing some advanced work. I don't know what they're doing actually, but it's very cool. The love is there. We've been building this for about four years. We're obsessed  with building products that developers really love.

I'm a developer. I've been writing code for 32 years, I believe, something like that, professionally for 25 years. We really think about how do we make engineers productive and how do we take away the boring parts of their job, which in many cases is the infrastructure and the configuration. We love infrastructure, so I like building this stuff. We take the boring stuff away from you and we build all the infrastructure for you so you don't have to think about it. Especially with these AI applications, they're pretty hard to manage.

Our typical customer tends to be machine learning engineers. It tends to be people who are building models or fine-tuning, building quite advanced things, and they really love that they get this full control. We're not trying to take away the power and the control from customers. In fact, we're actually enabling them to move faster and deploy more complex things and get the power of the scalability in the cloud and get even more GPUs than they previously had. It's really about making engineers move faster and scale up at large scale.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/840.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=840)

 A lot of what people do is really, again, we're not an AI API. We don't have a set of models. You can run any code or model. Whatever code you have, if it's a proprietary model in the case of Suno, or if it's an open source model, you can deploy it and we can help you with that. We have a lot of templates and a lot of examples for how to get started. It's consumption-based, so you only pay for the time the GPUs are actually running, which is a really nice feature because it means with these unpredictable scaling demands that you often have doing inference, you don't have to pay for the periods in between when the GPUs are sitting idle because we can run other workloads on those.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/890.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=890)

So how do you get started? If you want to check it out, it's very easy. Go to your terminal, do pip install Modal,  you can start writing code and running it in the cloud immediately. Every user gets $30 a month of free credits. If you're a startup, you can actually get up to $50,000 worth of credits, which goes a pretty long way. We have many startups who initially started at Modal, many Y Combinator companies and so on, started scaling at Modal or doing the prototypes with Modal, and now they're running at very large scale in production.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5a8b2de3a2af162b/940.jpg)](https://www.youtube.com/watch?v=qb9hZx3ZTb0&t=940)

We're there for you when you're building the prototypes. We help you with the developer experience. We help you move fast. We also help you scale it up and run it on thousands of GPUs when you're ready to put some production workloads. So thanks a lot. If you want to reach out to me, you can follow me on Twitter. I'm Bernhardtson. You can check out my blog, which is, sadly, I don't blog as much as I should these  days, but I used to. Or you can email me. I'm Erik with a K at Modal.com. We'd love to stay in touch. Any questions or any thoughts? I don't know if we have time for questions. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
