---
title: 'AWS re:Invent 2025 - Technical deep dive: Composable AI agents for partner solutions (MAM217)'
published: true
description: 'In this video, AWS Transform team introduces composable AI agents that enable partners to integrate their migration and modernization tools into a unified platform. The session demonstrates how AWS Transform evolved from fragmented point solutions to a centralized experience with specialized agents for VMware, mainframe, Windows/.NET, and custom transformations. Key features include workspaces for collaboration, job orchestration, human-in-the-loop patterns, and continuous learning. Technical deep dives cover base agent modules, Bedrock Agent Core runtime integration, and agent registry mechanisms. Live demos showcase building agents with the AWS Transform package, a DORA compliance agent for mainframe applications using flexible agent-to-agent communication, and Wavicle Data Solutions'' BI migration agent for Power BI to QuickSight conversion. Partners like Accenture and Capgemini are augmenting knowledge bases for regulated industries and mainframe modernization, demonstrating the platform''s extensibility and marketplace distribution capabilities.'
tags: ''
series: ''
canonical_url: null
id: 3085127
date: '2025-12-05T03:20:30Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Technical deep dive: Composable AI agents for partner solutions (MAM217)**

> In this video, AWS Transform team introduces composable AI agents that enable partners to integrate their migration and modernization tools into a unified platform. The session demonstrates how AWS Transform evolved from fragmented point solutions to a centralized experience with specialized agents for VMware, mainframe, Windows/.NET, and custom transformations. Key features include workspaces for collaboration, job orchestration, human-in-the-loop patterns, and continuous learning. Technical deep dives cover base agent modules, Bedrock Agent Core runtime integration, and agent registry mechanisms. Live demos showcase building agents with the AWS Transform package, a DORA compliance agent for mainframe applications using flexible agent-to-agent communication, and Wavicle Data Solutions' BI migration agent for Power BI to QuickSight conversion. Partners like Accenture and Capgemini are augmenting knowledge bases for regulated industries and mainframe modernization, demonstrating the platform's extensibility and marketplace distribution capabilities.

{% youtube https://www.youtube.com/watch?v=yLYKD5nXGvY %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/0.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=0)

### Introduction to AWS Transform: From Point Solutions to Unified Migration and Modernization Platform

 Welcome to MAM 217, Technical Deep Dive. We're going to talk about building composable AI agents for partner solutions with AWS Transform. My name is Murtaza Chowdhury. I lead product management in the AWS Transform team. I'm going to be joined by two esteemed colleagues, Alexis and Ravi. Later in the presentation, we'll also have a partner join us and show how they have worked with us to build composable agents.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/50.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=50)

 Here's the roadmap for today. We're going to talk about the journey with AWS Transform and how we started with several point solutions that we had at AWS. We combined them together to provide a centralized experience with consistency across the different migrations, and then how we are extending forward to bring partners along with us so that there is one unified experience for customers. We're going to talk about the components that enable composability. We'll cover use cases. We're also going to touch upon best practices that we recommend to our partners. We have several demos in this session, so if you want to take pictures, please feel free.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/110.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=110)

 Here's where we are with AWS Transform. We made the service generally available in May of this year. If you're following the announcements that we had this week, the current state of the service is that we have several specialized agents for some of the key workloads that we have identified based on customer feedback. This starts with migrating on-premises data center workloads to the cloud, which involves mapping the technical artifacts on-premises to what it means in the cloud. This includes identifying which applications and workloads should be grouped together, the servers, the applications, and the databases. We also figure out the mapping between the networking topologies on-premises and in the cloud. That's what constitutes our migration agents for VMware.

Then we have modernization for mainframes. There are mainframes that have been built several decades ago, and customers have asked us to help them identify what those applications do and comprehend those solutions. We have capabilities like extracting business rules out of legacy mainframes and generating documents which help them understand what those applications do, taking the step forward in terms of modernizing them to modern languages and migrating them to the cloud.

Then there are Microsoft workloads for Windows. We started with .NET workloads, .NET Framework-based applications which are Windows-bound. Customers asked us to provide them the ability to make them compatible with open source frameworks. Where we started is helping with agentic experiences to convert .NET Framework-based applications to .NET Core, which can be run on Linux-based either VMs or containers. This week we expanded that experience to become full stack. We included SQL Server modernization there so that when you are looking at Microsoft workloads as a whole, you can modernize not only the application but also the databases along with them.

The fourth pillar that we have here that we announced this week is custom. AWS Transform custom. We understand that the variety that you have in workloads and your needs in terms of frameworks, applications, and APIs cannot be just handled with specialized agents. We want to provide you the ability so that you can scale based on your needs. What we offer with AWS Transform custom is the ability to build custom agents tailored to your specific requirements.

AWS Transform Custom offers a set of out-of-the-box agents for use cases like Java upgrades, Node upgrades, or Python upgrades. In addition to that, you can build your own transformations using AWS Transform Custom by providing input output samples or documentation. You can create them centrally, provide them to the practitioners in your organization, and have them run that at scale.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/350.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=350)

### Navigating the AWS Transform User Interface: Workspaces, Jobs, and Continuous Learning

We revamped the entire experience with a new look and feel in the user interface.  What I'm showing you here is a glimpse of the web application for AWS Transform. We also have IDE-based plug-ins for use cases like .NET and Java. Let me walk you through the information hierarchy here.

To start with, we have workspaces. Think of them as the security boundary or a collaboration space where your teams can work together and you can assign different roles to your team members based on how your teams are structured. There can be administrators, contributors, and read-only users. Within these workspaces, your practitioners can create different job types based on the capabilities we discussed earlier. You see there are four workspaces here: one for VMware, one for Windows, one for mainframe, and one for custom. You can have them together if you want to have multiple job types in the same workspace, or keep them separate depending on how your teams are structured.

Within a workspace are jobs. That's where you can execute those transformations. How jobs work is that it assesses what your needs are, assesses what your environments have, and creates a job plan for you. Then it goes and executes those steps in the job plans. The job plans comprise several task agents that we have. In fact, across all the specialized agents and custom transformations that we have here, there are dozens of task agents that come together to provide you this seamless unified experience.

We also have the chat experience. The entire experience can be led through chat where practitioners and you yourself can chat with the agents, and the chat can recommend you next steps. Human supervision is an important aspect. Whenever there is something to be reviewed, the chat agent recommends and takes help from you to take the next step. We also have artifacts here. Think of artifacts as the common place where your digital footprint of migration and modernization are stored for your team members to work on it together. You can manage collaborators there at a workspace level.

We also have connectors. As you're migrating and modernizing your applications, you will need connectors based on where your workloads are, either on premises or already running on the cloud. The connectors on both the source side and the destination side allow you to manage all of that within the boundary of a workspace here. An important aspect of all of this is that there is a continuous learning loop. As you execute your migration and modernization jobs in that secure workspace, the agents learn from what your practitioners do, and the next steps that it recommends are based on that learning, which enriches over time. The more you use it, the better it becomes.

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/560.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=560)

### AWS Transform Composability: Bringing Partner Tools into a Unified Ecosystem

 We have come a long way. When we started, we ourselves at AWS had dozens of tools that were specialized for specific tasks. We had a separate tool for migration, a bunch of tools for modernization, for rearchitecting, for mainframes. When we launched AWS Transform, we brought them all together. What you see in AWS Transform now is a consolidation where all agents and task agents work together to provide you that seamless experience. What we have now is a singular experience for execution of migration and modernization.

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/630.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=630)

However, we work with partners, which most customers leverage, and these partners have their own tools. This creates a challenge: we still lack a single governance layer across partner tooling and AWS tooling, which in many cases has led to duplication and friction. 

What we bring you today with AWS Transform composability is taking a step further from what we did with our tools. We are bringing partner tools and solutions together to have a centralized experience for practitioners and customers. If you are a partner with migration or modernization competency, you can integrate your tools, agents, and knowledge bases. If you have specialized domain-rich experiences in specific industries, you can bring them together to build a cohesive experience in AWS Transform.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/700.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=700)

How this works together is that there is AWS tooling, partner tooling, and then a personalized workflow customized based on what you want to provide your customers. Practitioners and customers no longer have to go to multiple places. They see one seamless experience, one seamless job type, or an augmented knowledge base that they can execute centrally, bringing down the time and effort to modernize and migrate. 

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/740.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=740)

We have moved from fragmented tooling to unified agents. This not only brings together the experience but also provides a singular governance and security layer. Our approach is to innovate with partners. If you are a partner with a solution in mind, functionality in mind, existing tooling, or if you are already on a journey of building agents, please talk to us. We will work with you and co-innovate, bringing those together and providing a central experience to customers. 

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/770.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=770)

The benefits apply to both partners and customers. If you are a partner, you can differentiate yourself with your specialized intellectual property and domain knowledge, creating personalized and customized workflows. For customers, the benefit is faster migration and modernization. Here is what it looks like from a partner standpoint. 

You start by telling us what you want to build. You can reach out to us either via your partner development manager or partner solutions architect. If you are already familiar with Partner Central, we are starting an experience there where you can lodge a request on what you want to build, including your business case, solution, and how you want to differentiate yourself. Then we build together, and you can publish the agents that you build with us.

You have flexibility in how you publish those agents: either within your own account, within your customer account, or if you want to make it broadly available, we have the ability to integrate with AWS Marketplace and provide a SaaS-based delivery from there. From a customer standpoint, it is as simple as plug and play. Customers, practitioners, or if you are a GSI or SI, you can simply enable those published agents and put them to use.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/850.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=850)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/870.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=870)

### Customer Experience: Enabling Partner Agents in AWS Transform

I will show you what the customer experience looks like once the agents are published. The starting point of the AWS Transform experience is configuring and enabling it in the AWS console.  Here we have a new experience called partner agents where any new agent that is collaboratively built by partners will be seen once it is published. From a customer standpoint,  it can simply be enabled. Once it is enabled, it starts showing up in the workspace.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/880.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=880)

What you see here is a set of workspaces  that you might have, and once that agent is enabled, it shows up as a job type. It also depends on what the use case is. If you want to create a new job type, it will show up there. If you are augmenting a knowledge base, it will personalize an existing job type. Or if you have another use case, that will be seen here as well.

Now that you have seen at a high level how this thing will work, I want to invite on stage my colleague Ravi, who will talk about the components that make this possible.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/930.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/940.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=940)

### Ravi's Demo: Integrating Tool Agents with AWS Transform Base Agent Modules

Thank you Murtaza. Before we dive into the components, let's take a step back and look at what it means for you to integrate your tools and agents  into AWS Transform, because that's the basic thing that you need to do before you can take advantage of the composability components.  I'm going to start small. I've built a simple tool agent that is similar to tools you may have in your source. This tool agent helps with network file conversion and follows a multi-step flow to get its work done. It helps with converting network files stored in an S3 bucket by downloading them, analyzing them, verifying the proper content, converting them, and finally uploading them to another location in the same S3 bucket.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1010.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1010)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1020.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1020)

You don't have to worry too much about the network file formats because that's not the focus. The mechanics and overall experience don't change whether it's a simple agent like this one or something you have yourselves from a tools and agents perspective. Let me kick start the agent here.  I have a local development environment, and this is my agent. I'm going to start running it.  The tool agent is running through its paces, and you can see what it's doing. Let's pause here for a minute. Once it's completed, you can see all the text output in the terminal. As you can see, it's very hard to follow what the agent actually did. There's no easy way to know what the distinct phases are or whether it's performing properly.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1060.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1060)

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1070.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1070)

Now let's think about what you can do with AWS Transform. I've taken the same tool agent and integrated it with our AWS Transform package, which comes with base agent modules that allow you to easily integrate with the AWS Transform primitives.  As we continue, you can see that I'm going to start running the AWS Transform enabled version.  You'll quickly identify that now you have a listening capability added to the agent. This is enabled by default when you integrate with the base agent module capability provided with AWS Transform. It also leverages the AWS Transform SDK and other capabilities that help you integrate your tools and agents into AWS Transform.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1100.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1100)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1120.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1120)

Now this is on my local development environment. If I want to see how it actually looks in the web application, I'm going to switch into the web application.  I'm firing up a browser and going to the URL which will take me to where the agent can be seen. This is the first time I'm running my browser, and the browser is connecting to it.  You'll see that it says you cannot do it, and you first need to make sure that you're properly authenticated. It's going to throw me into the username password input mechanism for the identity provider that I've been integrating here. Once I give the credentials, it's happy with it and will redirect me back after signing me in to the actual page that corresponds to my agent's job.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1140.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1140)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1150.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1150)

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1170.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1170)

 This is where it is. I just want to reflect on something you probably saw in Murtaza's slide show with the white background based theme.  You'll see here that I'm actually showing a dark mode. It's your preference. You can choose whatever you want. We now have all the capabilities to switch between the themes seamlessly. I'm a dark mode person, so I like this. Now let me look at what's in the job plan pane that we typically talk about.  Wait, there's nothing in here. Why is that? What happened is that we kicked started the initialization process of the agent locally, but we haven't given it a command to start execution. How do you do this? You do this by enabling the capability that comes built in to take JSON RPC or HTTP commands. This is all documented in the guide that accompanies the AWS Transform integration package.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1220.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1220)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1230.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1230)

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1240.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1240)

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1260.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1260)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1280.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1280)

To avoid long command lines, I've created a small script that allows me to streamline the process. Let me show you the demonstration.  I'll need to go through the demo from the beginning.  While it's running through the paces, I'll explain what it's going to do. The first thing you do is initialize your agent. The next thing you do is start running your agent.  To run your agent, you have to use JSON RPC or HTTP commands, which are documented as part of the package you received in the documentation guide. When you do that, you can go back and see your agent executing in the web app itself. Here it's going back to the login page because I'm rerunning the demo for you.  But soon we'll get to the point where you'll be able to see the agent in action. 

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1290.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1300.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1300)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1310.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1310)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1330.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1330)

We're back at the place where we started. The job has been initialized, but it's not executing yet.  I'm going to run my script, which allows me to kickstart the JSON RPC command I talked about. Now I'm going to switch back and open the job plan.  The job plan is where you'll be able to see what the agent is going to do. As you can see, it quickly populated with what it's going to do.  This is the plan of execution of what the agent is going to do by itself. You can clearly see all the salient phases it's going to go through to get this job done. As it keeps executing, you'll start seeing it complete one step at a time, as you can see here with step one just completed. 

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1350.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1350)

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1370.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1370)

Let's look into the artifacts and work logs.  Here you have the work log, which allows you to see the progress of the job as the agent is executing it, one at a time. You can also refer back to it to see what happened if you're not in front of your computer. As the agent runs through and completes multiple steps, we'll go back into the work log and you'll see it's populating the work log entries. What this helps you do is have a very crisp understanding of what your agent is actually doing. If something goes wrong,  you can go back and look at what went wrong. It was much harder in the case we started with, where you have a terminal with all the log messages interspersed. You could integrate it with some other tool and look at it, but that takes time and effort. With AWS Transform, you get all of this in a very easy integration package.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1400.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1400)

Now that we've looked at some of the capabilities, let's look at what AWS Transform helps you with.  Security is the underpinning of AWS Transform. As you've seen, no one can easily connect to the agent without being authenticated. By integrating with AWS Transform, you enable your tools and agents to seamlessly integrate with enterprise identity providers. Every action in AWS Transform needs to be authorized. We all know how hard it is to deal with credential lifecycle managementâ€”getting credentials, refreshing them, renewing them at the right interval, and handling revocations is a critical but painful aspect of managing connectivity. AWS Transform's built-in capabilities offload this pain from your development so you can focus more on your business logic and not worry about these other painful aspects. You've actually seen some of the capabilities where you can look at what the agent has been performing and get audit trails.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1470.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1470)

Murtaza talked about AWS Transform being built with collaboration in mind.  By using workspaces and roles, you can delegate job responsibility among your team members, while approvals provide critical oversight for large blast radius based actions. By integrating with AWS Transform, you get to leverage secure artifact storage which has data encryption at rest capabilities built in. It also allows you to take an optional customer managed key by integrating with KMS, which is the AWS Key Management Service.

Your agents can help you prevent and detect abuses by leveraging the built-in capabilities from AWS Transform. We all know how painful it is to impose this on our customers, so we don't want to do that. By integrating with AWS Transform, you get all of this with very easy integration effort.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1540.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1540)

### Building Composed Workflows: Components for Multi-Agent Orchestration and Deployment

The next aspect we want to talk about is how you can seamlessly see the workflow from the agent being executed.  You saw the job plan being orchestrated, and how easy it was to follow what the agent was doing. The job plan management and orchestration capabilities of AWS Transform span the entire lifecycle of the agent, enabling both single agent-based workflows for orchestration and multi-agent orchestration flows.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1580.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1580)

AWS Transform helps your agents gain human-to-agent interactions by enabling the human-in-the-loop or the HITL pattern, which is the most common pattern for human-to-agent interaction, and also agent-to-agent interactions.  Once you're ready with building your agent and you've tested it, you want to go ahead and deploy your agents. Normally, you would acquire compute, provision it, and plan for capacity. Here, AWS Transform comes with built-in integration with Bedrock Agent Core runtime, which allows you to use on-demand capabilities to seamlessly scale your compute requirements.

[![Thumbnail 1630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1630.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1630)

The optional autonomous mode that you can enable allows your agents to run without human intervention if your agents need that. The agents can also harden themselves against faults and be resilient to failures by leveraging the checkpointing capabilities  that AWS Transform provides you. As you run your agents in production, one of the things you want to do is monitor how they're doing and respond to events. AWS Transform's built-in integration capabilities help you get the appropriate logging, tracing, and metrics you need to monitor and put together robust dashboards and set up alarms so you can react to events.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1670.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1670)

Thus far, we've seen how you can take your existing tools and agents and integrate them into AWS Transform and light up your capabilities to become more enterprise-friendly. Now let's look at how you can use this to build more complex composed workflows to achieve larger transformation outcomes.  What does it actually mean to have a composed workflow? In a composed workflow, your tools and agents work seamlessly with other tools and agents, both from AWS and from other partners, to achieve much more complex transformation outcomes that you cannot achieve with just your tools and agents alone. That's the advantage of having a composed workflow.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1710.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1710)

The topic of this section is components. Now we're going to dive into the different components that you can use to make this composition much more seamless.  As we talked about earlier, we have the base agent modules that are shipped with the AWS Transform package, which allow you to seamlessly integrate with AWS Transform capabilities like the work log, the artifacts store, the job plan management, and so on. You can supercharge your agents with domain-specific knowledge that is very important for your businesses by integrating with the built-in knowledge-based integration capability.

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1770.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1770)

We talked about the Bedrock Agent Core runtime integration, which allows you to deploy your agents in a secure VPC-based environment even with private networks. Finally, when you're ready to have the agents consumed by other agents or be discoverable by other customers, you can use the agent registry mechanism that's provided through AWS Transform.  To get you quickly started on the overall transformation flow, this is how it looks. We started at step one in the dev build environment, where we use the AWS Strands agents and the Agent Core SDK and the base agent modules to light up the core capabilities of what it means to integrate with AWS Transform. We were able to test it in a local environment, and once we were happy, the next step was to leverage Amazon ECR capabilities to build an image which you can upload into your own AWS account and run on Agent Core runtime.

This allows you to test as if it's going to run in production in your customer's account when it's deployed there. To interact with this agent, you need to use the Agent Registry to make it discoverable. Once you're done with this, you can use the chat capabilities that Murtaza talked about earlier to interact with the agent. Your other agents can seamlessly discover the capabilities of your agent and perform agent-to-agent interactions as well.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1840.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1840)



In a nutshell, this is what the overall agent building experience looks like. We started with the build capabilities where with Airbliss transforms, lifecycle management like memory and knowledge-based integrationâ€”everything about how you can build a very integrated and supercharged experience for your agents. We did some local testing and tested for correctness. The next step is to deploy it for your customers' use. As Murtaza mentioned, you can use AWS Marketplace as one of the distribution channels to get it out to your customers so they can use it. We also touched upon how you can operate and monitor your agents when they're deployed at scale.

### Alexis's Demo: Flexible Agent Reasoning and the DORA Compliance Use Case with Kiro

Now that we have talked about how you can take your agents and integrate them into AWS and start building composed workflows, let's look at how we can do more in terms of scaling your development environment experience and walking through some of the use cases that we've been able to achieve with partners. For that, let me invite Alexis on stage. Thank you, Robbie.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/1920.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=1920)



When we started thinking about composability, a few questions came to mind. Typically, can we do better with our agents? Can we enable more capability? Can they actually start to have their own kind of thinking if we provide them with more knowledge? Because it is true that as human beings, the more knowledge we have, the more reasoning we can do. That is one point I would like to present to you today in the upcoming demonstration.

Another element is, assuming we want to build more capable agents, how can we give you the capability to deploy your own agent and own knowledge bases in existing workflows and transform or build your own workflows with your own agent? The third question we asked ourselves is, can we actually drive business and technical transformation simultaneously? When we do a migration, we generally do it for a business reason. We are not migrating VMware or business intelligence reports or mainframe or .NET applications just because we want to get rid of the technical debt. It's a reason, but there is always an expected business outcome. So we need the agent to be able to help us assess priority, risk, and do wave planning. We want our agent and your agent to be able to develop those imaging capabilities.

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2020.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2020)



So that leads us to another question. If we consider that agents are a group of digital people, what makes a group smart? Usually, when it comes to agents, people are going to say they need tools, they need knowledge bases, they need memory. But the reality is that if we look at how we manage teams, a group is smarter when people can communicate together, when they know how to share knowledge.

The use case I'm going to present to you is about flexible reasoning. It's not based on a study graph. Agents are going to communicate directly with each other, share information, and come up with conclusions and create new types of data that did not exist before. So they are developing emerging capability.

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2080.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2080)



In this use case, I'm an application manager in the financial industry. I'm owning a set of banking applications, and I'm based in Europe. Unfortunately, Europe released a new regulation this year, the DORA Act. So now, if you're operating in Europe, you must prove that you're managing your system properly in case of fatal outage. If a transaction goes well, you need to be able to notify your customers.

So when I look at that, my first reaction as an application owner is: how can I know that my system is built properly? How can I do that? Can I automatically detect if my application is compliant with that new regulation? If not, how can I remediate that? And I'm unfortunate because it happens to be a mainframe application, so it makes my life even more complex.

In this scenario, I'm going to create a new agent, the DORA agent. I'm going to reuse an existing mainframe agent, the BR agent, which is extracting business rules from code, and I'm going to use Kiro for both building my agent system and for doing the remediation automatically. The way it works: BR is going to extract business rules from the legacy source code. I have created a knowledge base, the DORA knowledge base, which basically describes the DORA act, and I have created steering files for Kiro to actually explain how to create a proper architecture for transaction management or for managing customer accounts and similar examples.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2190.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2190)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2200.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2200)

So I'll take you through the first  demonstration, which is the building of that agent. I am in Kiro. We have an MCP server that can guide you and automate  the building of something we call a base agent, which is basically what was introduced to you. In these examples, I'm just asking Kiro: what is an AWS Transform and what is a base agent? So you can ask any question from within Kiro, and you can be guided step by step about what Transform is and what are the capabilities that were introduced to you.

In that case, it's saying that it's basically a foundation where you have different agents that can communicate with each other, they can interact, you can deploy them to Agent Core, and you have a registry of agents with a marketplace-like experience. It's based on something called a base agent, which is basically an improvement over the vanilla Strands agent so that those agents know how to communicate together and make use of the artifacts or the human in the loop that was presented to you.

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2210.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2210)

From there, I can interact with Kiro and ask follow-up questions. I'm going to ask: what is a base agent, what can I do with a base agent, and how do they interact? How can I equip a base agent with tools? So Kiro is going to answer all those questions. I'm going to go through a few of them. Typically at the moment,  Kiro is giving more information about what base agents are, and basically we have two types of base agents. We have an orchestrator base agent and we have a sub-agent.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2270.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2270)

 So the orchestrators are there to be able to do the reasoning part at scale, basically, while the sub-agents are most task-specific. And typically, to declare a sub-agent, all you have to do is use a tool decorator from Strands, register the tools into the agent, and you're done. Everything that was explained to you is something you don't have to worry about, basically, because you're inheriting from the base agent. It is the same thing for an orchestrator.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2290.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2290)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2300.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2300)

So in the next step, I'm going to ask Kiro to implement an orchestrator,  and I'm going to ask Kiro basically to explain how the messaging and communication happens between the various agents. So Kiro is generating  an example for me at the moment. It generates an orchestrator and a few sub-agents, and basically it's going to explain to me with a diagram how the communication works.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2340.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2340)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2350.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2350)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2370.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2370)

So if I pause for a moment here, we see that basically I have two tools which are  used in the orchestrator. One is basically a tool to contact the registry that we spoke about before, which allows that agent to retrieve a list of sub-agents and other orchestrators it is allowed to talk to, so you have control over the visibility of agents. The second tool which is going to be used is the send message tool, and basically you're allowing that agent to talk to other agents, and that's how you have control over your communication between the different agents and their capabilities. It is as simple as that.  

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2410.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2420.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2420)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2430.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2430)

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2440.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2450.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2450)

So moving on, the diagram  explains the same thing and how messages are going to jump from one agent to an API and then be redirected  to another agent based on the agent name.  In order to save time, I'm going to go to the part where I'm actually asking Kiro to generate the entire system for me, if I can make it happen.   I'll stay here for a moment. There's an issue. I've been asking Kiro to take the example I provided to it and basically take me through the different steps to package the agent, create a Docker image, push the Docker image to AgentCore, register the agent, and Kiro implemented the pipeline end to end for me. So I don't have to know how those APIs work, and I can actually automate the making and deploying of an agent.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2490.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2490)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2500.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2510.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2510)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2530.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2530)

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2540.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2540)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2560.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2560)

The second demonstration is about the creation of the Dora agent.   What I've been doing here is I have created three different sets of data. One is the translation of the Dora Act into a YAML file  which describes each and every act, their main functional features and capability. It has been done automatically using an LLM and the Dora Act itself. From there,  I have described the Dora Act per business domain and per feature. Once this has been done, we create an equivalent knowledge base  for the remediation. So it is a set of Kiro steering files with YAML front matter, using the same approach, where we actually describe the features and capability that we want to be able to interact with or to develop with Kiro. And then we have a longer description of the remediation when we want to use Kiro for transforming the code that we currently have in our application. 

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2580.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2590.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2590)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2600.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2610.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2610)

That agent is completely generic. If you have different kind of knowledge into it, it will be able to automate the remediation of your application based on the additional rules.  Now I'm going to use that new Dora agent into a subset of the existing mainframe workflow.  I'm selecting the COBOL files that I want to investigate in that application. Transform is going to extract the business rules. It's then going to let the Dora agent connect to the BRE agent.   They are going to have a discussion together, and basically the Dora agent is going to get back to us with a list of recommendations. It's doing similarity analysis between the Dora Act, which I put into a knowledge base, and the extracted business rule from the source code. So that allows us to do a functional similarity detection.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2630.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2630)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2640.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2640)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2670.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2670)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2680.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2680)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2690.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2690)

Here is the result that I'm having.  In this case, the Dora agent detected 24 rules that are completely aligned with transaction management in the Dora Act and is recommending to modify 20 of those.  The detailed remediation are saved into the artifacts store and can be used by Kiro, but I'm getting a report in the chat which basically tells me which Dora regulation should be enforced in the application. Then in the next stage, I'm getting the list of business rules that I should actually enforce, and I'm getting a link to every Kiro steering file  that has been produced automatically by the agent to automate the transformation of the code from within Kiro.   Basically, the use case was developed based on the existing BRE agent from mainframe, the code analysis agent from mainframe, and the new Dora agent.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2730.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2730)

And then we push all of that to Kiro. We could have included more agents like the SMF agent to know the usage of those transactions on the real system and see their criticality and get more recommendations because maybe that transaction is highly critical or is used only once a year and I have more time to do my remediation. So the best practices when developing an agent is you need to think about them as intelligent modules.  So you want them to expose their capability as skills by using the two-layer character that we've seen previously, so that they can negotiate how they are going to interact basically. When they need to share data, you want to make sure that the way the data is saved in the artifacts store is known to the agent, so you can basically create the skills where they describe the schema of their data so that they know how to interact, or you may want to do like I just did, put the data into a knowledge base, because then the retriever is going to enable the agent to directly exchange data.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2800.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2800)

### Wavicle Data Solutions Partnership: Building Specialized Agents for BI Migration

I'm now inviting Muthanza to introduce us with the next use cases. Thank you, Alexei. Several partners have been working with us as design partners to build new capabilities. And one such capability is BI migration to QuickSuite. I'm going to invite Naveen on stage to talk about Wavicle Data Solutions and how they have been working with us. 

Good afternoon, everyone. I'm Naveen Venkatapathi, co-founder and managing partner at Wavicle Data Solutions. We work with enterprises to help modernize and build composable data platforms. One of the ways we have taken this to a new level with AWS Transform is that we have combined our proprietary accelerators and the powerful orchestration layer AWS Transform brings. We have combined these two to build specialized agents that can help our AWS customers or other AWS partners to be able to leverage specialized agents to be able to drive the modernization at an incredibly high velocity and lower labor arbitrage to be able to drive value to our customers very quickly.

One of the things that AWS Transform gives us is the capability to not only build these specialized agents at a very fast pace but it also helps create a very flexible customer experience on a very simple chat-like interface. I invite my colleague Anantha to demonstrate our specialized agent and how we have built this on the AWS Transform platform.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/2910.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=2910)

### Anantha's Demo: Power BI to QuickSight Migration with Self-Service Assessment and Conversion

Hello everyone. My name is Anantha Chopali. I'm chief architect for AWS practice within Wavicle Data Solutions, and by partnering with AWS Transform team,  we have been able to take advantage of the platform's inbuilt capabilities where we can now bring our solution which has high impact and high value for our customers for quicker adoption by the customers. In this case, a BI migration solution that helps customers migrate from their existing BI legacy platforms over to QuickSuite and specifically the Quicksite part of it.

There are several advantages that we have seen working with AWS Transform team over the last several weeks where we have seen that we can actually accelerate innovation without the procurement delays. What I mean by that is that we are able to bring our solutions to the customers very quickly, which is the customers would be able to adopt very fast and without having to go through longer procurement delays, which could take several cycles and require heavy paperwork. So if you are a partner or a customer, you know what I mean by those. Think about non-disclosure agreements, master service agreements, statements of work, and not to mention any change requests and so on. All of those can be eliminated because the solution is now easily accessible in a self-service manner.

Another advantage that I would say is enhanced security posture.

While Vivial as a premier consulting partner brings best-in-class solutions to our customers rooted in best practices and well-architected frameworks, customers often have concerns about their data. There are data security and privacy-related concerns about data leaving their network. Not anymore. V Solution now runs on Bedrock agent core runtime, which includes the security posture that's offered built-in. Now customers can take advantage of running the agent core runtime 100% within their environment. This means there is no need for additional risk assessments or infosec reviews because their data does not leave their network. What happens to their data stays in their environment.

Our offering is also available through the familiar marketplace deployment channel, which means customers do not have to navigate through newer procurement channels. They can get access to the offering through Marketplace using the same channels they are already familiar with. The next advantage I would highlight is rapid time to value. For proofs of concept, customers can take advantage of doing quick POCs without having to think about the paperwork, and this helps them validate the product fit for their use cases before they consider full-fledged migrations.

Lastly, this helps customers unlock a better, faster, and cost-effective self-service path to ROI. What I mean by that is customers can actually do the entire BI platform assessment that typically takes several weeks to months in just a few minutes to hours. Once they have the exhaustive report, which we will look at shortly as part of the demo, they can use that information to come up with an ROI that they can take for internal approvals. Once they are ready, if they have the skill sets internally, they can use the self-service mechanism to migrate all of their reports and dashboards to Amazon QuickSight. Alternatively, they can use the white-glove consulting engagement by partnering with their existing SIs or GSIs or even reach out to Wavicle Data Solutions because we have done it several times over.

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3220.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3220)

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3230.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3230)

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3240.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3240)

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3260.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3260)

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3270.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3270)

[![Thumbnail 3280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3280.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3280)

With that context, let's take a look at the demo. In this particular case, before I move on, we will be looking at the Power BI analyzer, where we will first look at the Power BI interface if you're familiar with that, the Power BI reports and the workspaces, and we will look at how we can use the guided self-service AWS Transform approach where customers can interact with the transform platform using natural language and help analyze their entire BI platform.  Like I said, first we will look at the Power BI workspace.  A workspace in Power BI is very similar to a folder in QuickSight, and in this case we have three reports in the Power BI workspace.  The first one is a low complexity dashboard, which has one filter, a couple of different types of charts, and a few KPIs and metrics. The second report is a high complexity report.  The complexity is based on how many objects are used in creating those dashboards. As you can see here, we have different types of charts from  heat maps to pie charts and bar charts, and there are some custom actions that when someone clicks on them, they actually do something specific.  And lastly, a very high complexity report where again there are several different types of charts that the customer has.

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3300.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3300)

Think about this in the multitudes of hundreds of thousands of reports in an enterprise setup. Migrating one or two is easy, but when you want to do it at scale,  AWS Transform and Wavicle's offering for BI migration would help.

[![Thumbnail 3310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3310.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3310)

[![Thumbnail 3320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3320.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3320)

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3330.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3330)

[![Thumbnail 3340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3340.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3340)

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3350.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3350)

The customer would navigate to  AWS Transform, select the right offering for the Power BI analyzer, and create a job. This starts the agent core runtime  within the customer's environment to set up the environment. You can then start interacting with the Transform platform where you would ask it to analyze your Power BI  workspaces. You can either provide new credentials or use connectors that are pre-set up to connect to your  Power BI service and ask for all the Power BI workspaces that are available. Once you see all the Power BI workspaces, you can ask it to select a specific  workspace that you want to focus on, and then you can either analyze one or all of the reports within that workspace.

[![Thumbnail 3360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3360.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3360)

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3370.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3370)

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3380.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3380)

[![Thumbnail 3390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3390.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3390)

As we can see,  it actually does a quick check to the S3 because any intermediary log files and all the data gets stored in your S3 bucket. While this connects and  performs the activity of analyzing your Power BI workspaces, it would go through and show all the details for you for complete transparency. Once you have all the details and the data is uploaded to S3,  it would generate a report in Excel form, which is your complete  analysis and inventory details that is ready for use to do any ROI or TCO analysis.

[![Thumbnail 3400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3400.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3400)

[![Thumbnail 3420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3420.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3420)

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3430.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3430)

In this example,  this is a report where you can see there are several tabs, and this particular tab shows all the calculated fields that were used in your reports and the formulas that were used, so you can easily translate that with a click of a button when you're converting them over to QuickSight. Next, it also shows those types of charts  that are not natively supported in QuickSight and automatically provides suggestions for the native supported QuickSight chart types. Once those are configured and you're okay with  it, you can actually continue with the migration process of converting those dashboards.

Lastly, it shows the confidence ratio of how much it actually thinks it can convert automatically versus those case scenarios where it would require some manual intervention that may need to be looked at case by case. With the help of AWS Transform and the BI migration offering, you can unlock insights to your business intelligence platform. You can accelerate migration and do it faster, better, and cheaper. We are available at our expo at booth 611. If you have any questions or would like to have a demo, please feel free to reach out. We are also available on Wavicle.com should you have any questions. Thank you. With that, I would reinvite Murtaza.

[![Thumbnail 3520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3520.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3520)

### Partner Ecosystem Expansion: Accenture and Capgemini Mainframe Modernization Solutions

Thank you, Ananta. You saw how Wavicle Data Solutions is extending the migration capabilities of AWS Transform. We are also working with other partners. Here's what Accenture is doing for mainframe modernization, where it typically targets regulated industries like financial services, healthcare, or telcos.  There are mainframes that were built several decades ago. While migrating and modernizing those workloads, especially for the regulated segments, the practitioners typically have to consult domain and industry experts. Instead of having to talk to an expert separately while performing migration, what Accenture is doing with AWS Transform composability is that they're augmenting the knowledge bases that are available in AWS Transform and making the chat even more powerful.

[![Thumbnail 3590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3590.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3590)

The practitioners can do it directly within the AWS Transform interface via the chat experience instead of going and talking to domain and industry experts separately. Additionally, they're also extending the components that are available in AWS Transform like business rule extraction and catering them to what the industry-specific migration needs have.  Another partner who is co-innovating with us is Capgemini. Capgemini is working on another use case for simplifying mainframe modernizations. One of the key aspects of mainframe modernization is that many times it requires understanding of what the current applications and workloads are doing, extracting the business rules, and then taking a forward engineering approach to create the application again.

[![Thumbnail 3660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3660.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3660)

[![Thumbnail 3670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/b6acc1c139ac2123/3670.jpg)](https://www.youtube.com/watch?v=yLYKD5nXGvY&t=3670)

This involves reimagining it while keeping the business rules and business logic intact and creating the entire architecture in a modernized form. That's where Capgemini is leveraging AWS Transform composability. These were the design partners, and we'd like to hear more. If you want to build with us on AWS Transform, you can reach out via your partner development manager or partner SA or tell us what you want to build in Partner Central. The link is available via the  QR code here. Thank you so much for spending the time and attending this session. 


----

; This article is entirely auto-generated using Amazon Bedrock.
