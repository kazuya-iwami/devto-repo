---
title: 'AWS re:Invent 2025 - Coding cost-efficient multi-tenant knowledge bases w/ time-to-live (TTL)-SAS412'
published: true
description: 'In this video, AWS Solutions Architects Mike Gillespie and Steven Warwick demonstrate building cost-efficient multi-tenant knowledge bases using Bedrock Knowledge Bases with different vector stores (OpenSearch, pgVector, S3 Vector). They explain how to enforce data isolation through metadata filtering with tenant IDs and user IDs, implement tiered performance based on customer needs, and manage data lifecycle using DynamoDB TTL triggers. The session includes live code walkthrough showing CDK infrastructure setup, custom data source configuration, document ingestion with metadata, filtered querying, and automated cleanup processes. Key advantages highlighted include abstracting vector store implementations to avoid rewriting queries for different backends while maintaining security and cost optimization across tenant tiers.'
tags: ''
series: ''
canonical_url: null
id: 3085109
date: '2025-12-05T03:05:06Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Coding cost-efficient multi-tenant knowledge bases w/ time-to-live (TTL)-SAS412**

> In this video, AWS Solutions Architects Mike Gillespie and Steven Warwick demonstrate building cost-efficient multi-tenant knowledge bases using Bedrock Knowledge Bases with different vector stores (OpenSearch, pgVector, S3 Vector). They explain how to enforce data isolation through metadata filtering with tenant IDs and user IDs, implement tiered performance based on customer needs, and manage data lifecycle using DynamoDB TTL triggers. The session includes live code walkthrough showing CDK infrastructure setup, custom data source configuration, document ingestion with metadata, filtered querying, and automated cleanup processes. Key advantages highlighted include abstracting vector store implementations to avoid rewriting queries for different backends while maintaining security and cost optimization across tenant tiers.

{% youtube https://www.youtube.com/watch?v=IQ9YWcgbAY8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/0.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=0)

### Introduction to SAS412: Building Cost-Efficient Multi-Tenant Knowledge Bases

 Welcome to SAS412. We're going to talk about cost-efficient multi-tenant knowledge bases. If this isn't the talk you want to attend, you're welcome to find one that's a better fit for you. My name is Mike Gillespie. I've been with AWS for a little over 9 years, and I really enjoy these types of engagements where we're talking with our customers and learning about what problems they're facing.

As a code talk, this is going to be very interactive. These sessions are quite dull if you don't participate, so we're depending on you. When the time comes, I'll bring a mic around so everyone can hear your questions. There will be times when you stump us, and that's fine. We don't know everything, but I'm confident that won't happen today. Steven, do you want to introduce yourself?

I'm Steven Warwick, a Solutions Architect at AWS for the last 5.5 years. I'm definitely a builder. I was an engineer for about 30 years building various things across different industries. I've particularly enjoyed working with SaaS-based architectures and have been working with customers on that ever since I started at AWS. That's what we're going to dive into today. We both work with software companies, so we're familiar with the challenges that software companies face every day. The origin of this came from something I was working on. People came to me asking how to build knowledge bases, and I was happy to help. Then it evolved into a more complex question: we need to have different tenants and different sections so people look at the same stuff and get the same answers.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/110.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=110)

 We'll start off by talking about what our knowledge base is. I'm sure with AI many of you are already familiar with that, but we want to set a baseline of what we're talking about today and then discuss some of the challenges you face running RAG type architectures with multi-tenancy. We'll focus on a few of those. We have a demo which will then go right into the code and show you where in the code these decisions are being made and where the logic is happening. That's where you come in. That's where you can ask questions and see how we think through these things. I love these code talks and chalk talks because it gives you as a customer a window into how AWS thinks about these problems. We see these every day working with our customers on AWS, and we have a certain point of view. This will help you see that and be able to translate it into something you see every day.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/160.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=160)

 There's not always one right answer either. Our answer is definitely one of the ways you can do it, but we've had lots of comments from the sample code that's out in the world too. If there's one thing we want to get to, it's feedback from today. What do you see out in the world and how are you using things? That's part of what our point of view is. This is how we see things, but you may approach things differently because you have different problems to solve.

### Key Design Challenges: Data Isolation, Tiered Performance, and Lifecycle Management

Jumping right into it, what are some of the main design decisions you have to face when you're building multi-tenant knowledge bases? One is data segmentation. You have tenant-specific search. One tenant wants to upload documents or other resources into your application and then be able to have a chat against those or search against those in a semantic way. But of course, the one thing you can't do is let the data spill over from one tenant to another. So what are the mechanisms you can use to enforce that? We also have hybrid search, so it's not just semantic search but also keyword search. Then enabling that with AI. How do you hook that up to a chatbot or be able to run AI type queries against that within the scope of that tenant? There's another paradigm we're going to talk about today too: how to do that easily across different types of vector stores. We want to make sure that Customer A might want super low latency, while Customer B doesn't care as much about latency and just wants to pay less and have bulk processing. How do we do that in an easy way across the board so we don't have to do a lot of work to set that up? That way you don't have to write a whole new application for each way you access your data.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/260.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=260)

 I've talked about this already a little bit, but data isolation is the core tenet of multi-tenant applications. How do you enforce data isolation? The philosophy I have is don't leave that to chance. You have to build the infrastructure around it to enforce that it happens. Steven called out tiered performance. Not all of your customers are the same. You may have a free tier where you're just trying to get customers a sense for what your application can do, but you want to minimize the costs or really use a pay for what you use model versus established customers that really value latency. Working with software companies that build chatbots, the number one complaint customers get is latency. I type in my question, and they have to wait 30 seconds for an answer. That's not acceptable.

So how do you make your application be able to balance both the cost effectiveness but also meet the latency requirements of your customers? And because the vector stores that power the knowledge bases are some of your pricier data stores, how do you make sure that you have the right data lifecycle management?

There are a couple of reasons for this. One is certainly for cost, but also unstructured data tends to age very quickly. You may have an updated document that now reflects a new version of the state of reality. How do you age those out in a way that makes sense for that particular tenant? This is just different industries you'll see this a lot. So like the real estate industry, they might want to have segments of their market set up, but that real estate cycle is changing quickly. Or patient records if they're throwing it in a knowledge base, the doctor might see someone every six months and they might want to throw that in and then take it out, maybe for regulatory things as well. They want to make sure it's only in there for a certain amount of time, but also allows them to shrink the amount of usage that they have to use and only do it when they need to do it.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/380.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=380)

### Comparing Vector Store Options and Architecture Overview with Bedrock Knowledge Bases

So looking at tiered performance,  the underlying vector store that powers your database makes a big difference. Comparing three of these options, and there are many more, but looking at OpenSearch is a very popular vector store for the customers that I work with. It's very low latency, it allows you to do hybrid search, and you can have the provision cost to meet your demand. Compare that to pgVector, which has the advantage of being also low latency, but it's also integrated with your structured data. So if you have hybrid queries that query against structured data and your unstructured data, you can do that in one spot.

And then last year at re:Invent we announced the S3 vector bucket which allows you to run vector queries against data stored in S3. Really cool because you only pay for what you use. You pay per query and for the data that's being stored. You don't have that provisioned cost that you would in OpenSearch. However, that does come at the cost of higher latency per query. So the case that I mentioned before where you have a free tier, you may want to have that on the vector bucket so that you're only paying for that evaluation use case that your customers are just kicking the tires. Whereas your more established customers may want to have lower latency.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/470.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=470)

But as Steven mentioned, we don't want to write three different applications more or less to interface with our vector store. How do we abstract that out so that we get the simplicity of one way to access our data, but the flexibility to drive the backend how we want it?  Another challenge with the lifecycle management is how do we define rules for data retention? How do we ensure that you're running against the most up to date and accurate information? I know working at Amazon we have an internal wiki which is usually out of date shortly after you publish to it. So how do we make sure that the data that's in there is actually up to date and age out the data appropriately?

And the tenant tiering that we talked about is how do you have different rules based on the tenant? As Steven mentioned, you may have real estate with a very high churn rate of data in your documents in your vector store versus ones that may have much longer lifecycles and be able to configure that per each tenant. It could be file sizes, it could be file types, that kind of stuff as well. So given that challenge, the architecture that we're demonstrating today, and again as Steven said this is not the only way to do it, but we found this to be a very effective way to abstract out what that vector store is so that we can simplify our code but also give that flexibility on the backend.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/520.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=520)

So walking through it, you have the user interface secured with Cognito and that's important because Cognito stores which tenant that user belongs to. So that will be part of the JWT token that gets sent through so that the backend can then make decisions based on which tenant they belong to. That gets sent to the Lambda function which uses Bedrock to do the inference on the LLM and then uses the Bedrock Knowledge Bases to interface with the vector store on the backend whether it be in OpenSearch, S3, or elsewhere.  I think that's like one of the key things we're going to show is how we're using the custom stores and the custom data types and stuff so that you can do this in other ways where you're storing your files in different areas of S3 or something like that and you can use metadata files and things to pull it in, but it's an ingestion pattern that is kind of like a fire and do a batch pull in.

We're going to dive into how you can do this in a more customized manner, and we'll go through some of the code to show you what that looks like at a high level. We only have limited time, so we won't go through all the code, but we want to make sure you understand the key points you need to build into the system to derive individual files that you can pull into these stores for tenants, for users, or however you want to break it down.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/640.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=640)

### Live Demo: Multi-Tenant File Management and Chat Functionality

This session is interactive, so if anyone has questions, feel free to ask.  We're going to do a quick demo to show you what's possible. This is just a technical demonstration to show you what's available. The first screen you're seeing here is Tenant A. I've logged in and added a bunch of files. I'm trying to pre-populate this because we know that demo environments can be unpredictable, and we want to make sure everything runs smoothly.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/690.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=690)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/710.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=710)

What you'll see here is I have several files for Tenant A, which uses an S3 vector store. You'll notice that some files have timed out and others have an expiration date. Those are the ones I've marked to add to the knowledge base. The system shows you how much capacity you have. When you're building these systems, this capacity management is probably baked in, though you typically won't show these screens to your end users.  Imagine that file gets ingested. Now I'm Tenant A, a user in the system with my indexed files, and I can chat with those particular files. 

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/730.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=730)

I've already done a chat before, and as expected with demos, I'm only seeing and interacting with the files I've ingested. This could be organized on a per-tenant basis or per-user basis. I've ingested these S3 files and asked some questions about them. Now let me switch over quickly to Tenant C  to show you how tenancies work with OpenSearch. Tenant C has permissions to ingest many more files and has higher query rates. You'll see that I've indexed files again, but they have a much longer expiration period as well.

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/760.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=760)

While they're processing, they can do another chat and will be chatting against their own files, but it's per-tenant and per-user in this case.  When I go into a chat here, you'll see my indexed files and I can ask a question like "what programming language is this?" Let me see if I can actually get a response. My network seems to be failing, so we're going to pretend that response came back. That's why I had the other chat pre-populated. What we want to do is give you an overall view of what's possible. You can break these files down in different ways, but we want to get into what the code looks like and how we make this happen.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/800.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=800)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/810.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=810)

### Deep Dive into CDK Infrastructure: Setting Up Knowledge Bases and Storage Configuration

I'm going to switch over to our code here and zoom in so people in the back can see it better.   In here, this is my main stack, and what I'm doing is creating a couple of knowledge bases. You'll see that I have ones where I might want to set it up or I might not. I want to show you this because we're pausing for a second. When you say main stack, what do you mean by that? That's right, this is the CDK application that will build out the infrastructure inside of AWS for us. This part of the stack is the primary stack that runs first and builds out. I'm calling out to a bunch of other things to build out the knowledge bases and do the connectivity between everything.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/890.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=890)

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/900.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=900)

I want to call this one out because we're going to jump into this knowledge base class in a second. You'll notice there are a couple of them. There is the OpenSearch one and there's a Postgres knowledge base one. I should note that the CDK constructs and CloudFormation constructs for S3 Vector are still in the mix, or being turned on. That is built off in a different stack because I have to use the APIs directly for those right now. So there are custom resources  to build that out, but for the end result you're still going through a knowledge base for the querying, which we'll get into in a second.  The key takeaway there is that you're building infrastructure to have the knowledge base and it's connected to the backend vector store.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/920.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/930.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=930)

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/940.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=940)

Yes, exactly, so this is that knowledge base stack that I was talking about and I'm going to zoom in here. We've got a bunch of stuff at the top that's setting up a few things like sanitizing names,  and things like that, but one of the key things we want to do first is set up the knowledge base. I'm just going to copy  some things in here to save time because nobody wants to see me just type away. So we've got our knowledge base, we're setting up the name and things like that. 

[![Thumbnail 950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/950.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=950)

Now the next most important thing is the knowledge base configuration, so I'm going to copy that in here. You'll see what I'm setting up is the vector type. You can set up these embedding models and things like that, whatever you want. This is going to apply across all the knowledge bases, or you could pass these in as variables for the particular knowledge base that you're using.  I've set them across the board for the knowledge bases that I'm going to be setting up.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/970.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=970)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/980.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=980)

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/990.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=990)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1000.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1000)

Then we want to do the storage configuration. In this instance, this is where I'm passing in and creating the knowledge base storage configuration, and I'm passing in my OpenSearch for my Aurora stuff.  If we jump into there, you'll see that for RDS I'm setting it up a specific way. I've got my embeddings and chunking for RDS,  and if it's Aurora, then we're going to be setting up a different way.  Then for OpenSearch, we have to use OpenSearch server list, and I'm going into each individual one and setting up the individual vector stores. But once we get to the actual querying and things like that, I don't have to worry about that.  I can just say this is the one that I'm going to use, and each one is set up so I can just point to it and it can go.

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1030.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1040.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1040)

That was a lot to cover. I really want to open up to questions or comments. If there are points like why did you make that decision  or what, you know, why did we do that? And if no one asks questions, then I'm going to ask questions, and I'm probably not going to ask the same question you are.  Any questions, raise your hand, and I may have to step to the side to see. While he's going to find it, there's the next thing that we need to set up. We've got the knowledge base configuration, but we do need the data source, right? We need to tell it where the data is coming from. So I am going to set up a data source in my code.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1070.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1070)

One question is related to the different vector stores. What are the pros and cons for each one?  I can't see. Did you hear the question? I did not. The question is, what are the different pros and cons for the vector stores that you had set up? So the pros and cons, I think Mike went over a little bit there, but it's typically what I've seen with customers is the price to latency ratios that they're seeing. So if it's a customer like you, you might have a free tier where you really don't want to spend a lot of money on that. That's just the way to pull the customer in to see what you're doing, so you might use an S3 vector store there because the latency on every question might be a little longer depending on the file types you're using and things like that. So it's kind of on you to do a little bit of research to figure that out.

We have seen, you know, also there's if you're using Postgres already and you're using OpenSearch, you might just want to go those routes to add on to that. I mean, that's typically what I've seen is like we are familiar with Postgres, we know how to run it, we know how to operate it, and we need a low latency vector store. It's a good fit. For versus OpenSearch, you know, we have many customers that run OpenSearch for other purposes. It's just a natural fit to put their vector store in Postgres, but you know, there's many others. Pinecone would be an example. So there's probably a whole talk just on picking your vector store, but you know, the really the point here is one size does not fit all. You'll likely want to have multiple options with different dimensions, and the kind of the extreme with latency being S3 versus OpenSearch or one other. Does that help? I'll be right back unless you can yell and I'll repeat the question. I'll come back. You can jump back there. There you go.

I was curious, so right now we have a knowledge base with fairly small tabular data that's loaded in S3 and use OpenSearch for that. I was wondering if for small tables, it will be worth it moving to PostgreSQL with pgVector.

Will there be any accuracy implications for the agent that uses that knowledge base? That's going to be a tough question for me to answer. There are some samples out in the AWS samples GitHub repo that let you run your own performance testing on this kind of stuff. What we found is it really depends on the type of data that you have and the size of data and to your point about how many columns there might be in there, different types of things that have to chunk out, and there's quite a vast difference between the embeddings that you're using and the chunking size that you're using. So it's really difficult without looking at your data and doing some performance testing first to really figure that out.

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1290.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1290)

That's why we usually go back to the customer and say let's do a couple demo runs of this and use some of the samples that we have out there on your data to figure that out. I'll stress that the embedding and the chunking have a much bigger impact on the quality of the search than necessarily what the engine running the math is. The key thing I want to add in here is the data source. When you're  creating the data source for the knowledge base, we're setting the data source configuration to be custom, and this is one of the key points. This allows us to actually say instead of going to the data like S3 and just pulling everything in when the files are there, we want to do this very bespoke and custom so that we're setting it up to be exactly like that.

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1320.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1320)

You'll see once we get into the ingest part. I'm going to just copy the rest of this in here so you see what it looks like.  I've set up my data source now with the custom configuration. We're going through stages here, right? Set up the data source, set up the knowledge base, we're going to get into the ingest next and the query, and then what it looks like when we time out data and clean that up. Did you still have a question on that?

Yes, if we need to inject a caching layer on the previous architecture, how can it fit? How would we put a caching layer on top of that previous layer on top of the vector store? That's a very good question, not one that I've thought of. If it depends on whether you're using CloudFront as a fronting mechanism or something like that, you could try to do some caching of the results in there. I'm sure you could probably use ElastiCache and some other tooling in there if you wanted, but I don't have a bespoke answer for that from here, and the caching wasn't part of this system per se.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1410.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1410)

### Custom Data Ingestion: Implementing Metadata-Based Tenant Filtering

One more question then we'll move on. What are some advantages you've seen to adding the knowledge base layer versus going directly to individual sources? Actually, we'll cover that stuff now. We're going to go into that and show why we might want to do this. Ingest and querying  are part of those things. I'm going to go to the ingest section here. This is a Python file that's going to be doing my ingestion of my documents. This is when I clicked a couple of those files and said run the ingestion for just these files that they're sitting in S3. My users uploaded them and I don't want them to be in my vector store at all times. I just want to do it at particular times.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1460.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1460)

You'll see in here that I'm running through the files and I'm going to set up a document structure. This is one of the key things that we're going to go through here. We're going to set up a content type. So we've got a document and we've got content, and again the data source type is custom  and we're giving it a file identifier and the location of the file and we're saying it's S3. So we're setting up saying we know where the file is, but next we're going to add in some extra stuff which is arguably the most important thing, the metadata for these files that we're passing in.

So if you're doing this in a bespoke manner, you're going to have to do this for OpenSearch differently for S3 Vector, different PostgreSQL, whichever vector store you're using, you're going to have to figure out how it's doing these filtering mechanisms and do it through those. If you're using OpenSearch, you're also on the query side going to have to figure out how do I do that query, or if it's pgVector I have to write the SQL query to do this with the filtering in place. I'm going to add this in here so we all know exactly what I'm talking about.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1510.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1510)

I'm adding in metadata  and again you could do this with S3 if you want to do bulk ingestion. There is the ability to put metadata files in there, but what this is allowing me to do is check those files and individually pull them in. I'm also setting the metadata, the user ID, and specifically the tenant ID so that when I'm doing the querying I can come back and do the exact same thing in reverse. I set the tenant and say I want a query for that tenant.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1550.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1550)

There's one thing that I am not showing in here. I'm also adding these into the knowledge base, into the vector stores, but  I'm also adding a record into a DynamoDB database. In that database I'm setting a time to live, a TTL, which is arguably one of the most underrated things that people just don't use for some reason. It's awesome. You can do TTL or events and triggers from other systems in AWS, but the per-record way of doing it in DynamoDB is very cost effective. That's what lets me set up that expiry date in the files that I showed in the demo. There's an eight-minute window or eight-day window or a day window, twenty-four hours, whatever it is.

I've now gone through the ingest. I've set up the metadata to be able to know that I can filter on those, and this metadata is in the vector stores tied to the vector stores. This isn't in DynamoDB. The key is that regardless of what vector store you're using, it's using this structure. That way you don't have to know that it's a specific index in OpenSearch. I don't have to duplicate this. This code is one set of code, one set of queries that I have to do across the board for whatever vector stores that Bedrock knowledge bases supports.

So to answer the question that came up, why wouldn't you just talk directly to the vector stores? Well, this gives you an abstraction layer that allows you to work at the metadata level that would then push that into the vector store regardless of how that's being stored. We're plenty good on time, so I feel like I should be standing too, but then I'm typing really odd. My question would be how would the system react to nested metadata? For example, let's say we have a book. A book has multiple authors, so authors would have names, and these names could be in Japanese characters in Japan or English characters in the UK or US.

How would you design your metadata for Bedrock or for more efficiently? I think I understand your question about Unicode characters or other types of characters if they're in there. In this form I'm not sure if that would really come into play because in this form I'm saying a tenant ID and I'm filtering by a tenant and a user, not specifically the content. The content gets ingested as whatever vector store is ingesting. So it's using that embedding model to break it apart. I think the question would be going back to that embedding model and making sure that it supports that.

In this schema, can you have a list or an object type as part of the schema? I'm not sure. I'd have to figure out the backend and answer. I'll have to research. I think that's how I would start looking for it, but I don't have the answer off the top of my head. There's not a definitive answer. On the querying side you can do all kinds of things. We're going to get into that a little bit in the next step.

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1780.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1780)

### Querying with Security: Filter Expressions and Retrieve-and-Generate Operations

Inserting the metadata is an individual key as far as I'm aware, but I haven't confirmed that. You certainly could have author one, author two, author three, but you'd have a finite list of authors for your book. If that's a question I'd have to get back to someone on if the metadata supports array fields and things like that. We've gone through and set up our knowledge bases, indexed files into the knowledge base. The next thing obviously is I want to ask it some questions, so we want to go into the querying of the knowledge base. 

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1800.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1800)

What I'm going to do first is you'll see here we want to set up a filter and I'm going to zoom in because like everybody else I can't see. So in here we're going to set up a filter expression.  This is where we're going to be pulling in the exact same stuff in the opposite direction.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1820.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1820)

When we're doing a query, this is another Lambda call that is coming in from the chat session. Through the JWT token, I'm able to get my tenant ID, my user ID, and other information like that. I've pulled it out up above in this code, and the key thing to look at here is this filter expression.  What it's doing is setting up that tenant and the user ID, and then I'm also passing in a list of files. This could actually be useful later onâ€”even if they're indexed, you could say I actually only want to query on 3 of the 100 files that are in there. Because to Mike's point, sometimes wikis go out of date or other files go out of date, and if the TTL hasn't expired but I know I'm really focused on my search, I could query on just those files.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1890.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1890)

In this particular example, I'm just passing in all my file IDs that have been indexed so that I'm searching for my files. I'm filtering layer after layer to make sure that I'm getting just the stuff that I want to query on, and it's leaving nothing to chance. So if someone tries to inject in the prompt and send me all of the data about tenant 2, that's not going to really help them at all. 

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1910.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1910)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1940.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1950.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1950)

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1960.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1960)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/1980.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=1980)

So then we have to set up our retrieval parameters.  The retrieval parameters are setting up the query and the model we're going to use for turning our query into embeddings. Then we're passing in our filter expression, and we can control the number of results as well. Once we do that, now we can go into the retrieve and generate section.    What we've done here is set up our parameters. Once we've set that up, we would be calling a perform retrieve and generate call. Inside of the knowledge-based querying,  from the Bedrock agent runtime, we're going to call the retrieve and generate and we're passing in those retrieval results.

This is where, again, back to the point of why you would use Bedrock Knowledge Basesâ€”I set that up as a query for Bedrock Knowledge Bases. I didn't have to really understand what's going on underneath the hood or what vector store I'm going to. I'm doing this, and I could do this across every single vector store that Bedrock Knowledge Bases has, and I don't have to change this model at all. So if another customer came to you and said they've got a bespoke use case where they want to use a different type of vector store, maybe there's a government regulatory requirement or whatever it is, you don't have to worry about it. You can just spin that up as part of your infrastructure and then just point the code to it, and you should be good to go. It gives you a nice clean separation of concerns.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2050.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2050)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2060.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2060)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2070.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2070)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2080.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2080)

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2090.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2090)

Regarding the vector store with the query, the chunking is where the chunking happens and where do you configure that?  It's part of the knowledge base configuration. When we're setting up the knowledge base, we're setting up the data source and we're going to set up some of the chunking strategies inside of there.  But also when we go back to the indexing,  we've set up some of it in here. When we're setting it up, we're setting up individual data sources and a specific chunking strategy and things like that.  Then on the query side, we are doing the same thing with the query.  You're setting up the model that you're going to use, the foundational model, and some of the options that you have there. It's not really the chunking thereâ€”you're just basically picking the model to be able to do the embeddings.

After the question, we move on to the last part, which is just the cleanup. I have a questionâ€”maybe it does this automaticallyâ€”but if Tenant 2, for example, is chatting with their own index of documents and I update one of those documents, how does the tenant know that the document is updated and the embedding is regenerated? I don't want to lose the context of the chat. The document is updated and the embeddings should be regenerated, but the contextâ€”if I get your question right, it's like if you're continuing a chat and you want to search for more things, you just have to keep passing the context from the previous messages. So you do start getting a longer and longer context window, and I don't have a good answer for you on whether we can slim that down at all.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2200.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2200)

### Data Cleanup with DynamoDB TTL and Audience Q&A on Vector Store Implementation

I think you'll see even in systems like Quiro and some of the newer systems out there that they start hitting a limit and then they'll summarize for you and try to push that off to something else. So those are kind of the patterns that have come into play these days until someone smarter than I comes up with a better pattern for that. Thanks. Any other questions? Alright, the last bit of this is just the cleanup.  We've gone through the stages again. We've ingested, we've queried, and as we mentioned, we've got a TTL system. So whenever the TTL triggers, we need to go back and make sure that because we're using a custom store, there's nothing that specifically tells us to clean up all that information. We actually have to go in and extract the file information from the TTL trigger.

In the DynamoDB database, there are a couple of things: the file ID, the data source that it's in, and the knowledge base that it's in. We're going to use that information and extract it, then delete those records from the knowledge base. In this particular spot, we don't really have to care what knowledge base it isâ€”we're just giving it a command and it figures out what needs to be done and goes off and deletes it. I'm going to grab that chunk of code here. It's a rather large chunk of code, so I'm not going to type it in here.

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2290.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2290)

One second, while you're doing thatâ€”when Steven showed this to me, I thought this is one of the more clever parts of the solution because time-to-live is a really important problem. You can't just have a single rule for all of your documents. How do you manage that? Keeping a record of that index  in DynamoDB and then using the DynamoDB stream to essentially trigger that action gives you a lot of flexibility. You can use the built-in time-to-live within DynamoDB, or you could delete the record manually if there's a right to forget or if you're trying to expire a document. You just delete the record in DynamoDB and it will flow through the whole system.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2340.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2340)

So I was a little off thereâ€”it's not a super large chunk of code, but this is definitely, as I mentioned, all I have to keep track of in the DynamoDB database: the knowledge bases coming from the data sources coming from, and the file ID that I want to remove. Technically, I could probably put some extra parameters in here if I want to be very specific about the tenant ID and things like that to make sure we're going through, but for deleting from a knowledge base, I really just need the file ID from wherever the data source was. So I'm passing in that file ID  and then it's going off and deleting it out of whatever knowledge base. It handles creating the query and things like that.

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2380.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2380)

That said, it's just deleting out of the vector store. It's not deleting out of the underlying storage. So these are separated systems that we're dealing with. I've still got files sitting there in S3 that I could re-ingest later if I really needed to, but I'm just not taking up all the use of those particular vector stores. So that kind of ties it up. We've gone through all the stages and we've cleaned up the data, and now the users are hopefully  happy because they've got better utilization of their system and hopefully their costs are going down a little bit.

I would love to hear from everybody tooâ€”what vector stores is everybody else using? Did we kind of nail it with PostgreSQL and OpenSearch? Has anybody started using S3 Vector yet? One person there said yesâ€”has anybody found it useful and good compared to others? Yeah, okay.

I think when CloudFormation catches up on that, we're hoping that people will start really using those things. Any other vector stores that you're using? Well, I had more of a question. We use OpenSearch serverless, and one of the major issues we're having is that when listing the documents within the data source using OpenSearch serverless, the API that we use from our internal services to query the database times out if you have even greater than about 10 documents or something. Is there a workaround we can use to pre-filter the knowledge base so that we retrieve the results in less than 10 seconds or so?

You can hold on to that. I have some follow-up questions for you. What type of metadata do you have on the documents? It's basically file ID mainly, and then we have the agent ID in there to know which file we need to query and get the results, and that works perfectly. The major issue we're having is when we have the user interface for the customer to see the files they've ingested into the knowledge base. That's where it times out because the pagination happens very slowly. Is there a workaround we can use when listing the knowledge base documents? Is there metadata that we can pass to pre-filter the documents so that it's faster?

That sounds a little bit more like an OpenSearch-specific question. Exactly. I would love to get you set up with an OpenSearch specialist if you haven't talked to one so far. It sounds like if you went behind a Bedrock knowledge base and used something like this, you could maybe get the users to track down or have different sets of files or documents that they're looking at. I think one thing we've noticed is knowledge bases can get really big, and the stuff that individual people want to look at is maybe 10 percent of the overall. So you could segment it down by department or by something like that. Putting in some of that metadata so you could filter it down that way before you start to do the retrieve and generate might help you out on that.

So a follow-up on that. Since it's a multi-tenant system, I think there's an upper limit or quota for the number of knowledge bases that we can have in a single account versus the number of data sources. How do we tackle that, I guess? So we run into a bottleneck eventually in terms of scaling. That's the main issue. How Steven addressed that in his demo is he kept that record of files in DynamoDB. So if you're just listing the files, it's not actually hitting the knowledge base or OpenSearch, just hitting DynamoDB instead. And then with the tenant ID, you don't have a knowledge base for each tenant. You do the segmentation using the metadata. So maybe moving to a DynamoDB-based file information system could help. Yeah, if you're just tracking pure file information, I would say OpenSearch is really good at what it does, but if you're just querying flat-based information, DynamoDB is probably going to be a good spot to put it in. It alleviates some of the load on your OpenSearch as well, so you don't have to increase the size of your cluster at all.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/25c1d9da146ccba7/2650.jpg)](https://www.youtube.com/watch?v=IQ9YWcgbAY8&t=2650)

All right, thank you so much. Good question. So in terms of vector databases, a year ago before starting to use Bedrock Knowledge Bases, we were using MongoDB Atlas.  We found out that the LLM wasn't very good at doing text-to-MongoDB queries, so we had a few issues. Since now we're using OpenSearch and not S3 but pgVector, we still have a footprint on MongoDB. We're doing normal RAG without Bedrock Knowledge Bases. I'm wondering if we could use Bedrock Knowledge Bases with MongoDB at some point.

We just checked this too. I don't know if MongoDB is on the list. I'd have to get back to you. I'm not sure if MongoDB is on the Bedrock Knowledge Bases list of vector stores or support. But if it was, then yes, you could do that, and it would just generate the queries and everything for you. I told them they'd stump us. Yeah, and it's funny. I was just looking over these things. I know someone's going to ask about that.

We've been interested in using Neptune Analytics for general knowledge graph functionality, but also for vector storage since that feature is available within Neptune Analytics. How have you found the integration between knowledge bases and other vector stores? Is there more customization needed for Neptune? I haven't personally come across many examples of mixing those together, though Neptune vectors has been around for a little while now.

When you're setting up data sources to the knowledge base through Bedrock, it's pretty much the same configuration that you would do for whatever vector store you're going to use. You have a lot of the knobs and dials that you're going to tweak. I would say if you're tweaking them without it, you're probably going to be doing the same thing through the knowledge base. You're just saving the switching between knowledge bases and having to rewrite queries and things like that.

It would be an interesting question for the knowledge base team to see how they're doing performance enhancement, since they're going to be doing the queries for you into the knowledge store. It would be very interesting to see if you're finding performance issues. I'd love to get that team in front of it and see how they are doing those queries and what data specifically you're using. Maybe bump into me after this and we can figure that out.

If there are no more questions, I think we've solved everything, so that's good to hear. I really appreciate you taking the time. Make sure to put the feedback in the app. I think we're probably between everyone and lunch as well, so letting out a couple of minutes early is not the end of the world. Thanks everyone for attending.


----

; This article is entirely auto-generated using Amazon Bedrock.
