---
title: 'AWS re:Invent 2025 - The New York Times: From Google Cloud Platform to AWS (MAM337)'
published: true
description: 'In this video, Glenn Buckholz and Harleen Kaur from AWS, along with Ahmed Bebars from The New York Times, discuss their cloud-to-cloud migration from Google Cloud Platform to AWS. The session covers how The New York Times built a unified platform using EKS with shared Kubernetes architecture, implementing tools like Cilium, Karpenter, Istio, and OPA for isolation and scaling. A key highlight is the zero-downtime database migration from GCP Datastore to AWS DynamoDB using double-write patterns and PubSub for live data sync. The migration achieved a 90% reduction in service creation time and improved latency through cloud interconnectivity. The partnership approach addressed technical challenges like handling objects over 1MB and mapping incompatible data structures between the two platforms.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/0.jpg'
series: ''
canonical_url: null
id: 3085324
date: '2025-12-05T04:58:46Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - The New York Times: From Google Cloud Platform to AWS (MAM337)**

> In this video, Glenn Buckholz and Harleen Kaur from AWS, along with Ahmed Bebars from The New York Times, discuss their cloud-to-cloud migration from Google Cloud Platform to AWS. The session covers how The New York Times built a unified platform using EKS with shared Kubernetes architecture, implementing tools like Cilium, Karpenter, Istio, and OPA for isolation and scaling. A key highlight is the zero-downtime database migration from GCP Datastore to AWS DynamoDB using double-write patterns and PubSub for live data sync. The migration achieved a 90% reduction in service creation time and improved latency through cloud interconnectivity. The partnership approach addressed technical challenges like handling objects over 1MB and mapping incompatible data structures between the two platforms.

{% youtube https://www.youtube.com/watch?v=87O2r1br_ns %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/0.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=0)

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/50.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=50)

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/60.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=60)

### Introduction: Making Cloud-to-Cloud Migration Less of an Adventure

 Good afternoon everybody. Welcome to Reinvent 2025. My name is Glenn Buckholz. I'm a principal modernization and migration architect with AWS. I'm joined today with my colleague Harleen Kaur and a very special guest, Ahmed Bebars from The New York Times. He is their principal engineer. So the reason we're here today is we're going to talk about migrating from Google Cloud Platform to AWS. That sounds like a little bit of an adventure. I have my adventure hat on. Who here is ready for a little bit of an adventure? Show of hands. Okay, we have to talk. If as a solutions architect, I came up to you and I said you're going to migrate, it's going to be an adventure, does that necessarily inspire  a lot of confidence in the activity? No. Today we're going to talk about how we made it less of an adventure. So no adventure hat. 

We're going to talk about cloud to cloud migrations in general, some of the patterns that we've observed. We're going to talk about why AWS. We're going to talk about the thing that started the relationship between AWS and The New York Times. We're going to talk about The New York Times cloud vision. Remember, The New York Times mission is not to run the best or the fastest Kubernetes cluster. They have a mission to deliver media. While it helps to have the best or fastest Kubernetes cluster, it's about how AWS can help them achieve that mission, and that's the vision they're going to talk about.

Then we have these ideas of paved roads and blessed paths. First we're going to talk about paved roads and how that eliminates some complexity with the container portion of the migration. Then we're going to talk about the blessed paths, and that's how we did some of the database migration, and this one is particularly interesting because it involves two dissimilar services, which I'll go into a little bit more depth on in a minute. After that, Ahmed will talk about some of the outcomes from the migration and then we'll take questions. Are we ready for our non-adventure?

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/150.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=150)

### Rethinking Value Propositions: Why The New York Times Chose AWS

So one of the things that I wanted to talk to you about first  is migrations and cloud to cloud migrations. Five years ago when I started working for AWS and we would talk to customers about migrations, we would talk about five major differentiators. Those differentiators were instant global reach. If you want to go somewhere else on the globe, you no longer had to negotiate a contract in another country or something like that. Second, AWS would reduce the undifferentiated heavy lifting that you would have to do to take care of your infrastructure. Third, on demand. Paying only for what you need. Fourth, scalability. You can scale up and down as you need. And fifth, experiment quickly. You want to experiment with something.

Now let me ask you a question. If you're coming from another cloud service provider, are these necessarily differentiators? Probably not. Well, maybe the resiliency and we can talk about that one because you'll see it up on the slide right there, but for the rest of them, we have to think about the value proposition differently. With AWS and The New York Times, there were three key reasons that we saw that elevated the value of using AWS. The first was resiliency and capacity. Remember, The New York Times mission is to deliver their content and their media to other people. Whether one person is interested in that content or a million people are interested in that content, they need the ability to deliver it seamlessly. They thought that AWS was the right partner for them.

Second is partnership. Amazon has their leadership principles, and one of those leadership principles is customer obsession. My role and Harleen's role as solutions architects is kind of the embodiment of that. We are your partner and we're going to try to help you get the value out of your IT spend. We do that in a variety of different ways. We do that by making sure that your business need is fit to the right technical solution. We do that by looking for other ways that we can be your partner that might not be technology related, and we have account teams dedicated to that. And also the unsung heroes, the technical account managers.

AWS is your partner in more than one way. We don't just provide a service where you can run things. We provide people to help you get the best value out of your IT dollar, and that can take many different forms. For The New York Times, the building blocks of AWS helped them create their cloud vision. They saw this as the best and most efficient way to do it. We helped them create New York Times services on top of AWS services that allowed them to centralize their compute and data into a platform that was purpose-built for accomplishing their mission.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/370.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=370)

### Overcoming the Fear of the Unknown in Cloud-to-Cloud Migrations

Now that we've discussed how the value proposition differs between an on-premises to AWS migration and a cloud-to-cloud migration, let's talk about some of the blockers of cloud-to-cloud migration.  The largest blocker can be summed up as fear of the unknown. When you're working with another cloud service provider, you're familiar with that provider and all its quirks. You've probably spent considerable time and education with that provider, and change is difficult. The fear of the unknown applies to two different categories, which are two different things you'll have to approach when migrating from another cloud provider.

The first category is what I would call commodity services. These are services that look and feel the same almost regardless of which service provider you're using. For example, a virtual machine typically has memory, block storage, a number of CPUs, and probably a play button that starts the virtual machine and a stop button that stops it, or an API call that is somewhat similar. These are commodity services. One way we alleviate the fear of the unknown for commodity services is through education. AWS has a variety of mechanisms we can apply, from our training and learning needs analysis to immersion days and jam sessions, experience-based acceleration, and gathering the right experts to have conversations with you about it.

Additionally, commodity services usually are shaped that way because they look very similar to on-premises services. This means not only do we have a way to alleviate the fear of the unknown, but we also have a well-trodden migration path that was established when we were first bringing the cloud into existence. We know how to migrate SQL servers from on-premises to AWS, and we also have specialized knowledge to migrate managed SQL servers from other cloud providers to AWS. The tooling is pretty much the same. You use DMS or something similar. The same applies to instances and other things.

Because the rules are a little different when you're going cloud-to-cloud, there are sometimes acceleratorsâ€”things you can do faster that you couldn't do if you were on-premises. We have the knowledge and expertise to help there as well. In terms of commodity services, AWS can help you get past the fear of the unknown by implementing this knowledge and these services that we already have. The next part is a little more difficult. Other cloud providers have services that are specific to them. They may have an analytics suite or a database suite that's different, and as a matter of fact, one of the databases is what Harleen is going to talk about later.

There's no direct one-to-one mapping here. You can't just do a database dump and restore. It's not going to work because that service doesn't really exist on AWS. In order to get past the fear of the unknown for the managed services, AWS's solutions architectsâ€”remember, we are your partnerâ€”know how to gather the expertise necessary to provide structure for these migrations and allow you to quantify the cost in both engineering effort, time, and resources.

We will go out and if a migration SA like myself doesn't know how to do it, we will pull in the proper experts. In this case, maybe a NoSQL expert, an analytics expert, or a storage expert. These people have real-world experiences on how these services are built from the ground up and maybe even expertise with that service on the other cloud provider. So it's no longer unknown. It's a path that we can chart together, and something that we can do with a level of confidence to ensure that the migration is successful and within the bounds of what you've accounted for in your business plan.

In order to make sure things work, sometimes we even engage experts to do a proof of concept. On a small scale, maybe SAs like myself will do it. On a larger scale, projects have also been commissioned with professional services or partners, so that you don't just have to take our word for it. That's how we make this less of an adventure and remove the fear of the unknown. With AWS, you can do these cloud-to-cloud migrations.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/710.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=710)

### The Spark That Started It All: From Amazon Connect to a Unified Cloud Vision

Now, how did all of this begin with The New York Times? Well, there was a spark, and to talk about that spark, we have our distinguished guest. So Glenn has been talking about cloud migration, and now you see Amazon Connect on the slide,  so you might be confused. But let me tell you how it all started back in 2017. It was a business problem, not a technical problem, and we weren't moving from another cloud. The problem was that subscribers would call our numbers and reach our IVR system. The IVR system would try to solve the problem, but when it couldn't, they would get to a live agent and then have to repeat the problem again and validate themselves again.

The problem was how to actually cut the handle time and ensure that all context moves along from that initial conversation with the IVR to the agent so they don't have to repeat that. We had to transition them from a very frustrating process to a better, smooth process. The business value and business proposition was that we wanted to build a better customer experience. So we went on a hunt trying to find what tools we could implement into our stack to have something like that integrated into our CRM. Then we found Amazon Connect. It was a big transition because we had our telephony system with a lot of pieces, and then we had to figure out how to move all of that into a managed service. It was a big ask.

So we did a proof of concept and quickly identified that Amazon Connect as a managed service was actually a great fit for something like that. Why? Because all of the things that build or you can integrate natively with Amazon Connect. We're not just talking about telephony. We're talking about Lex understanding the conversational aspect. We're talking about databases to get the customer context and customer information. We're talking about SDKs, which for us was the actual key that we needed to integrate with the CRM. From there, we had a good way of understanding what that looked like and we knew what a managed service looks like, so we started to think about what we wanted to do next. That takes us to another level where we look at our cloud vision.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/840.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=840)

What actually are we trying to achieve?  If Amazon Connect as a managed service is good for that vertical business-side problem, what else can we do based on AWS? So we started to think about building more compute, storage, other tooling, and AI, but we didn't want to focus only on these on their own on AWS. We wanted to focus on how to build that in a unified platform. We wanted to build standards and have this organized in a way that we don't have to think too much about how to ship new features. The idea and goal here is that we are not focusing on the technology itself. We are focusing on solving the business problem, and the business problem we are solving is that we want to deliver news, breaking news, and serve games to our subscribers. So we want to focus on that.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/910.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=910)

From there, we know where we are going to build this stuff, but how we're going to build it is interesting.  Looking back in time, I'm going to dive into that deeper. We already have some of our services in AWS and some of our services on GCP, and there are other places where some services are on-premises. But they are differentiated from each other. However, when you take a deeper look into the services, they are not completely different. They might share some characteristics, but because they are built across a scale of teams and different teams, they don't look the same.

What we wanted to bring here is our opinionated way of how we are actually going to deliver a service to our customers. Our teams have to focus less on all of the infrastructure built around it, and my team as a developer platform team is also less focused on how we manage all of that. At the end of the day, we want to have our standards and good standards so we can ship this to our product engineering team, which allows them to focus on the differentiated features and the quality products that we want to ship to our readers.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/990.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=990)

### Paved Roads: Building a Centralized Platform with Kubernetes and Containerization

From here, we are focusing on scale and efficiency, and I'm imagining that all of you are trying to do something similar and seeing what good looks like for your organization. This is probably a question that you have seen before.  We have already designed where our service is going to look like and how we're going to be open about it. If we take a step back and look at our product engineering team, I'm in the developer platform trying to build tooling for my engineers in other missions. I look at a business problem, then as an engineer I come up with a solution, and then I start planning and design. We write some code, we ship it somewhere, and then we release it. We have monitoring. This is pretty common to any engineering teams that you have seen.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1050.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1050)

What are we trying to do with that? We're trying to centralize on that. We're looking at what are the common patterns between all of these that we can centralize on. We actually identified that the business problem might be unique, the idea and the plan might be unique, but not all of them. The creation of the process and the code itself will be different, but the way that you shape your code and the way you deploy your code could  be the same. The way you monitor it could be the same. What happens if we centralize all of that? Instead of making you think about all of these steps, you will get them out of the box.

As a platform team, our focus is on building centralized CI/CD and building observability, treating observability as a first class citizen, not an afterthought. We make sure that our deployment pipeline is always there. That gives us the opinionated way and gives us exactly the boring way, but also the good boring because when things look the same, you don't have to think about it too much. The process of where I'm going to deploy this is actually what I want my service to do.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1120.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1120)

From there, we have already defined the process. We actually found out what problem we are solving as a platform team, and we have the life cycle. So now to the next action, which is how are we going to do that? What are the technical pieces that we have to involve in this process? When we start to look into that, one of the things that Glenn talked about is that cloud to cloud  is not a copy and paste. There are always unknowns that you encounter along the way because something built with specific thought in a different cloud on these primitives. Moving it to a different cloud or a different provider is always a challenge, and not everything is known beforehand.

The goal for us wasn't just to migrate, but to build a library, a library of patterns and good standards that we can share across migrations. From there, we decided on Kubernetes. We have been using Kubernetes for a while across multiple providers. What if we have a shared Kubernetes architecture across the stack that we will deploy all of our services to? This will allow the team to focus less on any infrastructure. I will dive into the details for how this project will go through, but the other thing that was important was to focus on containerization, which makes the applications less focused on how to build and shape everything, but more focused on containerizing the apps themselves and having a way that we can embed this into our process and ship it to our platform.

The last thing that we were looking into is that we want to have this parallelized across multiple teams. We don't want to do a one-time project of migrating or building a single library. When we take an initiative, we want to have all of the teams move to it.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1220.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1220)

Specifically, we want all of the teams to move to Kubernetes and start thinking about containerization or doing something else because we want to keep building these libraries and keep automating the platform.  We focused on paving the road for where we're going with containers. We already built Kubernetes and focused on reliability and resiliency. Now we're trying to think through all of the steps necessary to get there. We already have the platform built and all of our tooling in place. Now we're going to focus on what we want to do with this platform. We have all of these steps mapped out.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1250.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1250)

We focused on building a Kubernetes setup.  The setup here is interesting, but let me give you a couple of context points about it. We are already in AWS. We have what we call AWS multi-account architecture, where all of our product engineering teams have their own accounts that they can deploy their resources to. But we came up with a different approach. If everyone gets a cluster, that might be problematic. Now we would have hundreds, maybe thousands of clusters across the organization. You can automate it and operationalize it, but the effort required would take a toll on the team.

So we started simpler with a shared architecture. We have a single account across multiple environments, and that account is focused on resiliency. Out of the box, we built multi-region redundancy. Let me tell you something that paid off during last year's East One outage. We had our critical applications scale into the West region, and that was good. We definitely had some degradation, but it didn't impact us as severely as we would have expected otherwise.

We focused on what we need in that shared Kubernetes cluster. When we look at how we isolate it, you're going to see Cilium here. We use Cilium for network isolation because at scale, we have to make sure that each tenant from our product engineering team is isolated to their own spaces. We also focused on scaling. Karpenter played a good role in how we scale faster. We can scale with different parameters to fit our needs. We're also looking at Istio, which is important for our service mesh and how traffic moves between East and West. We look at OPA, which is how we tell someone you are not supposed to do this automatically. We don't have to go to a team and say, "Hey, don't inject your resources into that from a different account." That is done automatically.

This is how we set up our shared Kubernetes architecture, and it helped us focus less as a team and organization on all of the infrastructure spun across the organization. It allowed the team to focus more on actually building the differentiated work. They focus more on how they're going to deliver different things for the news or a new game or a new recipe or something else that matters to our subscribers.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1410.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1410)

To summarize all of this, it's finding the common patterns  between all of your steps and migrations and projects. Looking at applications is a good start. When I look at a lot of applications at scale, I see all of them doing something similar, maybe logging, maybe tracing, maybe something else that we can opt for with open source or we can build our own wrappers on top of these things. Then we look at shared services. Can we map these things together? We've done that part and moved to containers. Now all of the applications are containerized in a way that we can ship them from one cloud to another. The process earlier started by shipping applications from being on their own GCE or similar services to GKE, which is Kubernetes native, and then move them to EKS, which helped us move through the way.

### Blessed Paths: Zero-Downtime Database Migration from GCP Datastore to AWS DynamoDB

The migration here is actually giving us a toolkit that we can replicate across all of the projects and ensure that all of the product engineering teams use the same patterns over and over from a compute layer. That was our first step. But from here we're going to talk about database migration. Moving the compute is one step, moving the data is another step. So for that I'll pass it to Helene to start talking about the database migration. Thank you, Emma. I do have a question though for you. So when you're doing the database migration and moving the applications, how much downtime can we have?

For a news company and a games company, we cannot afford downtime. I have never seen a banner on the website saying we are down for maintenance, or for the game saying we cannot give you stats right now. Downtime is simply not an option for us. Interruptions happen all the time, but we have to plan for zero downtime when we do any migration.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1540.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1540)

The problem statement given to us was that the applications should not be aware that migration is happening underneath. Because The New York Times is one of the world's premier media companies, they do not have maintenance windows, as Ahmed mentioned. It is also the mission of The New York Times to make available the news and whatever is happening in current affairs. Whether there is a spike in internet traffic or any implementation of technical changes, it cannot have any downtime. 

Not only did we have to engineer a migration, but we needed to engineer a migration where the applications would continue seamlessly without any issues. It was not enough just to get there; we also needed to make sure that no one noticed. That was the challenge we faced when we planned for the database migration.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1610.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1610)

We created a pattern for the database migration from GCP Datastore to AWS DynamoDB. We had to move the application from GCP to AWS with no downtime, as Ahmed mentioned. We also had to keep the data in sync between GCP and AWS while the migration was going on.  The data that needed to be migrated between GCP Datastore and AWS DynamoDB had to be in sync and had to be migrated live.

We also had to create a mechanism to do a cutover to AWS once the migration ended. Not only did we have to do the forward migration, but we also had to plan for the scenario where, if the migration failed, we would have a mechanism to do a fallback back to GCP. The best part of all was that the user should not know about the migration and should be able to use the application while the migration is going on.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1680.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1680)

How do we get from the state where the application is running on GCP and using GCP Datastore to the end state where the application is going to run on AWS with data on DynamoDB, and all with no downtime?  That was a very interesting problem that we had to solve.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1710.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1710)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1730.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1730)

In the start stage, the application is containerized and running on GCP, pumping data into Datastore for the persistence layer.  Double write comes into the picture as the main hero of our story.  The application running on GCP has to write data to both GCP Datastore and AWS DynamoDB because the idea is to maintain the current state of the database. The data has to be in sync, and all active transactions should be the same on both GCP Datastore and AWS DynamoDB.

The application writes to PubSub, and we have created one topic per table for the Datastore piece. We are using PubSub to distribute the data to the new target on the AWS side. We have to migrate the data reliably and do it live, and PubSub will guarantee the data reliability and help us avoid downtime. This mechanism syncs the data between GCP and AWS, and even though the application is running on GCP, the data is being written to both GCP and AWS simultaneously.

This is done to avoid any inconsistency or discrepancy in the live application traffic. On the AWS side, there is a job running that takes the data to AWS Lambda. Lambda processes the data coming from GCP and transforms it into a format compatible with Amazon DynamoDB, then stores the data in Amazon DynamoDB. This is the first part of our approach where we perform double writes to ensure that the current application, current data, and current state of the subscriber or any data being migrated is current with no discrepancy or inconsistency between GCP Datastore and AWS DynamoDB.

Now, what do we do with the historical data? There is a lot of historical data stored in Datastore on GCP because the application has been running on GCP. We perform a one-time bulk upload from Datastore to AWS DynamoDB. From GCP Datastore, the data is pushed to BigQuery. We push it to BigQuery because the data is stored in a binary proprietary format. From BigQuery, a job pulls the data and stores it in Amazon S3 in a format compatible with our DynamoDB database model. Then we have an AWS Glue job running that performs the transformation of the data and exports the transformed data into the Amazon S3 bucket, from where the data is populated into Amazon DynamoDB. This is how we handle the historical data migration.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1870.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1870)



At this stage, you have the active data and current data already in sync between GCP Datastore and AWS DynamoDB. At the same time, the historical data is being pushed from GCP Datastore into BigQuery, then into the S3 bucket, performing the ETL with Glue, and finally into DynamoDB. This historical backfill is a one-time operation. Once the historical backfill is complete, the data should be in sync between GCP Datastore and AWS DynamoDB.

Now we have the application performing double writes on GCP with data on both GCP and AWS. How do we perform the rolling cutover while ensuring the application remains up and running? We start the application on the AWS side, replicating what we did on the GCP side. Once the traffic is moved from GCP to AWS, the application on the AWS side not only writes data to Amazon DynamoDB but also pushes the data back through replication to GCP, going to BigQuery and then to Datastore. The application performs double writes from the AWS side to both DynamoDB and GCP Datastore. We do this because if there is an issue with the migration or if the migration fails, we have the chance to roll back and return to GCP without the user knowing. This is the insurance we implemented for failover scenarios, and this was our plan for the rolling cutover.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/1990.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=1990)



Now we have the application running on AWS with data being synced from AWS to GCP. The current state of the data is being maintained on both AWS and GCP.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2110.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2110)

Now here comes the word that I've been struggling with ever since I did that.  It's "quiescent." Thank you, Glen. He made my life difficult because he will not change the word. It's one of those words that everybody stumbles on, so this has been my nemesis: QS. This is like a pause period, a peaceful time period, a quiescent period where you let things settle down. You make sure the transactions are being processed properly on the AWS side. You make sure the migration is successful. You're doing some sanity testing. You are making sure there are no issues being reported by the users. You are making sure your KPIs and metrics are everything working.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2170.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2170)

[![Thumbnail 2210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2210.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2210)

This is the QS time, and this is what we gave after the migration and the cutover were over.  So now with the successful migration of the data between GCP and AWS, it is time to decommission the services and the workflows that were on GCP. The application is now running on AWS. I'm going to hand it back to Ahmed to conclude the session and share some of the wonderful statistics that we collected. Thank you, Arlene. So again,  the migration is complete, but the migration in itself is not what we're looking for. We are actually looking to build standards and libraries across the board. That's why we found it really important to focus on how to parallelize all of that effort and build that library fast enough because teams will innovate. We need them to do that undifferentiated work.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2250.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2250)

### Migration Outcomes: Achieving Scale, Speed, and Partnership Success

All of that work that they are going to start to do to build the quality product, we need to make it move faster with our migration. I'm from a developer platform mission, so my focus is on engineering. When I look at the outcome for something like that, having engineering teams is my reality, is our reality.  We want to make sure that the tools we are building are something that they are actually using and that it helps them build all of the stuff that they want to build. We are going to focus on the plumbing, but when we start as a platform team focused on the plumbing, who helps us? That's where we go to a partner like AWS and work with them on all of the things that they deliver.

So we can go a couple of ways here. I can go build my Kubernetes cluster. I'm very Kubernetes savvy, and then I can start wrapping my head around etcd and how to manage it on an EC2 instance and how to keep my file system, or I can run it on ETS. I can do the same for other services. Should I focus as a platform team on building the things that matter, on building the tooling that matters for my product engineering team, or should I actually start having more managed services from AWS to work with? This gives us all of the nines and the availabilities that we need across regions so we don't have to keep doing the work as a platform ourselves. Then we can focus more on the other innovations that we need to build for our product engineering teams.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2340.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2340)

It's not just about the platform; it's about the process, the governance, the standards, everything that we ship as a whole. That's how the people come together to build their services. Another thing that we want to focus on is that when we know exactly how we are doing things,  it becomes much easier. One of the things that I have seen before is someone has an idea and they want to go to production ready, but they want to be production ready with all of the standards baked in place. That would take them time to test all of the processes they built. Our platform gives them all of that out of the box, so they would think about exactly their business logic and what they are shipping.

Here we're finding a reduction of ninety percent of service creation time. When they start with an idea, it's ready to go to production safely with all of the standards shipped in and baked into the process and the platform itself. Another thing is when you work at scale, it's different from when you focus on a couple of services. If I have a service and I'm trying to optimize for latency on that service and then I have to improve it by five percent, sometimes the investment doesn't make sense to focus on the small thing because I will spend too much time trying to improve that on a very smaller scale.

When we look at what we can achieveâ€”whether it's 10 milliseconds or 150 millisecondsâ€”it depends on the scale. When we look at scale and consider many services from our platform perspective, we've completed several projects to improve latency. One project we did was cloud interconnectivity between GCP and AWS, which routes all traffic through the internal network. This saves us not just latency, but also costs because we no longer have to route traffic through the internet.

When we have to do double writes, moving data from AWS to GCP or vice versa, all of that is done internally on the solutions that we build. Another consideration is something like STO or other features in the platform. When we improve on it, we focus on removing one core from an application versus removing one core from 1,000 applications or 100,000 applications. The scale here matters significantly. All of this happens when we start to focus on what exactly we are building as a platform team for our customers, and our customers are product engineering teams.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2500.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2500)

I'm building something for them at that scale with a partner that helps us build that library to make it available for all of our engineers so they can focus on the problem and build better products. We can deliver the news at any scale at any time.  Thank you, and if you have any questions, let us know.

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2520.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2520)

Really quickly before we go to questions, one of the things I want to point out is that when we went through the solution for the database migration, there was a step that we left out due to timeâ€”the how do we get from here to there.  One of the things that we left out, and this is where a partnership comes in, is that Datastore is not an exact match to DynamoDB. Harleen, what are some of the requirements that The New York Times gave us, like transaction rates?

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5366557b1bd9803/2530.jpg)](https://www.youtube.com/watch?v=87O2r1br_ns&t=2530)

Transaction rates were there, and there was one other issue. How do you handle large objects? Because DynamoDB does not accept an object greater than 1 MB, there were a couple of strategies that we implemented.  One was flattening it out and just taking the fields that were actually required and being mapped to DynamoDB. Second, you can always have a pointer within DynamoDB, but the actual record is stored in an S3 bucket for lookup. Or you can break it down into chunks, and all those chunks can come together with one particular key.

Those are some of the technical challenges that we also faced. One of the other things was that map objects were not easily mappable into DynamoDBâ€”we don't have anything called map on DynamoDB. So just so you don't think by looking at the cleanly structured diagrams and the smooth flow of the presentation that everything just happened, there was a significant amount of planning that went into this. That is the partnership that we talked about at the beginning. I just wanted to go under the covers a little bit and expose some of the rough edges that AWS helped smooth over during the process of this migration.

All right, and now, for real this time, questions.


----

; This article is entirely auto-generated using Amazon Bedrock.
