---
title: 'AWS re:Invent 2025 - Measuring AI Impact with Amazon Q Developer and Jellyfish (DVT219)'
published: true
description: 'In this video, AWS and partners Jellyfish and Genesys discuss measuring AI impact in software development. Joe Kirby from AWS presents the "eliminate, automate, assist" framework and Amazon''s Cost to Serve Software model, which reduced costs by 16%. Krishna Kannan from Jellyfish introduces a three-step measurement framework covering adoption, productivity, and business outcomes, revealing only 30% of companies benefit at scale despite 100% organizational adoption. Craig Dollinger from Genesys shares their journey implementing Amazon Q Developer, initially using PostgreSQL for metrics before scaling with Jellyfish''s AI dashboard. Key insights include the importance of baseline metrics, leading versus lagging indicators, mapping usage to internal taxonomy, and understanding that AI amplifies rather than replaces developers. The session emphasizes that measuring AI impact requires multiple metrics across adoption rates, engagement levels, and business outcomes, not just individual productivity gains.'
tags: ''
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Measuring AI Impact with Amazon Q Developer and Jellyfish (DVT219)**

> In this video, AWS and partners Jellyfish and Genesys discuss measuring AI impact in software development. Joe Kirby from AWS presents the "eliminate, automate, assist" framework and Amazon's Cost to Serve Software model, which reduced costs by 16%. Krishna Kannan from Jellyfish introduces a three-step measurement framework covering adoption, productivity, and business outcomes, revealing only 30% of companies benefit at scale despite 100% organizational adoption. Craig Dollinger from Genesys shares their journey implementing Amazon Q Developer, initially using PostgreSQL for metrics before scaling with Jellyfish's AI dashboard. Key insights include the importance of baseline metrics, leading versus lagging indicators, mapping usage to internal taxonomy, and understanding that AI amplifies rather than replaces developers. The session emphasizes that measuring AI impact requires multiple metrics across adoption rates, engagement levels, and business outcomes, not just individual productivity gains.

{% youtube https://www.youtube.com/watch?v=l-mjftGWueA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/0.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=0)

### Introduction: From Developer Productivity to AI Impact

 Good afternoon and welcome to Reinvent. My name is Joe Kirby. I'm a global go-to-market lead in our next-generation developer experience team here at AWS. We're the folks that talk to customers and partners about their software development life cycles and how to take advantage of AI in that space. Today I'm joined with Krishna Kannan, head of product for one of our partners, Jellyfish, and also Craig Dahlinger, one of our joint customers with Genesys. So let's dive in.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/40.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=40)

Today we've got a story in three parts. I'm going to tell part one, which is how we've moved from the conversation about developer productivity that we've had over  the past couple of years to get us to this place now of talking about AI impact. What does that journey look like and how has that changed the way we think about it? Then I'm going to hand over to Krishna, who's going to talk about the way that Jellyfish thinks about this situation. I'm going to give you the Amazon perspective, he's going to give you the Jellyfish perspective, and then I'm very excited to have Craig come up and share his perspective as a customer. So you're going to get all three perspectives in this next 45 to 50 minutes or so.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/90.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=90)

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/110.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=110)

I've had the pleasure of talking to customers about this space for about three years. Before we launched Amazon Q Developer back in 2023, all the way through 2024, we've been really diving deep into this space. We've seen this evolution in tooling. In 2023, we saw this idea of fancy autocomplete, and then we moved into 2024 with this idea of chat and more complicated transactions and interactions.  Moving to 2025 today, what we're seeing is all of these more agentic type experiences with reasoned models and other things. They become much more interesting types of  experiences, but it really has challenged this idea of what productivity means in this age of AI.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/120.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=120)

When we first started having these conversations back in 2023, it was very much about individual developer productivity.  At Reinvent 2023, we started talking about development productivity at the team level and the system level. Last year at Reinvent, we were very clear in talking about the distinction between productivity and toil, right, two sides of the same coin. Then this year at the New York Summit, Amazon released our framework for how we think about impact in software development, this idea of cost to serve, and I'll be diving into that a little bit later on. As the tools have evolved, the conversations have evolved, and with all of the conversation this week about agents and other types of experiences, this is only going to get more interesting and more complicated.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/170.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=170)

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/180.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=180)

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/200.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=200)

### Debunking AI Impact Myths and the Eliminate-Automate-Assist Framework

 Before we get started on what works, there are some things I come across pretty consistently with customers, and I call them impact myths.  The first one is that AI can fix complexity. That's not true. If your processes are complicated with lots of steps, lots of people, and lots of interactions, AI can't fix that. The second myth is that it's all about creating 100x engineers. This also is not true. We see a lot of stories on LinkedIn about people creating applications by themselves, but what we know is that generally speaking, you gravitate to the performance of the team that you're on.  Yes, you may have one or two individuals who are able to do amazing things, but in the whole of your development organization, they gravitate to the performance of the team.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/230.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=230)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/250.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=250)

The third myth is that focusing on a single measure is enough. I wish that this were true. I wish that there was some metric we could say is the one. It simply is not the case. It is not possible. As I go through the next 15 to 20 minutes, I hope you'll get a sense of why and understand how these other platforms and our customers work with a range of metrics.  The final myth is that we can just turn it on and people will figure it out.  I've had a number of conversations with customers when we're talking about enablement and training and how you're going to let your folks know how all of these things work. They say, "I'm just going to turn it on." That doesn't work. If you look at things like the DORA report for this year, there's a lot of conversation there about structured enablement. We see it very much as well. You cannot simply turn these things on and hope it will work out.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/280.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=280)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/300.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=300)

So that's what doesn't work. So what does?  If there's only one thing that I would hope you would take away from this conversation, it's this, and it's how we think about applying AI thoughtfully, yes, to software development, but in many ways to any process. First off, there's this idea of elimination. What can you eliminate? I said AI can't fix complexity, so  how do we start to remove some before we start doing other things? Manual steps, tools, and processes.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/320.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=320)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/330.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=330)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/340.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=340)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/360.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=360)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/380.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=380)

We also found internally a very high number of manual interventions in pipelines.  How can you get rid of those things? What do you need to fix to make those things go away? The answer is to automate. What is it possible to automate with AI?  These are typically lower value, repetitive tasks, maybe relatively low complexity type of things. I would say standard things like documentation and test generation.  You've also probably heard us talk about our Java JDK updates that we did. So automating the sort of runtime upgrades, you'll see that in some of our AWS Transform agents, those are the things where we think about automating through the use of AI. And then finally, there's assisting.  This really has been true, I think, maybe in the last six or eight months as the models have become better. But how do we now take advantage of AI running on your laptop as a dev, as a thought partner, as an assistant, as something that is capable of complexity and thought and really being able to achieve  high complexity in frequent tasks. So you'll hear me refer to this framework: eliminate, automate, and assist. It's a really good way to think about where you're going to prioritize and use AI.

[![Thumbnail 400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/400.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=400)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/420.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=420)

### Accelerating Impact Through AI Fluency: Prompting and Context Engineering

So what are some accelerators for impact? AI is a new way of working. I've had a couple of customer conversations just this morning  about this. We are in a place where everybody's journey to their aha moment, where you say, "Wow, this AI thing is really cool," is different. It's very personal. So how do we get to that point where your devs are able to say, "Oh wow, this is really cool"?  By giving them permission to play. This is something that we do here at Amazon a lot, and we have a leadership principle called "Learn and Be Curious," which encourages us to take advantage and experiment with these new kinds of tools. What we know is that if you have a development team that is on a delivery schedule with story points and features to deliver, and then you're asking them to rethink the way they do the job at the same time, it's not going to work. You have to be willing in the organization to give your dev team some slack in order to give them the brain space to actually be able to play.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/460.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=460)

[![Thumbnail 490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/490.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=490)

Software development is a team sport. We know this internally, and what we find is that developers gravitate to the performance of the team as a whole.  It coalesces to the performance of the team. So as you're thinking about these things, how do you accelerate the performance of the entire team? How do we focus on those kinds of enablements and that kind of activity to really drive acceleration? Embracing sense making is another key accelerator. What do I mean by sense making? One of the things that we're talking to customers about even today  is around legacy code or bringing new developers into existing projects or even onboarding new developers into your organization. AI can speed up your new hires' ability to onboard to projects and to onboard into your organization. When you're looking at legacy code and you want to be able to describe it and understand it, there's an enormous amount of value in using AI in that role. You'll see this showing up in things like steering documents, all of these various ways to make your AI better. You can have your AI help you do those things.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/530.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=530)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/560.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=560)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/580.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=580)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/600.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=600)

And finally, encourage AI fluency. What do I mean by that?  I'm going to say that we have the mechanics of your developers understanding how to use Amazon Q Developer and those kinds of tools, but there is real value to be had from understanding more about the underpinnings of that. My team and I have done some really deep dives with customers this year, and what we found is that this idea of AI fluency is really important to get the most impact possible out of these kinds of tools. So let's talk about fluency for a little bit.  There are two broad buckets here that I think of. The first one is prompting. Who heard of prompt engineering a year ago? It was a big thing. Everyone was going to be like prompt engineers. Well, we know that that's helpful, and understanding how to write a good prompt is really valuable. The tools can't read your mind. It's sort of trite to say it, but it's very true.  We have to work on this idea of making the implicit explicit. I've been an engineer pretty much my entire career. I have things in my head that I don't say out loud, and I expect people to know. AI can't read your mind in that same way. We have to be precise. Precision matters. If you wanted to write test cases, tell it to write test cases in the format that you want and the framework that you use, and it will make it much more effective. 

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/610.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=610)

Structured problem solving  is something anyone heard of spec-driven development? A couple of folks, right. What we're starting to see with recent models is the ability to build a plan for how they're going to break things down and do things in small pieces when we're interacting with models. I certainly have seen this in the early days where you would say something like "build me a shopping cart." Sure, but that's not precise. It doesn't break it into tasks, and it doesn't really tell you what you want. What you're trying to do is have the model give you what you want and not go off track, and the way you do that is with more structured problem solving.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/650.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=650)

Finally, the thing that I think a lot of folks forget is you can ask the tools for help.  Hey, can you summarize this conversation into a prompt? What else do you need to know? How else might we write this? What is missing from this prompt? Using the tools to help yourself be better with them is really powerful. So let's talk about context, the other side of this. Context engineering has really been a conversation mainly that I've seen sort of this year. As the models have become better, the context windows have grown, and the reasoning has improved. The same problem occurs, right? We're trying to use additional context to make implicit explicit.

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/690.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=690)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/710.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=710)

A couple of examples: Q could create its own rules files. Kiro can recreate steering documents.  You may have seen agents.md, right? Ways to have context in your repos that can help guide these tools every time. More is better, but only to a point.  In the same way, if you were onboarding a new developer into your projects, you wouldn't necessarily have them read every repo and every application you've ever built. You wouldn't ask them to read all of your Confluence. What you would do would be point them to more specific examples that would be valuable. That's the idea of getting to the right context.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/730.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=730)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/750.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=750)

You'll see a number of innovations that we've had with Q and Kiro this year around constraining agents in the CLI, being able to pin context, and being able to be more specific with tool usage.  All of those things are trying to drive to the right context, not just more. MCP is incredibly powerful.  Where we were six or eight months ago was, well, just more MCP tools, right? I'm going to give it connections to everything I see. But what we know is that that can very quickly overwhelm both the tools and the models and everything else. So it's very useful, but be thoughtful and again ask for help.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/770.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=770)

Which of these tools would be helpful?  What else do you need to know? I have this other file. Does that make this better, right? Engage in this idea of it being assistive. Rather than you having to prescribe everything, ask it how it can help you be more effective. So we've gone through what doesn't work and some things that we know do work in order to get this idea of how we then measure the impact, right? We talked about how to make it better.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/790.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=790)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/820.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=820)

### Measuring Impact as a Journey: Leading and Lagging Indicators

Measuring impact is a journey.  If any of you have followed the content that we've had for the past 23 years, we've talked about this a lot. There are industry frameworks like DORA and SPACE. A couple of my customer meetings this morning were going and talking about the same things. I'm going to talk about the Amazon framework, Cost to Serve Software, which is something that we've explained how we think about it.  But it is a basket of measures. There are a number of metrics and data points that, depending on where you and in fact your teams are in their journey, will vary.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/830.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=830)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/840.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=840)

Qualitative and quantitative indicators are incredibly valuable.  Qualitative is the why, quantitative is the what, right? So quantitative comes out of tools, qualitative comes from your humans.  There's a lot of value in pulling those things together, and I'll give you an example in just a second. If I was going to say to you today, which is a lot of customers ask me, "Hey, we were looking for gains in productivity. I would say, well, how do you baseline it today? What tools do you have in place? How do you think about measuring productivity today?"

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/860.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=860)

My suspicion is, I know many of my customers say we don't.  It's ad hoc. We've guessed. We've got some bits and pieces. I suspect that a number of you today are sitting here in that same boat. You've got some idea, some tooling, some data points, but really the idea is that you baseline where you are today and then look at those trends shift over time. That's the way to think about this. Finally, there are both leading and lagging indicators. What we often find is that customers, and a lot of times it's the finance people, want to jump to the ROI piece, the qualitative and quantitative impact.

But we haven't gone through the leading indicators around things like adoption, engagement, how are people using it, what's the feedback, do they feel good? Both are important, so I'll explain more about that. There's basically this idea of leading and lagging indicators. Let me give you an example of how those things play out. So let's assume you wanted to roll out Amazon Q Developer in your organization. How might you use the metrics and how might they change over time?

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/940.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=940)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/960.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=960)

In the first  instance, what you care about is adoption. Who's got a license, who's signed in, who has used the tool, who's enabled it, has anyone got authentication problems? You're working through all of those basic things. You're basically managing a rollout. What you want to do is observe that early usage and see if you can get any interesting insights from it.  But at this stage, it's a lot more about does everybody have it, do they know how to use it, rather than trying to drive to an ROI conversation.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/970.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=970)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1000.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1000)

Then we move to engagement. Now I'm looking at different metrics.  I'm looking at who are the highly engaged humans, which are the teams that have strong adoption, which are the teams that don't. You've heard me describe multiple times that the team is where the impact is. You're looking to enable teams, and team impact is important. So which teams are on board, which are not? How are your individual teams internally sharing wins? Are you sharing wins? Do you know of people that have had some really interesting stories? And how can you then mitigate low engagement?  Craig and I were talking about this internally, and he has a great story about how Genesys was able to use these metrics to identify and mitigate low engagement. You might go and find, talk to people, and find out that it's something simple you can remediate.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1020.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1030.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1030)

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1060.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1060)

The qualitative, the why, comes from surveys, talking to people, walking the halls.  All of those are things that we use internally and we've seen customers use to understand why. Look over people's shoulders, really just observe and understand what's happening.  And then finally we get into the quantitative. Now we're talking about measuring impact to your baselines. In order to have conversations with executives and finance folks, you need to be able to have a baseline and then show some improvement. Hey, we improved X to Y. That's why we encourage you to look at the quantitative as a set of lagging indicators after you know that people are using the tools, they're engaged, and they enjoy it. 

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1070.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1070)

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1080.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1090.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1090)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1100.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1100)

So what about the tooling? If I think about the tooling that comes from Amazon Q Developer, and frankly what is likely to come  from all of these various agents that we're all talking about, it's going to be activity that maps to the A in the SPACE framework.  You'll notice there are four other letters there that we don't touch. The data that we provide can be used to drive engagement, figure out adoption, and figure out engagement to a point, and I'll give you an idea of the differences in a minute.  But what we cannot tell you, because we don't see enough of what's happening inside your organizations, is the AI impact.  You can't tell AI impact simply by looking at the use of a tool. You have to look at other things, other baskets of measures, other data sources within your organization.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1120.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1120)

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1140.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1140)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1160.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1160)

### Second-Order Impacts: Beyond Code Completion to Organization-Wide Value

The other thing that's really interesting and why this whole conversation of developer productivity has moved into AI impact is this idea of second-order impacts.  When I talk to a lot of executives, the idea of these tools is, oh, can I write more code? Sure. But where else is there value? What else is happening? We measure this internally, I've talked to a number of customers about it, and it's this idea of reduction of onboarding time.  How long does it take for a new developer to reach the speed of the team that they're on? Time to first commit, time to tenth commit, these are typical measures, but that can be a huge second-order impact of the use of AI. Both of these apply here at Amazon.  We tend to move developers around between projects quite frequently, and so the ability to reduce the time it takes to get a new developer or a developer onto a new team and get them up to speed is really important and it's impactful for us.

The increase in skill flexibility is something I was talking to earlier when a customer was asking me about skill makeup. What do you think team makeup looks like? We talk internally about a couple of different stories, one of which is when we're writing the Q CLI, we wanted to be able to write that in Rust. We had a shortage of Rust folks, so we were able to take Q basically and build itself and have experienced Rust programmers do the code reviews. So we're able to increase the flexibility of the skills.

[![Thumbnail 1220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1220.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1220)

You might also see this, for example, if you have a shortage of front-end folks but some back-end folks with available time. Is there an opportunity to take advantage of the tooling as an assistant to expand the skill set and increase flexibility? That's a great second-order impact. 

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1290.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1290)

Shifting into cloud operations, we've seen a lot of customers take advantage of tools like Amazon Q and Kiro moving into their cloud ops teams. Site reliability engineering, cloud operations, and troubleshooting can all take advantage of AI context engineering and prompt engineering. That's not directly related to the developer necessarily, and this would show up in things like DORA metrics and other places that would show you the impact of AI in your cloud ops team. 

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1320.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1320)

We also see it recently moving left into product management organizations. I've talked to a number of folks there who are trying to understand the value of vibe coding. We've got technically minded product managers and product owners who have been able to build prototypes, and it takes a lot of time out of the spec design process. People can react to those prototypes. How are you able to represent that in the metrics and data that you capture? 

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1360.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1360)

One of the things that came up consistently when the three of us were talking about this is: what are the projects that would never have been done if this tooling didn't exist? We talk internally about a team at Amazon that was responsible for heating and air conditioning who needed a simple app built but couldn't seem to get time. They were able to take AI and build it themselves. What are the opportunities there to undertake projects that would have never happened? How do you find that, represent it, and calculate it? There's value there. 

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1370.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1370)

The other thing that happens a lot is when I talk to customers, especially ones who are undergoing a lot of change in their development organization, is how do I isolate the impact of AI relative to these other changes that we're making? We have a complicated system. I might be doing a dev portal and changing my tooling and merging with a company, all of these things add complexity. We've gone from this idea of an individual human getting code completion to being able to take advantage of essentially the same tools in multiple places in the organization to drive a much larger AI impact. 

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1400.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1400)

### Amazon's Cost to Serve Framework for Software Development

Luckily here at Amazon, we're a large organization. AWS and Amazon have lots of different businesses, and so we tend to look around and see where there are ideas of things that have worked before and where we can learn. When we looked around, we looked at the supply chain work that we've done with Amazon.com and this idea of cost to serve. It's something we have talked about in shareholder letters. 

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1440.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1440)

The cost to serve at Amazon is all about this idea of units, packages, and customer delivery, and how can we reduce the friction, delay, waste, and defects in that process. Anytime we can do that, we are reducing the cost to serve, the cost for us to be able to get value into our customers' hands. Software, especially here, is a complicated supply chain with all kinds of different interactions. We have everything from hardware build to hardware software, software running on our hardware devices to AWS to Amazon.com to all of these various businesses using a variety of different things, and it's complicated. 

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1450.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1450)

If we look at this opportunity for friction, delay, defects, and waste anywhere, that can improve our cost to serve. There's actually a science paper, and we've published more information about this if you really want to dive into the science. The fundamental equation is that the cost to serve software is the infrastructure costs plus the human and tool costs divided by the number of units. 

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1500.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1500)

The number of units will vary if you're shipping a mobile app to an API to a monolith to something else, so the number of units is really whatever the unit is that your team typically thinks of as a unit. Ultimately, this is something that you can track over time, and it does allow for the complexity and does allow the acknowledgement of some of those second-order impacts that I described. Luckily, we have teams of folks that can do this work, but we were also able to isolate some of the ways that we improved or were able to reduce our cost to serve. CI/CD works. We have data. 

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1530.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1530)

One of my colleagues was very happy about being able to make that statement. Managing templates and abstractions is something we found to be really useful, and of course AI. Any individual one of these was helpful, but by bringing them all together, we really were able to drive a lot of value and a lot of improvement across our processes, ultimately with a reduction of almost 16% in our cost to serve across the organization.  For a development organization the size of ours, that is a very material impact. But you'll notice I didn't say a coding companion did that. This is the impact of change across your software development processes.

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1560.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1560)

### Jellyfish's Approach: The AI Paradox and Measurement Framework

Now we're lucky that we have the size and scale and folks to be able to do this for ourselves. But what I'd love to do now is call Krishna up to the stage to talk you through how our partner Jellyfish thinks about this.  Thanks, Joe. My name's Krishna Kannan. I work for Jellyfish. I lead our product organization and we're really excited to be here talking with our friends at Amazon and our partners at Genesys about measuring AI impact.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1590.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1590)

I'm going to take a half step back here and talk about measuring engineering productivity because that forms the backbone of what we'll talk about in terms of measuring AI.  Jellyfish has been measuring developer productivity for several years now and thinking about how to do that. Our approach has been first to take a look at data from your issue tracking system, whether that's Jira, Linear, or Azure DevOps, and combine that with your source control data. This gives us the business context and project context for what you're working on and why.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1650.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1650)

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1670.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1670)

Combining that with source control data like your GitHub or Bitbucket helps us understand how long it actually took to do that, what language it was in, and all the other metadata. So we can create a signal of what folks worked on through that. Then we combine that with data from your HR system, your CI/CD pipelines, and your error reports and incidents to really get a complete picture of how engineering is happening at your organization.  We've been doing this for a number of years now, and through that we produce developer productivity metrics among others. I expect many of you are probably measuring things like throughput, cycle time, and team velocity, either with Jellyfish or a similar tool or maybe with something that you built yourself.  Having a good understanding of engineering productivity is a prerequisite, of course, to measuring the impact that comes from using AI.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1680.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1680)

 Shifting gears here into our approach for measuring AI impact, I'll start with a few general observations and then I'll present a framework that we recommend you use that aligns with what Joe's talked about and with what Craig's going to share in a few minutes. The first observation is that across our database of six to seven hundred customers, virtually every software company we've talked to has adopted AI at an organizational level. There are a few stragglers, of course, but organizational adoption is not the barrier today.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1710.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1710)

 The barrier today is that despite that 100% organizational adoption, you really only see about 30% of those companies benefiting at scale from improved productivity through AI tools. This is our conclusion at Jellyfish. But if you look around at other studies throughout software or the knowledge economy as a whole, you get very similar findings. I read a BCG report just recently where they interviewed a public company executive and found a very similar report.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1750.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1750)

 A couple of things that we're seeing here: Jellyfish did a State of Engineering Management Report over the summer where we talked to a ton of CTOs, VPs, and directors and interviewed them about their AI journey thus far. This is a qualitative statement, but the question was what percent improvement in productivity are you getting today or do you expect to get over the next year through AI? The surprising thing here was that it was a very open-ended question that sort of begged for an optimistic response, and yet from that, only 30% said they were getting 50% improvement or greater, which is of course great for that 30%.

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1800.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1800)

But you see that the bulk of the curve is actually at 10 to 25 to 50% gains, which is good, but that's not the expectation that we're getting from the hype over the last two years about what AI could do. We expect more from this.  We also see that of the companies that have adopted AI fully across their organization, you actually do see fairly dramatic improvements in productivity.

This is a quantitative take from another study we did across 13,000 company engineer week observations. We found that over a 2x gain in PRs per engineer when you get to that top end of the curve. So the improvement in productivity is there, yet many are still struggling. We've been calling that the AI paradox at Jellyfish and trying to help companies through that.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1840.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1840)



In thinking about that, we interviewed a ton of companies, talked to a lot of company leaders, and talked to a lot of developers. What we found is that the gap between organizational adoption and individual adoption is generally not one of effort or interest in using the new tools. Engineers by personality and training, and many of us are engineers here of course, are tinkerers and experimenters. We want to use new tools, we want to use AI, and we want it to work. But we're also many of us skeptics. We're not going to use a thing because someone told us to, and we're not going to use a thing that makes our jobs harder, slower, or worse.

The key to unlocking greater adoption and productivity doesn't come through organizational mandate or fiat. It comes from understanding what are the blockers and then how do we get past those blockers to get true success. Through our work with companies, we identified a three-step journey to go from adoption to productivity and then to business outcomes in that order. We then generalized that into an AI measurement framework. I'm going to talk about what that framework is and I encourage folks to hear the framework and figure out how it can work in their organization because it's not a specific set of instructions around metrics to track or numbers to look at, but it's a way of thinking about those metrics to try and drive success for you.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1900.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1900)

### Three-Step Framework in Action: Adoption, Productivity, and Business Outcomes



[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1920.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1920)



The first part of the framework is adoption. When we say adoption, we are really thinking about whether engineers are using the AI tools that you've procured, how many tools they are using, how often they are using them, and then what percent of their actual work comes through the use of those tools. You can come up with a whole litany of metrics that satisfy this criteria. I have an example I want to walk through with you. This example here is from a sample company that I worked with closely and then anonymized for this purpose.

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1940.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1940)



[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/1970.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=1970)



What they did was they said out of all the potential adoption metrics, they want to focus on four, which is a good number. Two to five metrics is a good number. They want to say first, are we actually trying enough different tools? In their case, they're trying five, which is awesome. Then they want to say, are enough engineers using something at least weekly to begin to learn how it's working? From that, they want to go to power usage, which we equate to daily usage, and then actual output from those folks. So what percent of the work they're doing was assisted by AI in some way? They selected four that align with their methodical step-by-step approach and compared them each to an industry trend.

The good news is they actually compare very favorably to those industry trends, as you can see here, but there's still room for improvement. Even though they have 44% of their organization using AI daily, what about the other 56%? Why is that the case? This is where segmenting the data becomes your friend. A naive organization might go in and create a mandate or some sort of demand about it, but we recommend that you identify the 56% that are not using AI daily by segmenting your data, cohorting your data, and understanding whether that's specific teams, specific locations, tenure of employee, or if you can even understand if it's a certain type of work where AI is simply not effective yet for your engineers.

That's why that connection between your GitHub and your Git repositories and your issue tracking is super important so you can understand what types of projects maybe AI is not being used, where maybe it's less effective, and maybe that's driving these numbers.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2090.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2090)



[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2100.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2100)



The second part of the framework is productivity. When we talk about productivity, we're thinking about the actual outputs from your engineering team.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2130.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2130)

So are you getting more things through your process per unit time? Cycle time, throughput, counts of commits, pull requestsâ€”these are examples of productivity of your engineering team. In this example, the sample company centered on the pull request as their atomic unit of measurement.  It's important that they aligned on what was important for them to measure, because there are many different ways you could approach this. We've seen companies look at cycle time, issue throughput, epic throughput, and any number of things. It's important for your culture to all be aligned on the same thing that you want to look at.

They chose pull requests because that is something where they are generally smaller and voluminous enough that outliers and other noise sort of drop out, and you can get a good signal on what productivity looks like. They were looking at pull request throughput as their main metric. Now in this case, despite the fact that they're going faster, they are trailing the industry trend. Our recommendation was less about segmenting on cohorts of individualsâ€”we assumed it was less of a situation where this group is either junior or senior engineers, or this location versus that location. It was much more likely about the type of work.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2220.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2220)

What we found is that the age and complexity of the repository you're working in, whether it's an old codebase or a new codebase, and what type of project it is often matters more for productivity gains once you've got that base level of adoption and engagement for your engineers. In this case, we recommended they segment by the type of work they were looking at. The final one is business outcomes. Business outcomes are often the most misunderstood  because this is really where your organization's culture and what you're trying to get out of AI matters the most.

Once you have people using it through adoption and they're shipping more work more quickly, you're probably expecting some sort of change to occur in your company. When Joe talked about this a few minutes ago, he talked about Amazon's cost to serve model. That's just one way to look at a business outcomeâ€”are you able to produce software more cost effectively? That would count as an outcome here. Other outcomes might be things like R&D savings or the type of work you're delivering. Are you delivering more innovation work per unit time? That's what this company was looking at as well.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2270.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2270)

This company was very concerned with  the amount of time they were spending on maintenance work, keeping the lights on work, and bug fixing work. The entire goal of driving AI and driving adoption was less about cost for this company and more about whether they could produce more innovation, roadmap, and growth features per engineer than they could before. In this case, they had an improvement of 37 percent, which was excellent and ahead of the trend. Interestingly, they wanted to do that through using AI to fix bugs. They were doing that, but they actually trailed the industry trend here by a little bit.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2330.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2330)

So they were succeeding at their top-line goal despite that intermediate goal not being met. There's an opportunity to find out why they were succeeding despite the initial strategy not actually playing out the way they expected it to. This was good news. They could then investigate that and find out if they could actually go faster with the benefit here.  Before I pass it off to Craig, one thing I want to note here at the end is that Joe talked about how we went from autocomplete to code assist and now to an agentic world. The early data suggests that there's a lot of talk about agents, but the actual adoption of those agents is not quite there where the code assist tools might be yet, but it's probably coming.

The point is that we actually don't know what mode the AI tools will take over the next 12 months, let alone the next 24 or 36 months. The framework that we've presented is really intended to be mode agnostic. It doesn't matter if you're using code complete, chat, agents, or augmentation. If you're not adopting those tools, seeing increased productivity, and then seeing some type of business result on the other end, then regardless of what tool you adopt or what mode it works in, we recommend some type of framework like this to help measure progress and drive impact. So with that, I'm going to pass off to Craig to take you through the journey at Genesys.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2410.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2410)

[![Thumbnail 2420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2420.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2420)

### Genesys Journey: From Makeshift Metrics to Enterprise-Scale AI Measurement

Hi, my name is Craig Dollinger. I'm the Senior Director of Platform Engineering at Genesys.  I'm going to walk you through our journey when we first adopted Amazon Q Developer, and then I'll show you how we scaled it out with Jellyfish for our reporting metrics. Along the way, I'll share the lessons we learned and how we achieved where we're headed, including the makeshift reporting we did in the beginning and why we're using Jellyfish now. 

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2440.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2440)

When we first brought Amazon Q Developer on, our goal was simple. We wanted to give developers a boost in their productivity and velocity. We also wanted a way to chip away at some of the technical debt we had, including AWS SDK migrations from V1 to V2 and deprecation of Lambda runtime versions. 

We started getting some unexpected results. We got better documentation in repos and much stronger opportunities for our unit testing. Amazon Q Developer was being adopted and developers were happy with it. However, the challenge was we didn't understand how it was being used, who was using it, or who the power users versus idle users were. The gap in visibility was the problem we needed to solve.

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2490.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2490)

We realized there was no clear way to understand how developers were actually using Amazon Q Developer. We started looking at the Q metrics. We use AWS SSO for sign-in, so the user IDs were good. However, we couldn't really tell how the engineers were engaged.  We hacked together a makeshift metrics database so we could tell our leadership how Amazon Q Developer was being used. We basically took the data from Amazon Q Developer, piped it into PostgreSQL, and then our internal teams visualized everything in QuickSight.

But we started realizing we couldn't scale that because every team wanted a different view. We had to make different dashboards for each team, and it was becoming increasingly difficult to scale that out to everybody's different needs. We really knew this was a stopgap and we needed something automated at an enterprise level to actually do the reporting on Amazon Q Developer and understand how our developers were using it.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2540.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2540)

During one of the Amazon Q Developer check-ins with Amazon,  Joe mentioned that we were partnering with Jellyfish. We already used Jellyfish for our Jira intake, so it seemed like a perfect opportunity for us to start using their AI dashboard. We joined the program and were able to retire that PostgreSQL setup and move the metrics right over to the Jellyfish AI dashboard.

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2590.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2590)

The impact we have now is that Jellyfish consumes all the Amazon Q Developer data and gives our leadership near real-time stats on Amazon Q Developer. Since Jellyfish is already involved in our Jira taxonomy information, we're able to roll everything up to our team and service level and actually get the Jira taxonomy.  Our leadership groups are now able to understand how Amazon Q Developer is being used by their teams and by their product taxonomy. This solved a lot of the questions that were being asked.

Once we actually connected Jellyfish, we were able to see who our power users were and who our idle users were. It sparked a lot of conversation with different team members about how they wanted to use Amazon Q Developer. People who weren't adopting it as fast were able to engage in conversation and understand why. Basically, the dashboard answered probably 90 percent of the questions that were coming to my team.

Something really important happened: it sparked a lot of conversation with teams about how AI was amplifying their work. It's not a replacement; it's amplifying the developer. Engagement with the team leads and the developers started growing from there, and when they were able to see the impact, they pushed to adopt Amazon Q Developer faster.

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2640.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2640)

Looking ahead,  we've basically embedded AI across our entire software development life cycle. AWS and Jellyfish ensure our consistent, unified metrics across all our tools, so it's a one-stop shop for dashboards. It reduces the time that developers spend on mundane tasks. I usually tell my team members that we're not replacing developers; we're amplifying them. It's just another tool in their toolbelt to make them better at creating the features they want to create and help solve the hard problems moving forward.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2680.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2680)

What you're seeing there is basically how AI metrics help us qualify the real productivity gains,  things like faster code reviews and faster pull requests. The improvements are not theoretical; they're visible. We can actually see the teams making progress. It gave us something we've been missing for a long time: clear proof that AI was helping individuals. If it wasn't helping individuals, we knew we had to engage in conversations to understand why. Maybe it was prompt design, or maybe they just didn't want to use it.

The point is the metrics aren't black and white. They engage you in conversation with the developers.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2710.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2710)

 Later on, Amazon Kiro came out. We adopted that right away. Everyone in the team really liked it. They integrated right into VS Code, introduced spec-based programming and vibe coding, so two modes. We started seeing teams whip out really cool utilities and tools, but it really changed how developers thought about building stuff with AI. It was actually with spec-based programming more structured and intentional. Developers came more efficient and productive. We make a point of score here the same way we describe all of our other AI tools. It's an amplifier. It's not a replacement. It's not just an assistant, but we tell the developers it's just another tool in your tool belt to make you better.

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2760.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2760)

We're still waiting on some of the Kiro metrics. Once we consume those, we'll work with Jellyfish, and Jellyfish will consume those Kiro metrics.  So we'll have actually Q usage and Kiro usage in one place. That makes life a lot easier and everything also gets broken down the same way as that for our team taxonomy and everything. So one unified dashboard, one single space for usage adoption, your return on investment across the whole platform. For us it meant teams can choose the AI tools they want to work with and the leads and directors can actually go to one dashboard for reporting to see how developers are actually using the tools.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2790.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2790)

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2830.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2830)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2850.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2850)

### Key Takeaways: Building a Sustainable AI Measurement Strategy

 Using Q and developer Kiro for us is a journey. AI is helping us build a culture internally with creativity at the forefront and speed. Basically we want to move forward from experimentation to real measurable impact within the groups. So what are some key takeaways from this conversation? Some things I hope you'll take away from it. The first one is measuring AI as a journey.  You've heard me describe it. You've heard Craig and Krishna describe this. Baseline what you have. Pick a framework and start right. There's no, it's not going to get any better. Just go work with what you have. And then think about this idea of leading and lagging indicators.  A lot of the finance folks are going to want the lagging indicators, the ROI improvements, all of these things, but what are the leading indicators that would make sense in your organization to help you move forward?

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2870.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2870)

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2880.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2880)

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2890.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2890)

Map your usage to your internal taxonomy. I think one of the most interesting things that I found working with folks like Jellyfish  is the ability to overlay the usage across both organizational and product and feature taxonomies.  It's been really interesting to be able to see how adoption and engagement works.  Impact is not going to be universal and it's not going to be consistent, so being able to explain to the executives, here's why this team appears to be lower, here's why this team appears to be faster, and what the lessons are is incredibly important in those kinds of conversations. And then as Craig talked about, think about your organization. Having the good data enables you to help leadership understand what does and does not matter, what is and is not relevant.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2910.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2910)

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/ab1c8a400e23ef72/2950.jpg)](https://www.youtube.com/watch?v=l-mjftGWueA&t=2950)

Finally have a very clear vision and a clear message.  With that, thank you very much for your time. If you want to learn more, we have a chalk talk session tomorrow, which is myself and a colleague where we're going to be able to talk about all these various things more. And if you want to learn more about cost to serve, there is a session tomorrow, DVT 207, which is a session like this where my colleagues from AWS will go super deep into how we think about measure and manage our cost to serve framework.  So with that, thank you very much. Please enjoy re:Invent.


----

; This article is entirely auto-generated using Amazon Bedrock.
