---
title: 'AWS re:Invent 2025 - Universal data connectivity with ETL and SQL queries (ANT209)'
published: true
description: 'In this video, AWS Analytics team presents three universal data connectivity approaches to solve enterprise data integration challenges: AWS Glue connectors for self-managed ETL pipelines with 100+ pre-built connectors to third-party applications like Salesforce, SAP, and ServiceNow; AWS Zero-ETL for managed data ingestion supporting 23 sources including Aurora, DynamoDB, and SaaS applications with near real-time replication; and Federated Queries for ad hoc cross-source analysis without data movement. The session demonstrates how data engineer Alex uses these methods to unify customer data across Snowflake, Salesforce, SAP, Amazon Redshift, and RDS. Real customer examples include Vyaire Medical achieving 50% cost savings with AWS Glue, and Pionex reducing latency by 98% using Zero-ETL. The presentation includes live demos of creating Zero-ETL integrations from Salesforce to Redshift and setting up federated queries in SageMaker Unified Studio, showing how organizations can eliminate ETL bottlenecks and accelerate insights delivery from hours to minutes.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/0.jpg'
series: ''
canonical_url: null
id: 3086765
date: '2025-12-05T15:05:08Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Universal data connectivity with ETL and SQL queries (ANT209)**

> In this video, AWS Analytics team presents three universal data connectivity approaches to solve enterprise data integration challenges: AWS Glue connectors for self-managed ETL pipelines with 100+ pre-built connectors to third-party applications like Salesforce, SAP, and ServiceNow; AWS Zero-ETL for managed data ingestion supporting 23 sources including Aurora, DynamoDB, and SaaS applications with near real-time replication; and Federated Queries for ad hoc cross-source analysis without data movement. The session demonstrates how data engineer Alex uses these methods to unify customer data across Snowflake, Salesforce, SAP, Amazon Redshift, and RDS. Real customer examples include Vyaire Medical achieving 50% cost savings with AWS Glue, and Pionex reducing latency by 98% using Zero-ETL. The presentation includes live demos of creating Zero-ETL integrations from Salesforce to Redshift and setting up federated queries in SageMaker Unified Studio, showing how organizations can eliminate ETL bottlenecks and accelerate insights delivery from hours to minutes.

{% youtube https://www.youtube.com/watch?v=JsuEPLg50fc %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/0.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=0)

### The Data Silos Challenge: Why Organizations Struggle with Fragmented Information

 I hope you all had safe travel and are excited for re:Invent for the rest of the week. Before we get engrossed with the great sessions that we have planned for you for the entire week, let's think through a scenario. You get up, you want to grab a cup of coffee, you go to your favorite coffee shop and you order your usual coffee. The barista says, "Sorry sir, I can't do that right now. I don't have all the ingredients available in one place. The creamer that you like is in the side cooler. The special milk that you like is in the back warehouse and the beans are across town altogether." Sounds absurd, right? But isn't that how organizations are running our data today?

Our data sits in different systems. Customer data is somewhere else, our sales data is in different places, our service analytics data is completely different places. Then you have all your usual questions that our executives come up with. "Hey, can I know what was the sales last month?" Or you know, this new product that we launched, how is it doing based on the opportunities that we had? And it takes us weeks to get the data in the right place so that we can just run basic analysis on it. This is why we are here.

I'm Shrey Malpani, Senior Product Manager for AWS Analytics, and with me is Shruti Worlikar, Senior Manager, SSA, AWS Analytics. We have talked to hundreds of customers and what they have told us is they wish there was an easy way to integrate data from all these different locations into a singular place. In today's session, universal data connectivity with ETL and SQL queries will show you how you can transform your data and make it actionable. How you can simplify the data pipelines with AWS capabilities such as AWS Glue, zero-ETL, and Federated Query so that you can get actionable insights from it faster.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/160.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=160)

We'll show you how we help customers eliminate ETL bottlenecks, accelerate data ingestion and maximize return on their data investments. You are in for a ride.  Quick show of hands, how many of you are using a variety of operational databases and enterprise applications today and are having to stitch it together? Correct, all of us have to do that.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/170.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=170)

 So why is this important? There is a fundamental challenge when we have to tackle all of this. In today's organization, everybody says they have to be data driven. This is not just a buzzword anymore. This is important because we need to get to actionable data faster. We need to understand what are the market changes and how do we understand our customers deeply. These are all critical for growing our organization's needs.

Here is the fundamental challenge that we have. All of our data lives in different silos. The data that we have is stored in different systems, all in different locations altogether. Sometimes even in different cloud providers or on-premises data centers. In addition to that, we have different personas who are using this data. A new trend that's accelerating right now that we are hearing from our customers is convergence of analytics and AI ML workloads on the same kind of data. Your data scientists need access to the same data that your data analysts have, which they are using to build dashboards.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/270.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=270)

The AI ML models are built on the same data that feeds into your real-time analytic systems, which makes it additionally more complex. So you might be wondering what does this complexity look like in a real-life enterprise environment? Let's take a look at that.  As you can see here at the bottom we have a bunch of data sources: finance, telematics, customer experience, manufacturing, sales, and product quality. Each of these systems are generating mission-critical data for our organization. In the middle we have our data lake, which hosts the raw data sets. Then all of your structured data is sitting on your data warehouses. You are using AWS Glue Data Catalog to do the metadata management. You have transformation engines such as AWS Glue and Amazon EMR.

And then your query engines such as Amazon Redshift and Amazon Athena. And then last but not the least, you want to run AI and ML on top of it. So you are using Amazon SageMaker or Bedrock. Now this makes it complex. The additional piece of complexity comes into picture with all the different personas who are accessing the systems. Your data scientists are creating the ML models. Your data engineers are building the pipelines. Your data analysts are creating the dashboards. Your data stewards are managing the governance and your data owners are managing access. Now this is just one piece of the puzzle. We are not even scratching the surface about third party data integration or even marketplace integrations. All of this makes the data landscape more complex and more critical to navigate.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/370.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=370)

### Universal Data Connectivity: AWS's Three-Pattern Approach to Data Integration

We do have a solution in mind  which we call the universal data connectivity approach. At AWS, we are taking a look at it very differently. We believe there is no one size fits all. Different scenarios require different tools, different levels of controls and different trade-offs between flexibility and simplicity. What we have built is a unified data connectivity strategy. What that means is purpose-built tools and solutions so that your user personas can utilize them for what they need at that given point in time.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/410.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=410)

To make this concrete, I'll introduce a user persona.  One of our customers, Alex, is a data engineer. He works for a large enterprise. He is familiar with ETL pipelines and has deep Spark expertise. He can work with multiple data sources at any given point in time, but he is always struggling with similar questions. How can he drive value for his organization? How can he connect to all of these data sources seamlessly? We are talking about dozens if not hundreds of data sources. What tools can help him simplify this data pipeline so that he does not have to build deep expertise for every new tool and every new technology that he is going to use?

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/470.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=470)

Let's look at what Alex's environment looks like.  Alex today is using Snowflake for historical data analysis for one of his use cases. The company's CRM data sits on Salesforce. The operational data sits on SAP and service IT data sits on ServiceNow. All of these are different disparate third party systems. In addition to that, he has all the AWS systems that he is using. Amazon Redshift serves as his central data warehousing platform. Amazon S3 is his data lake, which is hosting raw logs or structured data that he has created and transactional data that he has created. In addition to that, we have Amazon RDS and Amazon Aurora, which connect the operational databases that are fielding or running the actual enterprise applications on top.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/540.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=540)

These are all different systems that the data flows through, but it needs to flow through seamlessly so that we can build in the right insights that we are looking for. So what does Alex need to accomplish with all of these different systems? Alex has three tasks that he has to achieve.  First, creating ETL pipelines to ingest data from multiple sources. All of you are familiar with this as the bread and butter of data engineering: extracting data from multiple sources, polishing it, transforming it into usable format, and then loading it into destination systems. It is foundational work, but it is time consuming and complex.

Second, reducing the operational load of managing the data ingestion pipelines. How can he drive value faster? Every time he is building new applications or new pipelines, he has to manage them and fix them if something goes wrong. That is frustrating. Schema changes, adding new pipelines, and so on. And last but not the least, ad hoc requests. We have all been in these shoes. Our VP comes in and we are looking for combining data across multiple different sources that he needs answers to like yesterday.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/640.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=640)

We have to quickly put something together. The data analyst comes in and they are asking for access to run a single query at one given point in time wherein the infrastructure may not even be used at any given point in time. So these are all the tasks that Alex is struggling with today. AWS provides three distinct patterns to address each of these needs. Let me show you our comprehensive approach. AWS what we call universal connectivity options. So there are three options.  Self-managed ingestion using AWS Glue connectors, managed data ingestion using AWS Zero-ETL, and in-place data access using federated queries. Each of them has unique benefits for unique use cases.

### AWS Glue Connectors: Self-Managed Data Ingestion with 100+ Pre-Built Integrations

For teams who want control over their pipelines and want customization, AWS Glue connectors is their go-to solution. We have built over a hundred pre-built connectors for you to AWS and third-party applications. With managed data ingestion, if you want a pipeline but you don't want to manage the operational overhead with respect to this pipeline, with Zero-ETL you select the source, you select the destination, and AWS takes care of managing that entire pipeline. You don't have to worry about maintaining it or anything of that sort.

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/740.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=740)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/750.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=750)

Last but not least, you have data where you need to query it on an ad hoc basis or there is data that shouldn't be duplicated across multiple systems because of security reasons, or you are combining data across multiple systems. For this, we have in-place data access using federated queries. Now each of these are available for you. Let's dive deep into each scenario one at a time. For the first one, we utilize AWS Glue.  AWS Glue is our serverless data integration service that thousands of tens of thousands of customers are utilizing today. 

What makes Glue interesting? Glue is an all-in-one data integration service. You want to manage the data quality and know what anomalies are associated with it. You want to have the data catalog so that you can query the data, know where the data sets are available and they're searchable. All of that is available with Glue. It's cost effective because it is serverless. There's no need to deploy infrastructure. You will only pay for the infrastructure that you use based on the jobs that are running.

This allows you to scale from gigabytes to petabytes and only pay for what you utilize. It's tailored to support all of the data users. Your data analysts who are not used to coding languages and are more accustomed to UI-based infrastructure can use the visual infrastructure to build pipelines without utilizing code. You have data engineers who are familiar with Python and Scala and want to do in-depth customization. Great, they can use AWS Glue. Or you have data administrators who are more comfortable with SQL for doing transformations and so forth. Perfect. They can use AWS Glue too.

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/860.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=860)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/870.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=870)

So it gives you tailored tools for each of these different user personas. And last but not least, it supports all workloads in one place. Think about traditional ETL, modern ELT, streaming data ingestion, or batch analytics. All of that is possible with AWS Glue. So it's a one-stop shop for you. What makes this more interesting is the diversity  or the depth of the platform that's made available for you and how have we done that? We have done that with hundred plus data sources that are available for you. 

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/890.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=890)

These are pre-built connectors that are available, which showcase years of engineering effort that's made it possible to virtually connect any source, any destination in AWS or in third-party applications available  to you at your fingertips. Then we have three types of connectors. Native connectors are the ones which are managed by AWS. They are pre-built, they are maintained, they are updated. Everything is taken care of by AWS for you. We have custom connectors.

If you have unique requirements or unique transformations that you want to do on the platform or pipeline, you can build your own connectors utilizing AWS SDKs, and they are available for you. And last but not least, we have marketplace connectors. These are connectors that our third-party partners have built so that you can utilize them and connect to multiple third-party sources that are available out there.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/940.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=940)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/970.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=970)

Managed catalogs are fascinating.  Utilizing managed catalogs, you can manage your catalog using Amazon S3 or Redshift Managed Storage to create Apache Iceberg tables directly. What that allows you is modern open table format accessibility directly on your platform, asset transactions, and time travel. All of these new features are available to you at your fingertips.  And last but not least, we ensure reliability and security. We provide encryption at rest and encryption in transit, identity-based access controls so that your authentication and authorization happens seamlessly, and all the data sits in your VPC control so that the data doesn't leave your private network.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1000.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1000)

### Real-World Success: How Vyaire Medical Achieved 50% Cost Savings with AWS Glue

This is what AWS Glue connectors do for you. Now let's see how Alex can utilize everything that we talked about  to actually solve his first use case. Alex wants to enrich and unify his customer profiles. He is using two key systems. In SAP, he has his operational data, which includes purchase history, payment terms, and credit limits. In Salesforce, he has his customer profile with all the sales data, sales transactions, and tickets that were opened by the customer.

What Alex is able to do is utilize AWS Glue connectors to connect Salesforce to AWS Glue, and the connector takes care of all the Salesforce API integration. He doesn't have to worry about pagination or anything of that sort. The Glue connector takes care of the entire overhead associated with it. The same applies for SAP. Just plug in the SAP connector into Glue and that is all. All the API-level transactions that need to happen are taken care of by the connector. You don't need to learn a new API for these third-party solutions on your own.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1110.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1110)

Now the best part about it is that utilizing AWS Glue, Alex can look for anomalies. Is this the right email address format? Are these the right ages? Does it look fake or not? Are these the right amounts? Do I have any negative sales amounts associated with it? Why is this important? This is important because you can catch anomalies in your pipeline before they even reach your analytics platform. So there are fewer surprises down the road, making the life of all the other user personas that are going  to use this data much easier.

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1130.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1130)

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1150.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1150)

Finally, Alex can use AWS Glue to aggregate and join all of this data together to create a unified customer profile. That data is then available on Amazon S3  as Iceberg tables so that it can be queried by any analytics platform or used for any AI and ML application. This entire pipeline is serverless, scalable, and operates on a pay-as-you-go model. We are constantly making this better. Let me show you how.  Right now we have 60 plus connectors to third-party solutions, including Salesforce, Zendesk, SAP, ServiceNow, Google Ads, and many more, where we take care of all the connectivity for you.

We are making it better. Just in the last quarter, we added 20 new connectors. To call out a few, we have Datadog. Think about it. You can use your application monitoring data and see how that applies to your business utilization. You have Okta integration. How is your user interaction? How long are your users interacting with your application? And does that influence the sales that you are doing within your platform? We also have Adobe Analytics, which covers all the top-of-the-funnel marketing activities that you are doing.

Asana reveals how my organization operates and which projects are actually moving the needle. All of this is available at your fingertips. All these new connectors use standard industry authentication methods such as OAuth 2.0, so you don't have to worry about storing credentials in files. Everything is automated in the system and accessible to you. It just works securely and reliably.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1270.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1270)

With these connectors, customers can test the connection, validate the connection, and then browse the metadata as needed. To close off this journey, let me tell you about a customer use case. Vyaire Medical is a respiratory care company  headquartered in suburban Chicago. They deliver respiratory care products to hospitals and healthcare providers worldwide. Like any other healthcare company, they had the same challenge: all of their operational data sitting in silos in multiple places, with data in SAP, JD Edwards, ServiceNow, Salesforce, and more.

They needed to get all of these disparate data sources together so they could understand their customers better, manage inventory more effectively, and improve sales management. What they did was utilize AWS Glue and AWS Glue connectors to pull in all of these disparate data sources and point them to AWS's Lakehouse architecture. Over eight months, they transformed hundreds of ETL jobs from their legacy ETL tool into AWS Glue and achieved huge benefits: 50% cost savings compared to their previous solution. That's not something easyâ€”it's a fundamental difference in their cost structure.

The key benefit was improving developer productivity for their data engineering team. These data engineers who used to take weeks to create new pipelines are now able to create them in days. Previously, getting new analysis used to take days, but now it takes hours. This is the kind of fundamental operational and productivity change that AWS Glue delivered for our customers and which we can do for you.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1430.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1430)

### AWS Zero-ETL: Eliminating Pipeline Maintenance with Managed Data Ingestion

We have discussed Alex's first challenge of building ETL pipelines, but don't forget that he has two other challenges we discussed. For that, I'm going to call my colleague Shrey Malpani, who will come here and talk to us about how Alex can solve those challenges. Thanks Shruti. Using the AWS Glue connectors, Alex was now able to simplify his ETL pipelines.  Now his next goal is to keep his analytical data in sync with operational databases without adding much operational overhead.

His current ETL pipelines run on an hourly basis, so any data he gets access to in his analytical systems is at least an hour old. He's also responsible for maintaining the pipelines and ensuring their reliability. Just like all of us, he hates getting paged at 2:00 AM because something broke in his data pipeline. He's thinking to himself that he needs a solution that can keep his analytics data current without having to implement complex change data capture systems, which bring additional overhead.

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1500.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1500)

This is where AWS Zero-ETL can help Alex. Before we dive into what AWS Zero-ETL is, let's look at the typical requirements that will be needed if you are running your own data application and ingestion shops.  You might have data stored in multiple sources and may need to apply certain business logic which is specific to each of those sources.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1520.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1520)

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1530.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1530)

You might also need to implement some complex retry logics to ensure that the pipeline does not break because of some transient errors.  You would also be responsible for maintaining the infrastructure and monitoring the pipeline to ensure everything is running smoothly. Additionally, you'll need some specialized ETL skill sets  to make sure that the pipelines are running in the most optimal manner.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1550.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1550)

At AWS, we understand that all of these requirements can make the task of replicating data from multiple sources for even the simplest use cases very complex. That's why we are investing in a zero-ETL future.  AWS zero-ETL is a set of AWS managed integrations that allow you to ingest data from multiple data sources without having to build an ETL pipeline. You can ingest data from multiple transactional, operational, and enterprise SaaS application sources.

With zero-ETL, you can get access to fresher data for your analytics, AI and ML applications, or reporting use cases. You may be thinking that AWS already offers multiple ways in which you can operate your data pipelines and ingest data into your analytics data warehouses and data lakes. As we discussed earlier, you could use AWS Glue ETL to build and maintain your own ETL pipeline. You could use visual ETL to get a no-code, low-code interface, or we offer other services as well where you can operate your ETL pipelines.

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1640.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1640)

So what is so special about zero-ETL? Well, zero-ETL is a purpose-built AWS managed capability that helps you operate and ingest data from multiple sources in a simple and secure way. You just have to create the integration once, and then we take care of the rest. It's also highly performant  and delivers data from multiple sources to your target systems within near real-time lag. For example, if you have zero-ETL integrations from Amazon Aurora or Amazon RDS to Amazon Redshift, any changes on the source will be replicated to the target within a few seconds. For other sources, these changes can be captured within minutes.

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1670.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1670)

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1680.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1680)

What this means is it reduces your time to insight.  Zero-ETL is also very efficient and it runs with minimum disruptions at the source. Finally, it has the right mechanisms in place  so that you can trust zero-ETL to replicate data accurately to the target systems. You could leverage encryption mechanisms. You could operate the zero-ETL integrations in your VPC systems and ensure the security of your data.

Many of you and other AWS customers have shared feedback with us that you want your end users of the data warehouses and data lakes to be able to ingest data from multiple sources and use them in the analytics engine of your choice. These end users may not always be data engineers. They could be data analysts who lack the specialized skill sets that are needed to build and operate data pipelines. With zero-ETL, our goal is to enable these end users with a no-code and managed capability so that ingesting data from multiple sources becomes a transformational experience for them.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1750.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1750)

Now let's take a look under the hood and see how zero-ETL works.  We'll look at an example where you have to ingest data from an Amazon Aurora database to an Amazon Redshift database. You might want to capture the initial snapshot of data from Aurora into the Redshift database and then periodically capture any changes that are made to Aurora and have them reflected in the Redshift database. In this case, Aurora becomes your source and Redshift is your target.

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1780.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1780)

[![Thumbnail 1800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1800.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1800)

Once you set up a zero-ETL integration between Aurora  and Redshift, it starts with an initial load of the full snapshot of the data. It creates the table on the destination Redshift database and creates the corresponding data types to match the source without you having to write a single line of code. 

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1820.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1820)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1840.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1840)

After the initial load is done, it starts capturing any inserts, updates, or deletes in near real time and reflects them in the target database, Amazon Redshift in this case.  So if a new record is added to your Aurora table, zero ETL will add the same record in the Redshift database as well. In case you drop a column, the same will be reflected on the target database in near real time. That is the simplicity with which you can operate zero ETL. You just create the zero ETL integration once and then we take care of the rest. 

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1870.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1870)

[![Thumbnail 1880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1880.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1880)

Let's look at a demo, which will show you how simple it is to create zero ETL integrations. In this case, we will create a zero ETL integration from Salesforce to Redshift. A prerequisite for setting up this integration would be for you to create an AWS Glue connection to Salesforce, just like Shruti mentioned earlier. So you could navigate to the AWS Glue console,  click on the zero ETL tab, create a zero ETL integration, select Salesforce as source, and click next. 

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1890.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1890)

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1900.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1900)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1910.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1910)

Select the right connection, provide the IAM role that has access to the Salesforce data, and then you can start filtering on the multiple Salesforce objects that are available.  This is very helpful when you don't want to replicate all of the data from Salesforce, but want to limit it to the specific objects that are relevant for your use cases.  Once you have made all the selections, you have the capability to review the data as well. This gives you an opportunity to check that everything looks as it should before it's too late in the system. 

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1920.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1920)

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1930.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1930)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1950.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1950)

Now that the source is configured, you provide the target details. In this case, that would be the Amazon Redshift data warehouse.  Now that the source and target are set, click next and you can now configure the integration.  You could provide the encryption keys or modify the replication intervals for change data capture. You could also add the integration name, let's say we call it Salesforce integration in this case. And click next, review all the details. And finally, if everything looks good, click on create. 

[![Thumbnail 1960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1960.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1960)

[![Thumbnail 1970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1970.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1970)

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/1980.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=1980)

It'll take a minute or so to set everything up and notice how in this case it prompts us that we have not yet created the Redshift database where the replication should happen.  So just with a click here without navigating to a different console, you can create that database. Let's say we call that database Salesforce db.  Click okay, and it's all set. If you go to the Redshift console, you can see that the zero ETL integration is now active. 

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2000.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2000)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2020.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2020)

The new database that we created is also active, and you can quickly start querying that data in the Redshift Query Editor. So if you look at the databases that you have access to, you would notice that it shows the Salesforce db database that we just created, and it has four different tables, each one corresponding to one object that you selected while configuring the source.  Start writing your queries and get the results. You could also dive into the integration metrics and look at the data that has been transferred because of this integration. 

You could also check the status of each of the tables individually and figure out if any step is needed to debug any errors that might have come up. Such is the simplicity of setting up zero ETL and just within a few clicks, without writing a single line of code, you can start ingesting data from multiple sources into the analytical systems of your choice. So now that we understand how to create zero ETL integrations, let's look at what are the different sources that you can ingest data from using zero ETL.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2090.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2090)

### Expanding Zero-ETL: 23 Data Sources and Multiple Target Options

This year, we have had support for 11 new sources for zero ETL integrations, and now zero ETL supports 23 distinct data sources. These can be classified into three broad categories. The first one is AWS databases. You can create point-to-point integrations from different Amazon databases like Amazon DynamoDB, RDS for MySQL, and Amazon Aurora. In addition, earlier this year we added support for RDS for Oracle, RDS for PostgreSQL, and Oracle Database at AWS for zero ETL integrations. 

The next category is third-party enterprise applications.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2110.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2110)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2130.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2130)

You can create managed data ingestion using zero ETL from eight different SaaS applications including  Salesforce, SAP, ServiceNow, and Zendesk. Last week, we launched a whole new category of zero ETL integrations from self-managed databases. You can create zero ETL integrations from MySQL databases, PostgreSQL, SQL Server,  and Oracle databases, which are either hosted on premises or on EC2 instances. That's eight additional sources.

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2140.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2140)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2160.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2160)

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2170.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2170)

If you are creating zero ETL integrations  to Amazon Redshift, you can ingest data from all of these 23 different sources. Alternatively, if you want to ingest data in your data lakes in Amazon S3, you can do so from 12 of these 23 sources. You can create integrations from Amazon DynamoDB.  Earlier this year, we also launched support for RDS for MySQL and Amazon Aurora databases to create zero ETL integrations into Amazon S3.  In addition, you can also ingest data from the SaaS applications we discussed earlier.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2190.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2190)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2220.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2220)

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2230.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2230)

Another thing we launched this year in addition to the different sources is a whole new target: Amazon S3 Tables. For those of you who don't know,  Amazon S3 Tables provide S3 storage that is optimized for analytics workloads. It has features designed to continuously improve performance on tabular data, and it is ideal for storing tabular data, which may include transactional information, your sensor data stored in tabular format, or other forms of tabular data.  You could ingest data into Amazon S3 Tables using zero ETL from Amazon DynamoDB or the eight enterprise applications. 

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2250.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2250)

Now that we understand the scope of zero ETL, let's go back to our friend Alex and evaluate his situation. The sales team in his organization is trying to understand what revenue is coming from some of the top opportunities that were identified earlier.  The revenue or sales data is stored in Amazon Aurora, and the opportunity information is available in Salesforce. The sales and marketing teams want to build a customer lifecycle model using data from both these sources and also create reports on some of their top accounts.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2280.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2280)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2290.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2290)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2310.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2310)

The task for Alex is to integrate data from these sources so that the marketing team is able  to achieve both these outcomes. Alex thinks that he can build an ETL pipeline to achieve this outcome.  He could take a streaming approach just like the one shown at the top of the screen and start ingesting that data, or he could build a pipeline that looks like the one shown at the bottom and have a batch integration approach. However, his existing pipelines are already taking up most  of his time, so he does not want to add an additional burden of managing these new pipelines. He turns to zero ETL instead.

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2320.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2320)

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2350.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2350)

He sets up a zero ETL integration from Aurora to Redshift and then another one from Salesforce to Redshift with just a few clicks like we discussed earlier. Just like that, in a matter of minutes, the data from these two sources automatically starts flowing into Amazon Redshift, and this is available in near real time for any of the analytics needs for the marketing teams.   For Alex, zero ETL means eliminating an entire category of pipeline maintenance work. AWS takes care of the replication of any schema translation and error handling, and Alex can now invest more of his time into business critical tasks. So Alex was able to achieve two of his outcomes now.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2390.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2390)

### Zero-ETL Enhancements and Pionex's 98% Latency Reduction Success Story

Before he moves to his third task, let's look at what are the other new things apart from the sources and targets that we discussed that we have launched in zero ETL. The first category is features  that provide you more control when creating zero ETL integrations. We've added support for on demand ingestion using zero ETL for cases where you want to ingest a full snapshot of data immediately.

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2430.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2430)

We have also added support for history mode for zero-ETL integrations into Amazon Redshift, which preserves the history of any changes made at your source within the Amazon Redshift databases.  This allows you to track and analyze any changes in Redshift itself rather than having to go to the source to track those changes. We added features for more flexibility when you are creating zero-ETL integrations. You can now customize the refresh interval when creating zero-ETL integrations from SaaS sources between 15 minutes to six days.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2470.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2480.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2480)

You can also create up to five integrations from an Amazon Aurora cluster to your Amazon Redshift data warehouse, and this could be to the same or different data warehouse. You can also now ingest data from Amazon Aurora PostgreSQL tables that have declarative partitions. We improved observability for zero-ETL integrations  and started logging the client errors into Amazon CloudWatch logs.  This will help improve the debuggability of zero-ETL integrations.

Let's take a look at a case study of one of our customers, Pionex. Pionex is a crypto exchange and wants to use the latest market data to deliver insights to their end users. It is critical to deliver these insights on the latest data so that their users can make decisions based on the latest data available. Earlier, Pionex operated an ETL pipeline managed by themselves, which led to a lag of 30 minutes. This meant that any insights delivered to their users were based on 30-minute-old market data, and this was a key blocker for them to deliver the best experience for their end users.

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2550.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2550)

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2570.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2570)

They shifted to zero-ETL and since that shift, they have seen a 98% reduction in latencies, an 80% reduction in maintenance costs, and a 66% reduction in overall operational overhead costs.  Such is the impact that zero-ETL can have on your workloads as well. In the words of the senior director of data analytics at Pionex, "Zero-ETL has helped us achieve real-time access to our operational data, something that was not possible before." 

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2620.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2620)

### Federated Queries: On-Demand Data Access Across 30+ Sources Without Moving Data

Now that we understand the benefits that zero-ETL can deliver for our use cases, let's go back to Alex's tasks. He has simplified his data pipelines using connectors and has eliminated a lot of his operational overhead using zero-ETL. Just as he gets his pipelines running smoothly, he realizes there is a whole different category of requests that he has received that he has not addressed yet. These are typically ad hoc requests that come from his leadership, and more often than not, they are required in a matter of hours. 

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2630.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2630)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2650.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2650)

Now Alex wonders how he can deliver those analyses and insights in the timeline that has been requested when there is just no time to build a new data pipeline. This is where federated queries can come to Alex's rescue.  Federated queries provide on-demand access to data from multiple different data sources into a single query without having to move a single byte of data. With federated queries, there is no need to build an ETL pipeline at all. You can analyze data directly where it lives and write a single SQL query  which accesses data across multiple sources.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2660.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2670.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2670)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2680.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2680)

We offer federated queries in both Amazon Redshift and Amazon Athena,  giving you flexibility to choose the right querying engine based on your use case. You can query and analyze data stored in relational, non-relational, or object storage,  or any custom data sources and write a single query to get near real-time insights from the different data sources where the data might be located.  This approach works perfectly for ad hoc analysis, for data exploration, for cases where you might have sensitive data which cannot be replicated at multiple locations, or scenarios where you are trying to optimize your storage footprint without having to store multiple copies of the same data.

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2710.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2710)

Let's see what you can achieve using federated queries.  Today we support 30 plus federated connectors, and you can connect to transactional databases such as MySQL or PostgreSQL databases and AWS databases like Aurora and RDS. You could connect to other storage systems and data warehouses and data lakes like Amazon S3, Redshift, and Snowflake. You could also connect to multi-cloud systems like Google Cloud Platform or Microsoft Azure. You could choose to connect to enterprise applications like SAP or any big data platforms like Cloudera.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2790.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2790)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2820.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2820)

Let's take a look at how it works under the hood. Say you want to use Amazon Athena as your centralized querying engine, and your data is stored at some of those places we discussed in the last slide. You can use Lambda-based connectors available in Athena, which will use Lambda functions to access data across these different data sources. You could either query or process the data from these data sources, read the schema, or get the records themselves.  You can modify them to achieve the desired analysis that you want. Once the analysis is received from the federated query, you could decide to generate an ad hoc report, create a visualization, or store the output persistently in your data lakes. That way you are not copying the whole of your data, but only the analysis that has been generated from the query. 

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2830.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2830)

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2840.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2850.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2850)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2860.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2860)

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2870.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2870)

Let's take a look at setting up Amazon Athena federated queries using SageMaker Unified Studio.  If you go to SageMaker Unified Studio, go to the data tab and in your lakehouse click add to add a new data source.  You can select the data sources and provide the name and provide the credentials that will be required to access the data in the query.  You can also do the same for other sources and evaluate the data or preview the data from a single source.  You could also go to the SageMaker Unified Studio Query editor  and join data across multiple data sources that are available in your lakehouse. This will result in the analytical outcomes that you want to achieve within a minute without having to replicate the data into your analytical systems.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2900.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2900)

Now that we have set up federated queries, let's see Alex's situation. Alex has recently received an email from his VP of sales,  and the ask is that they want to understand the customer tickets that are raised by their top customers. To do that, Alex needs to join data from Amazon Redshift, which has the customer size information, Amazon RDS, which has all the customer orders, and SAP where all the customer service tickets are located. Alex writes a single query to integrate data across these three different sources using Amazon Athena Federated Query and gets the output within minutes.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2950.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2950)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2970.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2970)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/2990.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=2990)

He can then store the output in Amazon S3 or visualize it using Amazon QuickSight. Just like that, he has the output that the VP wanted.  In this manner, he was able to achieve the ability to say yes to any ad hoc requests rather than having to explain why it will take days or weeks to build a new ETL pipeline and replicate the data. Now let's take a look at what is new with Federated Queries this year.  Earlier this year, we launched support for OAuth authentication mode for Snowflake connections. This mode uses authorization tokens and refreshes the tokens automatically, which makes it perfect for long-running federated queries. 

With federated queries, you can also leverage the multiple performance enhancements that we have introduced to our query engines. This includes enhancing the querying speed for common file formats and generating outputs quickly when the query results are returned.

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3020.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3020)

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3030.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3030)

Specifically, we're generating outputs quickly when query results are more than a hundred megabytes, and we've expanded DDL support for Iceberg tables and S3 tables. Finally, we also launched managed query result storage,  which allows you to store the outputs from your federated queries in AWS managed storage for up to 24 hours at no cost. 

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3050.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3050)

### Combining All Three Approaches: Alex's Transformation and Your Path Forward

Now that we've looked at all three approaches we started with, let's wrap up and understand how these approaches can deliver value. Let's take a look at Alex's situation again. By leveraging these connectivity approaches offered  by AWS strategically, Alex was able to transform his data architecture. Complex custom pipelines have now been replaced with AWS Glue data pipelines that leverage AWS Glue connectors, providing him with control as well as pre-built functionality. Critical operational data now flows automatically using zero-ETL integrations, minimizing Alex's operational overhead and maintenance work.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3100.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3100)

Any ad hoc requests are handled through federated queries, minimizing the need to build new data pipelines. For Alex, this means he now spends much less time dealing with data connectivity issues and focuses his attention on business-critical insights.  Just like Alex, you don't have to choose just one approach. The most effective data strategies combine all three approaches together. When you need complete control over your data pipeline, you can use AWS Glue connectors for self-managed data ingestion. When you want to focus on insights and not infrastructure, you can leverage AWS zero-ETL integrations for a managed data ingestion experience.

[![Thumbnail 3160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3160.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3160)

When speed and flexibility are key, you can leverage federated queries for in-place data analytics. I encourage you all to pick just one use case and apply these strategies. You could start with creating a Glue ETL job using connectors, start with creating your first zero-ETL integration, or write your first federated query. If you want to learn more about any of these topics  that we covered today, you can refer to these resources. The first QR code will direct you to a blog that will help you get started with creating zero-ETL integrations, and the second one will take you to a demo for setting up federated queries in Amazon Athena.

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3190.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3190)

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/8a2d1f67b3bbd08f/3220.jpg)](https://www.youtube.com/watch?v=JsuEPLg50fc&t=3220)

In addition, you can refer to the AWS Developers Guide to learn more about any of the features we discussed today. You could also visit AWS Skill Builder  to leverage the abundant learning resources available there and practice with hands-on exercises. With that, I would like to thank you for being such a great audience. Please take time to share your feedback on the session using your mobile app. Shruti and I will be available right outside this area to answer any questions you may have. Given that some of the other sessions are still ongoing and are silent sessions, I request you to be mindful of them while moving out.  I wish you have a great re:Invent ahead. Thank you.


----

; This article is entirely auto-generated using Amazon Bedrock.
