---
title: 'AWS re:Invent 2025 - Using GenAI to profile, scale, and optimize your multi-tenant SaaS architecture'
published: true
description: 'In this video, AWS Solutions Architects Dave Roberts and Dhammika Sriyananda demonstrate how GenAI can accelerate SaaS maturity by incorporating best-of-breed attributes into control planes. They explore three key areas: cost analysis using AI agents for cost per tenant attribution, margin calculation, and pricing strategy optimization; customer insights through feature usage analysis, persona identification, and automated churn prevention with personalized outreach; and resource efficiency by implementing anomaly detection agents and pattern recognition to solve noisy neighbor problems. The session showcases practical architecture patterns using Amazon Bedrock agents, MCP servers, and supervisor agents to create modular, scalable solutions that deliver quick wins while building toward comprehensive agentic workflows for SaaS operations.'
tags: ''
series: ''
canonical_url: null
id: 3085110
date: '2025-12-05T03:05:21Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Using GenAI to profile, scale, and optimize your multi-tenant SaaS architecture**

> In this video, AWS Solutions Architects Dave Roberts and Dhammika Sriyananda demonstrate how GenAI can accelerate SaaS maturity by incorporating best-of-breed attributes into control planes. They explore three key areas: cost analysis using AI agents for cost per tenant attribution, margin calculation, and pricing strategy optimization; customer insights through feature usage analysis, persona identification, and automated churn prevention with personalized outreach; and resource efficiency by implementing anomaly detection agents and pattern recognition to solve noisy neighbor problems. The session showcases practical architecture patterns using Amazon Bedrock agents, MCP servers, and supervisor agents to create modular, scalable solutions that deliver quick wins while building toward comprehensive agentic workflows for SaaS operations.

{% youtube https://www.youtube.com/watch?v=vcYT6INWpa4 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Using GenAI to Transform Multi-Tenant SaaS Architecture

Hello, everyone. Good afternoon. Thanks for joining us today. Welcome to SAS 305, using GenAI to profile, scale, and optimize your multi-tenant SaaS architecture. In this session today, we're going to look at how you can fast-track your SaaS maturity using GenAI to introduce best-of-breed SaaS attributes into your SaaS product. This is a level 300 session, which means we're not going to be diving into demos and code, but we will be doing a lot of architecture patterns and showing you real-world use cases that you can take away and use yourself.

[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/0.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=0)



My name is Dave Roberts, and with my colleague Dhammika Sriyananda, we're both Solutions Architects here at AWS. We've been working with SaaS for the last couple of years and in the recent year or two with GenAI as well. So what are we going to look at today? Well, first, I'm going to look at what makes best-of-breed SaaS and how you can use GenAI to incorporate some of that into your control plane. Then Dhammika is going to look at cost analysis, things like cost per tenant, pricing, and forecasting. Then I'm going to look into customer behavior, the insights that we can get out of those, customer personas, and packaging. And finally, Dhammika is going to round us out with looking into resource efficiency, so solving the noisy neighbor problem.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/60.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=60)



### The Growth Challenge: Moving Beyond Basic SaaS Operations

Let's start off and look at a very common situation. You've just built your SaaS application. It's awesome, right? You're proud of it. You've put a lot of work into it. And you've got customers using it. To operate and manage that, you've got some business model going on as well. It's been a couple of years and hopefully you're getting some revenue and making some money.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/120.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=120)



Where you want to go from here is to grow your SaaS. So the question is, what do you need to do to grow your SaaS? Well, there are three main areas that we can look at. One, we can get more customers, right? More customers equals more revenue. More revenue equals more profit. Great stuff. The other way is that we can look at getting more revenue per customer, so moving customers onto higher margin packages, which is going to give us more profit. And of course, the other way is to lower our operating costs, so get more resource efficiency into there.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/150.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=150)



### What Sets Best-of-Breed SaaS Vendors Apart

These are the things that we need to do, but how are we going to do it? Well, let's look at best-of-breed SaaS vendors and what they do. When we talk about best-of-breed, we're talking about those SaaS vendors out in the market that you probably know. They are standing above their competitors, right? You can see them and they always seem to be one step ahead and keep pulling forward. So what's their secret? Well, it's not really a secret. It's that they use data.

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/160.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=160)



They know the cost to serve their customers. They've got things like cost per tenant and cost per feature metrics. And they understand what their customers are doing. They've got insights into their customer behavior. Of course, they focus on their resource efficiency because when you first go live with a SaaS product, you're focused more on just getting it going live and getting products out, and you're not really thinking about building the most efficient architecture.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/190.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=190)



[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/200.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=200)



Now, the problem with that is that you get some resource wastage. And as you scale your SaaS, that resource wastage scales as well. Best-of-breed knows this and they focus in on efficiency because those efficiency gains are going to scale as well. And this will give them the edge because they're able to use this data to create packages and pricing that the customers are going to want, right? Because if you're choosing a SaaS product and you've got a couple of competitors and one of them has exactly what you want at a price that fits for you, well, that's the one you're going to choose.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/240.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=240)



Because they've got these insights, because they have this data, they're able to forecast the revenue that's going to be coming in, right? That's pretty useful as a SaaS business. And because they're ultimately more profitable, they have more profit to reinvest in themselves.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/260.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=260)



[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/270.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=270)

### Why SaaS Companies Struggle to Implement Data-Driven Insights



So you get this nice cycle, right? They have more profit to reinvest, they can pull further ahead, and so on and so on. So if it's so simple, why aren't you doing this right now? Well, odds are that you probably have other things to do, right? You've gone live with your SaaS, you're onboarding new customers, you're adding new features in. You started off as an architect, right? Then you turned into DevOps as you've got more and more customers. Now you're just ops. You're not even doing architecture anymore. So nobody's got time to think about implementing these sort of features.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/290.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=290)



[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/310.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=310)

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/330.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=330)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/340.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=340)

So nobody has time to think about implementing these sorts of features.  Engineering will think this is a business problem, right? So until you have business people assessing company scales where you have these business functions pushing engineering to deliver them, you're going to focus on what's important for you. Ultimately, there's a cost to doing this. There's a cost in terms of engineering points and a cost in terms of your time.  So it gets put onto the back burner until you reach a level of SaaS maturity and then start incorporating these features. 

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/350.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=350)

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/360.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=360)

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/370.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=370)

Now, the problem with this is that as you operate your SaaS, you have to make decisions. At the moment, you'd be making decisions without this data.  So you're already behind in the game because your competitors could potentially have these insights. If your competitors understand what your customer base wants, well, you're behind in the game.  And even more dangerous, you may have unprofitable customers. It's a common scenario that I see with SaaS vendors where they've been going for three years, they start looking into their cost per tenant, they find out they've got these customers that are unprofitable, that are hurting their bottom line.  It's not a place that you want to be in.

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/390.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=390)

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/410.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=410)

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/420.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=420)

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/430.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=430)

### GenAI as the Solution: Fast-Tracking SaaS Maturity

Now, lots of doom and gloom, right? But there is a light at the end of the tunnel, and that's why you're here today.  This is what the session is about because GenAI can really help in this regard. All these things, this data that we're talking about, it's all about looking into our data and finding patterns and finding insights. Guess what? This is exactly how LLMs work. They're great for identifying patterns.  Not only that, you can use them to get some value really quickly. You can get an environment going up, look into the data you have, and get some insights in an afternoon.  This is really awesome, right? You don't have to take months to build up this environment. Straight away you've got some value or not, and if you don't find something else. 

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/450.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=450)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/460.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=460)

And you get this lovely natural interface, right? You can use natural language to query for these insights. You don't have to have this intermediate translation from business question into technical requirement.  And it also gives us the opportunity to move beyond just solving these normal use cases and looking at what we can do beyond what's currently now, so things like real-time or predictive analysis. 

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/470.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=470)

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/480.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=480)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/510.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=510)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/520.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=520)

### Building Your First GenAI-Powered Control Plane

So let's look at an example SaaS application, right? You've got your application plane there. This is where all the good stuff's going on, right? The customers are getting their value.  And this is going to scale over time and you're going to need to operate and manage that. So you're going to introduce a control plane. And inside the control plane, we've got a few components.  We've got some data sets. So our application metrics, for example, our usage metrics. We've got a data set which will handle our tenant metadata. And when I talk about tenant metadata, it's information about our tenants, about our customers, like which resources are they allocated to, what sort of entitlements do they have, that sort of thing. And to do all of our heavy lifting, we have some compute logic, right? These are our workflows. So onboard a new customer, this is where it's going to happen. Change in entitlement,  update our resources, et cetera. And to interface with these, we've got this lovely integration layer. And we can see that we have a lot of tools, a lot of resources that an agent  could utilize.

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/530.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=530)

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/550.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=550)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/560.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=560)

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/600.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=600)

So what would this look like as an example? Well, let's say you're inspired. You come and you do the session, you're excited about this and you rush, don't even think about dinner. You just rush back to your hotel room and you want to start playing.  So you get up, you're in a nice career environment. Get onto the AWS lab site and see, hey, we have this CloudWatch MCP server. All my logs are in CloudWatch, so I'm going to use that.  I'm going to go ask some question, right? Do all my tenants have the same usage pattern? That could be interesting. So our little LLM friend  is going to go and use the MCP server and hopefully look in the right place. And I say hopefully, because your SaaS architecture is probably pretty complex and it's hard enough for us to understand, let alone an agent. So we can expose the code base and look into there and find how you've done your architecture, but it doesn't really tell us much about how you operate as a business. So we want to be kind to our future robotic overlords and give it a bit of context. Tell us a bit about how our SaaS application works. Because the better context you're going to provide, the better results that you're going to get. 

So now we add that context, ask that same question, beep beep beep, we get a result. This is really cool.

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/620.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/630.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/640.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=640)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/650.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=650)

We should be able to see some insights that we potentially didn't have before. You're thinking, well, this is great, right? It didn't take me long and I'm getting some value out of this. So where can I go from here? We can go back to AWS Labs and ask, what other MCP servers do we have? There's one for Cost Explorer.  That's interesting. Now I know I have a resource to look into the usage of my customers.  I know how to find the costs for serving that. Let's find out our cost per tenant.  We can say, let's just divide that total cost by the number of active tenants. And just like that, we've got some data and some insights about cost per tenant. 

I talk constantly with customers who have been serving SaaS for years and they don't have an idea about the cost per tenant because it seems so hard. Well, we've done it in less than an hour. Easy, right? The thing is it's not perfect, but now you've got some figure that you can work with. We can look at improving the accuracy because at the moment we're just looking at the number of active tenants that we have. Maybe we have some inactive tenants. That's going to change our cost per tenant values.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/710.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=710)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/720.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=720)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/740.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=740)

We have information on the number of tenants that we have and their subscriptions. It's sitting there in our tenant metadata. We just need a way to expose that so that our agent can use it. We can go and extend our integration layer, add some MCP servers there, and add these into our agent. Now we can get a more accurate cost per tenant.  This is awesome, and all we had to do is expose the control plane a little bit. That's really not so hard because, for example, look at our tenant management. This is the tenant management service inside our control plane.  We can see our CRUD operations there. To expose these as MCP servers, we can use something like Agent Core Gateway. It makes it really easy. We create this tenant management gateway and add some targets through to our API methods that we want to expose. We can repeat this pattern with our other control plane capabilities. 

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/780.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=780)

### Creating a Scalable Architecture Pattern for GenAI Capabilities

Resource management, entitlement management, billingâ€”anything that could be useful for us to get insights into. Once we've done this little proof of concept and we're seeing that we're getting some insights that are valuable for us, either as architects or as a business, then we can decide that we want to keep this and make this a permanent capability inside our control plane. One option is that we could just lift and shift what we have and deploy that. The other is that we could make it a bit better and a bit more accurate. We want to think about some sort of architecture pattern that we can use because we're going to be deploying these things over and over as we get these new insights. 

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/820.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/830.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=830)

It's basically going to look like this. We're going to have the agent itself. We're going to have some deterministic tools because if you want to do X divided by Y, well, create a Lambda. It's easier and cheaper than getting an agent to do it, and it's reliable. We're going to have a dataset or datasets. This is going to be the results from anything that we're deriving in terms of insights and some sort of gateway to expose those for our end users or other agents.  For example, we might have a deterministic tool that says every night I'm going to go off, look through our metrics, aggregate some results, and put them into our dataset. Now I can expose that to our end users with a nice simple web interface or maybe some BI tool like Amazon QuickSight.  Awesome stuff. Now our end users and our business are able to see the insights that we've garnered.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/850.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=850)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/870.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/880.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=880)

We can also expose that to some other agent, and some other agent can then come through the gateway through some MCP server that we've exposed and they can access that data.  We can see as we've created this capability, we're also able to share those insights and have a dataset that another capability could utilize to get even further insights. We can do a similar pattern when we talk about invoking the agent.  We can provide a natural language interface for our business to query, or we could expose it to some other agent. This is awesome. Then our agent can use its own tools or some external tools to go and do its thing. 

We can see by having this nice pattern, it lets us be modular and scale out our insights more and more so we can get more and more information about that. This is a nice pattern. You're creating value and you're creating more value and more value.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/940.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=940)

### Cost Analysis: From Cost Per Tenant to Margin Intelligence

To give some examples of how this would look in another context, I'd like to pass back to my colleague Damika, who's going to look into cost analysis. Thanks, Dave. Everyone, welcome again to the session. Now that you have a better perspective about how GenAI capabilities can be leveraged in SaaS applications, let's take a look at some examples starting from cost analysis. 

In general, cost analysis starts with instrumentation in the application plane to meter the consumption matrix of the tenants so that we can aggregate them in the control plane to get the infrastructure cost per tenant attribution. This helps us understand which tenants are not making much profit in the business so that we could do something about that. Traditionally, this is where we all settled in most of the SaaS applications in terms of cost analysis.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/980.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=980)

GenAI fundamentally changes this equation. While it can provide automation for cost analysis, it can also provide access to advanced cost intelligence much more easily. Before getting into the cost analysis, our story still starts with cost per tenant attribution.  Dave explained how we can get some quick wins by leveraging GenAI tooling in cost per tenant, but here we will see how we can make it a more permanent design in the control plane architecture by using GenAI technologies.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1000.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1000)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1020.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1020)

In the control plane, we already have the consumption matrix and the logs from the tenants. Now, instead of having hardcoded cost allocation rules and bulky microservices, we are going to introduce an AI agent.  That agent can be triggered using a prompt and will orchestrate the entire workflow of cost per tenant attribution by using the reasoning capabilities of a large language model. 

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1030.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1030)

[![Thumbnail 1040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1040.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1040)

During the workflow, it will look at the current usage of the tenants and then aggregate the metrics from that so it will know the list of AWS services that have contributed to the cost.  Then it will work with the Cost Explorer MCP server to get the unit cost of these metrics so it can calculate the cost per tenant accurately. 

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1060.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1060)

When you are building this for your SaaS application, you will feel that we need more tools and more steps in the workflow. That's totally fine. The agent will be able to orchestrate them all for you.  Now when I know the cost per tenant, I can easily get the margin value at the same time, and that's great because now I can have both the cost and the margin in the same dataset and save it in the control plane for future use. This is going to be the base for our cost analysis architecture.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1080.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1080)

### Advanced Cost Intelligence: Insights, Forecasting, and Optimization Recommendations

If you look at where the cost analysis implementation really fits into the control plane architecture, there are multiple ways of doing that. One way to think about it is to extend the SaaS admin panel and introduce a cost analysis dashboard so that we can populate the insights, patterns, and recommendations we are extracting from the cost analysis module there.  This way, the SaaS admins will have access to them.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1120.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1120)

With that setup, we already have the historical cost and margin dataset that came from the previous agentic workflow. One of the low-hanging fruits for us to start with is to look at this dataset and identify the insights and patterns so that as a SaaS provider, I have a better perspective about what happened during the last few months in terms of cost and margin. 

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1130.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1130)

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1160.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1160)

For that, we'll have a cost insight agent with a very simple workflow. It will look at the last twelve months of the cost and margin dataset and then leverage the LLM to ask for insights and patterns.  There's no business logic here, just the power of the LLM. During this analysis, I can ask for things like trends, margin variations, and temporal and behavioral patterns, and this can be entirely customizable because this is what will go into the prompt that we send to the AI agent. 

The results of that would be very data-driven, deep insights that will show me how the margin and cost have been varying over the last twelve months. This is awesome because now I can even understand some of the opportunities where I could have done better to improve my profit margin. Now another feature that we can add to the cost analysis is cost forecasting. The idea is simple: you look at the historical data and then predict the future values.

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1200.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1200)

For that, we're going to have a cost prediction agent.  It will look at pretty much the same dataset as previously, the last twelve months of data for cost and margin, and then do the prediction for the next six months.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1230.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1230)

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1250.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1250)

As a SaaS provider, I really want to have this prediction as accurate as possible so that I can use it with high confidence for my business decision making. To achieve that, I can either use a library or business logic of my own that will give me accurate predictions, or I can deploy this agent in the Amazon Bedrock agent  core runtime and let it know what kind of prediction algorithm and simulation I want to have. The agent will then be able to generate the code using the LLM and run that within the agent code interpreter to provide the prediction dataset.  In that case, I'm not even managing the business logic that will be literally generated by the agent for me.

[![Thumbnail 1270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1270.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1270)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1280.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1280)

There are many options with an agentic setup like that. When you have the predicted dataset, you save it for future use and then leverage the LLM to ask for insights and patterns.  During this predictive analysis, I can ask for things like forecasting trends, growth patterns, and cost and margin variations.  Now I have a better perspective about what's going to happen in the next six months in terms of SaaS economics, which is awesome. I can now get credit for the business in different forms. I only had the cost per tenant value some time ago, and now with the help of a couple of agents, I have the historical patterns and the predictive analysis as well.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1310.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1310)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1330.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1340.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1340)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1350.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1350)

Before moving ahead, we want to improve our architecture because so far  we have been discussing what individual agents can offer to us. If you put them all together into a SaaS architecture, it's important to look at how they work with each other as an extendable design.  We have the insight and prediction agents that we discussed before. One of the changes I would make here is to introduce a supervisor agent that will orchestrate these two agentic workflows and anything that comes up down the line to extract the response for the prompts coming from the front end, in this case the insights dashboard.  As a technical decision, I would probably have the cost per tenant workflow separately so that I can  individually trigger that when I want to get the latest cost and margin values into the control plane.

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1360.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1360)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1380.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1380)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1400.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1400)

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1420.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1420)

With that setup,  it's really interesting to look at another area that is AI recommendation. Can an agent provide me recommendations to optimize resource utilization and maximize my margin, for example? To explore that, we'll introduce a resource optimization agent here,  and I'll provide all the knowledge about the infrastructure of the system and the tenant usage of the infrastructure as a preloaded knowledge for these agents, so it's already configured. When that agentic workflow gets triggered from the supervisor agent,  this agent can leverage its own LLM along with the historical and forecasted cost and margin dataset and ask for recommendations for critical infrastructure changes I need to make to maximize my profit margin.  This is awesome because now there's an agent telling me what things I can consider in terms of infrastructure changes to maximize my profit.

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1460.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1460)

### Pricing Strategy Optimization Through Cost Per Feature Analysis

If your main focus is to improve your profit margin in your business, which is in every SaaS application, this is just one side of the storyâ€”optimizing the infrastructure. The other side would be tier pricing. You have to have the tier pricing optimized as well. But if you look at our architecture, there's one important dimension that is missing to have recommendations for tier pricing.  That is the unit cost for features, which stands for the average infrastructure cost required to host each feature in the architecture. Why is it important? Because most SaaS applications are driven by product features, and we categorize or group these features into different tiers and provide a pricing value. Because of that, the unit cost of the feature will have a direct impact on tier pricing.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1490.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1490)

How can we get it? It's actually simpler  than we think. We already have the consumption matrix in the application plane that has been metered by tenant ID. All we need to do now is identify the features and their workflows along the architectural components and then inject the right feature ID into the consumption matrix based on that.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1530.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1530)

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1550.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1550)

After that, send them onto the control plane where I'm going to aggregate the matrix by the feature ID, calculate the cost, get an average, and then determine the cost per feature dataset. Now I can introduce the new pricing strategy agent that we are going to add to the system.  This agent will provide recommendations to optimize the pricing strategy. I will provide all the knowledge about the current tiering strategy, packaging structure, and cost into this agent. When it gets triggered, it will ask for the recommendation and now it has a rich  set of datasets to draw from for getting those recommendations, such as all the costs, the margin dataset, and the new cost per feature data as well. Because of that, these recommendations are really comprehensive.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1570.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1580.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1580)

The recommendations can come in terms of pricing adjustments where the agent will tell you exactly what prices have to be changed  to maximize profit. It could be packaging recommendations where it will tell you what should be the list of features that each tier should have to maximize the margin.  There are even new tier opportunities where we can introduce a new tier, what the price of that should be, and what list of features it should have. Now this is awesome because we can start optimizing the pricing strategy as well.

[![Thumbnail 1600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1600.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1600)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1620.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1620)

If you look at our cost analysis architecture, on the right-hand side you have a nice  agentic workflow that is growing. We can keep adding new capabilities, with the supervisor agent orchestrating them all and then extracting the response for the prompts coming from the front end, which is the dashboard in this case. An easy feature that we can actually implement here is  the natural language interface. Now SaaS admins are not limited by what has been shown in the dashboard. They can have a conversation or interaction with the agentic workflow and get to know more about the insights and recommendations as well.

When you are building this for your SaaS application, I have two recommendations. Number one, you don't have to start with everything. Start small but have an extendable architecture so that you can keep adding new features and new agents without worrying about how to integrate that to your SaaS architecture. Then work backwards from your business requirements so that you know what you need from the front end, whether it's a dashboard or an AI bot or something else, so that you can prioritize the relevant agentic workflow at the back end.

### Customer Insights: Understanding Personas, Packaging, and Feature Usage

I hope this will help you start off with your cost analysis workflow in your SaaS application. Cost analysis and custom insights take high priority in SaaS applications where generative AI can help a lot as well. To talk about that, let me turn it back to Dave. Awesome, Samaka. So customer insights, knowing what your customers are doing, was one of the big promises of SaaS, right? You were all on-premise and didn't know what your customers were doing. Well, let's operate and manage this in our own environment and we're going to get insights into our customer behavior and understand what they're doing. Awesome, let's do it.

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1720.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1720)

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1730.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1730)

Now the reality is, I see this with a lot of customers, is they're not getting these insights, and this is a bad place to be because customer insights are really important. Why are they so important?  Well, basically, they grow fast.  Because if we know what customers are using, we can spend our engineering points and make our strengths even stronger. I've seen more than once where a customer starts to look into their insights to find out what their customers are using and they find that their customer base is using features which aren't really the core product. So they decide to pivot and focus on those aspects of their product and they become more successful that way.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1760.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1760)

The other side is it lets you sell in the way that customers want to consume,  right? Because if I'm going out there and looking for sales products, I'm going to choose the one that has the package and the pricing that relates to what I want to use. You can use this packaging and pricing to create conversion journeys as well, creating a way to get someone from a basic tier that gets value from your core product into a more premium tier with higher margins. So we can explore customer behavior if we go back into our nice care environment with some context like we had before, or we can ask some basic questions, right? Do we have different customer personas based on our usage?

And our little LLM friend is going to do its thing and give us some data.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1810.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1810)

These insights could be useful. For example, we can see here that we have a bunch of customers  with low usage. That could be interesting for our business because those customers could potentially leave. We don't want that, right? We don't want to lose the revenue. On the other side, we have a bunch of customers with high usage, which is also interesting from a business perspective because these customers could be moved into a higher-paying, more-performing tier.

[![Thumbnail 1850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1850.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1850)

We'd like to get a few more insights into this, but we need to add some more dimensions. We need to look into feature usage, right? What features are the customers using? So we come up with a problem of where do we get that feature usage from?  A common conversation I've had with SaaS vendors is that they're looking into feature usage, but they think that they have to instrument their whole application, and this becomes a roadblock. It's a lot of instrumentation, you never get data out, and when you do, it's incomplete views. But it doesn't have to be so hard.

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1890.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1890)

We're building our applications on the cloud, and you need some sort of way to access your application. This ingress is the perfect place to take feature usage. For example, if I'm using an API, most of our APIs are going to have some opportunities for access logs,  and most likely each feature is going to have their own path. We can use a path per feature as a way to get feature usage dimensions. That's nice and easy, right? We didn't take any time at all, and now we're getting insights into our feature usage.

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1910.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1910)

We can get more granularity this way as well, because our operations within that service feature are probably going to be different based on the HTTP method.  We can use this as another method of getting some more granularity in there, and straightaway, we're getting a lot of insights into feature usage without really having to do much. Now, this doesn't apply to every use case. Maybe your product is taking millions of requests per minute or second, and you don't really want to turn on those access logs because your bill is going to explode.

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1950.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1950)

So then you need to look at actually instrumenting your code.  This is going to give us a bit more complexity because we actually have to go into our application, touch the code, and if you've got multiple feature teams, you have to think about how you're going to standardize across the teams. You want some sort of standardization because standardization scales fast. But the benefit to this is that you're able to get much richer insights because you can instrument for the dimensions and the metrics that are relevant for your business.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/1980.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=1980)

We have to think about that trade-off and balance that higher engineering cost with these deeper insights.  There's a place for both approaches. My general advice is to start off by getting that feature usage with your ingress logs, right? Get some data on the table, stuff that you can work with straightaway. Then when you need deeper insights into particular areas, start instrumenting your application code from there.

[![Thumbnail 2010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2010.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2010)

[![Thumbnail 2020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2020.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2020)

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2030.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2030)

Now we know how to get feature usage. Let's add that as some context for our agent.  Now we can ask some questions, right? What features are my customers using? And it gives us some results. Because it's a natural language interface,  we can follow this up. We can say, well, based on that feature usage, do we see some different personas?  Customer personas are really strong. When we talk about customer personas, what we mean is a group or a subset of our customers that we can market to because they have similar attributes and they use our product in a similar way.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2050.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2050)

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2060.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2060)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2070.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2070)

It would be nice if all of our customers were the same, but just like the real world, customers are different.  We need to find out what sort of different customers we have. Once we know what different customers we have, we can find out their profitability.  It would be really interesting to understand which ones are profitable, more profitable, or unprofitable because we can use that data to set our strategy.  If we're planning our customer acquisition for the next quarter and we're going to be spending this sort of money, well, what personas are we going to target? I'm probably going to choose the ones that are more profitable. But now we have some data to do that.

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2100.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2100)

We can explore these personas by building up these capability sets as well.  I've created this persona analysis capability. I'm getting good at it. I've created ones for usage and cost analysis. The agent's just doing half of this work for me anyway.

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2130.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2130)

When I query that, what's the cost per persona, it can use these other capabilities recursively. It's going to ask what personas we have, what sort of feature usage those personas do, and then we can use that cost per feature to get some insights there. We can ask which ones are profitable and whether we have any unprofitable ones. So we're getting this information that we can use as data for creating more decisions. 

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2150.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2150)

Part of those decisions and their strategy is looking at our packaging. We're creating packages for the personas that we've discovered because you can target your customers better. We can plan these packages to foster an upsell  and to foster a conversion journey. Another common conversation I've had with customers is that they create this awesome SaaS product solving a great use case, but they only have one service tier and it's really expensive. So they can't get new customers on because their market base is a small subset of enterprises that have all that money. You need to think about how you can create a basic value tier that gets customers onto your product, getting value out of that integrated into their business workflows, and then pushing them into more higher value for the customer and higher margin for you as a SaaS vendor in the premium tier.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2200.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2200)

Once you're planning these packages, you can forecast the cost per package as well.  This is going to be really important. If we go back to that customer acquisition example, if I know which personas I'm going to target and I have an idea of what it costs to serve that and some sort of conversion rate success, well, I can forecast what sort of return on investment I'm going to get by spending in customer acquisition. Or maybe you have some RFQs. If you're in the public sector, RFQs are really common. Well, you need to understand how low you can go in terms of your offer and also make an acceptable margin. Now you've got that data, and you can increase your revenue with this approach.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2250.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2250)

There are other ways of increasing revenue per customer.  One way is by reducing churn. When a customer leaves, it's a bad thing because not only are they leaving and taking their revenue stream with them, but they probably still have the core need why they came to you in the first place. So they're probably looking at a competitor. Now that competitor has that revenue stream and they're reinvesting that to get better than you. You don't want to be in that place. This is why Best of Breed vendors have this as a core mechanism to retain customers.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2280.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2280)

### Reducing Churn and Driving Upsell with AI-Powered Customer Success

The other way of increasing revenue is upsell.  We talked about planning this conversion journey and upselling. Well, you need to look and find the customers that have potential for upsell. Best of Breed SaaS vendors know this as well and they build this as core components inside their business models. So if you're in an early stage and don't have big teams that can do this, how can our little agent friends help us here? Well, if we come back to these capabilities that we created, let's look at how we can detect churn.

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2310.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2310)

[![Thumbnail 2320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2320.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2320)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2340.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2340)

I can go and ask to set up some scheduled automated requests to say every night, go and look for low usage customers.  It's going to go check our usage metrics.  If it finds some, it needs to know what to do. Now this is a decision for you as a business because this is ultimately a business process. What do we do with potential churn risk? It could be that you say, well, how important is that customer to my business? Because if they're really important, well, we need to get feet on the ground.  We need to be out shaking hands, finding why they're not happy and making sure we can retain them.

[![Thumbnail 2350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2350.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2350)

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2360.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2370.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2370)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2380.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2380)

But that doesn't always scale. So we end up building in some sort of automated workflow.  We reach out with some templated email. We've seen these in our inbox.  You get this email saying, please stay customer, here's a ten percent discount, whatever. It's very non-personal, but it's got a rate of success. That's why we do it. But we can improve that. We can invoke our usage analysis agent and say, hey, this is the customer that's at risk of leaving. Based on their persona and their usage, what would be a good way to try and retain them?  So we get a personalized response for the customer, and this should yield a better retention rate. 

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2400.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2400)

This is cool, but we can also improve this further. We can turn that customer reach out into a customer success function.  This customer success capability is going to do the same thing. It's going to create that personalized response and reach out to the customer.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2410.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2410)

We're now tracking these customer interactions and seeing which ones are more successful than others. Basically, the system is learning from itself over time.  We can apply a similar approach by looking at new customers. When a new customer comes on board, we want to ensure they get a short time to value and start using your product so that value gets locked in. We can use the same approach and say that if they have low usage and they're a brand new customer, we'll send them something that's more of a quick start rather than trying to retain them.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2450.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2450)

We can use these same tools for upselling. Instead of looking for low usage customers, we're looking for high usage customers  or maybe customers that use specific feature sets. Once we see an opportunity, we follow that same approach, reach out to our customer success capability, and create a personalized approach. Except now we're looking to upgrade the customer instead of retaining them. These capabilities aren't brand new things. I didn't invent customer success. If you've been in the SaaS field for a while, you know that customer success is a standard business unit and a standard capability.

Usage analysis has been done by product owners since product owners started. The difference is that now you're able to implement these with agents without having to build up headcount or physical teams to do this. This is really awesome because it means you can start getting these best-of-breed, mature SaaS attributes straight from day one. GenAI is fundamentally changing not only how we build our SaaS products but also how we operate them. To give another example of this, I'd like to pass back to Damaka, who's going to look at resource management and resource efficiency.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2560.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2560)

### Resource Management: Automating Anomaly Detection and Noisy Neighbor Resolution

Resource management in SaaS is always a balancing act. If you provision too little, it will impact performance. If you provision too much infrastructure, it will kill your profit margins. That's why it's critical but also very challenging to control plane service. This is where GenAI can help a lot to automate, improve operational efficiency, and increase the agility of the overall module. Resource management always starts with resource monitoring. 

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2570.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2570)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2590.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2590)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2600.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2600)

In traditional SaaS applications, we already had the consumption metrics and access logs of the tenants in the control plane.  We then defined the Service Level Objectives, or SLOs, which are the infrastructure level thresholds that tenants are allowed to use at the maximum level. The monitoring microservice constantly looked at tenant usage  and checked if there were any SLO violations. If so, there was a notification to the DevOps team. The DevOps team would come in, identify the issue, identify the fix, and then verify once the fix was done.  That's a whole bunch of manual work.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2610.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2610)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2630.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2630)

If you really want to bring some acceleration and quick wins using GenAI, you can lean into a preloaded AI agent like Kiro CLI, provide the context, let it know how the solution works, and brainstorm about the notification you may have received or anything directly about the tenants.  It will provide some guidance. But if you really want to make this automated as part of the SaaS architecture,  one of the first things you want to do is understand how the SaaS resource monitoring and management workflow would work and how you can automate that.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2650.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2650)

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2660.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2660)

Everything starts with resource monitoring. Why? Because then we can understand the anomalies, which are the suspicious tenant activities.  Ideally there's still a violation, but with GenAI you will see that there is a broader scope than that.  Once we know the anomalies, we need to figure out whether they are actually leading to anything critical or concerning, such as noisy neighbors, resource misuse, or API spikes. For that, with GenAI we can leverage pattern recognition.

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2690.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2690)

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2700.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2700)

Once we know the potential critical condition, we can get focused GenAI components looking at them and identify the corrective actions,  which are the infrastructure level changes that we need to do to mitigate those conditions. Then we can go and fix them, monitor them, and then verify if the condition has been resolved. 

[![Thumbnail 2720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2720.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2720)

When you want to automate this for your SaaS Resource Analysis System (SARS) application, certainly there will be more steps coming up, but I would consider this as the bare minimum that you need to automate. So let's see how this flow can be implemented using GenAI technologies. Now for the anomalies, we are going to implement an anomaly detection agent that can be triggered using a prompt.  It's a simple workflow that will take the current usage of the tenants and their Service Level Objectives (SLOs) and leverage the LLM to ask for any anomalies.

[![Thumbnail 2730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2730.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2730)

In the prompt, you can ask  the agent to cross-check the current usage with the SLOs and also consider additional factors such as business impact, tier configuration of the particular tenant, and the impact to other tenants in the tier. Based on your SARS application, you can keep adding more and more considerations. What happens is the agent now has a better and broader scope to understand or identify anomalies without missing anything. Once we know the anomalies at this stage, we are going to log them into the resource analysis data store and then move on to the next step.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2770.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2770)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2790.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2790)

If you look at these anomalies,  they are filled with data points that justify why the agent thought this would be an anomaly, such as statistical analysis of threshold violations and LLM analysis that also provides details. Once we know these anomalies, the next step is to identify whether they are leading into anything critical.  For that, we're going to add a pattern recognition agent. For the pattern recognition agent, we will preload or provide knowledge to the agent upfront about potential critical conditions that could occur in the SARS architecture, such as how to define a noisy neighbor, how to identify resource misuse, and security issues.

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2830.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2830)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2850.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2850)

With that knowledge already in place, when the agent gets executed within the workflow,  it first takes the anomaly that was identified, then gets the tenant ID, and retrieves all current and historical usage data from the control plane. Then it leverages the LLM to ask for potential critical conditions. Through the prompt, we can  specifically mention conducting usage, temporal, and behavioral pattern recognition of the given anomaly against the historical usage data set. This way, the agent will know how many times this anomaly or something similar has been repeated in the past and what the criticality and importance of that is, so it can map the anomaly into a critical condition.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2880.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2880)

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2890.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2890)

Once we identify any potential patterns like that,  we will lock them into the resource analysis data store and move forward. These potential patterns will clearly mention what condition is concerning.  For example, in this case, tenant 004 has been identified as a potential noisy neighbor. Why? That anomaly has been repeated about 100 plus times during the last few days, with 70 percent of which occurring during the last two hours. The agent has also predicted that 80 percent will likely happen in the next three hours as well. It relates to heavy CPU and resource usage during off-peak time. Certainly someone has to look into this immediately.

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2940.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2940)

However, if you look at our architecture, we are not yet ready to process those critical conditions. We just have a couple of agents, so it needs improvement. The first improvement is we are going to have a supervisor agent for orchestration, and then we will introduce a condition-specific agent layer  where there will be a dedicated agent per critical condition. One for noisy neighbors, one for handling resource misuse, and maybe one for security as well, who will be handling cases like unauthorized cross-tenant data access and so on. The advantage is that now I can provide focused knowledge, tools, and capabilities agent-wise based on their focus area and the condition they are focusing on, so their decision-making capabilities will be much more advanced and accurate.

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/2980.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=2980)

The final enhancement to the architecture at this  stage is if you look at the resource analysis data store, we have been keeping on adding all the anomalies, the patterns, critical conditions, and down the line you will see we'll add all the corrective actions and the actual fixes that we are doing into the same data store. Over time this data store is going to be the single source of truth that represents the entire SARS monitoring and management module of the SARS application. It's a huge amount of knowledge. So what we can do is convert this into a vector store and build a knowledge base out of that

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3010.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3010)

 and then inject that knowledge to each and every specialist agent that I have. Now they are more capable, and the decision-making would be much more accurate with the historical knowledge as well.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3030.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3030)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3050.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3050)

With this architecture, if you have a noisy neighbor  to handle, like in our previous example, the supervised agent just needs to trigger the noisy neighbor agent, which is already configured with the knowledge and the tools. It will refer to the potential pattern and then ask for two things from the SL LLM . Can you confirm whether this is actually a noisy neighbor? The agent will use the existing knowledge to decide whether that is a noisy neighbor or not.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3070.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3070)

If yes, what are the corrective actions we need to do to mitigate the situation? If you look at the output, it will clearly mention the confirmation  and also the corrective actions that we need to do at the infrastructure level. This format will be the same for all the critical conditions that you are going to have in our solution.

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3110.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3110)

This is great, but the big question is, who is going to verify this? Traditionally, we had our DevOps team coming in and doing these infrastructural changes, but now we have an agentic player doing most of the stuff for us. So there are many options. My recommendation is, when you have these kinds of corrective actions or critical conditions found in your SARS application , prompt that to a natural language interface and have a human in the loop.

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3120.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3120)

Ideally, an expert in the house on DevOps who can come in  and review these critical conditions and the corrective actions. Then maybe ask questions from the agentic layer about these options, get to know them more, and verify what needs to be executed out of these, if not all. Then use the existing DevOps and CI/CD tools to do the fixes and let the supervising agent know about it.

[![Thumbnail 3150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3150.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3150)

That way, it can self-generate a prompt that will trigger the existing pipeline , but this time only for this particular tenant and only to verify the fixes for the critical condition. We are reusing the same workflow and same pipeline without needing any additional instrumentation for monitor and verify. We can even have a verification window during which we will run this prompt a few times just to absolutely make sure that the fix worked.

When you want to adapt this to a new SARS application, I highly recommend having the human in the loop option. This will improve the confidence of the infrastructure level changes that you are doing in your production system and also help you understand the overall accuracy of the output of the agentic layer. Any improvements can be identified so you can keep modernizing the agentic layer until you have better confidence about the output. That way, you can even plan to pass or hand off some of the responsibilities to the agentic layer as well.

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3230.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3230)

This will help you to entirely automate your SARS monitoring and management layer by using GenAI technologies . We have talked a lot about GenAI today and how we can bring GenAI capabilities into the SARS control plane. We took a lot of examples in explaining that as well.

[![Thumbnail 3250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3250.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3250)

### Key Takeaways and Call to Action for SaaS Modernization

In terms of summary, the key takeaway is, if you are early into the process, start small. There are enough preloaded AI agents and MCP servers out there that you can use to put together and get something up and running that shows some results real quick in your control plane . Always lead your teams to that to start with. If you are there already, identify a few areas that you can modernize in your control plane and bring the agentic capabilities in.

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3270.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3270)

[![Thumbnail 3280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3280.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3280)

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3290.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3290)

I highly recommend you have a closer look at Amazon Bedrock Agent Core service and its features . It has a lot to offer to you. Feel free to bring the right tools  and bring the right capabilities to the agents so that their decision-making capabilities will be higher. Also, always start with the data. If you do not see the right data set that an agent requires , start from instrumenting the application plane and bring the data set first into the control plane so that you can then talk about the agentic workflow.

[![Thumbnail 3300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3300.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3300)

Finally, take the leadership to provide the guidance required to your team and the organizations to bring the GenAI capabilities .

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3330.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3330)

Now that you know it's not only about automation and operational efficiency, but it can also help maximize the profit margin of your SaaS application as well. We went through these three use cases in depth in terms of design and implementation to showcase what is possible and how it is done.  I highly recommend that you plan to implement them in your SaaS application. In general, my call to action for you all would be to identify the remaining control plane services that you want to modernize and feel free to bring the agents and GenAI capabilities into the implementation as well.

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/6f5d71b40835fd2a/3370.jpg)](https://www.youtube.com/watch?v=vcYT6INWpa4&t=3370)

So this is the kind of content that we want to share with you all. I hope this is what you expected from the session, and I believe now you have a better perspective and better confidence about bringing GenAI and the agents and agentic capabilities into the control plane. Before wrapping up, I have a quick couple of things to mention. These are some of the SaaS sessions  that our team is hosting during the next few days at the reunion as well. If you're interested, please take note of them. There are some workshops and builder sessions that will be very hands-on as well.

Finally, there should be a survey link for this particular session in your mobile app. Please let us know your feedback. It's super important. We'll be looking at them with high interest to learn from you and also to shape our future re:Invent sessions as well. A huge thank you to all of you for being here this evening. Thanks, Dave for cooperating with me as well. I wish you all have an awesome remaining part of the reunion. Have a wonderful evening.


----

; This article is entirely auto-generated using Amazon Bedrock.
