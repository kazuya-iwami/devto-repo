---
title: 'AWS re:Invent 2025 - Build generative and agentic AI applications on-premises & at the edge (HMC308)'
published: true
description: 'In this video, AWS experts Pranav, Fernando, and Chris present strategies for deploying generative AI and agentic AI at the edge to address data residency, corporate policy, and low latency requirements. They demonstrate the AWS AI continuum spanning Regions, Local Zones, AWS AI Factories, and Outposts, emphasizing model optimization using llama.cpp framework achieving 40-50 tokens per second. The session covers practical implementations including RAG architecture with re-ranking for 90-95% accuracy, prompt engineering techniques, and small language model deployment on g4dn.xlarge instances. Live demos showcase chatbot applications, Chef Antoine recipe generation, and agentic workflows using Bedrock Agents and Llama 3.2 models, proving edge-based AI feasibility with significantly reduced TCO compared to LLM alternatives.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/0.jpg'
series: ''
canonical_url: null
id: 3093149
date: '2025-12-08T20:45:17Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Build generative and agentic AI applications on-premises & at the edge (HMC308)**

> In this video, AWS experts Pranav, Fernando, and Chris present strategies for deploying generative AI and agentic AI at the edge to address data residency, corporate policy, and low latency requirements. They demonstrate the AWS AI continuum spanning Regions, Local Zones, AWS AI Factories, and Outposts, emphasizing model optimization using llama.cpp framework achieving 40-50 tokens per second. The session covers practical implementations including RAG architecture with re-ranking for 90-95% accuracy, prompt engineering techniques, and small language model deployment on g4dn.xlarge instances. Live demos showcase chatbot applications, Chef Antoine recipe generation, and agentic workflows using Bedrock Agents and Llama 3.2 models, proving edge-based AI feasibility with significantly reduced TCO compared to LLM alternatives.

{% youtube https://www.youtube.com/watch?v=MW7Afh17dFo %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/0.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=0)

### Introduction: Bringing Generative AI and Agentic AI to the Edge

 Good afternoon everyone. Welcome to re:Invent 2025. Today we have an exciting presentation for you on the evolution of generative AI and agentic AI at the edge. As many of you have heard, AWS is making a lot of advancements when it comes to running your AI workloads in regions. However, there are a number of customers who cannot deploy their workloads in regions either because of data residency, their compliance needs, or low latency requirements. That's why we're here to talk about how you can harness the power of agentic AI and generative AI and drive innovation at the edge.

I'm Pranav, and I'm excited to be here with Fernando and Chris. Over the past few years, we've spent a lot of time helping our customers design and architect their application workloads outside of regions. This hands-on experience has really provided us unique insights into challenges and opportunities in this area. In this session we'll share our practical guidance on how you should think about deploying your edge-based AI workloads. I just want to clarify our intention is not to provide one size fits all solution. We want to provide you the right framework, the right optionality for you to best architect your applications and your workloads.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/90.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=90)

So let's get started. We have an exciting agenda lined up for you today. First, we're going to start by unpacking what we mean by generative AI and agentic AI. Next, we'll talk about some of these use cases that are already driving tangible  business value for our customers. Then building on that foundation, we'll talk about how you can optimize pre-trained models to run efficiently at the edge. Optimization is only half the battle, so we'll also talk about how you can enhance the accuracy and improve the performance of these models as well. Finally, we'll dive deep into how you can deploy agentic AI workloads to run efficiently at the edge as well. By the end of this session, you'll walk away with a comprehensive understanding of how you should deploy your edge workloads to drive innovation with AI and agentic AI.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/130.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=130)

### The Evolution from POCs to Production: Understanding Generative AI Agents and Agentic AI Systems

 Before we go through some of the use cases and various AWS offerings, I wanted to walk you through some of the journey that we're seeing with our customers who are using generative AI at AWS. Overall, I'm super impressed by how far we have come along, especially in such a short amount of time. If you look at year 2023, this was the year of POCs. Customers were still doing testing, they were trying to figure out what they can do with generative AI.

Then in 2024, we saw a big shift. This is where customers started focusing on prioritizing real world use cases, focusing on the business outcomes they can drive. Here we are in 2025, where customers are really focused on driving the business value while still caring about the things they should care about, including security, privacy, and just doing it in the right way. A big shift we have seen in the last couple of years is also customers thinking about more and more AI workloads to be deployed on-premises or at the edge. The journey doesn't really end here.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/190.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=190)

 This is where we're seeing a next evolution of agentic AI systems, and that's where things get really interesting. Let me just talk about what we mean by generative AI agents before we jump into agentic AI systems. Generative AI agents are much more goal oriented. They can handle a much more complex range of tasks compared to generative AI systems. They can really automate your entire workflows without a lot of human intervention. At the same time, they can evolve to changing ecosystems.

On the other hand, agentic AI systems are fully autonomous. This is where agents can interact with each other, they can make really complex decisions and really mimic human-like reasoning capabilities. When you think about deploying your AI workloads, it's really important to also think about how you should leverage the power of agentic AI systems to improve your business processes. As I mentioned, in this session we'll cover both generative AI and agentic AI systems.

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/250.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=250)

### The AI Continuum: AWS Offerings from Regions to Edge, Including AWS AI Factories

 Before we dive deeper into use cases, I wanted to walk you through the AI continuum and various AWS offerings that exist in this space. When people think about deploying their AI workloads, they tend to think about two options. It's either regions or it's on-premises. The reality is it's a much more nuanced conversation. In fact, we're seeing a lot of enterprises asking for a consistent experience when it comes to on-premises, regions, and even edge deployments.

So how are we meeting the requirements of these customers who have varying deployment needs? We're basically doing that by bringing AWS wherever our customers need them. On the left side you see AWS Regions, which are available in 38 locations. They continue to provide you access to the latest and greatest in NVIDIA GPUs, our own Amazon accelerated chips including Trainium and Inferentia.

These regions also provide high-performance storage and EC2 Ultra Clusters that deliver better petabit-scale networking. In these regions, you also get access to AI services like Bedrock and SageMaker. When we shift towards the right, this is where we have AWS Local Zones, which are available in select locations like Atlanta, Dallas, and Phoenix. These Local Zones provide you a similar experience to some of these regions with high concentrations of NVIDIA GPUs, our own Amazon accelerated chips, as well as EC2 Ultra Clusters and high-performance storage.

Two days back, we also announced something called AWS AI Factories, which provide you a very similar experience in some of these regions and these select Local Zones, but in your own data centers and your own on-premises facilities. We'll talk a bit about what we mean by AWS AI Factories in the next slide as well. But if you look at all three offerings I discussedâ€”regions, these select Local Zones, and AWS AI Factoriesâ€”they provide you a consistent experience that allows you to deploy any kind of AI workloads, whether it's inference, fine-tuning, or even training that requires a lot of GPUs in a single location.

On the right side is what we call the edge. This is where we have other Local Zones, which are distributed across multiple locations globally. These Local Zones provide you access to compute and storage without the need to run your own data centers. We also have AWS Outposts and Dedicated Local Zones, which provide you a similar experience but in your own on-premises facilities. And then we have AWS IoT offerings, which are meant for the far edge. When you think about all of these edge deployments, they're best suited for inference and fine-tuning workloads.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/430.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=430)

Overall, across all of this AI continuum, one thing which we made sure is a consistent experience when it comes to services, APIs, tooling, as well as infrastructure. So now that you've understood the AI continuum, let's quickly talk about what these AWS AI Factories are  before we jump into the use cases and some of the business drivers. These AWS AI Factories are dedicated deployments that combine the latest AI chips alongside EC2 Ultra Clusters as well as high-performance storage like FSX and S3 Express One Zone.

Overall, these AWS AI Factories allow customers to rapidly deploy their AI-powered applications while still meeting digital sovereignty requirements. We deploy these AWS AI Factories in customers' data centers, leveraging their own power. At the same time, customers also have an option to use their own GPUs and their own network connectivity, which basically allows customers to make use of any investments they made in this space and really helps us meet customers wherever they are in their AI journey.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/520.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=520)

Another key part about these AWS AI Factories is the access to AI services like Bedrock and SageMaker. With these services and AWS AI Factories, customers can gain access to foundation models without the need to negotiate contracts with different model providers. At the same time, these AWS AI Factories are connected back to the region, which means that customers would get the same security, the same privacy, and the same tooling that they use on AWS Cloud. And as I mentioned, the AWS AI continuum provides customers a consistent experience  all the way from the regions to the edge.

### Business Drivers and Use Cases: Why Customers Deploy AI Workloads at the Edge

When it comes to the edge, there are three key business drivers that we see across all our customers. The first one is around data residency, where customers with certain regulated workloads cannot deploy these workloads in regions. It's either because of country-specific residency requirements or because of country-specific sovereignty requirements. Overall, there are limitations on moving and processing data outside of certain geopolitical boundaries or geographical boundaries.

The second business driver is around corporate policy, where certain corporates or enterprises just cannot deploy or run their data, especially model weights, outside of their own sites and their own on-premises facilities. The third driver is around low latency. This is where certain customers with certain workloads just cannot tolerate the latency they get by deploying these workloads in distant regions. Overall, all these three key factorsâ€”including data residency, corporate policy, and low latencyâ€”really form the foundational business drivers behind why more and more customers are thinking about deploying their AI workloads at the edge or on-premises.

[![Thumbnail 600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/600.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=600)

Across all these three business drivers that I discussed,  customers across a variety of industries are using these edge deployments for a variety of AI use cases.

We've observed that from an architecture perspective, they tend to fall into three big categories. The first one is around enhancing customer experience. A great example is a chatbot, where customers are using generative AI from all the way up to the general purpose models to industry-specific models. We've also seen another great example of call center support, where call centers are looking to use generative AI to transcribe audio calls into text and really analyze those for insights. We are also seeing a lot of customers looking at generative AI to do sentiment analysis and really understand interactions with their own customers.

The second category is around driving productivity and creativity. This is where a great example is what we are doing internally at Amazon as well. After every call, after every meeting, we transcribe voice to text and summarize the key decisions, key action items, and the next steps we discussed in a specific meeting. Another great example of a use case is content creation, where we are seeing strong uptake. Customers are looking to use generative AI for the first draft of their marketing collaterals, their sales assets, and even other technical assets as well.

The third category is around optimizing business processes. Recently, a bank reached out to us and they were looking to leverage generative AI to slash the cost and time it takes for them to generate monthly critical business reports. That's another great use case where we're seeing strong uptake from customers. In fact, the use cases that you see in yellow are the use cases where customers have already moved from POCs to production, while the other ones highlighted in the slide are the use cases where we're still working with customers on testing and POC.

Now that you've understood the use cases and the business drivers, I'll invite Chris on the stage, who'll talk about how you can optimize pre-trained models to run efficiently at the edge. He'll also talk about how you can improve the model accuracy and drive performance with these models as well. Then Fernando will talk about and run you through the demos and really bring all of this to life. One thing before Chris starts, I want to clarify is that after the end of the session, we'll make sure you have access to the codes of our demos as well so you can try it yourself and just relive the excitement here. Thanks.

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/780.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=780)

### Model Selection and Optimization: Choosing the Right Small Language Models for Edge Deployment

 Good afternoon. We'd like to share with you our journey for the last year, where we've been working with customers to deploy edge use cases for generative AI, and we've jointly learned a lot of lessons as we went along. The format we're trying to show is explain the technology, show the architecture, and then show a demo of the system so you can actually see it working in real life. As my colleague mentioned, at the end, we will be around for questions if there's other stuff you want to talk about.

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/800.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=800)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/810.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=810)

 So the first thing to look at is the whole area. The business drivers, as we talked about, are self-managed SLMs for customers. For various reasons, they have to manage  the generative AI application on-premises for data residency, transparency, and so forth. What we've set out to do is create a cookbook where you can actually leverage this system, not to run a specific model, but actually the cookbook allows you to choose relatively any model on Hugging Face and actually apply this methodology and architecture to it.

We went with the llama.cpp framework because it gives us a lot of controls on how to run the models. But more importantly, it allows us to compress the model and get it into a small area, because running at the edge, power, cooling, and GPUs are all commodities that are scarce and hard to get hold of. In the work we've been doing for the last year, we've improved the performance of the models using this framework by about five times. We started off with a few tokens per second, and we're now up to 40 to 50 tokens per second for exactly the same SLM running on exactly the same GPU.

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/890.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=890)

 So the first thing to think about when you're entering this engagement is the model selection. How do you actually pick a model? What we found, looking at the left of the slide, is each model is strong at different things, especially if you go down to small language models, where we define the small language model as less than 10 billion parameters.

We've actually found that certain models like Mistral and Phi are really good at mathematics. Qwen is very good at text generation and also very good at specific languages and specific domains. From a deployment strategy, the first thing to decide is how you're actually going to deploy the model. Do you want to go for an LLM strategy where you have an enterprise LLM running on hundreds of GPUs, or do you go with the industry trend of having small language models that work on a particular task or a particular system?

What we've learned over the last years is that it's really important to understand what you want the model to do, not just at the high level, but in detail. You need to have really good evaluation criteria, and the use case needs to be defined to a fairly low level. Then you've got choices to make. The choice is how many tokens per second do you need. You can have a very small language model that has a huge number of tokens per second, but its accuracy is not that great. Or you want to balance the context size that you're feeding into the model, because what we found is larger context sizes seem to slow down the models, or quantization of the model to squeeze it into a smaller footprint.

We have customers that are experimenting on voice to text, for instance, on CPUs, not even GPUs. We always recommend as a best practice to start with a small model, try it, see if it works, and then go to the next size up, the next size up, the next size up. What we found is a lot of customers start with an LLM, it works fine, and they don't actually see if they could use a smaller model to actually get the same accuracy and performance.

The last thing to do is to look at the sustainability and TCO. I had one customer last year that basically had a manual process. It's a financial services bank. They had a financial report they have to run every month that they use humans to pull together because there's no shared index on the databases. It's basically a hard report to generate. They actually started off and went for a large language model, and the cost to do it manually was roughly $60,000. When they went to the LLM, it worked fine. They got about 80% accuracy with not a lot of work, and they were quite pleased with that. Then they discovered it was $120,000 to actually pay for the LLM running the tokens.

We got involved, and then they deployed a small language model and went through the process we'll talk about in a second. They got the whole thing running for about $7,000 or $8,000. So they actually had a good TCO, and that's what we're trying to say here. It's really important to understand the whole ecosystem you're working with and not just go for the biggest model you can find that does the job.

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1090.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1090)

### Architecture and Demo: Running Self-Managed SLMs at the Edge with llama.cpp

 This is the architecture we'll be showing you. We've got three levels of this architecture. This is the one we're starting with at the moment. Starting on the right of the diagram, we have the users where they present a prompt to the system. We have an application load balancer that load balances across a number of application servers. The application servers themselves are actually doing prompt engineering work, so they're taking the prompt, enriching it with a prompt template. If it's a real-time application such as a chatbot, it goes through an ALB into the models.

We also have text summarization and text generation use cases that use the same architecture framework, just a different context and different training on the LLM, and that goes through a queuing mechanism into the system. The architecture is basically designed to support a range of different use cases and give you a scalable system from one user up to hundreds of thousands of users.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1160.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1160)

 The other thing to say is we had a lot of discussions with customers where they wanted to go for a really large GPU, and then they got into the issue of having water cooling. This was a test result we did with one legal firm where they had ingested all the legal text. I'm not a lawyer, so I don't know exactly what they were doing, but they had all the texts in there, and they were actually using chatbots to advise the solicitors in the company and respond back.

The white line here shows the required performance, which they defined as six tokens per second as the requirement. You can see,

starting with one user and running up to 48 simultaneous users, all the models we're testing here are just a sample set of the models we test. We test different models all the time. Then you can see around 48 simultaneous users, it starts dropping off in terms of tokens per second, whereas some models were able to keep performing up to 64. The model they picked for their evaluation topped out at about 50. So for a scaling system where they had up to 10,000 people using the model, they worked out that they had 1% or less than 1% simultaneous users, so they basically just were able to multiply the number of users by 50 to give the GPU sizing.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1260.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1260)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1280.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1290.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1290)

I'll now hand over to my colleague Fernando to do a demo. Thank you, Chris. So  now we have this Python application that is used to demonstrate local inference at the edge. This is a naive Python application. I'm not using any framework to develop it. And here you can see that we are using a Llama 3.2 model.  It's a 3.84 billion parameter model. It's a very tiny model for this kind of application.  We have a system prompt here, and one topic that's very important to treat is related to prompt engineering, so we take care about some use cases here. And also we have the temperature and Top P just to demonstrate it's running at the edge.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1310.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1310)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1350.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1350)

 Talking about the instance that we have behind this application, we can see that we have a g4dn.xlarge, so it's a small GPU. We use one GPU Tesla T4 per instance, and we also have the application and the vector database running in-situ. These instances can be deployed in a Local Zone and also in Outposts. We are not using anything specifically from the region. If I ask a question here,  like what is Service Link, you can see the answer. That's very fast because we are working with a very tiny model. And here we have some metrics about the input tokens, output tokens, latency, and tokens per second.

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1380.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1380)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1410.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1410)

Another interesting part that we can do is, this is where I'm running Llama server. Llama CPP is the Llama server  that we are using to run the local inference in that case, and from this side I have the chatbot. On the top you can see the memory, the GPU memory in use, and the utilization. So if I ask the same question, what is Service Link, you can see the utilization.  They start consuming and after that stopped. So it's a very fast model for this use case, and as Chris mentioned, it's the best practice to start using a small model in that case and start testing until you find the right model that's important for your use case.

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1440.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1440)

So another topic that's important is related to the temperature. One thing that we observed here is  when we work with big models and we would like to have more creativity, you can increase the temperature. So we start with 0.7, 0.8, sometimes 0.9. When we work with small models, we need to take care because they can start hallucinating very easily. So it's a parameter that we need to take care of when you define the right value. That's the way that you need to have the evaluation process inside to detect and to find the right value for this kind of parameter.

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1490.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1490)

So the architecture here, we have Milvus  running as a vector database. We can also run it using RDS Postgres and pgvector. You can enable this extension if you wish, or different vector databases that you can run.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1520.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1520)

We are using rerank, another instance that we explain later in this presentation. The SLM that we use to run is Llama Server in the 54 Instruct. And  the vector embeddings, it's another module that we use that's part of the RAG that we will explain in a few minutes. So now, please go ahead with that.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1540.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1540)

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1570.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1570)

### Enhancing Model Accuracy: Prompt Engineering, RAG Architecture, and Re-ranking

Thank you, Fernando. So what we should hopefully see here is a framework  for running efficiently the models. And the first part of our exercise we started off probably two years ago was how to get models running really efficiently at the edge with a low cost of ownership. What we found this year is customers have come to us and said, well, the 70% accuracy was great last year, but I want 95% this year. And the next set of slides we go through is how do we actually take a generic model and get it ready to actually provide really high levels of accuracy  in terms of the operation.

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1580.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1580)

So we strongly recommend that for all deployments, particularly at the edge,  you should strongly consider prompt engineering, using a RAG database, and also optionally fine tuning, because some models don't actually need fine tuning for a particular task. Sometimes they do. But what we also say is basically any retraining of the model, where you're actually doing open heart surgery on the model and reworking the complete thing, should be done in the region. And we advise our customers on creating synthetic data where they're not exporting their infosec information or rejecting the data to train the models in the region, and that's a strategy for regulated markets that works quite well.

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1620.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1620)

 So starting off with prompt engineering, as I said, Fernando just showed you a snippet at the moment, but it's really important. And it's also described as a dark art, where you actually need to go through and tell the model what you want the model to do. That's the art of prompt engineering. You need to give instructions to say respond with 10 words or respond with one word. You can control the model in great detail. You need to give it a context.

And one of the funny things we found recently in terms of context is we have an AWS product called AWS Wavelength, and we asked the question, what is Wavelength? And the model came back and gave us the physics definition of the speed of light. It had no context that we were actually asking about an AWS-based product, and that's where context is so important. You need to really guide the model into the behavior you want. The user input is really important in terms of who the user is, what they are, are they non-technical, technical, etc. And also you can add a security component to say what they can actually access and not access.

And then the output is the tone and style. I want an architect response, or I want an engineering response, or I need a lawyer's type response, or in our demo you'll see later, a chef's response. All that can be controlled by the prompt engineering side. And you can also have must-haves in there. You can say if you don't know the answer, don't make it up, or all this sort of thing.

And the other thing we do with our customers is fine tuning, where we recommend parameter efficient fine tuning as the method, where basically you take the model, freeze most of the layers of the model, and then work on a few layers to change the weights, the weights being the data effectively in the model, and actually equip the model to be able to respond, say, in a chatbot environment or a summarization environment, etc.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1750.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1750)

So we're going to show you the architecture  now of all of this working. So you've had a quick snippet of the context engineering already, but we have two slides here that show the RAG architecture. On the right side of the slides, we have a number of data sources, everything from a file repository with a PDF in it or Word documents, databases. A lot of our financial services customers are actually using this to access data around customer accounts, information, etc.

We also can reach out to the internet, you'll see in the later demo, and also the external data sources like data lakes, etc., and pull that in. The ingestion process detects that there's new, is an application running on an EC2 instance, detects that there's a new file or a new set of data, pulls that in. If it's a PDF, it also does the chunking strategy for the PDFs.

Because you don't want to put in a massive file as one chunk, you break it into maybe four chunks per page. That's then fed in the right format into the embedding model, and the embedding model basically converts it to a vector alongside the chunk, and they're both stored in the vector database. We're showing PG vector here on RDS Postgres, but we also have a demo using others. The architecture is not fixed on any particular type of knowledge base.

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/1840.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=1840)

So then, once you have the system up and running with the RAG database,  consumers come in and they issue a question or a prompt. That hits our application that does the prompt engineering. Then it sends the prompt to an embedding SLM that basically creates a vector. That vector then is used to do a top K search. In our case, I think it's 25 that we've got as the output, so it gives the nearest 25 chunks that it has in the database to the query.

One of the things we found with that was a lot of the SLMs were getting confused because they were being hit with a lot of data, and also some of the top chunks may have a similar vector, but they may be incorrect data. Both of those things were confusing the model because it would have to go through all 25 chunks, work out when it's doing its inference which were useful and which weren't. That caused a lot of inaccuracy.

So we introduced this idea of a re-ranker, where we take the top K output and feed it through another SLM. That SLM goes through and does two important things. One is it detects bias in the responses from the knowledge base. It also can put any guard rails you have in the system, so you may not want to expose this type of data or something like that. You can set all that in the re-ranker. More importantly, it goes through and filters the list that you have from the knowledge base and says, I only want to send these two fragments to the SLM that's doing the inference, because the rest of them are not that relevant.

It has a really big impact on the performance of the SLM because it's not going through 25 chunks now, it's just looking at two, and it's not having to work out if some of the chunks are misleading it in some way or other. Then the application packages up the prompt, the prompt template, and the top K chunks returned from the re-ranker, and then we go through the process of the inference and return the answer.

With this architecture, I would say we are getting really good results on small models like certainly 8 billion models down to 3 billion models. They're actually performing at 90 to 95% accuracy all the time with very little hallucinations. Our customers are actually really surprised and happy, and we get into lots of discussions with data science teams about how we're managing this with this architecture.

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2000.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2000)

So just to talk about the re-ranking a bit, because it's one of the new things we've done. Normally in a semantic search, you basically  have the application call the semantic search and the knowledge base goes through and selects the vectors as close as possible to the incoming prompt, and then that's returned to the application. As you can see here, if I'm using five chunks in this example, we have the five chunks that are added to the prompt, and then the five chunks plus the prompt go to the SLM.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2040.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2040)

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2050.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2050)

What that means is the SLM has to process a large context size because all five chunks are in there, and more importantly, some of these chunks may be misleading, so the SLM has to go and think about it more than it would normally. By doing the re-ranker,  we actually feed it through another system and then we can filter the list down to in this case one response that's really relevant.  That means that the context going to the SLM is really small and it's really accurate in terms of how it augments the prompt.

We found that this has a really beneficial setup, and you may ask why not do all the search in one go on the knowledge base. But what we found by doing it in two steps, we could actually do bias detection and have guard rails on it, but also it was faster than doing a really complex lookup in the knowledge base in one go. Now I'll hand back for demos.

[![Thumbnail 2090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2090.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2090)

### Comparing RAG vs. Non-RAG: Live Demonstration of Improved Accuracy with Service Link Query

Thank you, Chris. So we also have a compare mode here to  show the use of RAG and without RAG running at the edge. This application is running exactly the architecture that Chris presented in the last slide.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2110.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2110)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2160.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2160)

Just to remember, we have two instances. One that we use to run the inference with the context of RAG, and another one that we are using without  context. If I ask a question here, on one side I expect on the left side we can see inference running without RAG. On the right side we have with RAG. So I can ask the question, what is Service Link? In this RAG environment, we ingested AWS official documentation about Local Zones and Outposts and something about Bedrock. So when I'm asking about Service Link, on the system prompt we worked to define the chatbot so they can have the responsibility to use this information to answer the  question.

[![Thumbnail 2170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2170.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2170)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2200.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2200)

So we can see here, what is Service Link? From this side, we can see Service Link is an AWS Outposts facility  for customer VPC traffic between the Outposts and the associated region. If you see the other side, we can see the model hallucinating because they are using information inside this small model. They are not using the accurate answer or accurate information. From this side, we also have the source used, and here we can see the chunks that we are using. The distance on the RAG, the  naive RAG in that case, you can see this chunk, the distance, the similarity in that case is 19%. After the rerank, the rerank reclassifies this chunk to 99%, which makes sense. The same for this one. So now instead of 26%, now we have 92%. Instead of 27%, we have 90%. So the rerank is running here to reduce hallucination, to reduce the amount of noise that we send to the model.

If we send to a small model, when we work with big models, the big models can handle complexity. So they can understand what's noisy, what's not, and they can provide an accurate answer. When you work with the small models, you need to take care because it's not a thing that the small model can do, and they can start hallucinating very easily. We tested with different data sets in that case, and we could observe exactly this behavior.

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2280.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2280)

Okay, Chris. Yep. So just to finish off that section then, what we're saying is  we've gone through this process multiple times with customers. First, it's really worthwhile spending time on the evaluation criteria and creating a good quality data set that we can use for fine tuning, but also to check the model and make sure the model is behaving correctly. Prompt engineering, as we said, is really important, and it's the context for how the model is supposed to behave. It's your guide to the model, and you can also, in the demo we have, you can also see the instructions say don't make stuff up, don't, if you don't know the answer, respond I don't know the answer, et cetera.

And then using RAG is really good for getting data, fresh data into the model where you don't have to retrain all the time, but it also helps the accuracy of the model, especially in the small language models that we're deploying at the edge. And then we go through an optimization process and the testing process. That's the blueprint we've successfully rolled out into a number of customers now, where we're doing everything from specific Q&A chatbots to summarization and also compliance checking and things like that. So that was, if you like, the generative AI part of the puzzle.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2360.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2370.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2370)

### Agentic AI at the Edge: Architecture, Components, and When to Use Autonomous AI Systems

The next thing is we're moving over to talk about agentic AI, and again,  we have customers that want to run agentic AI, but running it at the edge is a challenge. Firstly, just to make sure we're all  on the same page, we've defined agentic AI as an autonomous artificial intelligence system that can independently act to achieve predetermined goals without constant human oversight. I like to think of it as basically it's a way of having a system work out what you want and then go and get the data for you.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2400.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2400)

In terms of the key elements and the architecture, AI agents are independent. They keep running  all the time, and they're semi-autonomous. You can have human in the loop checkpoints.

But they can run on their own. They're goal oriented. They use Natural Language Processing to actually have a sentence coming in from a human into the agent, and the agent then breaks that down, uses contextual reasoning, and creates a workflow for how to respond to the customer's response. More importantly with the agent, which is different to Generative AI, it's continually learning as you go along.

One thing to consider as an architect is agents are token hungry. So you only want to use an agent where you really have to. You don't want to have agents everywhere because they are very expensive to run in terms of tokens per second. The main benefits really are adaptability, autonomy, and faster decisions in processes that may have difficult workflows.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2470.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2470)

Just trying to work out some guidance as to what use cases agents are good for, because we're not saying agents are good for everything.  Agents are good for tasks where customers are coming to them and asking them questions that they haven't seen before. They're good for creating workflows that are dynamic or created on the fly, and they basically work towards a task list and they interact with other tools and other agents, so they can mesh with different tools. Our demo will show that where we have an agent calling a Generative AI system to create some text, but also to create an image.

You should also consider where the agents are less relevant. In a structured environment where there's little change, an agent is overkill. Or if the workflow is always the same, it's always the same pattern you go through, you don't need an agent for doing that. You can equip a normal Generative AI application to do the same thing or just a standard ML model. The key thing here is, if it's limited changes or you're cost sensitive, agents may not be the best starting point for your deployment.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2540.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2540)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2550.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2550)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2560.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2560)

 One thing to bear in mind is hallucinations in Generative AI, hallucinations are bad. In an agent, if it goes off the rails, it can be really bad.  So you always want to try and constrain the way the system is working and also how it behaves.  One of the things we recommend for agents at the edge, because it's a constrained computer environment, is running small language models as the inference engine.

They're really good in terms of they can be specialists, so they can be a specialist SLM working in an agent. You can tailor it for each set of domains or operations, and they're very customizable, which is good. More importantly, you can control all the aspects of the agent in terms of how it behaves, what data it can see. You can basically have a low infrastructure cost because the cost per token running an SLM is much, much lower, like two or three percent of the cost of an LLM. You don't need the same number of GPUs at the edge to run it. It's also highly performant, especially in terms of low latency. Some of the models are actually really quick at coming back with an answer, which is important for the agent.

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2620.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2620)

We've actually built the demo  we're showing you here using the AWS Bedrock Agents, and we've done that because it was pretty easy to use. It's actually a really robust set of capabilities. It's very extensible, and we've extended it into the edge, and it allows you to rapidly create an agent and deploy it. That's just some background as to how we built the agent, I'm not going into Bedrock Agents in detail.

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2650.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2650)

From an architectural point of view, we have the same setup again where we  have an application and the application's doing the prompt engineering, checking the user. That's sent forward to the agent, and the agent, I'll break down what's inside the agent in the next slide. We've also put in a semantic cache, because what we're seeing is a lot of times with an agent, the query or the question is similar to the one that's already been received. So you don't need to go through the whole workflow. You can just pull something from a semantic cache, and that gives quite a good saving in terms of costs.

Then we can have human in the loop step where a human needs to check off or click on something. And then we have a set of tools, and the key difference with an agentic architecture is the tools are recursively called. The agent can call the tools over and over again if it needs to, whereas in a Generative AI RAG system, the RAG database that I'm showing here will be called where the prompt is received, but it's not called again.

Whereas an agent can actually pull data from the database over and over again. If it doesn't like the answer it's received from the RAG database, it can go back in again. It can also call other agents to do specialist tasks or tasks that it's not equipped for, say maybe a math agent or a graphics agent, and then it can call tools to do simple tasks like query a database or write some SQL query to get the data from the database, and it can do that over and over again.

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2750.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2750)

Looking inside the components of the RAG database, we have the first step which is to perceive or to understand the questions that are coming in.  The model needs to break down the text and say, if I'm asking for something, what does that mean? Then it goes through a reasoning stage where basically people go through outside the workflow, working with the SLM and a planning function, which creates an understanding of what the problem is and an understanding of how to address the problem. Then it has an act function that can actually go and call tools repeatedly. It also has, importantly, memory, so it has short-term memory for the context of what it's doing, but also long-term memory to learn from the system, oversight, and learning, as I mentioned.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2800.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2800)

### Chef Antoine and Chef AmÃ©lie: Demonstrating Domain-Specific Agentic AI Applications

I'll now hand over to a demo. Thank you. So SLM works very well for  specific domains. When you have a very defined domain, it's something that you can consider to use. In this demo we can show a general AI chef, the French chef, Chef Antoine, where we can define some ingredients and they can prepare the recipe. Everything is running on-site as well, and everything is generated by AI.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2840.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2850.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2850)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2860.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2860)

So I can select my protein. I have four steps here. I would like to select the beef.  Now, the enrichment source, so I would like to use only button mushrooms and tomato.  And the aromatics, so I can select here maybe this. And finally we have the ingredients and now when I click here we have  a specific system prompt for the agent. The model that we are using is Llama 3.2 1B Instruct. So we send to the model, the model will prepare the recipe and also prepare the image description because I'll send this description to another model running to generate the image of this dish.

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2900.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2900)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2910.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2910)

[![Thumbnail 2930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2930.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2930)

So when I created here, prepared the recipe, you can see they chose Boeuf Bourguignon. We can see the recipe being created.  We have chef tips and we have a remark here at the end, 27 tokens per second was for this model, and here we see the image.  This image is generated by one SLM. Based on this SLM they send to another model to generate the image related to that dish. It's a naive Python application, but we also have Chef AmÃ©lie. Chef AmÃ©lie helps you to  prepare, to organize your party or one event at home.

[![Thumbnail 2950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2950.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2950)

[![Thumbnail 2970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2970.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2970)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2980.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2980)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/2990.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=2990)

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3000.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3000)

We have here in natural language, so I can ask, I'm hosting a casual Sunday lunch for six people, no one's vegetarian, natural language in that case. But  instead of showing here on this side, you can see the application log that we show. On this we can see the application running. So this is an agentic  AI application. We are using Strands Agents SDK, and you can see here, the SLM analyzed the  available tools and now they start using the tool analyze menu requirements, calculate cooking time.  These are tools that we provide to this SLM model, to this application to send to the SLM model, and they decided which tool to use  to combine all the information that they need to prepare the organization at the end.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3010.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3010)

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3030.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3030)

Including here we have one that  suggested wine pairing. All the logs, of course, are happening here, and they will process the last part when they combine all this information and prepare a final organization plan in that case. Again, we are using  small modules to process that.

[![Thumbnail 3040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3040.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3040)

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3050.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3050)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3060.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3060)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3080.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3080)

Let me increase. Here, oh, casual Sunday lunch. So menu  overview, we have starter, main, side, serving 6 people, cooking timeline, shopping list that we use,  the budget breakdown in that case, wine pairing. They consider the final notes and some metrics that we have here. So the agent is running at  the edge, running on Outposts for a specific domain in that case. It's of course just a generic example, but you can consider inside the organization what makes sense for your application. 

[![Thumbnail 3090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3090.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3090)

Cool. So that was a summary of Agentic AI. What we've shown here is that it's absolutely possible and feasible to run Agentic  AI at the edge on relatively small GPUs. The SLMs are the foundation of the whole system, and we've explained the architecture, and also we've looked at getting the TCO as low as possible for these systems.

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/71c5726db9fb3ac2/3110.jpg)](https://www.youtube.com/watch?v=MW7Afh17dFo&t=3110)

So summary overall, hopefully you've  enjoyed listening to us about how we actually run SLMs at the edge. We've provided some best practices for you. We have blogs and also code samples that we've published if you want to look at this in more detail. Shared very quickly a path to production and some of the steps we've gone through and then talked about Agentic AI. So I'd like to thank you and ask, is there any questions?


----

; This article is entirely auto-generated using Amazon Bedrock.
