---
title: 'AWS re:Invent 2025 - PostgreSQL performance: Real-world workload tuning (DAT410)'
published: true
description: 'In this video, Baji Shaik and Vlad Vlasceanu demonstrate real-world PostgreSQL performance tuning on Aurora PostgreSQL by fixing five critical queries causing high CPU and slow execution times. They show how rewriting a function call with a SELECT subquery enabled index usage, reducing execution time from 60 seconds to 800 milliseconds. A partial index on active products improved performance by 100%. Using Aurora''s pg_hint_plan and APG Plan Management extension, they switched from a single-column to composite index without code changes. Implementing heap-only tuple (HOT) updates by adjusting fillfactor to 80% optimized non-indexed column updates. Finally, reducing daily partitions to monthly partitions eliminated lock manager overhead. Dropping unused and duplicate indexes freed resources. These optimizations increased query throughput from 15,000 to 24,000 queries per 30 secondsâ€”over 100% improvementâ€”demonstrating systematic performance tuning using EXPLAIN ANALYZE, Performance Insights, and pg_stat_activity.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - PostgreSQL performance: Real-world workload tuning (DAT410)**

> In this video, Baji Shaik and Vlad Vlasceanu demonstrate real-world PostgreSQL performance tuning on Aurora PostgreSQL by fixing five critical queries causing high CPU and slow execution times. They show how rewriting a function call with a SELECT subquery enabled index usage, reducing execution time from 60 seconds to 800 milliseconds. A partial index on active products improved performance by 100%. Using Aurora's pg_hint_plan and APG Plan Management extension, they switched from a single-column to composite index without code changes. Implementing heap-only tuple (HOT) updates by adjusting fillfactor to 80% optimized non-indexed column updates. Finally, reducing daily partitions to monthly partitions eliminated lock manager overhead. Dropping unused and duplicate indexes freed resources. These optimizations increased query throughput from 15,000 to 24,000 queries per 30 secondsâ€”over 100% improvementâ€”demonstrating systematic performance tuning using EXPLAIN ANALYZE, Performance Insights, and pg_stat_activity.

{% youtube https://www.youtube.com/watch?v=t4COpw6A-EE %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/0.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=0)

### Introduction to PostgreSQL Performance Tuning: Meet John and His 3 AM Alarms

 Hello, everyone. Welcome to DAT 410: PostgreSQL Performanceâ€”Real-world Workload Tuning. Let me ask you this: Have you ever received alarms at 3:00 a.m. in the morning due to database performance degradation? It could be high CPU utilization or high query execution times, or query plans switching. If so, today we're going to see a few common performance challenges which most of us face and fix together. My name is Baji Shaik, and I'm a Senior Database Engineer for RDS and Aurora PostgreSQL databases. I have a co-speaker here. Hi everyone, I'm Vlad Vlasceanu. I'm the Senior Principal Database SA for databases here at AWS. I'm here to help Baji. If you have any questions, please raise your hands, and I'll come with the mic to you so you can ask the question so everybody can hear it, and then we'll try to answer your questions to the best of our knowledge.

So let's start with it. Before starting the content, I just want to start a load generator. I have a slide to talk about this load generator. I just started the load generator. So, meet Mr. John. He's a Senior Database Engineer at AnyCompany. He's responsible for PostgreSQL databases in his company. He did a pretty good job in installing and setting up PostgreSQL databases. He tuned it very well and he set up all monitoring and all the alarms. But eventually, data grows, and he started seeing alarms at 3 a.m. in the morning due to high query execution times and high CPU utilization. Does that sound familiar? Then let's fix John's PostgreSQL performance issues together.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/110.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=110)

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/120.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=120)

  Before that, let's look at the key areas to focus for performance tuning. I would start with CPU utilization. It could be due to suboptimal queries which are going to do full table scans, which is high CPU intensive. Your application workload could be undersized, for instance, where you see high CPU utilization. By default, PostgreSQL uses parallel queries. So if you have a large number of parallel queries, you'll see more connections using CPU. That's where you see high CPU utilization.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/150.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=150)

 Next thing to look at is memory. Again, suboptimal queries and memory intensive queries are factors. PostgreSQL uses process-based architecture, so each connection is a process which consumes some memory. If you have a high number of connections, you'll see high memory utilization. PostgreSQL has some memory-related parameters based on which you control the memory. But if those parameters are overconfigured, that's where you see high memory utilization.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/180.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=180)

 Next thing to look at is storage and IOPs. PostgreSQL uses multi-version concurrency control mechanism. So every modification to the database will have two versions of rows: old and new. The old version of rows is called dead tuples, which will be cleaned up by maintenance activities like VACUUM. Otherwise, you'll see high storage and IOPs utilization. If you have more indexes which are unused or duplicate, every modification to the database will lead to updating those indexes unnecessarily, which is where you see storage and IOPs utilization. PostgreSQL uses work memory to control query operations such as sorting. But if that memory is insufficient, it creates temporary files on disk, which causes high IOPs activity.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/240.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=240)

 Next is application pattern. If you have a large number of queries which are blocking each other, that's where you see slow database performance, not due to query executions, but because those queries are conflicting with each other. If you have long running transactions or idle transactions for a longer period of time, which will block maintenance, eventually the database will slow down. If you have more idle connections which are doing nothing, they still consume resources. That's where a connection pooler would help to optimize the connections. So these are a few key areas to focus.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/270.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=270)

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/280.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=280)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/290.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=290)

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/300.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=300)

    Next, we can look at query tuning methodology, which is a step-by-step process. You would start with looking at Active Session Summary in Database Insights, Performance Insights, or you can look at the pg_stat_activity view inside the database. You can look at top SQL and top wait events consuming those resources. You will generate an EXPLAIN ANALYZE plan with the BUFFERS option where you can see shared buffers information as well. Then investigate from that. It's a step-by-step and iterative process. You need to find out the top SQL and start fixing from there.

[![Thumbnail 310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/310.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=310)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/340.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=340)

How does an EXPLAIN ANALYZE plan look like?  This is a simple plan. Every arrow mark represents a plan node, and the top row is the consolidation of all the plan nodes. If you observe the first row and every plan node, there are two sections to it. The first part is estimations and the second part is actual. Let's look at the numbers. The first number is estimated startup cost, then total estimated cost, then total estimated rows, and the average width of the rows. The next section is actual, which starts  with actual startup time in milliseconds, then total time in milliseconds, and the total number of rows for that particular query. Every plan node has these two sections.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/430.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=430)

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/440.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=440)

If you look at the second part of each plan node, that's the execution time of that particular plan node, and the consolidation is the top row. The top row has the number of rows executed with that particular query, and next is the loops, the number of loops. If you have a complex query, you'll see more number of loops. That's the basic foundation for EXPLAIN ANALYZE plan. What problems should you look at in EXPLAIN ANALYZE plan? In the demo, we have several problems to look at in detail, but at a high level, we'll start with bad estimates. The planner is dependent on statistics. If the statistics are not up to date, you'll see bad plans and execution times are high. You can look at sequential scans and full table scans, which are CPU intensive, where an index can help. But if an index is already there, a strategic index can help improve the performance. You can also look at buffer rates with the buffers option in EXPLAIN ANALYZE plan.  As I said, you need to look at each plan node and pick up the slow operation in that plan node and start investigation from there. 

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/450.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=450)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/470.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=470)

### Setting Up the Demo: An E-Commerce Application Under Load

This is an e-commerce application. We have a simple e-commerce app with a load generator that I've started. This app has simple e-commerce tables like orders,  products, and users, and we have the load generator. It opens the connection to the database with more than 1150 connections and it has balanced reads and writes, and it provides a summary every 30 minutes on how many queries  run. We can see that in the demo. These are the five critical performance issues we are going to cover. We'll cover them in detail in the demo, but at a high level, the first one is how rewriting a query could help. The second one is when the query goes for a different plan, but another plan can help. How do you switch between the plans? The third one is when you already have an index. How can a strategic index help improve the performance? The fourth one is heap-only tuple updates. How can that help improve the performance? And we have some lightweight locks we can see in the demo, and what are those locks and how can we fix them?

[![Thumbnail 550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/550.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=550)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/570.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=570)

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/580.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=580)

This is the content. We can start with the demo. I'll just switch back to the demo. This is the load generator I've started. It has eight queries that are running, and it gives you information about how many times each query ran and the aggregation of the queries.  For that particular thirty-second interval, as we fix the queries, we can see improvement in these numbers of queries. Switching back to the console, this is the instance where you can go to Performance Insights.  Let's select the last ten minutes, and you can see different wait events with CPU being the most top wait event.  You can see more than sixty to seventy sessions are consuming CPU, but we have a lot of wait events. I'll go to Database Insights, which is an extended version of Performance Insights, and continue monitoring from there.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/610.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/620.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=620)

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/650.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=650)

[![Thumbnail 670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/670.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=670)

If you select the last ten minutes, you'll see the same wait events where CPU is the most.  I'm switching back to terminal now. Let's connect to the database. I was talking about pg_stat_activity as well. You can query the pg_stat_activity view, selecting a substring of the query for maybe 20 characters from the pg_stat_activity view where the database name is e-commerce.  So you can see all connections to the databaseâ€”more than 150 connections. These are all running against the database.  If you look at all the inserts, selects, updates, and all sorts of queries, you can see it has balanced read and writes to the database. Let's go back to the console and select the CPU wait event, which is the most significant.  If I scroll down, you'll see the top queries that are consuming the CPU. Today we're going to fix the top five queries one by one.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/680.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=680)

[![Thumbnail 690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/690.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=690)

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/710.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=710)

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/720.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=720)

[![Thumbnail 740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/740.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=740)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/750.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=750)

[![Thumbnail 760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/760.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=760)

Another thing to monitor is the database telemetry section.  You have individual metrics on CPU, memory, I/O, and all sorts of monitoring.  If you weave metrics and CloudWatch, you can create your own dashboards to monitor these metrics. I have a simple dashboard created with metrics like CPU, read ops, write ops, and all those basic metrics. We have commit latency and all those.  If you see the CPU over the last 15 minutes,  it's touching 100 percent. So now let's go back to Database Insights and pick up those top queries and fix them one by one. I'll select the last 10 minutes and then the CPU weight event.  If you scroll down, you can see the query. This is a select function call which is consuming more CPU resources.  If I check that query, this is the query that's running against this database. Let's see the DDL of this query and this function to see what's inside it. 

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/780.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=780)

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/800.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=800)

### Query Rewriting: Fixing Function Calls with SELECT Subqueries

It has only a single select query with a predicate to a function call. So if you execute this function, sale_movement_type is coming to this query and the overall function gets the count of all sales movement types.  But if you execute an EXPLAIN ANALYZE on this query, it's going to take time. A few minutes back, I executed EXPLAIN ANALYZE on this function just to save some time since it takes more than one minute.  If you run EXPLAIN ANALYZE with BUFFERS on the function call, you don't see much information on EXPLAIN ANALYZE directly on the function. For this, you can use the PL profiler extension, which gives you the internal query details of the function. Alternatively, you can check the function and manually execute EXPLAIN ANALYZE for the queries inside it. Since we have only one query, I just ran EXPLAIN ANALYZE on that query. We're going to spend a couple of minutes deeply looking at this EXPLAIN ANALYZE plan, and then for other queries, we can spend less time.

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/840.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=840)

This is the EXPLAIN ANALYZE plan.  The arrow mark that I talked about is this plan node. We have one plan node since we have only one query, one table, and a single straight query. That's why we have one plan node, and the top line is the aggregation of all the plan nodes. Now, the first thing to do is determine which plan node is slow. Since I have only one plan node, if I look at the plan node, the estimated cost is over 3 million and rows are nearly 2 million. The actual total time is around 60 seconds, and the rows returned by this plan node is only 1 million. Now, another couple of things to look at is that rows removed by filter are 10 million rows. But if you observe this plan node is returning only 1 million rows.

[![Thumbnail 920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/920.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=920)

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/930.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=930)

This discrepancy exists because if you look at the number of rows in the same table, there are 11 million rows, and if you  group the movement type for sale, this is 1 million. So this sale movement type is what is written here. If you  exclude that sale, that's 10 million rows, which is where it's removed by the filter.

Now what's happening is that for each row, this function call gets executed. That's why it gets executed for all 11 million rows. Then the planner realizes that the sale movement type is only 1 million rows, so it needs to remove all the rows that were processed for 10 million rows. If you observe this, it's unnecessary work for the planner to go through 11 million rows to execute that function. If you look at shared hit, these are in blocks. So 87,008 KB blocks are fetched from memory. That's high. If you multiply 87 by 8 KB, it's around 800 megabytes or something, so that much data has been read.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1000.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1000)

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1020.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1030.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1030)

Now, if you look at this planned node, it's going for a sequential scan, which is a full table scan. But if I look at this table DDL,  with the backslash d option, you can see the table structure. I'll connect back once again and then look at the DDL for this table. This is the movement type index.  So there is an index on the movement_type column, but it goes to a sequential scan for this query. This is because  the planner doesn't know the value that's coming from this function. If you execute this function with a select call, then it gives you sale, but the planner doesn't know what value is coming there. So that's why for each row it's executing that function.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1070.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1070)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1090.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1090)

To make the optimizer aware of the sale movement type, if I change the query, maybe I'll take this query from here and explain analyze buffers. If I use a select call for that function, if I go back to the function  name, so if I add a select call to that function and see what happens, it should pick up that index because it knows that sale comes to that filter column. The first time it's going to take time because if you see,  it's going for an index scan, that's what we wanted. So now the select call gives an index hint that you should go for an index scan.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1110.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1110)

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1120.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1120)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1130.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1130)

If you observe the IO timings, this is for the first time the index has to be loaded into memory. That's why you see IO timings there. But if I run it again,  as it's already in memory, it took 10 seconds earlier and now it took 800 milliseconds, but initially it was nearly 1  minute. So we brought down the execution time from 1 minute to 800 milliseconds. Not only that, you don't see that rows removed  by filter here. So the planner exactly goes to the sale movement type rows here. If you see the shared buffer hits, it was 87,008 KB blocks. Now it's just 2,008 KB blocks, which is very much less work for the planner. That's why execution time is slow. But if you look at all of the metrics, the cost was 3 million earlier. It's 443,000, so everything came down.

[![Thumbnail 1170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1170.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1170)

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1180.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1180)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1200.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1200)

So to fix this query, we just need to rewrite this query to use the select call. I'll just do that.  I'm just modifying that function to use that select call.  And now if you see the definition detail of that function, you can see that the select call has been added. So every connection that comes after this modification should go to an index scan. If I go back to my load generator, so function calls for every 30  seconds are only just 9 queries. If you keep monitoring, if you wait for the next summary to be displayed, then you can see

[![Thumbnail 1210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1210.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1210)

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1230.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1230)

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1240.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1240)

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1250.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1250)

the improvement in those function calls as well.  The function calls have now improved to 12 queries. If you wait for a few minutes, you'll see the actual improvement. Another thing I'm doing is switching back to the console to look at other stats. If I go to the dashboard and scroll down,  I'll pick the commit latency metric. If I select for the last 15 minutes,  you can see it's slowly coming down. It was around 23 microseconds per transaction and now it's coming down to 21. So commit latency for those function calls has been reduced, and it  gives more resources to other queries as well. You can see a little improvement in the number of queries for other queries as well, because the resources consumed by this function call are now less.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1280.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1280)

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1310.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1310)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1330.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1330)

We can see other metrics like queries finished. Maybe I'll go for this metric. If you go to CloudWatch and deselect all metrics and select only queries finished,  this should also get improved over time. You can see the number of queries is improving. If you look at the total query time, this should get reduced. It started with 5 million and now it's at 5.25 million. Because other queries are also running, it created resources for other queries. But eventually,  it comes down. That's how we can rewrite queries to improve performance. This is an example of rewriting a query. If you're using function calls as predicates, make sure you use a SELECT subquery. Otherwise, the optimizer will get confused and it will take a sequential full table scan instead of index scans. 

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1340.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1340)

### Strategic Indexing: Creating Partial Indexes for Active Products

If I go back to Database Insights,  is there any reason you would not always do this? Or why would you ever not prefix it with a SELECT? To call a function directly. If I understand the question, why would we need to add that SELECT subquery? Why would you ever not do that? I didn't even know this was a thing, and I'm thinking about all the queries I've written. I should be doing this everywhere. Is there any reason we wouldn't want to do this?

Let me paraphrase the question. She's asking why the database engine would not automatically do that. Yes, understood. So the question is, why doesn't the database engine itself pick that up as a SELECT subquery? That's the question, right? Yes, that's how the optimizer works. It only looks at the function call. If it doesn't have a SELECT subquery, it has to execute that call and get the value for that filter type. But if you have that SELECT subquery, it already executes that call first because it's a subquery now. It's not the actual filter; it's a subquery. So it executes the subquery first and then gives the value to the further rows.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1450.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1450)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1460.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1460)

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1470.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1470)

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1480.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1480)

If we observe the next queries, I'll fix this third query first. Any questions? Yes, I had a few questions on this example too, just before you get too much into this. Did you, when you were running those EXPLAIN plans earlier,  did you run a second one before you made that tested fix?  This is because of the I/O timings. The first time it has to load the index into memory, so all the extra lines  are related to I/O, and it fetches this I/O timing line. That's where it is. Did you have to add a parameter or something to EXPLAIN  to have it come out with more details, or is this just the regular EXPLAIN? It's just the regular EXPLAIN.

[![Thumbnail 1570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1570.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1570)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1580.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1580)

That was the difference. Thank you, that's what I was asking. I wanted to know. And then one other thingâ€”all of these PostgreSQL command line things you're doing, such as the EXPLAIN PLAN or the backslash D function, are those something you could or would want to do in the AWS console? Not in the AWS console, but any client tool like PGAdmin or other client tools. You can use those client tools to see the details of the tables and what indexes, what functionsâ€”everything can be done through client tools. Just one more quick follow-up: Is there a significant difference between what you've shown here and if you're doing Aurora PostgreSQL?   This is Aurora PostgreSQL. I'm so sorry I haven't mentioned that. If you go back to the console, this is my Aurora instance that I started looking at. The EXPLAIN plan is a standard for that, so you can use it for any PostgreSQL flavor database. There is nothing special if it's RDS Aurora community PostgreSQL.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1610.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1610)

[![Thumbnail 1620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1620.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1620)

Now, let's look at the third query that I want to pick. If you look at the query, it's getting the average price for products in a certain category, only for the active products. If I select this query and generate an EXPLAIN ANALYZE plan,   it goes to an index scanâ€”specifically a Bitmap index scan. The difference between a normal index scan and a Bitmap index scan is that for an index scan, it takes one row and fetches it from the index file, so for one row, there's one iteration. But with a Bitmap index scan, it takes a group of rows and fetches all the rows at once, so it reduces the number of iterations to the index files. That's why a Bitmap index is more efficient than a normal index in some cases.

[![Thumbnail 1690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1690.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1690)

So it's going for a Bitmap index scan, and the two things we looked at in the last query, we are again looking at here. You can see 30,000 rows were removed by the filter and 13,000 8 KB blocks are fetched. If I look at the query, it's getting the average price for a certain category, which is activeâ€”only for the active products. But if I look at only active and inactive products from the products table,  only 33 percent of the products are active. So if I know that I always select the active products and not the inactive products, then I can clearly create a partial index on the same category column, but only for active products. Let's try to create that index. I'm using the CONCURRENTLY option because all the queries are running in parallel. So IDX for category and only active on the products table on the category column where active equals true. This is the partial index I want to create. Let's create this index and execute the EXPLAIN plan and see the difference.

[![Thumbnail 1740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1740.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1740)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1770.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1770)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1780.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1780)

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1790.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1790)

Now it goes for a Bitmap index scan for that new index. If you observe these two metrics, the rows removed by filters are only 900 now, where they were 30,000. The execution time is 360 milliseconds, and now 23 milliseconds. The buffers are only 10,000, where they were 13,000. So there's less work to do.   Now that we have created the index, I'll go back to the load generator. If you observe,  the function calls were 912 before we fixed the first query, and now eventually you can see 418 queries are running for that function calls.  So it's a lot of improvement due to that index scan for the first query. Now let's look at the status index. The number of queries is 1,200, 1,200.

[![Thumbnail 1810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1810.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1810)

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1820.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1820)

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1830.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1830)

[![Thumbnail 1840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/1840.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=1840)

After creating that index, you can see improvement from 1200  to 2000, which is almost 100% improvement. The aggregation of queries is now 20,000 after we fixed a couple of queries , but initially when we started, it was 15,000. So we have 5,000 more queries running for the same workload on the same instance. I haven't changed anything at the  instance level. Just fixing the queries improved performance from 15,000 to 20,000 queries. Now let's go back here  and move to the next query to fix.

Can we not use a combined index instead of getting an index on every single column? Would a composite index not have helped? If I understand the question correctly, why can't we have a composite index on category and is_active? The reason is that we can't be creating so many indexes for different statuses. That's right. But again, if you create a composite index, is_active has inactive products as well. We want to filter only active products specifically because we always search for active products. So if you go for a composite index, it still has a large number of rows removed by filter because the category may have any number of inactive products as well. A composite index might not be the right choice here. A partial index would work well. That's why we created an index specifically on the active products.

We thought for a long time that index scanning happens from the leftmost column in the predicate. Right now, we are picking something in the middle. Is that something we're supposed to benchmark before we roll out? This happens because of data growth as well. Let's say you only have active products for 90% of the products table initially, and your category column index works well. Now, eventually data grows and some of your products become active and inactive. Now the inactive amount is 66% of the table, so that's where you see the change. But if you benchmark initially, you would have gotten the best performance.

Building on that question, the more indexes we create, that essentially indicates that we need to periodically monitor, observe, and optimize. I think that's probably the whole plan, because otherwise what happens is you end up creating indexes on every single one of those columns, which you cannot plan ahead of time. The more indexes you create, you'll get less performance for modifications to the table. If you create that partial index, you can keep observing the first index on category and see if that index is not being used other than this index. You'll keep monitoring the index scans for that particular table and that particular index. If you observe that index is not being used, you can get rid of that index. We have ways to get rid of unused indexes as well going forward, but that's a good question.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2060.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2060)

### Query Plan Management: Switching Between Plans Without Code Changes

When you looked at that last example, what was the indication that there was something wrong with that query, and how did you know that an index would fix it? I looked at the inactive products and active products inside the  table, so I know that only 30% of my table is active. The red flag was that rows removed by filter are high. The query planner worked on inactive products as well and then realized and removed all those inactive rows. You want a low number of rows removed by filter because it's doing more work. That's every time, correct. That's a hint there. So let's go to this query. If I select this query, if you observe this query, I'm not sure if it's visible. I'll just show you this query with an EXPLAIN ANALYZE plan.

[![Thumbnail 2110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2110.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2110)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2130.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2130)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2140.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2140)

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2200.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2200)

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2220.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2220)

[![Thumbnail 2230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2230.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2230)

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2310.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2310)

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2330.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2330)

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2340.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2340)

[![Thumbnail 2380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2380.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2380)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2390.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2390)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2400.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2400)

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2410.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2410)

Let me show you this query with an EXPLAIN ANALYZE plan.  First, let me run it with EXPLAIN ANALYZE.  So let's observe the query. It has a hint, similar to what we use in Oracle. This can be done through the extension called pg_hint_plan.  I already created a pg_hint_plan, and in the initial days of my application, I provided a hint to use this particular index because that index performed well for my query. Now that I've hinted for this particular index, you can see an index scan using that specific index. So far, so good. But again, there are red flags: the same rows removed by filter are 400,000 and the buffers fetched are 400,000 8 KB blocks. But why did the optimizer go to that bad index? Because you hinted it. If you observe the predicates, you've created it on total_amount, and this is where a composite index can help. If you look at the details of the orders table, I already have that composite index.  If you see this index, I already have it, but it still goes to the single column index because I hinted for that. The simple solution is to just remove that hint, so the optimizer is smart enough to pick the right index.  Let's remove the hint and see what happens. It should pick up that composite index.  Great. So it went to create that composite index, and if you look at rows removed, there are no rows removed by filter, so it exactly knows how many rows to look at. The shared buffer hits are only 37,000 from 400,000, so that's a significant reduction. Now we know the solution: the simple solution is to remove that hint, and it works well, but it requires an application change. That query will be in different places, and removing that hint is not a short-term solution. You might plan for a long-term solution to remove it. But as a short-term solution, how do you make this query use a different index without changing the query? This is where the Query Plan Management extension from Aurora helps. Using this extension, you can capture the right plan and switch plans between queries. This can be done using the APG Plan Management extension. If I create this extension, it will give you a DBA plans table where it captures all the plans that hit the database.  If I create this plan and run that query, I'll just drop the extension and recreate it.  I have a question for you about the extensions. Right now, the Aurora product team has not guaranteed that extensions will be forward compatible.  Is that going to change on the roadmap? For example, if we begin to rely on this extension and then as the product moves forward and it's no longer supported, it could put us in a difficult situation. Are you endorsing that or how are you helping work with the product team to make sure that any extension we pick will be forward compatible? I mean, honestly, we are working with the service team to make sure that some of these capabilities exist and they become more compatible, but  we're not yet at a point where we can just guarantee it.  I think you can generally expect that, but we can't make that as a guarantee just yet. In time, as more of these extensions get a level of maturity that we're comfortable with, we would be able to make  that type of guarantee, but right now we're not quite there yet. But still, it's a tool. While it's not a guarantee, this tool is currently supported by AWS, so if you need  to use it, by all means go ahead and do that.

I had to reboot the instance because I created an extension and it dropped some time back, so that needs a restart of the database. I just restarted it.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2440.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2440)

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2450.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2450)

[![Thumbnail 2460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2460.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2460)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2470.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2470)

[![Thumbnail 2480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2480.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2480)

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2490.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2490)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2500.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2510.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2510)

Is this only available in Aurora or RDS also? This is specifically for Aurora. Now, if you look at the DBA  plans table, it captured plans for all the queries that are running. So all inserts, updates, selects, everything. If I  query specifically for our query that we are working on, which has created it,  is rebooting things. If you create and drop an extension, it needs a reboot.  Yes, this plan has been captured here, but just for  visibility and clear visibility, I have a query that just uses a JSON  function to bring the index that is being used for that particular query. It's on DPA plans, but it's a simple query. If I execute this query,  you can see that this query's plan has been approved in status, and this is the plan hash and this  is the SQL hash. So the plan for this query is already captured.

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2530.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2530)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2560.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2560)

That goes to create an index. Now let's remove that hint and run the query to capture the plan again. So now if we run this query,  in the note it says an approved plan was taken because that has, but instead of minimum cost plan. So this query plan has been captured already, but Aurora knows that there is an approved plan, so I should go for that. It still went to the created index plan, the old plan, but it captured the new plan. So what I need to do is look at this DBA plans table, and there is another plan which goes to created at  amount table, the composite index. This is unapproved. So if I unapprove this, reject this, approve this, it should go to the new plan. That's how we can switch back the plans.

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2600.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2610.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2610)

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2620.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2620)

Before that, we have Aurora stat plans function which will show you plans being used for the live queries. So if you are running queries and you are confused which plan is taking, you can use Aurora stat plans function to see which plan exactly it's taking. I'm just going for my queries tab,  and this is a simple query on Aurora stat plans function with filtering that particular query. If I execute this,  so index can hint query. This goes for orders created at single  column index still because I haven't approved the new plan. If you look at the number of calls, 1800, if I run the same query again, it is 2000 now. This is because these queries are coming in and it's taking the old plan still, so that's why you see the number of calls being increased for this particular plan.

[![Thumbnail 2660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2660.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2660)

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2670.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2670)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2690.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2690)

So now, how do you switch these plans? Aurora plan management comes with a function called set plan status. Using this set plan status function, you can reject or approve a particular  plan. You just need SQL hash and plan as inputs. So I already built two statements on this function  with SQL hash of new and old plans. If I just execute to approve the new plan, sorry to switch back between the windows, but if I approve and reject plans, and if I go back to this, now the plan with composite  index created at an amount is approved now and old plan is rejected. Now the assumption is it should take the new plan with composite index. How do you check that?

[![Thumbnail 2710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2710.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2710)

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2740.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2740)

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2750.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2750)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2760.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2760)

You can execute Aurora's query plan view for that particular query.  If you look at the results, there are two rows. The old index created shows 2,493 calls. With the new composite index, there are now 1,200 calls. If I run it again, this number stays constant because it's no longer using the old plan. This will keep increasing as we continue. So now that we've fixed that query, let's go back here.  Plan instability queries were around 255 to 260.  Now the plan instability queries are at 800, up from 255 to 800, and the total number of queries increased to 24,000, whereas previously they were 16,000.  Around 20,000 it hit 20,000 with two queries fixed. Now the query count has increased to 24,000.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2770.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2770)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2790.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2800.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2810.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2810)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2820.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2820)

### Heap-Only Tuple Updates: Optimizing Non-Indexed Column Updates with Fill Factor

 So we looked at how to rewrite the queries. The first approach is query rewriting. The second is that we have an index, but we can create a strategic index or partial index to improve performance. The third is that you have the right index already created, but how do you switch back between the plans to use the right index?  The next thing to look at is Claudia's metrics. The update query is consuming more resources.  If you look at this update query, it updates the products table on the notes column.  Looking at the DDL of this products table and its indexes,  there is no index on the notes column. PostgreSQL has a feature called heap-only tuples. If you are updating non-indexed columns within a data block and there is some space inside it, instead of updating the index because this is a non-indexed column, it will just create pointers inside that data block. It doesn't need to update the indexes, so the iteration of that work has been reduced. This is called heap-only tuples, but you need empty space inside the data block.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/2880.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=2880)

This is defined by the fill factor in PostgreSQL.  If you look at the fill factor for this table, it's 100 percent, which means write to the entire block and don't leave any space. That's the meaning of fill factor 100 percent. Now heap-only tuples need some space. Either that space is created by a vacuum for the empty spaces, or you specifically need that percentage to be free. I have a quick question. Is the query plan going to be the same for the writer and reader instance in the same cluster? If it's not the same, where do you see it's going to be different to be optimized for each of them?

Did you understand the question? What you're showing there on the screen with the query planâ€”the question is if you have a cluster of multiple readers or writers and readers, do you need to make that change only once, or do you have to make that change on every one of those instances? You have to make the change on the writer instance, so the reader instance will be updated. The changes propagate from the writer. They'll propagate to the plan changes. It's going to be a read-only workload. Are there cases when you need to have them being different between readers and writers? I'm not aware of any cases. I haven't seen any. If it's the same query, then it has to be the same index because that gives you better performance.

I mean, the results of a query are going to be deterministic no matter where you're running it, subject obviously to your replication lag. But if there is a slight change in the query, then the SQL hash will change, and then that will not work for the query.

So what you mentioned is that query optimization changes we do on the writer instance are automatically propagated to the reader instance, right? But it's not bidirectional. What I optimize on the reader instance will not propagate back because we often run into reader instances handling a lot of reads going to downstream systems like data warehouses. The reader instance is always resource intensive in nature, so we end up optimizing a lot on the reader instance. What kind of optimizations specifically for reader instances? What about query plans? We do similar optimizations on the writer instance, but these queries are hitting the reader instance. That's the same question. It's not bidirectional in nature. But if you want a different plan on the reader instance, that's the same question. So if it's the same query, it has to be the same plan. I'm not aware of cases where the same query needs two different plans, one for the writer and one for the reader. I'm not aware of those cases. Structurally, the data is physically laid out in storage the same way. It's the same storage, so there's no reason you would have the same query needing different performance optimizations on different instances. The question is that you might only run that query on the reader instances because it's feeding downstream systems and it doesn't run on the writer, but that's not going to impact the performance of the writer. But the optimization is still valuable intrinsically because that query is deterministic in its response. It's always going to produce the same response given the same input data.

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3190.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3190)

[![Thumbnail 3200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3200.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3200)

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3220.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3220)

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3230.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3230)

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3240.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3240)

[![Thumbnail 3250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3250.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3250)

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3260.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3260)

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3270.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3270)

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3290.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3290)

Coming back to HOT updates, I need the fillfactor to be reduced to maintain some free space in each block for those index pointers. That can be done by changing the fillfactor for that particular table using this command: set fillfactor to 80%. Now I changed the fillfactor, but that's not an immediate effect because all my blocks are already filled. I need to reorganize the blocks to leave that empty space for every block. That can be done by VACUUM FULL.  VACUUM FULL acquires an access exclusive lock, so I need to get rid of the connections to improve the VACUUM FULL time. I'll just kill the connection, and the load generator is smart enough to restart when you kill the query. Using PG_STAT_ACTIVITY where database name is e-commerce and PID not in the PID that I'm currently working on, if that PID gets killed, then you cannot move forward with the session. So I'll execute VACUUM FULL ANALYZE on the products table.  It's going to take a few seconds to execute that VACUUM FULL, maybe five or six seconds. After that VACUUM FULL, now we will have 20% space for those hot updates.  If I go back to the load generator, let's check if it's changed. The table DDL now shows fillfactor is 80%.  You can check whether updates are going for HOT or not using PG_STAT_USER_TABLES. There is a column in PG_STAT_USER_TABLES called N_TUP_HOT_UPD.  This column shows how many HOT updates for that particular table. Let me get the percentage. I'm going to my query.  This is the HOT updates query. I'm selecting the update count, how many HOT updates, and the percentage for that particular table.  If I execute this simple select here, then there are 70% tuples going to HOT updates.  If you keep observing, then you'll see the performance ratio will get increased, so 72, 72.1, and it will keep increasing. So now there are more HOT updates to the table. If you go back to the load generator,  there are 2,300 HOT updates. Previously they were 3,800, but we restarted the instance, so it takes time to pick up. We will eventually see more number of HOT updates.

[![Thumbnail 3310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3310.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3310)

 That's where HOT updates help. If you have updates on non-indexed columns, it can create some free space on each data block to have those pointers instead of updating the indexes. Now you can see almost 4,000 queries. Previously they were around 2,000 to 3,000, so that's a lot of improvement.

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3350.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3350)

If I go back to this, the fifth query we have is this function call.  We originally started by saying that there's no index on that column, so it was going to basically update the space in the table. Then we said the way to improve it is to reduce the fill factor from 100 to 80. What is better long termâ€”would an index on that column or reducing the fill factor?

It's not because there is no index on the column. It's because PostgreSQL needs to update all the indexes for those DMLs irrespective of whether you're updating a non-indexed column or not. Those indexes still need to be updated. Imagine if I don't have an index on that column, I don't need an index on that column, but still I am making progress to update all those indexes unnecessarily. However, those indexes should be aware of these updates.

So how can we do that? That's where this feature helps. Instead of updating indexes, you can create pointers. You can at least reduce the time of updating. Those pointers should be available in the same block. They can't be in a new block. That's why we need space in that block to update those pointers.

Another disadvantage is if you have that free space in that block, your inserts need more blocks. Asking your operating system to give you a block to update the data is again overhead. So if you have more updates on the table and fewer inserts, then you can go for that fill factor to leave that space. Otherwise, your inserts need more and more blocks because you're leaving that space in each block.

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3450.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3450)

[![Thumbnail 3460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3460.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3460)

[![Thumbnail 3470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3470.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3470)

### Reducing Lock Overhead and Eliminating Unused Indexes: Final Performance Gains

If you look at this function definition,  this is a select on the order_items column. This is a simple select, but if you look at the order_items table,  this is a partitioned table with daily partitions.  You can see 731 partitions. Now we know that if we have a WHERE clause on the partition key, it goes to partition pruning. But before that, the optimizer needs to get a lock on each partition first, and then it prunes the exact partition for the data. But still, it needs to lock all the partitions.

[![Thumbnail 3540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3540.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3540)

PostgreSQL by default allows non-fast path and fast path locking. Fast path locking reduces the overhead of acquiring locks. By default, only 16 fast path locks can be acquired for each query. Now if you have a partitioned table with 700 partitionsâ€”so 700 different independent tablesâ€”it has to acquire 700 locks on that table. Obviously, it goes to non-fast path locking, which is some overhead. That's why you see particularly lock manager events for this query. 

[![Thumbnail 3560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3560.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3560)

[![Thumbnail 3580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3580.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3580)

[![Thumbnail 3600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3600.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3600)

The lock manager event occurs when your query has a higher number of non-fast path locking. So just to check, if I begin and execute this function,  select star from the function and let's check the number of locks for that particular query. I'll just reconnect to the database. Select count of locks from pg_locks.  It acquired around 1,300 locks. But if I remove the fast path locks, there are still 800 non-fast path locks.  This is because it acquired a lock on each partition.

[![Thumbnail 3610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3610.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3610)

[![Thumbnail 3620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3620.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3620)

[![Thumbnail 3630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3630.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3630)

[![Thumbnail 3650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3650.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3650)

[![Thumbnail 3660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3660.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3660)

[![Thumbnail 3680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3680.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3680)

I have a new table for these other items called order_items_new , which comes with monthly partitioning instead of daily partitioning. This has the same amount of data as the original order_items partition table , but it has a significantly lower number of partitions. If I update this function to use the partition table with fewer partitions , and then select this function in a transaction, I can check the non-fast path logs. Those are significantly reduced . All that locking overhead has been eliminated. If you keep observing, you won't see those lock manager events, or you'll see very few lock manager events here . This locking overhead affects query performance, and we've fixed this issue. We've fixed five queries, and there's one last thing I want to show regarding indexes .

[![Thumbnail 3690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3690.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3690)

[![Thumbnail 3700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3700.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3700)

[![Thumbnail 3720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3720.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3720)

[![Thumbnail 3740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3740.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3740)

If you have unused or duplicate indexes, the database unnecessarily needs to update or insert data into those indexes, which is heavy work. This query checks for unused indexes . This is not a query I created myself; you can find it in the Wiki. If I execute this query to find unused indexes in my database , I can see there are many unused indexes and duplicate indexes as well. Sometimes in development instances, we keep checking index usage and create more and more indexes, then forget to drop the duplicate ones. That's where duplicate indexes come from. If you check the duplicate indexes , there are 14 duplicate indexes. Not only that, these indexes are around 5 to 6 gigabytes in size, which is unnecessary storage. Now I can drop those indexes and see the performance improvement .

[![Thumbnail 3760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3760.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3760)

[![Thumbnail 3770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3770.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3770)

[![Thumbnail 3790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3790.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3790)

[![Thumbnail 3800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3800.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3800)

[![Thumbnail 3810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3810.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3810)

[![Thumbnail 3820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3820.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3820)

[![Thumbnail 3830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3830.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3830)

[![Thumbnail 3840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3840.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3840)

[![Thumbnail 3850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3850.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3850)

I'm getting rid of the connections because dropping indexes takes some time with active connections. If I execute this drop index command , it gives me a set of drop index commands. For duplicate indexes as well , there are 14 duplicate indexes. Now if I go back to the query where I'm terminating all the connections  and run the duplicate command , I just need to execute so that all those drop commands will get executed. The same applies for duplicate indexes as well . You just need to execute. So now all those indexes were dropped, all unused and duplicate indexes . We can keep observing as our load generator has restarted all the queries , so we can observe these queries and see the improvement. We started with somewhere around 12,000 to 14,000 queries, and after fixing our five queries, we are at 24,000 queries . That's more than 100 percent improvement with this tuning .

[![Thumbnail 3860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3860.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3860)

As it takes some time to generate the report, but meanwhile, if you have any questions, I'm happy to take them. Hi, I just wanted to confirm when you did the HOT updates and made that change, you didn't change the code in any way, correct? It was just because you had some spare space. I just changed the table's fill factor. Okay, thanks. The second question is, are you aware of any development underway for PostgreSQL or for Aurora PostgreSQL to consolidate query plans? For example, if you have a query with a certain comment but the rest of the text is identical, it produces a different hash. Or if you have spaces or carriage returns, same query, same number of parameters, but the spacing is a little different, it's a different query hash. Are you aware of any efforts to consolidate those queries that are actually the same query functionally into one hash or have some kind of mapping? 

Honestly, I'm not, but if you have hints, it will remove the hints to generate the SQL hash for that particular query. Or if you have explained whatever before that select, it removes those hints to generate a SQL hash. But if you have spaces or enters, then it's a different SQL hash. I'm not aware of anything that's being developed to enhance that.

[![Thumbnail 3990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/cbbf0064db48b95d/3990.jpg)](https://www.youtube.com/watch?v=t4COpw6A-EE&t=3990)

We can capture this as a feature request for the service team to see if they can work on that. The entire extension is available only for usâ€”it's our extensionâ€”so we should be able to do it. It's just a matter of prioritizing the work based on customer needs. We can definitely let the service team know about this request. Now that we know about it, we can put in this feature request for the service team.  All right, thanks everyone. Thanks, Baji, that was wonderful. Thank you for joining us.


----

; This article is entirely auto-generated using Amazon Bedrock.
