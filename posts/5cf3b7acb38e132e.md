---
title: 'AWS re:Invent 2025 - AI at the speed of news: Bloomberg Mediaâ€™s vision for the future (IND3331)'
published: true
description: 'In this video, Bloomberg Media and AWS Solutions Architects present an end-to-end AI-powered media platform for analyzing and creating content from petabytes of video archives. Bloomberg processes 13 petabytes of media growing at 3,000 hours daily, serving 60 million monthly viewers across 48 streaming platforms. The session covers their "disposable AI strategy" using versioned models, federated search across multiple embedding databases, hybrid search combining keywords and semantic vectors with OpenSearch and Amazon Neptune Analytics, and knowledge graphs for contextual understanding. They demonstrate multi-agent workflows using Amazon Bedrock that automatically repurpose a 30-minute interview into platform-specific cuts for YouTube, TikTok, and Instagram with different aspect ratios, while maintaining content accuracy and trust through automated review agents and human-in-the-loop validation.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/0.jpg'
series: ''
canonical_url: null
id: 3093087
date: '2025-12-08T20:04:02Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - AI at the speed of news: Bloomberg Mediaâ€™s vision for the future (IND3331)**

> In this video, Bloomberg Media and AWS Solutions Architects present an end-to-end AI-powered media platform for analyzing and creating content from petabytes of video archives. Bloomberg processes 13 petabytes of media growing at 3,000 hours daily, serving 60 million monthly viewers across 48 streaming platforms. The session covers their "disposable AI strategy" using versioned models, federated search across multiple embedding databases, hybrid search combining keywords and semantic vectors with OpenSearch and Amazon Neptune Analytics, and knowledge graphs for contextual understanding. They demonstrate multi-agent workflows using Amazon Bedrock that automatically repurpose a 30-minute interview into platform-specific cuts for YouTube, TikTok, and Instagram with different aspect ratios, while maintaining content accuracy and trust through automated review agents and human-in-the-loop validation.

{% youtube https://www.youtube.com/watch?v=5CkOnwmJkpQ %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/0.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=0)

### Introduction: Building an End-to-End Platform for Intelligent Media Analysis

 All right. Hey everybody, welcome to Friday morning at re:Invent. Made it this far, right? I also said I didn't realize we did sessions on Friday morning, but you guys are here. We're excited about that. We're going to talk about some really cool stuff today. My name is Brandon Lindauer. I'm a Specialist Solutions Architect focusing on media and entertainment workloads. We've got some great stuff lined up today. I just want to give you a quick primer, quick heads up. This is a 300 level session. We're not going to throw a whole lot of code at you, but we are going to throw a lot of concepts, a lot of deep concepts, and we're going to go through them really quickly. So the second or third time you watch the replay of this, you'll be good.

If there's something that you want to learn a little bit more about that we just didn't cover in depth, there are a lot of other sessions that were at re:Invent that get posted online. If you're watching online, hello from the past. So this session is not about a single point solution. It's not about a single service. We're not talking about OpenSearch or Rekognition in isolation. What we're talking about is building a complete end to end system, a platform that allows you to intelligently analyze, understand, and action on large media libraries.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/100.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=100)

So along with my colleague Robert Raver and Loic Barbou, we're going to cover Bloomberg's vision of the future. And here we go. We're going to cover Bloomberg's vision of the future. We're going to get deep into some analysis and creation of metadata on this media.  We're going to talk about how to ingest and organize petabytes of content. We're going to talk about how to build a hybrid search that understands both keywords and meanings. We'll get into a little bit of knowledge graphs, how to use AI agents to orchestrate complex content workflows, create content from that, and then we'll talk a little bit about where Bloomberg's at with this whole thing. So to hop in here, we got a lot to pack in. We'll just hop in. Loic Barbou from Bloomberg.

### Bloomberg Media's Vision: Data-Driven Insights Through Video Content

Good morning, everyone. My name is Loic Barbou. I'm the Head of Media Technology for Bloomberg Media. Raise your hand if you've heard of Bloomberg Media. That's a good amount, okay. And by the way, thank you for coming here this morning on the last day of the conference. As Brandon mentioned, we have a lot to pack in and there's a lot of knowledge that we want to share with you guys. On my side, I want to share the visions of Bloomberg and what we're doing in the world of AI and media.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/180.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=180)

 Bloomberg Media is a global news and business platform aimed at providing data-driven insights to business leaders. So we analyze the news, we extract all the information from the news, we curate it, and we distribute it in order for those business leaders to make decisions and navigate the complex global economy that exists today. And we do this by producing different types of content, text-based content like news articles, audio content through radio and podcast channels, and also video through our linear distributions of TV, through mobile, web, and social platforms. And that's for live content and video on demand content.

On the video side, that is an area that has the strongest area of engagement right now and that keeps expanding. The demand is very high. Just to put things in perspective, and that's why we're going to focus on the video side today, to put things in perspective, we have about 60 million unique viewers every month consuming those videos, with a reach of about 437 million households globally. And we're doing that through 48 major streaming platforms.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/280.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=280)

 Here's an example of two of our linear distribution channels. So the first one is Bloomberg TV. As you can see on the screen, it can be quite complicated. This is something that uses a lot of components to not only integrate videos, but integrate the news and provide all of those insights in real time to our users. The second one is Bloomberg Originals.

So this is a different type of content, long form content that can be seen as documentaries, but also highly in-depth analysis of topics that affect the world today.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/330.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=330)

### The Current Media Ecosystem: A Serverless Architecture for Dynamic Workflows

 In order to understand our AI journey, I'm going to talk a little bit about the existing platform that we're dealing with. So the current media ecosystem that handles workflows like having the ability to ingest content, produce content, manage content, transform content, and distribute content has been designed and implemented at Bloomberg. And to do it, we use different technology. We use services that are within our own network, but we also use a lot of cloud services, so it's really a hybrid global platform from a media workflow point of view.

We several years ago decided to invest a lot of time in order to figure out what we could do and how we could implement an ecosystem that will provide us the agility that we need to constantly handle changing business objectives and new requirements, but also align our distributions with the ever-changing platforms. And to do this, we created a serverless ecosystem running on AWS that really has the ability to leverage new components in a fairly easy way where we can introduce new services. The concept of dynamic workflow is also part of this, so we can really adapt our processing very easily in terms of the amount of content that we are dealing with, and also with the changes of these media workflows.

In our world, there's also complexity that comes with the many sources of content that we're dealing with. It's not just about distributing it, but it's about understanding where we get our content from. In fact, content can be ingested from live TV productions, could be contribution feeds, could be coming from raw content coming from an event, or could also be content that is already stored within one of our repositories, so that's what we consider historical content.

There is one thing that we have to implement with every workflow. We have to be fast. Nobody cares about old financial news. We have to be accurate, and we have to be trusted. So the selections of how we actually process our media, we make sure that we address every single one of those requirements in every decision that we make. And that's going to be the same for anything that we're going to be doing with AI.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/540.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=540)

### The Challenge of Applying Generative AI to Media: Archive-Driven, Not Artificially Invented

 Now let's talk about AI. So we've all used the ChatGPTs of the world, and we've all done some very cool things with it. But when you try to apply those concepts of generative AI to media, this is where things are a little bit more complicated. In fact, the statement that I made several months ago, I think, was not well understood by even a few of my peers at Bloomberg, but now it is clear that this is actually the only possible and viable way that we can move forward. The future of media AI is archive-driven, not artificially invented.

What does that mean? It means that a video of me doing a backflip on stage actually has to exist. If you search it, you're going to find it. It's not real synthetic content. This is not what we're trying to do as a media organization.

But what we're trying to achieve here is to use all of the content that exists, that we've ingested, that we've stored in our platforms to leverage data and extract the insights from those contents and assemble it in order to produce a compelling video that matches the specification of the distribution target. So that process is actually complicated because you have to put in place a platform that is capable of really understanding all of your content, processing your content, and dealing with it.

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/650.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=650)

 Our journey started by experimenting like everybody else, and we found some good stuff. When you start looking at some of those capabilities, you're going to see that this is really cool, it feels like magic. You can give a video to an AI model, and from there you can extract metadata from that media, who are the people in the videos, what are they talking about. That works well, but in the context of a single video, and as the video gets longer, things don't work. As the video gets more complicated, it loses context and the generated information may not be accurate.

There is a concept that is supposed to address that, and that is RAG. I'm sure you may have heard of it, and RAG is really the solution to say, hey, when you have a lot of data, you need to index and generate embeddings for that data, store it, and then all of your AI models and workflows will now leverage that information in order to produce something that is actually coming not from the outside world, but from your data itself. The problem with that is that when you try to scale this, it becomes extremely complex, as not all content is really created equal. The type of processing that you need to do on that content really changes based on your workflow, and you need to really re-engineer some of those workflows completely every time you're going to need to make a change.

In fact, the content that you indexed may not have the right information in order to achieve an output that you want to do later, so do you have to reindex everything? Well, when you do that for a lot of content, that becomes very expensive, that becomes very complicated. The other part that became clear to us is that as we're starting to design those media AI-driven workflows, you have to design the workflow, you have to implement the workflow, but you have to test the workflow, and that's something that takes a long time. By the time you're done testing it, there's a new AI model, there's a new way of implementing an AI-driven pipeline that is much better than what you've done. So how do you deal with that?

The last problem that we've seen here as well is that when you start implementing those complex workflows, you have to make a decision which vendor, which model you're dealing with. And when you see that the technology used by those vendors may not now provide what you need, it's extremely difficult to remove them from those workflows and from your ecosystem if you want to switch them into something else. So it's complicated.

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/880.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=880)

### A Disposable AI Strategy: Versioning, Federated Search, and Platform-First Thinking

 And what we decided to do is not to move forward in implementing something, but instead, we did an analysis of all of the moving parts that we think needed to be tweaked when a new AI model needed to be introduced, when we had to generate a new type of content, when we needed to extract information from content that didn't already exist.

We identified a couple of areas that are the dials we need to turn within our platforms every time we need to address one of those changes. The first one is versioning. We introduced the concept of versioning to models, embeddings, services, workflow pipelines. What does that mean? Well, it means that when you have a query and a prompt that is getting executed, within those prompts you can actually trigger specific versions and you can say I want to use this version over this one for a model or for an embedding. So there are multiple layers, there are multiple levels of embeddings that are stored with different versions that can be targeted through different pipelines and workflows.

Not only did we do that for versions, but we also did the exact same thing for quantifying the level of data that we extract as embeddings and that we generate through the different steps of the workflow. So there are workflows that are considered to be production, and there are workflows that I consider to be non-production within the same ecosystem. Now we can have multiple flavors of a given workflow that is generating production data and generating content and videos, but we have other ones that are not good enough or they have not done enough testing yet in order to get promoted to a real production output.

The fact that they exist together means we can actually tune those things as part of one platform, and we can easily promote or demote a certain part of the workflow. So it provides dynamicity and agility within the workflows where we can really bring in models, remove models, and tune processing pipelines in a way where we get total observability on the quality of the output, the performance, and the cost of each step of running those workflows.

The other concept that we put in place, and this one is very important, is the ability to deal with multiple databases of metadata and embeddings. The simple way of doing this is you generate and extract your embedding, you store that inside a database, a vector database, and then you basically use that for all of the workflows. What we ended up doing is we needed the ability to store multiple types of embeddings that could be created from an audio embedding model, video frames using vision AI, or even using the transcripts of the videos, so they're being stored into multiple repositories, multiple databases. There are different versions because we're using multiple models, so it becomes complicated in terms of the number of places where you actually store the data.

We created the concept of a federated search with one search service that has the capabilities of selecting the right database to pull the content. But when you think about deciding which database to use, that can be complicated. So we also have an AI layer there that analyzes the prompt, that analyzes the query that is coming to the search, rewrites the queries, realigns those queries to point to the right repositories, to the right database. It also has the ability to merge the output together, reprocess it, and return it.

So there's now that level of abstraction between all of the embeddings and the versions of embeddings and the metadata that can be semantic or even graph database, and the inference workflows that are using the data to actually create and generate content. And I know that statement, that the title of the slide may surprise you, but at the end of the day, that was really key for us as we realized that platforms matter, models don't. And what I'm describing here to you is something that we quantify as a disposable AI strategy. Before you introduce one of the services or one of the models, we really think about what we need to do, what it will need to be done when we need to remove it from the ecosystem. When you start thinking that way, the decisions you're taking are a little bit different.

Well, that's the concept. Now I'm going to transition to Rob who's going to explain to you how we actually achieve those goals.

[![Thumbnail 1240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1240.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1240)

### From Ingest to Creation: Building a Modular Architecture

Thank you, Loic. Good morning everyone. My name is Robert Raver. I'm a Principal Specialist Solutions Architect like Brandon, so I have the pleasure of working with customers like Loic to be able to build these solutions and platforms that enable our customers. I wanted to talk really quick about taking us on a journey from ingest to the end of creating  content. When we look at that, we want to break it down into these five buckets: How do we ingest content? How do we analyze it? How do we understand it? And then how do we create?

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1260.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1260)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1280.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1290.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1290)

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1300.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1300)

We're going to walk through this high level architecture.  You can see this resembles a lot of what Loic was talking about: loosely coupling together services, making it modular, being able to swap in new models, version prompts, and so on. On the left is really where we're going to start with ingest. We're going to go through this,  touch on it lightly, and show how to get content in. We're then going to look at how to analyze that content. How do we get  meaning? How do we extract information out of that? We're then going to look at how to understand it better. And then after that,  we're going to look at how to search it and also how to create new content.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1330.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1340.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1340)

When we're going on this journey, we look at agents in the top right, very similar to people. When agents need to search, they'll search just like you would. They will have the same experience. So let's hop in and talk a little bit about ingest first.  When you think of ingest, you want to make sure you account for the different options. How are you going to ingest file-based media?  How are you going to get that and land that in the cloud? How are you going to take live content, ingest it, and start to analyze it? What archives do you have that you need to bring in and also analyze? And then if you have third party feeds or contributions, how do you get that in?

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1370.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1370)

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1400.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1400)

Really what we want to look at here though is how do you create a funnel? How do you take all of those sources  and be able to get it into a central spot that you could trigger workflows from and analyze it? That way you don't have a bunch of unique snowflake kind of workflows, one-offs that you have to manage. You want to be able to make it efficient and have everything be analyzed the same. Once we get to that stage, we're going to talk about analysis a little bit. 

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1420.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1420)

### Three Approaches to Media Analysis: Task-Specific Models, Vision Language Models, and Vector Embeddings

When looking at analysis, from that last slide, we get content into S3 and then S3 sends an event saying, hey, I got a new file here, new asset. We're going to look at how to trigger workflows here. When you think of that, analysis on media has really changed over the past years.  If you look at this, there's three different types of analysis. There's task-specific models out there. These are your models previously like Amazon Rekognition or others. We have a new way of analyzing media, vision language models, or VLMs. And then we have embedding models. How do we generate embeddings and what do those mean?

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1450.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1450)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1460.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1460)

If we are looking at task specific models, these are models that are trained  and are looking for a very specific thing. They're looking at generating labels, looking at something that's predefined like a transcript.  If you look at vision language models, those are a little bit more free form. Those are where I could take an asset and ask a question, like describe this to me, not just tell me if a person is there, but tell me what that person is doing in natural language. They accept natively video, image, audio. When you look at all these modalities, you want to be able to analyze audio, video, image, even documents the same to be able to create that unified space.

Most importantly, these models often require no additional training to be able to do that. So it's zero-shot reasoning, meaning I could take my media, put it in there, ask a question, and get an answer back.

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1520.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1520)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1560.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1560)

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1580.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1580)

Then we look at vector embeddings. Vector embedding is  probably my favorite thing right now. What we have here is how do you take an image or video and transform it and understand it semantically, or a different way of understanding it, and represent it with numbers. If I create a vector, which is a flow of numbers, that allows you to compare meaning quickly. So is that just a dog, or is that a dog wearing a bandana on a beach? How do you do that? When we do that, we'll take things like text, images, and videos.  We'll put that into an embedding model that outputs these vectors, which is a bunch of numbers. The numbers themselves, if you look at them, aren't going to tell you much. But when you put that into the vector space, kind of a three-dimensional space, it starts to  create this 3D model of what is closely related or not. Is that a Yorkie over there? How close is that to a gorilla? You're starting to measure similarity and meaning.

[![Thumbnail 1610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1610.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1610)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1650.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1650)

When you look at that from a multimodality purpose, you can start to look at audio, compare it to images, compare it to video, and portions of your video. So the question is, if we have these three different ways of analyzing content  that have evolved over the past years, which one do you pick? There's a lot of options out there. But the short answer is all of them. You use each of them and use what they're best at to be able to create a rich understanding of your content that provides meaning. So what you do when you start building a media management platform is you're starting to create this unified architecture of bringing all of that together and correlating and relating that to surface insights. 

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1660.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1660)

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1700.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1700)

If we're looking at that, and then you take that to AWS Services and our partners, here's a couple of the services and models that do that.  You use things like Amazon Transcribe and Amazon Rekognition Video for labels or structural breakdown, shot, and scene. You can use Amazon Bedrock that hosts the models, our first-party models and our partner models, to do things like produce embeddings using Twelve Labs Marengo 3.0 or our new Nova multimodality embedding model to bring that together, keeping that content in there. So to look at that really quick, just to give you an idea of that, I wanted to kind of walk you through this demo of what that looks like. 

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1710.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1710)

So if I look at this, this is a previous session. You might recognize that person, guy maybe in the corner back there. If I start to play that,  how would you describe that? If I asked you what's going on in that video, how would you tell me what's happening? Would you say person, blue, hand? Probably not. You would talk about what's being seen, what's happening, providing that natural language.

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1750.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1750)

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1770.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1770)

[![Thumbnail 1780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1780.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1780)

So if you take that and you look at using a Nova model, this is a quick Jupyter notebook that I ran. This is using our Python Boto3 SDK. Really quick with Claude, which by the way, running with Opus,  a few sentences will produce this for you. Pretty neat nowadays. But this is how I described it. I asked, hey, let's see here. The prompt was, why don't you tell me about the second speaker in the video? Describe the second speaker in the video.  There was a speaker before Brandon, there was Brandon, and a speaker after Brandon, very similar to like here. This is what comes back.  It says the second speaker is a man with a beard, still true today, wearing a light blue shirt and blue jeans. If you look at the different ways, if you look at an embedding, it's just going to be a bunch of numbers. It's not going to tell you that. But what it is going to tell you is how similar it is to potentially a search term. If you look at labels, it's not going to tell you, it's not going to know who the second speaker is. It's not going to understand context or temporal timelines.

And that's really what we want to do, right? I want to start using each one of these methods, leveraging what they're good at, combining them, and doing it efficiently.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1820.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1820)

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1830.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1830)

### Hybrid Search: Combining Keyword and Vector Search with Dynamic Intent Analysis

So if we were going back  and we look at doing the analysis, we're now going to move to search. And so if you kind of look at this middle section,  this is really where everything happens, right? It's where you ingest, you analyze, you unify, and then you access it, and so particularly search and agents. If I look on the top right over here, if I have a conversational interface that talks to an agent and I ask it questions, it's going to use that. Or if I use a single turn search, it's still going to use that search API. That way you give the same experience to everyone.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1870.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1870)

Traditionally with search, people would use keywords, right? People might even use file names,  like they might get clever and put underscore IND331 and expect people to know that that's Brandon talking or myself. Didn't quite work out. So what we're really looking for is how to make it so people can find what they want in seconds and then understand what that is and use it.

[![Thumbnail 1900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1900.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1900)

But search is a tricky thing.  How I search for something versus how you search for something might be very different. How I describe something that's happening versus you is probably very different. And so search really needs to adapt. Search needs to understand you and what you're asking and not be static.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1930.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1930)

And so if you take that data that we're talking about, that analysis, you break that into three categories as well  of how to use it. Do I do a keyword search, right? Is that Robert Raver? You notice in that example I gave previously, I said, hey, describe that second speaker. Did it say that was Brandon Lindauer? No, it has no clue who that is. He's not that famous. Just kidding. I only wish I was as famous as you. But a keyword, right, if you tag him in it from a custom face perspective, keyword's going to match on that quickly. From a vector search, if you say, hey, find me people talking or presenting or on a stage, it's going to find similarity. You kind of want both, right? So that's why you come in with hybrid search.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/1980.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=1980)

So if you look at keyword search, traditionally it works off of data stores like OpenSearch.  That's going to use an algorithm and look at keywords in index fields, and it's going to look at frequency, it's going to look at closeness. Is it an exact match? Is it a fuzzy match? How often does it happen to give you that back?

[![Thumbnail 2000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2000.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2000)

If you look at a vector  and a vector engine, you're going to be looking at how do you take that, map that space out, and then query it. How similar is it, right? When you're doing a vector query, you're basically taking two vectors, those two numbers, comparing them and saying how close are they or how far apart are they. A Yorkie running downstairs compared to me on a stage, hopefully are pretty far apart, right, and they wouldn't be close.

[![Thumbnail 2040](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2040.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2040)

Now, when you look at that and you're finding similar content, what you're really doing is you're saying  here's my vector, right, a search term, a person on stage talking, and you're sending it to this vector space or this index of vectors, and it's going to give you a top K or a top number of results back. Similarity scoring, right? It's going to give you, hey, I have something super close to what you're asking for, or yeah, no, we don't have anything anywhere near that.

And typically when we think of it in multimodality models, like our Nova multimodality model, 12 Labs Marengo, some of the open source models out there, it's semantically understanding what it is. But it's important to note that embeddings are specific to the model and the meaning of those embeddings are specific to that model. Meaning it might be trained on semantics, the next one might be trained on my voice. Can I identify my voice from the heuristics?

[![Thumbnail 2100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2100.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2100)

So then we get into actually searching, right? We want to search this data. So I have a couple of search terms here, right?  Brian on a stage with two other people versus Robert talking about search techniques.

How would you rate those? How would you determine I'm on stage, Brian's on stage, or how do you know that I'm talking versus more than one person on stage? If you look at modalities, if you're going to say who is there or are there three people on stage, you're going to visually look and see. You're not going to listen to see if you could hear that. But if you want to hear me speaking about something, you're going to look at my transcript, you're going to look at what I'm saying. If I'm waving my hands up here, you have no clue if I'm talking about search or Yorkies. By the way, I like the Yorkies, I have a couple of them if you haven't guessed yet.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2150.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2150)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2180.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2180)

So what you want to do is to create a hybrid. The answer is, how do I  use search and break that down into search intent and be able to do that. OpenSearch has a way of doing that built in using an algorithm for ranking. You can also do that yourself at an application layer, really, and I encourage you to test with both. We're going to look at using doing that at an application layer, or a little middle layer.  If I say keyword, you know, Robert Raver, I'm going to weight the keyword much higher than a vector for inspiring movies. So what you want to do is you want to dynamically do that.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2220.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2220)

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2240.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2250.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2250)

If we look at our architecture in a little bit more detail, we have a search input box here. On the left we have kind of, you know, thumbs up, thumbs down to give feedback to help over time fine tune this. What we're going to do is we're going to put a search term in here, and it's going to send it to API Gateway. Lambda's going to answer it, and then it's going to take that  and say, here's the search term. What is the intent of that search term? What are they really asking for? So it's going to go to an LLM and it's going to say, hey, here's my search query. How should I look at this? At the same time, it's going to create embeddings for the data stores or the embeddings you have, and then it's going to be able to query those.  Bring those back and then really be able to give you a final answer. 

[![Thumbnail 2260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2260.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2260)

[![Thumbnail 2290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2290.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2290)

So if we were to look at that, here's another little Jupyter notebook I have, and I did something similar. So up here,  we have a prompt. It's the prompt that was just created. I'm not saying this is the best prompt in the world for it, but going back to Louis's point, this is what you want to be able to version, improve on, iterate on, measure. It says, hey, I have these data stores. Analyze the search term, bring back a weight, and tell me what I should pay attention to. So if you go back to three people on stage with two other people,  which doesn't really make sense how I look at it, but what it's actually looking at is down below you'll see the LLM weight analysis. You see visual is a 0.9. It understands that I'm going to visually look to understand how many people are on the stage. And you know what, there might be specific people, so I'm going to give it some weight, but from an audio perspective, that's not going to really help me. So I want to ensure that I'm searching the right areas to be able to do that.

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2330.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2330)

Now if you go back to that other search term we had, Robert talking about search, you see how that changes.  People goes up. It's looking for a person named Robert. Transcript goes way up. It knows that I'm looking for something about me talking. And then audio in this case is audio background. You know, somebody making a loud noise, car going off, whatever. It's like, okay, well, no, we actually really want to understand somebody talking.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2360.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2370.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2370)

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2390.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2390)

### Knowledge Graphs: Creating Contextual Understanding Through Relationships

So once we get there, we really want to understand what we have,  right? So if you do that, you get a bunch of results back, but that doesn't necessarily mean you get true contextual meaning as to what the search results are.  If it gives me a bunch of results back, say 100 videos of me talking, how do I make sense of that? How do I understand how those are related? And so we look at knowledge graphs, then. So we use these purpose-built data stores to be able to then relate content. 

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2400.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2400)

And there's a few different ways to, you know, you can use graphs. Ultimately what a graph is though, is you're taking these two nodes,  a person, an asset, and you're creating these relationships. And there's different ways you could do that.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2410.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2410)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2430.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2430)

You could do that temporally, like what's  happening over time, or by identity, domain, and so on. What we're going to do is split that data, and then we're going to use that graph, in this case Neptune Analytics, to be able to do searches and create meaning. When we look at that same analysis, that same data we produced earlier,  we're putting it into the graph as well as OpenSearch, and we're searching it in different ways. These are different data store types, different ways of searching to get meaning out of data, but you can use those same embeddings.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2450.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2450)

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2470.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2470)

Now you can start to correlate. If you look at this, you start to build this graph.  There's a person that appears in an asset, the asset is created from an asset, there's a story that happens in a place. When you start to do this, you create understanding. If I have 100 assets, but let's say I actually created a new asset from 4 other assets, and then I created even  more assets from that because I'm sending it to YouTube or TikTok, you get to understand that those 7 assets are actually really the same thing. You just created a new asset. If I have search results and 7 of them are the same thing, I probably want to identify that and make it so you know it and be able to say, hey, these are really the same asset or derived from each other, here's the unique ones.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2500.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2510.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2510)

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2560.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2560)

What we do  is we take the data, that analysis, we populate that knowledge graph at the same time as our OpenSearch, and then we query it the same way.  When you're looking at that, when you populate a graph and it's relationship based, you're taking that and we're doing something called creating a virtual entity. A lot of people will search for content and look at what's in the content and expect to say, here's 100 assets and that has this person in it, that person in it. But in your mind, do you think that way, or do you really want to say, I'm interested in Rob as a person, I want to actually then go to Rob and understand what information I have about Rob, what assets he appeared in, what was maybe recreated. You start to take that same data and create virtual entities in this graph,  which are those nodes, and start linking it together.

[![Thumbnail 2570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2570.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2570)

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2580.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2580)

[![Thumbnail 2590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2590.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2590)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2610.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2610)

You create  this entity that starts to understand my voice, starts to understand maybe my face, other characteristics, and you build those all together. When you look at that,  if we go back here  and look at it, we created this sample graph we want to look at. It's fictitious information, looking at two summits that might have happened, tech summits here, maybe one in the spring and maybe one in the fall. But you start to understand what were the  keynotes saying, what were the sessions saying, what were the stories being told.

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2620.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2620)

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2630.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2630)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2640.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2650.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2650)

When you start to look at this,  instead of just seeing a single bit of information, a single asset, I can start to understand what topics were talked about,  what assets relate to those topics, what were the connections here, who was saying that.  When you start to look at that, you get to the point where you start to be able to understand what's in your assets, how that's connected, and how things change  over time or how are they related. In this instance, using that graph, I can not only say Rob and Brandon and Loic, I have them in my assets and they might have been talking, but I could say who's returned speakers between my events, what is the topic of evolution, how does that talk track change. In 2024 they're talking about RAG, in 2025 maybe GraphRAG, and then you can start to see how stories evolve.

Once you start to understand how these stories are evolving, what do you have? You can start to use that to create new content, to create new stories. We want to spend less time searching, less time doing this manual work of trying to understand what we have, and have that context surface to you quickly

and let you use it how you want to create quality content, stories, and search. With that, I'm going to pass it back to Brandon.

### Automated Content Creation: Using AI Agents to Expand Reach Across Multiple Platforms

Thanks, Rob. That was a quick but deep overview of a lot of the technologies behind searching. But in reality, it's not just search. Nobody wants search. You don't want to pay for search. You want to pay somebody to search. You want to find, and that's the technology that lets you find what you need, lets you correlate things across multiple entities, events, things like that. But finding data is only part of the story. If you can find content, great, you found it. But if you can action on what you've found, that is where the real value comes in.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2770.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2770)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2780.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2780)

Rob and I sat down with Loic and we said, hey, let's talk about creating content. What does that look like for  you? There's a lot of things that you could do with that metadata that we generated with the different ways to search and find it. There's a lot of things we can do. There's a couple things that I'll highlight, and we're going to dive into  one of these. When you automatically create content from this, think about if you have thousands and thousands of hours, the petabytes of data that Bloomberg has in their archive. Traditionally, you have somebody sitting there that has to just go through all that manually. If you're able to find that, and you're able to find it automatically, suddenly the things that you can do, putting stories together, creating new stories, is amazing.

We're going to dive a little bit into the expanded reach though. Loic gave us this example. Imagine you have a 30 minute interview with a prominent actor. Great, you got to sit down with them. You get this great interview, all of this content. Two minutes of it makes it to air. Traditionally what would happen is, well, that two minutes made it to air, take the rest of that 30 minute interview, throw it in the archive, maybe one day we'll be able to use it.

Now in recent years there are lots of platforms. There are lots of ways to expand the reach. You can post it to all kinds of social media on the Facebookgram or the Insta Snap or whatever they are, whatever the kids are doing these days. And they all have different ways that you reach people, but that requires a little bit of curation of that content and there's actually a lot of labor associated with that. So if we want to expand this reach through YouTube, TikTok, Instagram, things like that, we're going to look at this automated process.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2880.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2880)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/2920.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=2920)

It's going to start with a user request,  your natural language query. A producer goes in there and says, hey, we have this 30 minute interview. Cut this down and make it available to these platforms. Make it available to our standard channels. Using things like agents, using parallel execution, assembling those things together and even allowing humans to review it if you want will give you this fully automated pipeline for content creation. So I'm sure agents is a word you haven't heard at re:Invent this year,  and AI.

This is what we imagine it looks like. If you say I want to do an automated content assembly and creation, your user comes in and says, hey, make this 30 minute interview available to our various social platforms. Well, YouTube, it's great, we can post the full thing there. Two minutes of it already made it to air, but we have platforms that want a 12 second version, they want a 30 second version, a two minute version. We may have multiple 12 second cuts that we want to do that reach different audiences or that get across different topics. That's a lot of stuff there.

So we start with an analysis, summarizing and understanding the metadata. That's a lot of what Robert went over, understanding all of this stuff, searching, understanding what the user's intent is when they go through that. At the same time, as you understand those and you understand what you need to find, what you need to search for, you can have a selection process going, a selection agent that's going to go through and it's going to find the different portions of this video that are useful in 10 seconds, in 30 seconds. This is your editor. This is the editor sitting there going, this is useful, this is the part that I want to use.

The assembly is also part of the editor, but this does the work of putting these things together. So it's one thing to say, cool, I have a five-second chunk here that I want to show. I have a 30-second clip that I want, or I have these two things that I want to put together as a 30-second clip. I might have different angles. We did an interview with this actor and we had two cameras shooting them. You're going to ingest those as two different files, they're going to be indexed separately, but your entities, your search, and everything that Robert just covered is going to be able to say, yeah, I'm correlating these two clips, these two portions, as the same thing.

And you can have a selection agent, an assembly agent look at that and go, you know what, this angle is better right here, this angle is better there, based on parameters, input, all kinds of things that you can give it. It puts it together, and then you take that output and you run it through and you get to another agent and you go make sure this meets our standards. Make sure that we're not making this actor look bad, make sure that we are not saying anything we shouldn't say, make sure we're not violating our content guardrails or not changing what's in here. I mean, I've worked in video for a long time. You can very much take a clip of somebody out of context.

As Loic said at the very beginning, trust is key for Bloomberg. It needs to be accurate, and they need the public to trust them. So having a review agent say, let's make sure that everything's good before we push this out. And in that area, you can also put in a process for a human in the loop. Hey, we want a person to look at these before everything. We want a person to look at it and get back a score from the agent that says, hey, we're flagging this, or these all look good to us based on these scores. All these things are possible.

And lastly, push it out. You can do this completely automated, starting with take this interview, make it available, and you can have the agent push it out onto all your portals. Now, something like that, you don't always have to use an agent. Let's not use AI where an if statement will work, but there are certain situations where you'd say, hey, let me use an LLM to decide if this should even go to YouTube or if this should go to TikTok or if these channels are better. Provide that feedback to the humans in the loop.

[![Thumbnail 3170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3170.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3170)

So as we get into that content selection, your content selection can actually be multiple agents.  You can break this out and have all kinds of things going in parallel. Find me the segments of that video. They can coordinate as needed. You can sync them together, you can find the clips. The assembly agent may communicate back and say, hey, I've got 25 seconds, and my target is 30 seconds. Can you find me some fill? I need some B-roll that's related to this. Got all those graph relations. Cool, let's find some images, some cutaways, some video, audio, something that is related to everything else we have going on.

Have those agents communicate with one another and automatically have this iterative process where they're collaborating. And again, you can always insert human in the loop there. And one of the coolest parts about this is because all of this is going on between these agents, because all of this is happening in AWS with Bedrock, you can take the output from each of those. You can save it with complete lineage so you can say which angles were used in this, why were they used, what was the score, the model score that shows this, which platforms were targeted with this cut down. You can get all that information and you have that full provenance of why did we do this the way we did.

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3270.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3270)

And understand what the model is doing and continue to improve and continue to tweak because again there's no final one way to do this, there's always room for improvement. Your multi-platform assembly, I mentioned  different platforms. You've got nine by sixteen, you get sixteen by nine, you've got one to one. You've got things with text overlay. Sometimes when you take a wide shot and you verticalize it, you need to follow the action. Sometimes it is a wide shot and you need to bring it down and throw a blur or something in the background, so you have the top and bottom that aren't just black.

There's different methodologies and different ways to do verticalization or different platform targeting, and you can use LLMs for that.

You can use these tools to decide what should be framed where and determine what will look better. You can decide whether to do a cutaway here. Ultimately, it's about creating what's right for the right platform, so you're not just doing one cutdown. You're not just doing a 32-second cutdown, you're doing a 32-second cutdown and versions for multiple platforms.

[![Thumbnail 3330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3330.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3330)

Like I said, it starts with a user request.  You can do the orchestration with your agents, execute a lot of this in parallel, assemble those results, and you can get humans in there. This improves your speed, your volume, your consistency, and your traceability. The fact that you can now reach more audiences on more platforms faster than ever before and you can do so in an automated or mostly automated fashion is incredible.

[![Thumbnail 3360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3360.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3360)

[![Thumbnail 3390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3390.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3390)

 Rob showed this earlier in the presentation. We started with ingest, we go to analyze this data, we search it, we understand it, and we action on it with creating content. So we've walked you through all of this, and the cool thing is that really opens up new revenue streams. Because you can build this on AWS, you can do this in a well-architected manner. It can be built and deployed  highly available, highly resilient, secure, and scalable with insights, controls, and monitoring.

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3430.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3430)

### Bloomberg's Journey Forward: Unlocking 13 Petabytes of Unstructured Video Data

I'm going to bring Loic back up here. He's going to explain to you a little bit more about what Bloomberg is doing with all of that that we've gone through. Thank you. Thank you to Rob and Brendan for having very well described how we're doing certain things and also the media workflows that will be leveraging this new  technology.

I want to take a step back to reiterate the goals here. Now you understand the complexity that we're dealing with. The goal of doing this is really about having the ability to create a platform that will be able to handle all of the business of the new business objectives and requirement changes, but also to handle all of the evolutions happening within AI. This is something that is complex, it's complicated, there's a lot of moving parts. It's not possible to re-engineer the whole thing tomorrow when something better is happening, so this is really the foundation of all of the work that we're doing here. Yes, we're going to be able to do all of that and we're going to be able to change it.

This platform is also going to unlock a couple of things for us and optimize how we manage and create content today. Number one, it should allow us to drastically reduce the time to market by improving discovery, management, and distribution of content. The second thing is to unlock new distribution targets. It's complicated to create a new format to target a platform and to do it well. Sometimes you have to really manage your workflow using least common denominators and push content that may not be the best for those platforms, but you still have a presence on those platforms. Well, using this, we will be able to create content that really fits those platforms and that can provide better monetization on these platforms, and we discussed that.

Everything that we've been discussing so far is done by humans. We're not replacing the humans, but we're really helping them in not spending time on things they shouldn't be doing. I kept the best for last. There is one thing that we're going to be able to do with this platform that is not possible today. It was said in a keynote, in the infrastructure keynote yesterday, that 90% of unstructured data is video. If it's unstructured, it is not well understood. With this technology, we will be able to really analyze and generate insights from 13 petabytes of existing media that is growing at the speed of 3,000 hours per day.

It's not possible for humans to analyze all of that, so we will be able to analyze real-time news, correlate that with everything that exists within our repositories, and create a new type of story or be able to tell the stories in a much better way than we can do today.

[![Thumbnail 3630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3630.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3630)

 I would like to thank AWS to have joined us in this journey. When I went to them and explained the concepts, and that's really, there was a concept, they didn't shut the door on me. But what they did was, well, you know, what you're trying to achieve is something that we cannot do today, but if we work together and we use the AWS technology and we shape it in a way that it's aligned with your visions, we will get somewhere. And this is exactly what we're doing here, so thank you. And I hope to see you soon with the evolutions of what we're going to be creating here.

[![Thumbnail 3700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/5cf3b7acb38e132e/3700.jpg)](https://www.youtube.com/watch?v=5CkOnwmJkpQ&t=3700)

Folks, thank you again. Thanks for being here Friday morning early, early. You guys are rock stars. There's some more resources right here if you guys want to check out some of the learning. If there is anything in this that you want to dive deep in, the re:Invent sessions will be posted soon.  You can level up your skills with AWS Skill Builder, so do that. And we will be hanging out here if you need anything, giving out free high fives, reminding you what to give us on that survey. So thank you everybody.


----

; This article is entirely auto-generated using Amazon Bedrock.
