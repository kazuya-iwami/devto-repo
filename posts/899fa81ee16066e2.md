---
title: 'AWS re:Invent 2025 - From Documents to Decisions: Unleashing AI-Powered Business Automation (SMB204)'
published: true
description: 'In this video, David Mefford from AWS and Priya Iragavarapu from AArete discuss how they built Doczy.ai, a generative AI solution that extracts metadata from healthcare contracts. AArete evolved from manual processing (100 documents per week per person) through AWS Textract with rules-based engines (50-60% accuracy) to a generative AI approach using AWS Bedrock with Anthropic Claude models, achieving 99% accuracy and processing 500,000 documents per week. The solution processed 2.5 million documents and 442 billion tokens in 22 months, delivering $330 million in client savings through identifying claims overpayments and reducing manual effort by 97%. The architecture uses Next.js, AWS Cognito, S3, Lambda, Textract, ECS, and Snowflake, with plans to launch as a SaaS platform for healthcare payers'' business process automation.'
tags: ''
cover_image: https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/0.jpg
series: ''
canonical_url:
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - From Documents to Decisions: Unleashing AI-Powered Business Automation (SMB204)**

> In this video, David Mefford from AWS and Priya Iragavarapu from AArete discuss how they built Doczy.ai, a generative AI solution that extracts metadata from healthcare contracts. AArete evolved from manual processing (100 documents per week per person) through AWS Textract with rules-based engines (50-60% accuracy) to a generative AI approach using AWS Bedrock with Anthropic Claude models, achieving 99% accuracy and processing 500,000 documents per week. The solution processed 2.5 million documents and 442 billion tokens in 22 months, delivering $330 million in client savings through identifying claims overpayments and reducing manual effort by 97%. The architecture uses Next.js, AWS Cognito, S3, Lambda, Textract, ECS, and Snowflake, with plans to launch as a SaaS platform for healthcare payers' business process automation.

{% youtube https://www.youtube.com/watch?v=kzglciudj1I %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/0.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=0)

### AArete's Challenge: Unlocking Data Trapped in Healthcare Documents

 Thank you for joining everyone. I'm David Mefford, AWS Commercial Sales Leader, based out of Chicago. I have the luxury of building teams to help our customers build solutions, and I'm excited to highlight one of those today with Priya here. Today's session is "From Documents to Decisions." AArete lives in the healthcare and financial services space and serves as a consulting firm helping customers extract data from documents and make decisions through their consulting firm and consultants. They face a unique challenge in getting that data out of the documents, and that's what we're here to discuss today.

Without further ado, I want to introduce the leader of the program, Priya. Thanks for coming to re:Invent. Welcome to your first re:Invent, and we're looking forward to hearing more about AArete and your problem. Thank you so much for having us, Dave. This is fantastic. Are you all having fun here? I feel like I'm a kid in a candy store. Exciting things are happening everywhere.

I'm Priya Iragavarapu, Vice President of Data Science and AI at AArete. AArete is a global management and technology consulting company headquartered in Chicago. We help healthcare payers, the health insurance companies primarily. We process their claims and help them understand how to improve their medical and administrative cost reduction. We also help with member engagement, improve member retention, and assist them through care coordination and care management. We also have a financial services practice, and I lead the AI practice that serves both industries. I'm really excited to be here and share our success stories.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/80.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=80)

Our success stories essentially involve finding a business problem and challenge, then applying generative AI to solve that problem at scale. That's really the story we want to share here today.  The challenge we have solved for is that data is locked up in documents. We have processed a lot of provider contracts, which are contracts between health plans and providers such as hospitals and physicians. These contracts detail all the payment terms and contractual terms. We have also seen member and vendor contracts that process for organizations through vendors with various SLAs, KPIs, and other metrics. The data in those contract documents is locked.

[![Thumbnail 140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/140.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=140)

In order to get it out, the current options essentially fall short. You need a lot of institutional knowledge, and you have to throw a lot of people at it. Anytime you want information extracted from documents, you need all of those additional resources. It's not scalable. If you have more documents, you need more people to do the work. Additionally, the contract lifecycle management solutions and products that are out there don't do a much effective job because they only cater to configurations for a few fields. For many other fields, there's no process for contract lifecycle management tools to incorporate them.  What happens is really a lot of friction and downstream use cases. If you want to use that data in documents for analytics, aggregated metrics, KPIs, and insights, you're out of luck. That's the challenge we have solved for using Doczy.ai.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/180.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=180)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/240.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=240)

 Our Doczy.ai is a metadata extraction and interpretation tool built on generative AI. Simply put, what it does is look at the document, understand it like a human would, and extract the information from it into a structured repository that is then usable for downstream needs. The primary uplift is in terms of removing subjectivity. You need a lot of people to do all the work, but different people with different experiences would do it differently. By using this technology, we've been able to eliminate that variance and subjectivity within humans. Every document is processed the same way across the board by a large language model. That's the primary benefit, and the scale of course is the secondary. 

The secondary benefit is that you have extracted that data and then you can use it. It's aggregable, queryable, searchable, and you can use it for downstream needs.

### The Evolution from Manual Processing to Generative AI: A Journey to 500,000 Documents Per Week

I know we're talking about the generative AI portion of it now, but like any good story, there's a start, middle, and end. You all have been on an evolution of trying to figure out how to get this data out of the documents. So talk to us a little bit about the journey you all went on at AArete to get to this point.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/350.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=350)

Definitely, Dave. That's a perfect segue because our journey did not happen overnight. We wanted to definitely start off with prior to 2020.  We were doing this manually. We had 17 years of ontology experience doing this manually. And obviously our consultants were not too happy with it because it's mundane work. They wanted to be able to do something more effective and more strategic with the clients. But the scale of that was we were only processing 100 documents per week per person using that manual approach.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/450.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=450)

Then in 2020, we decided we needed an accelerator for this because it's not really ideal to just put human resources on this problem. So we used AWS Textract as an OCR toolâ€”by the way, that's the best OCR tool out there if you're wondering. We used that to convert PDFs to text and then wrote a rules-based engine on top of it. We basically said go to this page, go to this line, grab these characters, and for this field put it into this JSON structure. So we created that rules-based engine, which worked for quite a good time. But then we also had a challenge with it. The challenge was that not all documents are created equal. If it followed a certain template, the results would be consistent. If the documents had varied levels of information at varied locations, we were out of luck and had to again go back to human resources for solving that problem. So it was giving us essentially 50 to 60 percent accurate results. 

With the advent of generative AI in 2024, we did rapid prototyping and said this is the perfect candidate for using generative AI for solving this problem. We actually got very positive results and our impact at that point was very significant.

I think it's great to see the construct and where you were going and the idea of getting the work off the consultant's plate. It was really low value to put them back in front of your customers. This is great. I think the numbers start to speak for themselves in terms of what you were able to deliver. So can you walk us through the numbers and what you saw as a business?

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/500.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=500)

Absolutely. As I mentioned, we were processing 100 documents per person per week, and we went up to actually 500,000 documents per week.  This is actually a lowball number, but the scale that we were able to process using our generative AI and Doczy.ai approach was tremendous. This is something that we could not have otherwise done if not for the technology, along with the ontology and the domain expertise that we incorporated into it.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/540.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=540)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/560.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/570.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=570)

But then, who would want a solution or a product that is not accurate?  It doesn't matter if you're able to do it at that scale. You need to be accurate, and we were able to get it at 99 percent accuracy level, which again a lot of our clients and our team members were extremely ecstatic with.  This also created a 97 percent reduction in the manual effort that was going into it from our past experience.  All in all, it enabled a lot of savings for our clients. We were able to get 330 million dollars of savings for our clients as a combination of direct and indirect savings.

The direct savings for our health plans was that we got the reimbursement information from the provider contracts and then compared it to the claims and identified overpayments. That's a direct recoupable opportunity for the health plans, and that's the direct savings that they were able to achieve.

From an indirect savings perspective, it's all of the human resources they have avoided in order to do that kind of work and the scale at which it was done. If Doxy wasn't that accelerator that was used, then they would have to use a lot of manual team members for this work, so that's really the savings we have seen.

### Building Doczy.ai on AWS: Architecture, Scale, and the Path to SaaS Integration

I appreciate how you shaped up the business problem and now you're seeing clear returns. The business is excited about new AI products through your team and what you're capable of delivering. I know this is often a technical audience, so can you walk us through the solution and how it's architected?

[![Thumbnail 650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/650.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=650)

Absolutely. By the end  of this year, we're also launching a SaaS platform on Doxy AI. That's a great achievement for us. We also submitted for patent, so it's patent pending. We've put all of our eggs into AWS baskets, so to speak. We've built our end-to-end solution on the AWS platform.

Our external users log into a frontend interface built on Next.js, authenticated by AWS Cognito. When they upload documents, they get stored into an S3 bucket. From there, it invokes a Lambda function that calls the Textract service to convert PDFs to text. Textract identifies tables, forms, checkboxes, signatures, and even handwritten notes, converting all of this into text within the Textract process.

That output data then gets processed using a containerized service running on ECS. We strategically chunk the document text and include our prompting, which is built on our seventeen years of ontology and domain expertise. Together, we make API calls to AWS Bedrock. We use Anthropic Claude models 3.5, 3.7, and even 4.0. We're very heavily invested in AWS Bedrock and were one of the top consumers of AWS Bedrock until recently.

From there, the output gets stored into a Snowflake database, which is then usable by the front-end users from our interface. I think a couple of things to point out here for the audience is that you've used four to five models and been able to swap those out when you found better returns and cost performance. The other thing is you work in a heavily regulated industry. You were able to put this in your own VPC in your own environment. For particular clients, high trust was a big issue, and you're able to accomplish all of that in the solution.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/830.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=830)

By the numbers, I think it's interesting to look at because everyone now knows what a token is and is familiar with the size. Talk us through what the actual numbers look like and the outputs of how you're leveraging this architecture. Absolutely, the numbers really speak for themselves. We've been able to process about 2.5 million documents  using our tool, which amounts to 442 billion tokens in just the last twenty-two months. The scale at which we were able to do this would not have been possible without Dave and his team, Brett, Sanketh, Garish, and everybody here.

It wasn't something we achieved in one go. We would constantly keep hitting the tokens per minute limit and also the rate per minute limit. We would keep hitting it and then work with Dave and the team to help increase those numbers. Finally, we've gotten them to a pretty high level so that we're able to process things at the scale and keep up the delivery that we had to do for our clients. Absolutely, great success story. We wouldn't have been able to scale without the Bedrock platform that is built by AWS and also this partnership. There's not a vendor-customer kind of relationship, but we're together in it to help and deliver. So very thankful to Dave and the team.

We appreciate the trust that AArete has put into AWS. I think what's interesting now is you have a solution that is in a consulting world living and breathing.

When you start to look at your health plan customers and how they can consume Doxy.ai, how do you see this product changing your business and how you serve those customers?

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/899fa81ee16066e2/930.jpg)](https://www.youtube.com/watch?v=kzglciudj1I&t=930)

Absolutely. Our product is not meant to be a once and done kind of thing. Anytime you process documents  through Doxy.ai, there are contract documents, but then you go back and renegotiate and do amendments on top of it. These documents are living and breathing and changing over time. What we want to do with Doxy.ai is make it an integral part of business process automation so that it lives within the ecosystem of our healthcare payers and their environment.

That way, each provider contract or vendor contract, when they are written and signed, gets processed through Doxy.ai to extract the metadata from it. Then we proceed towards either claims adjudication, claim accuracy, or even invoice accuracy and pricing at that time. We want this to be a living and breathing thing within our clients' ecosystem, and that is our goal and where we are heading through our SaaS platform and subsequent integrations through API that we are going to build within our payer ecosystem.

The other thing we have noticed is that it has unlocked other opportunities for vendor management and other contract management opportunities that are maybe outside of the core health plan space. You are able to move quickly now, and you have the confidence in the business. Your customers now have the confidence in you as well.

Thank you, Priya, for walking us through the solution, the challenge, and how you and Reed and AWS partnered together to solve this problem. Enjoy your first re:Invent. We will be down to the side here, and Priya will be signing autographs. Thank you all for coming. Thanks, Priya. I appreciate it. Thank you so much.


----

; This article is entirely auto-generated using Amazon Bedrock.
