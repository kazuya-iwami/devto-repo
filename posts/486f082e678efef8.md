---
title: 'AWS re:Invent 2025 - Best practices for serverless developers (CNS403)'
published: true
description: 'In this video, Julian, a serverless developer advocate at AWS, shares serverless best practices through P√¢tisserie Am√©lie''s journey from 40 to 320,000 monthly orders. He covers critical topics including right-sizing Lambda functions, transitioning from synchronous to asynchronous architectures using EventBridge and SQS, eliminating unnecessary Lambda functions through direct service integrations and Step Functions, optimizing performance with SnapStart and Provisioned Concurrency, implementing proper error handling with DLQs and idempotency, and cost optimization strategies. The session introduces new features like Lambda Managed Instances, Lambda with Durable Functions, and demonstrates integrating AI capabilities using Amazon Bedrock Agent Core. Julian emphasizes practical patterns like Event Source Mapping configuration, filtering to reduce costs by 92%, and achieving 400ms processing times down from 6 seconds through architectural improvements.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/0.jpg'
series: ''
canonical_url: null
id: 3088292
date: '2025-12-06T07:11:06Z'
---

**ü¶Ñ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


üìñ **AWS re:Invent 2025 - Best practices for serverless developers (CNS403)**

> In this video, Julian, a serverless developer advocate at AWS, shares serverless best practices through P√¢tisserie Am√©lie's journey from 40 to 320,000 monthly orders. He covers critical topics including right-sizing Lambda functions, transitioning from synchronous to asynchronous architectures using EventBridge and SQS, eliminating unnecessary Lambda functions through direct service integrations and Step Functions, optimizing performance with SnapStart and Provisioned Concurrency, implementing proper error handling with DLQs and idempotency, and cost optimization strategies. The session introduces new features like Lambda Managed Instances, Lambda with Durable Functions, and demonstrates integrating AI capabilities using Amazon Bedrock Agent Core. Julian emphasizes practical patterns like Event Source Mapping configuration, filtering to reduce costs by 92%, and achieving 400ms processing times down from 6 seconds through architectural improvements.

{% youtube https://www.youtube.com/watch?v=tMvwM_KXNPA %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/0.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=0)

### Introduction: Following P√¢tisserie Am√©lie's Serverless Transformation Journey

 Excellent. Well, good afternoon, everybody, and welcome to re:Invent. Everyone having a good time so far? Enjoying the keynotes this morning? Some cool things. Excellent. Well, welcome to best practices for serverless developers. But you know what, actually forget the serverless part. This is actually for all developers building in the cloud. Serverless is really the default way to get the most out of the cloud when building applications.

My name is Julian. I'm a developer advocate here for serverless, specializing in serverless at AWS. We've got lovely people in the room. We've got some overflow rooms in other parts of Las Vegas at other hotels. So hello to you all virtually. And if you're watching this from the YouTube recording, which will be posted shortly, I suppose hello into the future. So thank you everybody for joining me.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/40.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=40)

 Today I'm going to take you through a story that actually mirrors the serverless evolution of thousands of companies. And in this talk, we're going to follow P√¢tisserie Am√©lie's transformation from a single p√¢tisserie to international success. And I promise you there's nothing that I like more than a delicious pastry. It's absolutely my weakness and I'm just thinking I should have actually brought one now. So the serverless best practices I'm going to teach you is going to help to enable this incredible growth and understand the real-world scaling challenges that every developer is going to face.

[![Thumbnail 80](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/80.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=80)

And you can see the evolution from just getting something out there to a mature serverless architecture. You can see here, five years of growth, both business growth and architectural  growth and evolution. Each phase brought new challenges and also there were obviously lessons. Notice the dramatic scaling from 40 orders to 320,000 orders per month, a huge scale, and the architecture had to support this growth, and we're going to see how serverless patterns help this transformation.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/100.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=100)

### The Evolution from Lockdown Startup to International Success

So things started.  Am√©lie opened during lockdown following her absolute baking passion. A single Lambda function handled everything, just 200 lines of code, basic DynamoDB table for database storage for orders, and then manual payment processing via phone call. You know, 40 orders per month from local neighborhood customers, a great sort of minimal viable architecture with basic Lambda functions, great way to get started. Obviously, the pay per use model is perfect for uncertain demand and you know, zero infrastructure management during a crisis was perfect as well.

[![Thumbnail 130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/130.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=130)

2021, the simple architecture still holds,  you know, with some splitting of Lambda functions for payment and inventory tracking, a good idea. And then, you know, the first scaling challenges start to emerge. Lambda timeouts during the morning rush, so a performance degradation during peak hours, and you know, something that's always hard to troubleshoot and team coordination issues too.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/150.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=150)

[![Thumbnail 160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/160.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=160)

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/170.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=170)

2022, more locations, many more orders. Now 42 Lambda functions,  complex dependency chains and not enough error handling. The morning rush, cascading failures, spiking customer complaints, performance suffering, six plus second order  processing. That's crazy. Three percent failure rate during peak hours. Oh no, a breaking point that forces architectural transformation. And so with customers getting frustrated  with the slow service.

[![Thumbnail 190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/190.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=190)

And 2024 was the year to get down and fix things. A proper serverless architecture sort out, bringing in Step Functions for orchestration, EventBridge for events, SQS for reliable processing, and reducing 42 Lambda functions down to just 10, six seconds down to 400 milliseconds processing time, and a foundation set  for some cool international expansion.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/210.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=210)

And it all made a difference for global expansion. Sixty locations across Europe and North America, 3.8 million orders per year with great performance and reliability. Handling 40 times the holiday traffic spikes seamlessly and a 35 percent cost reduction despite three times the growth. Am√©lie's passion project  goes to international success.

And so we're going to follow Am√©lie's transformation through a number of different areas, show the problems, some solutions, and some of the lessons learned. And I'm going to give you actionable insights that you can apply to your own serverless applications. Now, I'm going to warn you there's going to be a lot here. I'm going to go both broad and deep, so there's going to be a huge amount of info. And I've included this QR code, which is a link to a handy resources page with the slides from the presentation and plenty of other reference links. And I'm going to do the QR code at the end, so if you miss it, don't worry.

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/240.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=240)

[![Thumbnail 250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/250.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=250)

 And also if you are watching this on the recording, this is probably not one of those talks that you want to watch at double speed. Maybe half speed is going to be a little easier, but I'll leave that up to you.  A lot we're going to cover.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/260.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=260)

### Sizing and Organizing Serverless Applications: From Monolithic Nightmare to Right-Sized Functions

So, let's start with the foundations, how to size and organize your serverless applications. By 2023, what started as simple order processing had become a monolithic nightmare.  A single Lambda function handling orders, validation, notifications, analytics, and a whole bunch of more stuff. Deployment time stretching to five minutes. Debugging is, I suppose, like finding a croissant in a bakery full of pastries and then a holiday rush incident, a single bug in the notification service brought down all the order processing. That is bad.

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/290.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=290)

And this crisis forced them to learn proper Lambda sizing principles. And there's some universal principles that do apply. Each Lambda function should generally have one clear responsibility,  avoiding a complex do it all Lambda functions. And what you want to do is actually size it for what it needs to do, not just for your convenience. Partly, we'll get to that. But Lambda allocates also CPU memory proportionally to memory configuration.

Adding more memory improves performance and may also reduce costs. Package size also directly impacts cold start times, so you want to make sure your function is small and nimble for optimal performance. However, you can also easily swing the pendulum to the opposite extreme with too many functions, repositories, or stacks to manage.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/320.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=320)

 Resist the temptation to create these sorts of micro-functions prematurely. Start with cohesive functions and split only when you experience real pain. The 42-function chaos was organic growth that really went probably a bit wrong, and the same applies to stacks and repositories. The goal is a pragmatic approach for your organization and your current needs, not some sort of future theoretical perfection. This is actually probably more of an architectural and organizational decision rather than just lines of code.

[![Thumbnail 350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/350.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=350)

When you do create clear boundaries, each team  can choose a runtime and tooling based on their expertise and needs. This is unique pretty much to serverless in that you can pick and choose the different kinds of things you want to do. Maybe the orders team is going to prefer using SAM or Java, the inventory team uses Python for maybe some great data-heavy workloads, and the customer experience team wants to use TypeScript because they want to use it for both the front end and the back end. Your infrastructure choices can align with your team's strengths.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/380.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/390.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=390)

This extends also to domain-driven organization, where each domain can also own its own data and own its own functionality  and be right-sized for the team, right-sized for what a team can effectively manage by itself. If you're thinking and you're building serverless apps, just use a framework. It doesn't actually really matter which one,  pick the one that works for you. There are many third-party ones, or we've got SAM, which is an extension of CloudFormation with some simplified syntax, or CDK to create CloudFormation in programming languages. Just use a framework; it's just going to make your life so much easier.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/410.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=410)

Many people also overcomplicate repositories too. Please don't have a repository for each function. I beg you, please, that's going to  be a nightmare. You can easily have a single repository for many services with separation between some domain-specific and also some shared infrastructure. The decision on size for actual repositories is also all about manageability.

[![Thumbnail 430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/430.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=430)

Am√©lie's transformation was thinking about sizing, making a function smaller, reducing the cold starts, deployment times, and also test runs.  They achieved 40% lower compute costs through right-sizing. Right-sizing is the key, and the key principle being keep it big until it hurts. Right-sizing for workloads organized by business domain enables team autonomy in your technological choices.

[![Thumbnail 450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/450.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=450)

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/460.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=460)

### Breaking Free from Synchronous Chains: The Power of Event-Driven Architecture

Next up, Emily learned that too much sync and not enough async  was killing performance during the morning rush, from blocking chains to event-driven flow. In 2023, synchronous architecture started to create some nightmares during peak hours. The morning rush brought cascading system failures  and, of course, frustrated customers. The synchronous chain meant one slow payment API call, for example, blocked everything downstream.

[![Thumbnail 480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/480.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=480)

Business impact was not good: 23% of orders abandoned, a huge increase in complaints, and revenue loss during the highest traffic periods. This crisis forced them to understand the fundamental problems  with synchronous architectures. The sequential wait train, if you look through the Mermaid diagram here, means every step must finish before you get a response. Everything had to complete in Lambda functions before the customer got a confirmation, so one slow service affected the entire workflow. It's all waiting, waiting, and waiting.

[![Thumbnail 500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/500.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=500)

[![Thumbnail 510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/510.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=510)

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/520.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=520)

[![Thumbnail 530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/530.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=530)

Long  chains can also approach API Gateway and even Lambda's timeout limits, and customers abandon orders while waiting for confirmation. This can manifest itself in two ways. You can  have one big Lambda function doing absolutely everything, and it becomes brittle and tough to manage, and also you're paying for the waiting time within a Lambda function.  Actually, I think probably even worse is multiple Lambda functions in series, each one a fragile cascading chain. If a third-party payment service has a failure, for  example, the rest just stops, resulting in a poor user experience and tough to come back from.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/540.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=540)

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/560.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=560)

Emily and her team did their research and realized an event-driven design would help, using an event bus for service-to-service  calls, stopping direct service dependencies, which allowed for loose coupling, some independent scaling, and means failures in one system don't break the others. You can still use async callbacks to update the original caller when the work does complete. This means bringing in some other of our serverless friends like Step Functions, EventBridge,  SQS, and even AppSync Events, which is great for the front-end notifications actually. What's the mantra for serverless? You want to use the best service for the job.

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/590.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=590)

The customer gets an immediate order confirmation instead of waiting for absolutely everything. Then EventBridge decouples all the services, each service processes asynchronously and publishes its own events when done, and AppSync Events provides real-time updates back to the client as work completes, so the customer is going to see the various progress updates: payment confirmed, kitchen preparing, and ready for pickup.  You get the best of both worlds: you get that immediate response back to your client plus real-time progress updates. You get resilience because payment delays don't affect inventory or notifications, scalability because each service can scale independently based on demand, and the clear event trail shows exactly where issues are occurring.

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/610.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/620.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=620)

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/630.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=630)

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/640.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=640)

### Lambda Event Source Mapping: Understanding Queues and Streams

And another thing to bring in for asynchronous  processing is Lambda's Event Source Mapping. One of the best use cases for Lambda async processing from a number of event sources was the direct Lambda integrations.  An Event Source Mapping is a resource that gets events from a source and ultimately sends the event to Lambda for processing. The Lambda service polls event sources automatically.  There's some content-based filtering which can reduce invocations for events you don't need to process. The ESM can batch or group records together for efficient processing and then ultimately invoke the Lambda function with the batched records. 

And there's actually an architectural difference between two types of event sources: queues and streams. Queues are great for task processing when each message is independent and messages are deleted after the processing. Streams is more event processing when perhaps you need multiple consumers who need the same data or order matters and messages are retained for replay. And so a key decision you may have is, do you have one consumer doing the work once, maybe you're going to use a queue, or you've got multiple consumers or need some sort of replay, then you're going to think about doing a stream.

[![Thumbnail 680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/680.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=680)

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/700.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=700)

And the Lambda ESM provides a bunch of functionality across all of these event sources. You've got the filtering, which I mentioned, batch controls, including for streams,  being able to split a batch to find a faulty record, and then choosing where to start in a stream, from the beginning, from the latest stream, or at a particular timestamp. There's some retry and failure handling options built in, some analytics for Kinesis and platform performance configuration options. So a whole bunch just called an ESM, but all of the functionality that is built in. 

And so Am√©lie needed a message buffer during traffic spikes to prevent system overload. And so SQS provides perfect decoupling between producers and consumers. Lambda ESM automatically polls SQS and then scales based on the queue depth and messages are then going to be consumed once and then deleted after successful processing. And SQS just automatically handles the complexity of any of the message durability and delivery. SQS is an incredible service.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/730.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=730)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/750.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=750)

And there's a new mode joining Kafka for SQS called Provisioned Mode.  And this allows you to provision event pollers in advance to handle sudden spikes in traffic, and you've got controls to be able to optimize the throughput. And there is an additional charge based on what's called an Event Processing Unit. Am√©lie doesn't actually need Provisioned Mode, but this can be super useful for high throughput, low latency queue polling. 

### SQS Configuration Best Practices: Managing Message Flow and Error Handling

So let's look at some of the best practices for SQS configuration. You want to set the visibility timeout to at least six times the Lambda function timeout so messages are available during processing during retries. I suggest also setting the redrive policy for the SQS to three or probably five to also give more chances for successful delivery. And then for message retention, make sure it's long enough to handle any possible break in the system. This is not just in your ESM but maybe in your whole company or maybe in the whole region, so messages aren't lost during any downtime.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/790.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=790)

[![Thumbnail 800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/800.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=800)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/810.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=810)

And then you must set a DLQ, which is a Dead Letter Queue, so any messages that Lambda  can't process are sent to this other queue and then are not lost. And then you want also some process to be able to recover these messages from the DLQ and be able to replay them into the main SQS.  And from the Lambda ESM side, you want to also ensure that you can process messages and also protect downstream sources from a surge. That is the point of having a queue. 

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/830.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=830)

For processing messages, the Lambda on-demand ESM can scale up to 1,250 concurrent invocations per event source and then use Provisioned Mode, as I said, if you want faster scaling and also up to 16 times higher concurrency, so 16 times higher concurrency, that's 20,000. That's an impressive throughput which you can use for your SQS with Lambda. 

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/850.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=850)

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/860.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=860)

Filtering also allows you to only process the messages you need. You can use positive filtering to pick which messages you do want to process, also negative filtering for a flexible query to say which messages you don't want to process. And this is a very powerful thing, saves you money, saves you time, and it also uses the EventBridge syntax for filtering, so it's also consistent.  And also this can save you a bunch of money and using this, Am√©lie was actually able to reduce costs for one of the ESMs by 92% by just using filtering. 

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/880.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=880)

Batch sizes, I recommend just 10 to get started. You may as well start with that. It does mean fewer invocations and still decent throughput and because the larger the batch size, the fewer invocations, and so that's one thing you need to think about. See how you go, but batch sizes can go up to 10,000, which is a huge batch size, and that can be super useful in some of your scenarios. 

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/900.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=900)

The batch window here is then to improve the latency when you actually don't have much traffic. So you get to process messages before a batch number is formed. So you can use these two different settings for different use cases. And then also, you've got the partial batch, the report batch item failures. So if you do have a faulty record in your processing,  you can return a list of failed records back to SQS and SQS is then going to delete the processed messages, which is more efficient rather than having to retry an entire batch.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/910.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=910)

To control the flow,  this is the rate at which Lambda consumes the messages, you want to set the max concurrency on the Event Source Mapping, and this is going to control how many concurrent invocations the ESM attempts to send to Lambda, which prevents overwhelming downstream services. These could be things like databases or APIs or any third-party services which can only handle a certain amount of throughput. This is actually the all-important buffering control system.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/940.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=940)

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/960.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=960)

Lambda reserved concurrency is a separate setting which you can set at the Lambda function level, and this actually reserves capacity for this  function, ensuring it can scale up when needed within the account concurrency. So you want to use max concurrency to manage the buffer, but if you do want to use both together, make sure that your reserved concurrency is higher than the max concurrency to prevent throttling. So two different settings for two different things, and you can use them together.  The last bit is for more error handling. This is configuring Lambda on-failure destinations for function invocation issues.

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/970.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=970)

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/980.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=980)

You may be thinking, why are there actually two different error handling parts? Well, the SQS  Dead Letter Queue captures messages that fail repeatedly during polling or processing, and that is on the SQS side, while Lambda on-failure destinations capture invocation failures like network issues,  throttling, or maybe even if you deleted a function or you've got some kind of IAM issue. So both serve different purposes and should be used together for comprehensive error handling.

[![Thumbnail 990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/990.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=990)

 Moving to async is a powerful architecture that actually should really be your default choice in my opinion. Avoid synchronous chains that can go wrong. You want to store first, you want to reply quickly for better user experience, and then process things later. This is a very powerful, resilient, and stable model. Understanding also when to use queues versus streams, async also means better failure isolation, but you then do need to think about retries and not losing any messages.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1020.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1020)

[![Thumbnail 1030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1030.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1030)

### Eliminating Unnecessary Lambda Functions Through Direct Service Integrations

 Next, Emily learned how to avoid unnecessary work through using Step Functions, orchestration, and some direct service integrations. In 2024, Emily had successfully moved over to async architecture but created a new problem.  Forty-two Lambda functions, as we said before, including many doing just simple data transformations and routing. That means high compute costs for operations that didn't require any sort of custom business logic, cold starts affecting performance for basically CRUD database updates, and complex failure scenarios across multiple function invocations. This led to the discovery that configuration as code could replace many Lambda functions.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1060.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1060)

You basically want to avoid Lambda when you need to  use native capabilities in other services. Less code to manage. I don't know about your code, my code, I want less code of that to manage. Your code is probably great, so you may be good with that. But for things like simple data transformations, direct service-to-service calls, only routing data from one place to another, or simple data enrichment, you don't need to use Lambda. If you do have Lambda functions that serve only as a proxy, for example, I'm going to go through some direct integrations between API Gateway and downstream services, you can optimize that.

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1100.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1100)

You can see on the right, API Gateway can actually connect directly to multiple AWS services such as DynamoDB, SQS, Step Functions, and many more. Once again, no need to use Lambda just as a proxy, and I understand that VTL is a bit  of a pain to test, but also once you do have it configured, it's done forever. For a simple example like this DynamoDB GetItem request, it's easy, cheap, and it's fast.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1110.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1110)

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1130.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1130)

 There are more great opportunities to remove code. One common pattern that a lot of customers use is using Lambda to capture DynamoDB change events or change data capture events from a lot of other services if they support it. Lambda then parses the events and then sends them to another service, an event bus or API for some other downstream  processing. EventBridge Pipes provides built-in transformation capabilities. JSONPath provides powerful data manipulation and filtering of only needed events. You can enrich data as part of the pipe, do data format conversions and transformations. You can also use Step Functions and you can also use Lambda as part of this step as well. Then it's going to batch it up for efficient downstream processing.

This is perfect for simple data transformations that don't necessarily require complex business logic. This is if you're using the pipe by itself without Step Functions or EventBridge. But yeah, this reduces your Lambda invocations for basic event processing and it's really simple to set up. But actually, the big winner in Emily's use case is using Step Functions. Being able to remove a bunch of Lambda functions for direct service calls, you can see all of the AWS SDK actions are available. So in effect, there are thousands and thousands of integrations.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1190.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1190)

[![Thumbnail 1200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1200.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1200)

Here's the configuration to update inventory in DynamoDB. Just a simple configuration call that you put in a Step Functions  state, and it's really easy to call and there's a whole bunch of stuff around it with no cold starts, and  safe retries are also built in. It's also got built-in JSON path support with loads of these functions including some Step Functions specific ones like generating a UUID.

So this is super useful. You want to count things, you want to sum up some numbers, the length, and all these kinds of things. No need to have a Lambda function. You can just use pretty much free JSONata expressions to be able to do it. So these are very powerful ways to build up a lot of functionality without having to maintain any of your custom code.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1230.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1230)

### Step Functions and Lambda Durable Functions: Orchestrating Workflows Without Custom Code

In many scenarios, Step Functions is then used in conjunction with EventBridge, and Step Functions is great probably within a domain  microservice. And then the pattern is to finish what the service needs to do, whether that is Step Functions or even a Lambda function, and the pattern is to then emit an event onto the event bus. And this is then going to enable the event-driven communication between different domains. Very powerful model, allows you to add as many domains as you need without sort of trampling on each other.

[![Thumbnail 1250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1250.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1250)

 And if you didn't know, there are actually two different flavors of Step Functions. There's Standard Workflows which can run for up to one year and are asynchronous, and Express Workflows which are fast and furious. And these are specifically built for high throughput. And they can run for a maximum of five minutes and they can also be synchronous. So it's super useful for doing that synchronous calling we were talking about earlier. The pricing is different. Standard Workflows are priced per state transition and Express Workflows are then priced by the number of executions and memory consumption. And Express Workflows also run in memory, so they're super fast.

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1280.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1280)

 And the best part is you can and actually you should combine Standard and Express Workflows. You can combine the durability of Standard Workflows with the speed of Express Workflows. And so this is perfect for scenarios where most of the process is long running, but some of the parts of your state machine need some real-time responses.

[![Thumbnail 1300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1300.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1300)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1330.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1330)

 And so here we can see the Standard Workflow is going to run start execution and orchestrate the complete order processing lifecycle with a full audit trail as part of the Standard Workflows, and then it's going to run the start execution and run the nested Express Workflow to handle real-time inventory validation in subsecond time. And what it can do in the Express Workflow, it can do other cool Step Functions things like running a parallel state to check the inventory and the pricing rules simultaneously. And then it's also using DynamoDB direct service integrations. So again, no Lambda cold  starts. All becomes nice and simple.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1350.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1350)

[![Thumbnail 1360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1360.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1360)

But actually, as announced in the keynote today, there's a new kid on the block for asynchronous processing, but still within the Lambda function. Lambda with Durable Functions being announced today. And this actually does completely change the way you think about Lambda functions. You can now build workflows in your favorite  programming language. You're able to use checkpointing to suspend and resume long-running operations with idempotency just built in. So this is actually useful when you have a bunch of Lambda  code in your workflows and you can now use the enhanced context object like I'm highlighting over here, and I'm showing you, you can use, you can run steps, you can do parallel processing, and even wait for callback all within an async Lambda function, and within the Lambda function, and you don't pay for that wait time as well, and they can run for up to a year.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1400.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1400)

So if your mind's going a bit, hang on, this is all a bit strange and different for Lambda, I'm with you, but this is a super powerful way, a new capability that we announced today. Step Functions, of course, is still great, particularly for those direct service integrations. But if your workflow does require a lot of your own code or you prefer just managing your workflow in code with async checkpointing and resume, this is certainly worth taking a look.  And my good friends, Michael Gasch and Eric Johnson are doing a dedicated talk tomorrow, and that's going to be diving deep into it. I think it may be Mandalay Bay, but I'm not entirely sure. It's CNS380, so have a look that hopefully should be released in the content catalog today. Durable Functions is going to be awesome. Well, it is awesome.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1420.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1420)

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1440.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1440)

So going all async helped Am√©lie, being  able to drastically reduce Lambda functions and cold starts, improving latency, better timeouts, better error handling due to reliable service integrations, and a monthly saving of $1,500 just from eliminating unnecessary Lambda functions. So this demonstrates the power of configuration as code over custom compute. Remember, the good old adage, the best Lambda  function is often the one you don't even have to write.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1450.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1450)

### Optimizing Lambda Performance: Memory Configuration and Power Tuning

So next up, Am√©lie optimized Lambda performance for functions that truly needed the custom logic, because Lambda still is a great service when you need to use  it. 2023, 35 locations, 180,000 orders monthly. The morning rush created a perfect storm of performance problems. Customers abandoned orders faster than Am√©lie could even bake croissants, which meant a revenue loss, and that's not good. And this was an existential business threat, not even just a technical problem.

[![Thumbnail 1470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1470.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1470)

 And so, quick sidebar for performance and cost, there's also another Lambda kid on the block announced this week, Lambda Managed Instances. And this actually gives you the Lambda operational simplicity and serverless operational model to go with EC2's full range and specificity. We manage the hardware and it runs in your VPC. You can choose specialized instances based on memory or CPU or network, and then Lambda is just going to manage and handle this instance. And there's multi-concurrency actually built in, another mind-blowing thing and another change that Lambda is going to be able to support for this model, but you do need to handle this in your code. There's going to be a lot of information coming out about how to do this.

[![Thumbnail 1530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1530.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1530)

The same timeouts do apply, but this can be a big cost saving for at-scale workloads where you can use also your EC2 pricing plans, so things like reserved instances. So this isn't for everybody. If you're at high scale and you've got really steady-state functions, it doesn't spin up as quickly and be able to burst and all that kind of thing, but at least it is an option at high scale if you want to be able to manage your costs when you grow with Lambda.  Again, there's another dedicated talk on Thursday from Steven and Oshana that's going to dive deep into it, and it's worth watching to understand how concurrency changes for this model and definitely check for this one and this is in the content catalog.

[![Thumbnail 1540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1540.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1540)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1560.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1560)

 So let's take a look back at the normal Lambda function lifecycle. Lambda creates a new execution environment or for managed instances, actually, it deploys a container onto the EC2 instances. It then downloads your code or your container image, starts the runtime, and then we have to run your function handler pre-initialization code and your first invoke. And then after that point, your function is warm and ready  to run the event that's been sent to it.

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1590.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1590)

There's actually a separation here between what you can control and what we control. And essentially everything up to the init of the runtime is AWS's problem. And the Lambda team spends an inordinate amount of time over the years here, literally shaving down to nanoseconds, trying to improve performance and trying to make everything that becomes on our side of the line faster and faster and faster. So the parts that you can control are memory allocation for performance, your initialization code, your function handler code, and  then the package size.

Lambda allocates CPU power proportionally to memory configurations up to 10 gig for on-demand functions and up to 30 gig for managed instance functions. Lambda also proportionally allocates CPU power based on memory, and for managed instances, you can actually configure the ratio of memory to virtual CPU that you actually want for your workload. For on-demand, once you're allocated more memory past about 1.8 gig, you can actually use more cores, so multi-threading becomes possible. And it's counterintuitive, but actually higher memory often reduces total cost.

If your code is CPU bound, adding memory improves the performance and may reduce the cost. And the key is to finding the sweet spot where performance gains justify memory costs. So you add more memory, it adds more CPU oomph, and so things can be processed quickly, and that can reduce your costs. Now for on-demand, if you want to take advantage of more power with more than one CPU, you can use parallel processing within your Lambda function code.

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1650.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1660.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1660)

[![Thumbnail 1670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1670.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1670)

So if we say here we're processing a batch of records, doing this sequentially  is going to take 300 milliseconds for 3 records. And within your code, when you use the process record in your code, this is going to process each record at a time.  But you can actually speed this up using multiple calls and running this in parallel. And so it's only going to take 100 milliseconds for all three because you can use promises to run the code in parallel.  And this is a super unlock for batch processing to get the most for your CPUs.

[![Thumbnail 1680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1680.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1680)

Now,  working out the optimal memory configuration, that could be a manual process, you've got to try all the options to do it all yourself. And so Lambda Power Tuning is an open-source tool that gives you a data-driven approach to help you visualize and fine tune the memory power of your Lambda functions. It runs multiple concurrent executions of your function at all the different memory allocations you pick and then measures how they perform and it runs in your account performing your real function calls, showing the real cost and speed, and this helps you to find the right balance in an automated way.

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1710.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1710)

[![Thumbnail 1720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1720.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1720)

### Cold Start Optimization: Package Size, Imports, and Native Compilation

Okay, so on to cold starts.  Here, Am√©lie has her figures and cold starts affect less than 1% of invocations in her production workload, and they primarily occur during scaling up or after function updates.  And so, you generally want to focus cold start optimization on latency-sensitive user-facing workloads. Async workloads can usually tolerate some cold start latency, unless you may be using something like provision mode and you need to do a whole bunch of throughput processing.

But generally, all those async processes, if it takes a few milliseconds or even a second for a payment notification email to come through, nobody's really going to mind. And so don't over-optimize cold starts for background processing functions. Although, I did mention there's a cost for doing this, but it isn't the biggest thing that you can do for performance. But there is a bunch of stuff that you can do to make your init more efficient.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1760.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1760)

 Focus on what you can control, the package size, the imports, and the initialization logic. You only want to import specific modules that you need. So you want to avoid importing huge big SDKs when you're only going to use a small part of it. If you're using Node or TypeScript or that kind of thing, you can minify production code to reduce the download time, lazy initialization if you do have functions that do some multiple processes, and so this can defer some heavy operations until they're actually needed for maybe only part of the invokes.

You can also think about establishing connections during the init phase and then reusing them during subsequent invocations when we're talking about warm starts. But of course, you do need to think and you need to make sure that you can handle reconnections in your handler to deal with any of those stale connections. Keepalives also maintain persistent connections to other AWS services.

And then also, you can cache reusable data, but you also need to think about cleaning up sensitive data. If you've got subsequent invokes and you've got information from different customers or things like that, or secrets that pertain to different IAM roles or things, you know, all those kinds of things, you do need to think about that to make your functions safe.

[![Thumbnail 1830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1830.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1830)

Now, there are also a lot of code optimization strategies. I'm not going to go through all of these,  because each runtime has specific optimization techniques. And really, there's nothing groundbreaking here. These are just often a lot of them are just normal code-based practices. Java can benefit from using SnapStart and proper SDK usage. JavaScript or TypeScript can use modular SDKs and specifically using version 3 of the AWS SDK and also things like tree shaking. .NET can leverage AOT compilation for faster startup, and then Python optimization is often all about your import strategy and the package size. But remember, for all of these, connection reuse is critical across all the runtimes to save you time.

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1870.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1870)

[![Thumbnail 1910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1910.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1910)

There are some native compilation options that can provide significant performance benefits. GraalVM  compiles Java to native executables with sub-second cold start times. .NET AOT does a similar kind of thing for .NET applications. Now, obviously, there are trade-offs with this approach. There may be some longer build times and some runtime limitations and a different sort of runtime that you maybe need to use, but this is a useful thing that you can look at. This is often best for CPU-intensive workloads with predictable execution patterns, so you can run these. It does obviously require some code changes and framework compatibility considerations, but more and more frameworks are supporting them. And so this is most effective for functions that are also frequently invoked or also  are cold start sensitive.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1920.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1920)

### Platform Features for Performance: Provisioned Concurrency, SnapStart, and Runtime Upgrades

And we also have some platform features like Provisioned Concurrency and SnapStart to eliminate cold starts  without any code changes, and we'll get into that. So Provisioned Concurrency is available for all languages. You configure it for a certain value, and then Lambda effectively goes ahead and pre-warms those execution environments, runs that initialization process in advance, and then we're going to keep those execution environments around for you. As they age out, we're going to replace them automatically. And see, here I've turned on Provisioned Concurrency for this function for version 10. You can see the inits are running in advance, and then when the invokes come in, they land on those already warmed execution environments. And so you won't see a cold start in front of these environments. And so this is helpful just before that sort of 8:00 p.m. patisserie rush, or from the morning rush that Emily would need it.

[![Thumbnail 1980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/1980.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=1980)

Two other things: don't actually pay for Provisioned Concurrency you don't use. There's a CloudWatch metric to monitor this, so don't waste any of your money. And I pushed over too fast. And then also, you must use the function version or alias, not dollar latest. That trips people out when they wonder why is my Provisioned Concurrency not doing anything. So when to use it, this is  for applications that need less than 100 milliseconds of startup latency. You need to configure the value for Provisioned Concurrency, so it's best for predictable traffic patterns which justify the cost of keeping those warm environments up and running. So obviously, it's going to be good for mission-critical APIs where any latency variable is unacceptable or some high-traffic applications needing consistent performance during heavy periods.

[![Thumbnail 2030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2030.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2030)

Now for implementation, I suggest you need to do some research and understand your execution patterns, what times of the day do you need to pre-provision the Provisioned Concurrency, and you can just start with a static figure and then maybe evolve it into something dynamic, and configuring it using Application Auto Scaling for just when you need it. And that can be very efficient to be able to ramp up beforehand and ramp down afterwards. And with Lambda's new, more recent, I suppose a year ago now, scalability things, it's really quick to get Provisioned Concurrency up and running in advance. 

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2050.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2050)

[![Thumbnail 2080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2080.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2080)

So then Lambda SnapStart for Java, Python, or .NET is another platform feature that runs the cold start process when you publish a function in advance, so not actually just before an invoke. And so when you need to invoke the function, it's going to resume the snapshot and it's generally going to do this in under a second, and so it makes it perfect for cost-sensitive applications with unpredictable traffic.  There's no additional cost, which makes it a great workload where Provisioned Concurrency would be expensive. Functions with heavy runtime startup or loading heavy dependencies, they're going to be the ones that see the biggest benefit. Java is going to see great performance gains due to having to start up the JVM. Python is going to benefit significantly when loading big ML frameworks, and .NET gains from eliminating the JIT compilation delays. And remember again to target a function version, and you also cannot use both SnapStart and Provisioned Concurrency at the same time. 

So what can you do to actually make that snapshot process as efficient as possible? For Java, you can use the beforeCheckpoint hook to preload dependencies and initialize resources using the init phase. And this actually uses the CRaC runtime hooks, which is from an open-source project. And there are two ways to do this. You can actually use invoke priming, which does have the highest performance, but make sure that you then use test data and your code must be idempotent as you're actually running real live invokes for your Lambda function to set this up.

Or you can choose to use class priming, and this is then going to load classes without actually the method of execution. So two different ways, but you want to run as much as you can in this init process so that's going to optimize your invokes.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2120.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2120)

 And for .NET, it's actually one on one. You want to run multiple invokes as part of the init using the register before snapshot, which is what makes .NET SnapStart really effective. And literally you want to run probably, you know, maybe even 10 or 20 times, and this signals the .NET runtime to perform tiered compilation aggressively. It's sort of, I suppose, think of saying to the runtime, this code is hot, optimize it as aggressively as you can. And again, you know, use dummy or stub data and avoid side effects during warm-up because you are running real invokes. And this works with many frameworks as well. So there's some universal best practices for SnapStart.

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2160.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2160)

 You want to re-establish network connections after restore. State isn't going to be guaranteed. You want to generate unique IDs and secrets after the initialization, and then you obviously want to refresh credentials and timestamps then in your handler. And of course, only executed code paths are captured. So you want to use those runtime hooks we spoke about to warm those critical paths. And then again, you must use function versions, not the latest, and obviously you want to test with your realistic workloads. And CloudWatch metrics can be your friend to validate the performance improvement with your business impact. SnapStart is actually a really great, you know, quick and easy win to be able to improve cold start performance.

[![Thumbnail 2200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2200.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2200)

Now another optimization tip, just upgrade your runtime. Seriously, it's often one  of the simplest things to do. You know, in this example, upgrading Python had a dramatic increase in latency, which improves performance and reduces cost. Languages are continually evolving and keeping up to date with runtime means you also get the best performance for your buck, and of course, there's a security implication for this as well.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2220.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2220)

So a systematic approach delivered dramatic  improvements starting with memory right-sizing, the main kind of thing, the biggest impact, the easiest implementation. Code optimization provided additional gains through better resource utilization. Cold start mitigation ensured consistent performance during traffic spikes, and native compilation provided that last little performance boost for CPU intensive workloads.

[![Thumbnail 2240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2240.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2240)

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2250.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2250)

### Building Resilient Systems: Idempotency and Comprehensive Error Handling

 So next up, how Am√©lie built resilient systems through proper failure handling and error recovery. You know, lots of locations, 35 of them processing thousands of orders daily.  That holiday rush I spoke about earlier created this sort of perfect storm of cascading failures, payment issues, a crashed order system, no notifications, manual recovery. Obviously, this is a big business impact. Lost revenue, angry customers, a confused kitchen, what's going on? So, of course, this crisis forced them to understand that resilience is also a business imperative.

[![Thumbnail 2270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2270.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2270)

[![Thumbnail 2300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2300.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2300)

 So with all of this, the most important thing to think about is idempotency. Idempotency ensures operations have the same effect whether executed once or multiple times, and this is critical for distributed systems where retries, duplicates, and out of order processing occur. Basically, I don't want to be charged twice, so please build all your idempotency operations. But you do then need to think about your idempotency, what that idempotency key is, that sort of unique transaction identifier, you  know, is it going to be an order ID or something that you can use for your idempotency.

[![Thumbnail 2310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2310.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2310)

And many AWS services provide built-in idempotency, which you may not know about. DynamoDB you can use conditional writes  and so this is going to prevent duplicate operations. SQS FIFO queues can use a message duplication ID for automatic deduplication. And did you actually know for Step Functions, you can just specify an execution name to prevent duplicates. So you've got a Step Functions workflow, just provide the name. If the same name has happened before and you can use that, you know, maybe an order ID or that kind of thing, Step Functions won't run it. So simple and a lot of people don't know. And of course, Lambda durable functions is another way because idempotency is just part of the checkpointing part of it.

[![Thumbnail 2340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2340.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2340)

 And for your code, Powertools for AWS Lambda, this provides, please, this is a tool and library for .NET, for Python, for Java, and for Node and TypeScript. Just use this for your functions. It has a huge amount of functionality. Idempotency is included, and it actually just uses DynamoDB behind the scenes. You just set it up in your code and idempotency is handled for you. Super useful.

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2370.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2370)

So we already covered SQS DLQs and Lambda destinations. This is all part of your failure handling toolkit. And remember,  retries are all built into Step Functions when you're using Step Functions set as configuration, part of your failure and error handling. And you have retry, you've got catch states, you've got choice and parallel states, and this can give you a whole lot of error handling goodness. If you're building saga patterns or other kinds of things, a bunch of functionality. And so Am√©lie's order processing workflow has comprehensive error handling at each step to prevent single failures from bringing down the entire order process.

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2400.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2400)

And if you prefer the full code approach, you know, we've  got new options now with Lambda with durable functions.

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2410.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2410)

### Advanced Observability: Embedding Business Context and Leveraging New CloudWatch Capabilities

Next, Am√©lie gained complete visibility into their serverless systems through  advanced observability. You know, 15 locations, system visibility gaps across services, difficult to trace orders, trying to identify bottlenecks causing the slow processing and where they're coming from, or trying to connect some of the technical metrics to the business impact. The breaking point was an undetected payment issue which happened for two hours, resulted in hundreds of lost orders and $12,000 in revenue loss during the dinner rush. Now that's not a good thing that any fledgling business wants to run into. No one wants to fly blind.

[![Thumbnail 2440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2440.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2440)

Now, observability is a massive topic,  and it's in a fast-moving space and there are many breakouts here at re:Invent which are all about observability. So I'm just going to give you some of the new things that you can explore to help you out. I love Charity Majors' work. She works for Honeycomb and she talks a lot more about more than just the pillars of observability, but a bunch of different signals of data which you can use for unified observability data. And seriously, read up on Charity Majors, read everything she writes about and talks about observability. She's an absolute guru and amazing.

So you've got things like metrics and logs and traces and events and profiling, and they can all be part of these signals and other kinds of business things that can come in. Observability is a big thing. But the thing I want you to take away is you want to really focus on embedding business context in all of the signals for rapid troubleshooting. So this isn't just a technical monitoring thing. This is observability and the business is very much part of it.

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2500.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2500)

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2510.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2510)

[![Thumbnail 2520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2520.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2520)

This shifts your thinking from not just monitoring failures to being able to answer  questions about individual things to understand your business. Again, Powertools is your friend here for observability. It just makes things so much easier. You  can import the libraries really quickly, configure your service names, and then it's easy to set up the logging and the metrics as well. And this is then going to use structured logging behind the scenes and using the Embedded Metrics  Format to create metrics from the log entries automatically.

[![Thumbnail 2540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2540.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2540)

A single log write creates both searchable logs and CloudWatch metrics, and this is also faster and cheaper. This is a great way then to incorporate that business context I was talking about, like an order ID for the logs without having it as a metric dimension, which is going to be pricey.  So here we see both entries written to CloudWatch Logs in a single log stream. The regular logs are queryable with CloudWatch Insights for debugging, and then the EMF entry lets CloudWatch automatically extract the metrics from it, a powerful two for one.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2560.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2560)

CloudWatch Metrics also has a bunch of new stuff like powerful tag-based automation with filtering on alarming instead of configuring individual function alarm tags,  and this makes it way easier. And it's cheaper. You've got one alarm instead of 50 individual alarms, which reduces the CloudWatch costs. There's really fancy querying capabilities now available and a cool new dynamic dashboard where you can actually build up your dashboard and then you can just select your environment from the drop-down selection and it changes all in real-time.

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2580.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2580)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2600.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2600)

 The query generator converts natural language to CloudWatch Metrics SQL and allows you to refine existing queries, which I found so much easier. The metrics history has also been extended from three hours all the way up to two weeks for historical analysis. CloudWatch Application Signals is also something to look at, a new observability solution which is based on OpenTelemetry, and this can also  automatically discover services and it's got a more advanced service map and a whole bunch of other things.

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2610.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2610)

So I can't do CloudWatch justice here and I don't want to just flip over things.  So the awesome Joe Alioto has done a great video on all the new things and there are going to be new things announced this week as well. So I just really suggest you take a look because there's a huge amount that's gone on in observability this year which can really help you with your business.

[![Thumbnail 2630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2630.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2630)

So talking about OTEL for Lambda. Well, the X-Ray SDK is coming to an end. X-Ray  as a service continues, but it's moving to OTEL for instrumentation using the AWS Distro for OpenTelemetry. If you want to start looking at this for Lambda, there is an ADOT layer. We actually recommend using the Application Signals Lambda layer. It's actually an advanced, enhanced version of ADOT specifically optimized for Lambda, with better performance and lower resource consumption.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2670.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2670)

You can also configure Application Signals to use only the OTEL libraries for instrumentation without all the other features for Application Signals by just setting an environment variable. So moving to OTEL at the moment does mean a little bit more memory and a small cold start penalty, but if OTEL is your thing, at least we've got some good options. And of course, we love partnering  with others so you can use your favorite third-party tools with Lambda to get the observability you need.

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2680.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2680)

And so Am√©lie did a lot of observability work, more than I can  cover here in the time, but it all pays off, a transformation from disaster-prone to a highly reliable system, better reliability, recovery times, less tail chasing. You know, Black Friday 2024 had zero outages during peak traffic and far less customer complaints due to better error handling.

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2700.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2700)

### Cost Optimization Through Architectural Decisions and Smart Logging

Next, let's talk about costs and how Am√©lie optimized costs  using design while scaling.

P√¢tisserie Am√©lie started the serverless journey with excitement but faced unexpected cost challenges. This can happen to you. Monthly AWS bills jumping from $200 to $2,000 without much warning. During a growth phase, Lambda invocations spiking crazily, maybe overprovisioned Lambda functions with too much memory, CloudWatch logs accumulating gigabytes of data daily. Each new restaurant adding more and more infrastructure costs, and data transfer and storage costs all adding up. Emily realized that in serverless architecture, decisions rarely impact costs. Understanding serverless pricing is crucial for cost-effective architectural decisions.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2740.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2740)

[![Thumbnail 2760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2760.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2760)

 Lambda is on-demand. It uses a pay-per-use model with charges for requests and also your compute time. Any time is charged from initialization to remember, and this was to avoid complexity with some invocations being charged and some invocations not being charged. So it's all across the board. Units are charged, and then per-millisecond billing is super powerful with each invoke, and of course each invocation is charged separately.  And so this is cost-effective when there are gaps in work to do. You can also consider Lambda managed instances for bigger steady-state workloads where you can run Lambda at scale in a cost-effective manner.

[![Thumbnail 2770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2770.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2770)

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2790.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2790)

 Now, Fargate uses a time-based model with charges for CPU and memory per hour. You pay for what you provision. Now, I didn't have enough time today. There was enough to talk about to go into all the Fargate best practices, but it is a super great solution for many workloads when you don't want to use an event-driven model. And it can also be cheaper than Lambda at scale.  Continuous processing or long-running predictable workloads are going to favor Fargate, and sporadic event-driven processing is going to favor Lambda. Even though Lambda can scale up to super high workloads, this is costing that you can work out.

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2810.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2810)

For Step Functions, Express workflows can also be significantly cheaper. Emily's using  both, but you can see our pricing can be cheaper. Many more invocations for one-twentieth of the cost in this example using Express workflows. So also take a look at your existing state machines and see whether you can potentially save some money, and then consider using Express workflows first when you're building your next applications.

[![Thumbnail 2830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2830.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2830)

And optimizing your logging costs can be an easy  win. AWS rolled out tiered pricing for logs this year. Basically, the more you log, the cheaper it gets per gigabyte. The best part, it's automatic. You don't have to do anything. It's just going to be cheaper. Use structured logging with smart log levels. Maybe you're going to set info as your default. Maybe you're going to sample debug logs sparingly and use Lambda Powertools for efficient logging without the custom code overhead.

You also want to match your retention to reality. Maybe you need 30 days for dev, 90 for staging, three to six months for production. I don't know, it's going to depend on your workload, but basically, don't stop paying for logs you're never going to use. Also, don't log what you don't need to. Skip massive payloads in your logs, just log key identifiers instead. You can also archive to S3 now for long-term storage or log directly to Data Firehose when you want high throughput and maybe you want to send the logs to some other destinations.

[![Thumbnail 2880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2880.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2880)

And lastly, monitor those costs. You want to set CloudWatch billing alerts and use Cost Explorer  to identify your biggest log spenders. So there's lots that Emily did to reduce costs, which also be covered in some other sections. Right-sizing was the biggest flex to not pay unnecessarily. Filtering is obvious to not just have to work on things that you don't need to, but it's going to be able to avoid a whole bunch of costs unnecessarily. And also using Compute Savings Plans, Emily is looking to explore that as well. She hasn't quite done that. That's another option. And so with some simple optimizations, you can make a big difference on costs.

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2910.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2910)

[![Thumbnail 2920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2920.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2920)

### Integrating AI with Serverless: Bedrock Agent Core, Tools, and Real-World Applications

 Every architecture decision has a direct cost impact. Understand your pricing models, monitor your usage patterns, and then optimize based on your actual requirements.  So next up, integrating AI capabilities into their serverless architectures. This is the year of 2025. I don't think anybody at any conference in IT in the world is unable to talk about generative AI, but it is an important and I think an interesting part that we can look at.

[![Thumbnail 2940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2940.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2940)

By this year, Emily's 60 global locations faced challenges  traditional automation just couldn't solve. If you've got 12,000 monthly inquiries that can overwhelm your support team, you know, if you need to particularly handle multiple languages, 4.3 million orders with minimal personalization, maybe you can do some more with that too, you're missing some revenue opportunities, and manual decision-making for pricing and inventory. Emily starts to look at this, to AI to help with this and maybe take their business to the next level.

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/2980.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=2980)

But Emily also knows that AI is a journey to also work out what's real and what's hype. And I know we're all probably struggling here at the moment trying to work out what the best use cases in your business are. And of course, all of AI could be a whole other session, but I'm pretty sure here at re:Invent this year, there are going to be a few other generative AI sessions  to help with whatever generative AI things you're going to be looking at.

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3000.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3000)

So the first thing they realized in the investigations is that generative AI is just another workload. The same rules apply. What do you need to do? You need to protect endpoints, you need to think about quotas, you need to handle security, you need to think about performance. Everything you know and love about serverless still applies.  There's less infrastructure to manage, variable compute demands, pay-per-use are all good.

Using asynchronous event-driven processing allows AI to respond to business events like orders, inquiries, and inventory changes. Automatic scaling handles traffic spikes without the need for capacity planning, and your existing serverless skills transfer directly to AI workloads. Event-driven architectures, cost optimization, and observability remain fundamental principles.

[![Thumbnail 3020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3020.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3020)

 Amazon Bedrock serves as an excellent example of a serverless service. The pricing is based on input and output tokens, with automatic scaling to demand and enterprise security and compliance built in. If you look around the industry at the generative AI landscape, most of it is just good old-fashioned serverless. Any kind of model you're using features token-based pricing and pay-per-use economics. So yes, serverless is definitely very applicable in the AI world.

[![Thumbnail 3050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3050.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3050)

 Of course, you can integrate Bedrock with all other serverless services. For compute, there are plenty of different options, and Step Functions is great if you want to orchestrate complex multi-step AI workflows. Event routing and response distribution are handled with EventBridge, and DynamoDB and S3 can also be used for storage of many different things.

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3070.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3070)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3080.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3080)

 We want to understand the two main parts of AI that are relevant: agents as the orchestrators and tools as the executors.  Agents run in an orchestrated loop to accomplish what you've basically told them to do. They call the tools to perform actions and figure out what the next stage is to satisfy your request. Agents are often long-running and need to maintain state. We could decide whether agents are short or long-running and pick the best serverless service to handle this, whether that's going to be Lambda or Fargate.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3100.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3100)

 But what we've actually learned from a decade's worth of serverless experience is to use a service that is purpose-built for the job. Bedrock Agent Core is built specifically for running agents and abstracts a lot of the heavy lifting, so you can move up the stack to a managed service, the true serverless way.

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3110.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3110)

 Agent Core has a bunch of capabilities, including a secure managed runtime to run your long-running agents with pay-per-use pricing. It actually runs a Lambda Firecracker instance for you, all abstracted and behind the scenes. We're taking the learnings from other serverless services like Lambda and applying them to more use cases. It includes an MCP gateway to connect to existing APIs and tools, conversation memory, and inbound and outbound identity management. Agent Core is also framework and protocol agnostic.

The cool thing is it's a bunch of capabilities you don't need to build yourself, and you don't have to use them all. You can just use one, use two, or use all of them‚Äîit doesn't matter. It's built very specifically so you can pick and choose the useful capabilities for what you're going to do. For identity management, for example, sure, you could spin up a DynamoDB instance, store your conversation history, and manage that all yourself. Or you could just use the built-in identity feature, and it's all included with semantic search as part of the package.

[![Thumbnail 3160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3160.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3160)

 Then there are tools, and these are what the agents call to actually do something. The tools are actually what you likely already have. Maybe that's something behind an API that reads or writes data from a database, performs some action, connects to your private data, something on-premises, or something within your own VPC. Actually, the tools are the deterministic part within generative AI. You know exactly how they work, what functions they can perform, and they help in avoiding hallucinations.

In fact, I think you actually want to focus more on the tool side. Even if you don't have robust APIs yet, with MCP or something like that in front of them, you've actually got more freedom to be able to experiment. APIs are forever, but because MCP abstracts them, you can have things internal that you can expose externally without having to worry too much about your APIs. You want to care about your APIs, but having this MCP intermediary layer can be super powerful.

[![Thumbnail 3210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3210.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3210)

 Of course, Lambda is great for tools, whether standalone or behind an API, because Lambda functions have fast startup times, they can connect to anything‚Äîthings in your VPC, things anywhere‚Äîand they can access your data in your VPC while being super secure.

[![Thumbnail 3220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3220.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3220)

[![Thumbnail 3230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3230.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3230)

[![Thumbnail 3240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3240.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3240)

 I've got just two examples here.  Emily built a new customer service AI agent to integrate with existing functionality. The customer inquiry goes into an agent using MCP.  Agent Core manages the AI agent runtime with Bedrock while using the gateway to connect to APIs and then to Lambda functions as tools. Agent Core handles the orchestration, the state management, and the observability automatically, and the team could then focus on building the actual tools without having to worry too much about building the agents.

[![Thumbnail 3270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3270.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3270)

Another intelligent recommendation system was a good idea they had. This is based on an order-created event which triggers EventBridge into action. Lambda processes the context, Bedrock provides the AI analysis, and DynamoDB stores the profiles.  Recommendations adapt during sessions. This is basically when someone completes a payment, it's going to pop up and suggest, "Do you also want to buy this?" It can use time, weather, location, or cross-location insights. Contextual personalized upselling is really good for customers and also for their business.

[![Thumbnail 3290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3290.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3290)

 So far, Emily's AI results are a good start and they are very welcome. They achieved 70% customer service automation using Agent Core and Lambda tools, 12% revenue growth by using the upselling feature with what's in the cart, and faster development by focusing on the tools rather than on the infrastructure. They also gained cost efficiency through pay-per-use pricing for both the agents and the tools. I haven't got time to cover more, but

there's not even including the AI coding assistance that the team is taking on. Using Cursor for serverless development, which is literally speeding up everything and building really good production grade applications, and then using the serverless MCP server which you can connect into Cursor and in fact any MCP client, and this really uplifts your serverless best practices. You can use natural language, you can query about MCPs, you can query about what infrastructure as code tools, Lambda optimization, Step Functions, all these kind of things built into that MCP server. I really suggest you take a look.

[![Thumbnail 3340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3340.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3340)

[![Thumbnail 3350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3350.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3350)

### Key Takeaways and Next Steps: Applying Serverless Best Practices at Any Scale

So after all that,  I'd said I'd cover a lot and I did. Let's recap to see what Emily's journey revealed. Is everybody okay? You're hanging in there. Good. Serverless works at any scale from 40  orders to 320,000. Serverless architectures had to evolve to support this growth, but serverless, of course, was the best way to achieve this.

[![Thumbnail 3370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3370.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3370)

We have some core principles. Here's the QR code again for the presentation and more handy jumping off links. Right-sizing, matching the function repo and stack size to your workload. Async over sync, EventBridge and queues for resiliency with still faster  customer updates. Avoiding work if you can, using direct service integrations, Step Functions, filtering early, using batch operations. Performance optimization, optimizing those cold starts, function code optimization, platform functionality with Provisioned Concurrency and SnapStart. Failure handling, got retries, DLQs, circuit breakers, a whole bunch of things. We've got observability, logs, metrics, signals with business context and lots of new CloudWatch goodness and cost being architecture, every decision has a price tag. And GenAI just another workload, your serverless skills apply. And these principles also apply at any scale.

[![Thumbnail 3410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3410.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3410)

So what could be your next steps? Well, this  week, maybe next week, I think you're going to be broken from re:Invent, but pick one Lambda function to optimize. Add some structured logging, including some business context, implement proper error handling using retries and DLQs if using SQS. Check function sizing, make sure you correctly size your memory and use EventBridge, not direct invocations. And then once you start there, you can build some momentum. Migrate one sync workload to async, build robust workflows considering Step Functions or Lambda durable functions, implement some event filtering, save yourself some money, review and optimize those costs, and then also document your patterns to be able to scale out your efforts.

[![Thumbnail 3450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3450.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3450)

There are lots of other sessions going on this week,  Lambda managed instances and durable functions I mentioned in the talk as well. There's also a leadership session, building the future with AWS Serverless and a shameless plug. I'll be back tomorrow with a Lambda principal engineer, and we're going to be talking in detail all about how events travel through Lambda, specifically with polling, and that's going to be back here tomorrow.

[![Thumbnail 3470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3470.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3470)

 This link also provides a bunch of additional information to continuous serverless learning. It's got links to Powertools, a bunch of announcements, ServerlessLand patterns. If you've ever used that, that is infrastructure as code and function code examples for, you know, all the serverless services and you'll just be able to do them and a whole bunch of them you can use directly in your IDE as well. Super useful. ServerlessLand.com as well is the best website to keep up to date with all things serverless on AWS. Lots of cool stuff that you can read over there.

But most importantly, thank you for joining me today. I appreciate you, appreciate you taking out the time. If you've had to move across Vegas and come to another venue, thank you so much. I also really appreciate if you could fill in the session survey that really, if you're hungry for, you know, more deep technical content, more broad, and more kind of things, this really lets us know the kind of things you'd be interested in. So I would appreciate, of course, a nice five in that as well, but also be honest with what you think.

[![Thumbnail 3540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/486f082e678efef8/3540.jpg)](https://www.youtube.com/watch?v=tMvwM_KXNPA&t=3540)

So thank you very much. I hope you enjoy the rest of your day here at re:Invent. Plenty more to learn, plenty more people to connect to. I encourage you to use the hallway track as well. Sit down with somebody at lunch and find out and talk about your kind of things. Talk about serverless and enjoy the rest of your week. Thank you very much. 


----

; This article is entirely auto-generated using Amazon Bedrock.
