---
title: 'AWS re:Invent 2025 - Scaling Data Transformation w/ dbt & Nasdaq: Unlocking Ent Power Global Mkts'
published: true
description: 'In this video, Michael Weiss from Nasdaq explains how the company uses dbt Enterprise to power its Eqlipse Intelligence Platform, which serves global financial markets. He describes Nasdaq''s three pillarsâ€”Market Services, Capital Access Platforms, and Financial Technologyâ€”focusing on how the Intelligence Platform collects and enables data-driven use cases for 4,000+ customers worldwide. The platform uses a hybrid tenant architecture with dbt for data transformation, providing standard analytical models while allowing customers to extend them. Key benefits include multi-tenancy support, dbt meshing for project dependencies, semantic layer for contextual enforcement, and governance capabilities. Nasdaq''s roadmap includes establishing common data ontologies, enabling customer extensibility, supporting AI/agentic use cases, and ultimately creating a data model marketplace where customers can monetize their custom models globally.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/0.jpg'
series: ''
canonical_url: null
id: 3092807
date: '2025-12-08T18:02:18Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Scaling Data Transformation w/ dbt & Nasdaq: Unlocking Ent Power Global Mkts**

> In this video, Michael Weiss from Nasdaq explains how the company uses dbt Enterprise to power its Eqlipse Intelligence Platform, which serves global financial markets. He describes Nasdaq's three pillarsâ€”Market Services, Capital Access Platforms, and Financial Technologyâ€”focusing on how the Intelligence Platform collects and enables data-driven use cases for 4,000+ customers worldwide. The platform uses a hybrid tenant architecture with dbt for data transformation, providing standard analytical models while allowing customers to extend them. Key benefits include multi-tenancy support, dbt meshing for project dependencies, semantic layer for contextual enforcement, and governance capabilities. Nasdaq's roadmap includes establishing common data ontologies, enabling customer extensibility, supporting AI/agentic use cases, and ultimately creating a data model marketplace where customers can monetize their custom models globally.

{% youtube https://www.youtube.com/watch?v=VBwX-w8wnMc %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/0.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=0)

[![Thumbnail 20](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/20.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=20)

### Nasdaq's Three Pillars and the Eqlipse Intelligence Platform

 Thank you, everyone. My name is Michael Weiss. I am a product strategist for the Nasdaq Eqlipse Intelligence Platform, and I'm here today to talk a little bit about how we have used dbt to enable our own markets but also enable markets on a global level. So I'm going to start with a little bit of an agenda.  Hopefully this is not going to run too long though. I know happy hour is around the corner and everyone's more excited about that than to hear me talk. But I figured we'd start today with a little bit of an introduction to who Nasdaq is, just so we all know what we're talking about.

[![Thumbnail 40](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/40.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=40)

So when we talk about Nasdaq, you can really think about Nasdaq in three main pillars. First and foremost, you can think about our Market Services  pillar, and this is the one that most people know Nasdaq for. These are the markets that Nasdaq owns and operates in both North America and in Europe. This includes the biggest trading market in the U.S. by trading volume. Second to that is our Capital Access Platforms. And this is the part of the company that's responsible for primarily our listings business, in which we list close to 3,500 companies today. Another big part of that is our index services in that area. And if you've ever heard of the Qs or anything like that, that's a big part of our portfolio. And the final pillar is our Financial Technology arm. And this is the one that most people probably don't think about Nasdaq in. Nasdaq services close to 4,000 customers globally for various solutions to support their needs in a financial market.

[![Thumbnail 90](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/90.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=90)

Let's drill into Financial  Technology a little bit, just so you understand the breadth of the services Nasdaq can offer to our clients. We cover everything from financial crime management with our Verafin products to regulatory technology with AxiomSL for regulatory reporting and surveillance to ensure compliance on the markets. And finally we have our Capital Markets Technology, which includes both Calypso and Nasdaq Eqlipse. These cover the full trading life cycle for both banks and markets.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/120.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=120)

 So let's just spend one more minute talking about what is Nasdaq Eqlipse and the products that make up that area, and then we'll get into the product that I'm here to talk about a bit today, and then where dbt plays a role there. Eqlipse includes four main technology areas. First is the trading system. This is the one that markets will deploy to execute trades on. We have clearing, which is supporting the clearing needs of our customers, and then custody is the final one in that main trading life cycle. And then the area I'm here to talk to you about today is our newer product, the Intelligence Platform. And the Intelligence Platform is responsible for collecting all this information and enabling our customers to leverage that to enable new use cases, new products, and new services to sell to their customers.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/170.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=170)

### Architecture and Components of the Eqlipse Intelligence Platform

So let's dive a little bit into the Eqlipse Intelligence Platform so you understand what it is.  And when we think about the Eqlipse Intelligence Platform, there are really four main areas we think about from a platform point of view. Foundational to everything is data management, and this is everything from how we acquire data from our own products to other third parties, how we store that information, how we ensure governance on that data, and how we keep an open ecosystem to allow a variety of different use cases and services to communicate with that data. Everything from other data transformation services to AI and agentic use cases.

The next main pillar we think about are the analytics, the insights you need to glean from your data to operate your markets or operate your business. And here we have a couple of things we think about, and this is the area dbt will primarily fall into. But really at the end of the day, what we're looking to service with the analytics side is the business. We're trying to give them access to as much information in the most intuitive way possible. Traditionally that's been done with more dashboards and reporting, but obviously in today's world we're looking at more agentic use cases and digital worker type use cases as well.

The third pillar is around our reporting. Despite everyone's best efforts, sending CSV files and PDFs is still a big part of what we have to do. And so we have a service that allows our customers to automate that and reduce friction with their customers by providing a self-service portal and capability to create reports without needing technology or engineers to really get into it. And the final one that probably doesn't seem to fit into the rest of the other three categories is our managed billing solution. And this focuses on how our customers need to bill their transactional or their non-transactional related charges to their customers.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/270.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=270)

When we talk about the Eqlipse Intelligence Platform,  this is kind of, I won't call it an architecture diagram, but it can give you an idea about how the components are laid out. Going back to Capital Markets Technology, first, the first thing we're looking to do is integrate natively with other Nasdaq products and services. When a customer buys the Intelligence Platform, we don't want them to have to think about how they're getting data from other Nasdaq services. We take care of that for them. And that's everything there on the left-hand side. In addition to our own data from Nasdaq, we allow our customers to bring their own data into the platform so they can be infused and sit alongside the data that we're already managing.

In those gray boxes, you'll see we have four main things here. The ingestion engine really runs there, and that's the one that's pivotal for the integration with all the other products and services. The cloud data lake, I think everyone probably has an understanding what the data lake is, but that's the primary retention and storage for the information. We provide a variety of different query and access endpoints that our customers can use to integrate with their own services if they want to bring their own applications into the ecosystem. And then finally, there's the contextual understanding and knowledge about the data, and that is what the data catalog services there.

And then on the far right in the green, you can really think about that as the business products and applications that sit on top of all that other stuff. So when I mentioned analytics, that's the Insights HQ product. That's focused on your more traditional dashboard use cases, the ability to create, publish, and share dashboards both internally and externally. The Report HQ product focuses on reporting, and then Revenue HQ is that billing component I mentioned. And as I said, these can be exposed to both the internal customers who are buying the product from us, Intelligence, or they can use it to provide access to their customers and reduce their friction points there and make sure their customers have the information they need to make decisions.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/380.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/390.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=390)

### Integrating dbt to Enable Data Transformation and Semantic Understanding

So let's get to what we're really here to talk about, the role of dbt in this platform, in this ecosystem, because dbt plays a  major part in what we do. Before we get into what they do, I think it's important we lay out a couple of our goals that we had going into leveraging dbt.  And in a lot of ways, these are the same goals we had starting back in 2021 when all of this was focused on only Nasdaq's use cases. We understood we needed to integrate with existing products and services for our internal customers to be able to make that information usable. We need to be able to provide better business models and analytical models to our end customers so they can actually parse and understand what is going on in their business.

On top of that, we wanted to allow our internal users, and now our external users, the ability to extend and participate in the model creation. A lot of times those data models and historical ETL processes were locked behind engineering resources. We wanted to lower the bar of entry so that more teams could collaborate and participate in the process, and dbt really unlocked that for us internally and is now allowing us to unlock that externally. And then the final part of this is to provide an access point that can enforce standards and contracts to both users and AI. In today's world, data without the context is a little bit limiting. When you're going to turn on an agent, you want to know it's talking to the data in the right way, and that's really a key driver in what we're looking to do with dbt as well.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/460.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=460)

 So why are we doing this? What is the strategic vision for us and why did we decide to even do the Intelligence platform and why dbt? And it really comes down to a couple of target outcomes we were looking to get out of this. We believe that because we do all this for ourselves, we can provide faster innovation to our customers by taking all the burden of managing the data platform and building baseline data models off their plate. Let them focus on where their real differentiators are, and let us do more of the heavy lifting for you.

The other thing to think about is we want to start thinking about the financial fabric of the globe more unified. So what we're looking to start doing is building standard understandings and semantics on that information. So whether you're operating in the US or Europe or Asia or Latin America, we're all speaking the same structure, the same ontology, the same semantics. And that'll be a big unlock for a more globally based economy, obviously, because now we all can share and provide information to each other. And that also then leads to the globally sourced model definitions, which is essentially a giant outsourcing exercise. The ability for our customers to not only use our models, but to contribute back to those models and share it with their partners and their users in a more seamless fashion.

[![Thumbnail 540](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/540.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=540)

So how are we really thinking about this from a dbt integration and  product point of view from an architecture? And I apologize this is high level, but this just gives you an idea of how we lay this out. The way we deploy the Intelligence platform is actually what we call a hybrid tenant approach. And not many people will probably know what that means, but really what we have here is we have separated and segregated services based on shared components, which are all multi-tenant, and then single tenant components, which is where the customer's data resides. And we do that for a couple of reasons, locality laws being the biggest part of that, and then of course, security and all that fun stuff.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/580.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=580)

For dbt we also take a similar approach. We have a main dbt account, and that's where we, that's where Nasdaq will do most of our dbt development,  where we're going to test our models, and then we're going to publish those models out. And then for customers who are interested in collaborating on dbt models, we will provide them their own dbt account, pre-populate it with a whole host of different things they don't have to think about. Everything from how code is sourced in GitLab to CI/CD processes to test things like test coverage and coding standards,

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/610.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=610)

all the way to how they can depend on Nasdaq data models within dbt to extend them and add their own information to that.  So what does this look like from a dbt project structure point of view? It's interesting how we thought about this. Everything on the left in the blue is what Nasdaq is going to do and provide out of the box. Obviously, first and foremost, we're always going to integrate with our own products and services, and we touched on that.

But you'll also notice we have a Nasdaq mapped data layer there at the contract section, and you might be asking why we would have that. There are a couple of reasons we think about that. One, that is the lowest level of the data we want to define in business terms. So think about defining transactional data in a more business-friendly manner. But two, we want to provide the flexibility and capability to support non-Nasdaq entities and services to plug into the ecosystem as well. By having that contract layer, any other tool or service that is in the same domain as any of the products we already service can instantly plug in and map their own data to that.

Once we get into the contract layer, that's where we start talking about the Nasdaq standard analytical models and insights models that we automatically give every customer within the domains they're interested in. But that's not always enough. We can probably cover 80% of what people need. We understand that there are going to be things our customers want to do that are custom to themselves. That's where the ability for them to provide their own information in the platform, then create their own contracts and their own models or extend our models is really critical. Because if we can't cover all use cases, we don't want to be the bottleneck to them unlocking true value and capability on top of their data.

The final interesting piece of this is the semantic layer. I'm not going to say semantic layer is a perfect term because I think the term semantic is not descriptive of what we're trying to accomplish. When we think about semantic, what we really mean is context. What we're looking to do is provide a layer of contextual knowledge about the data that whether it's a traditional BI use case or an agentic use case, they can understand the data they're talking to and know they're using it in the right way.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/750.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=750)

On top of that, we want to be able to enforce certain standards on the data they're pulling from. For example, if you have a calculation and you only want them to take an average, if you just go to the traditional data store, you have no control over enforcement. They're able to do whatever they want. With the semantic layer, we can enforce those types of capabilities, so they're only going to be able to leverage data in the way that makes the most sense. 

### Why dbt Enterprise and the Roadmap for Global Deployment

So why did we pick dbt and why are we using dbt Enterprise? Everything I just talked about, we could probably do with open-source dbt. That's a great open source project, and I think anyone who's used it would agree. So why did we go with dbt Enterprise? Well, first and foremost, they gave us the easiest path to multi-tenancy versus building our own. Second, the dbt meshing, the ability to depend on projects in the dbt Enterprise ecosystem, is critical for us to offer that extensibility to our customers.

The semantic layer, the ability to publish those rules and enforce those rules from an access point, has also become more critical to our overall offering. Governance and lineage and visibility are essential. We want to make sure the right users have access to the right things, they understand how data's derived, and they know what's going on. The final thing is dbt Enterprise has a lot of great tooling around continuous delivery and standard enforcement.

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/830.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=830)

One final thing I'll mention about dbt is just the general development experience. dbt Enterprise, if you haven't used it, has a great interface for creating dbt models. Whether you're a more technical user and you want to get into the SQL aspects of it, or with dbt Enterprise now having something like Canvas where you can do more drag and drop, you can enable a multitude of different users, both internally for Nasdaq but also for our customers, to participate in the management of data contracts and the data lifecycle. 

So how are we thinking about rolling this out? It's a lot to think about. When you're talking about deploying something at a global scale, there are a lot of moving pieces to it. One, how do you ensure you're doing it the right way? How are you doing it in a secure way, but also how are you rebasing on yourself to make sure your direction is going the right way?

The way we thought about this is, first, let's focus on the area we know the best, and that is defining the common ontology and semantics around the data we know. Because if we get those contracts right, those data models right, we can then move to the next phase, which is allowing the clients to offer that extensibility. Once you get to the extensibility aspect of it, now we want to talk about how we expose that information to AI and agentic use cases. How are you going to let digital workers talk to that data and make sure they're not going off the rails and doing the right thing?

The final phase, which is one of our goals that we're looking to get to, is a data model marketplace. When we talk about data and monetizing data, we always talk about the data itself. We never talk about the data models that you could potentially sell. If we all have a common definition and understanding of a baseline, customers now have the opportunity to create their own constructs and repackage and sell them to other consumers on the Intelligence Platform on a global scale. It's a new monetization methodology that previously wouldn't have been possible without something like dbt.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/910.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=910)



So just to wrap this up a little bit, what are some of the value drivers we're thinking about from our client point of view? What are we trying to measure against? How do we know we're being successful as we think about this over the next 12 to 18 months? First and foremost, do our customers have easy access to data? Do they understand that data? Are they making better decisions because of that data? That's obviously the easiest one we could think about because that's a universal thing that's been around for 20 years at this point.

The second is, have we made it easier to engage and use agentic use cases? Is the work we're doing allowing our customers to move faster in AI? The third one is, are we enabling new customer data access services? Are we giving them the ability to interact with data in new and interesting ways that they otherwise could not be able to do before?

We also want to measure how many of our models are actually reusable. We don't want to be in the business of creating custom one-off models. We want to create models that we know every customer can benefit from, so we want to make sure we're measuring the usability across the spectrum here.

And then one of the other ones I'll point out is that no-code experience. Are we enabling non-technical people to really engage with the data in ways they haven't been able to before, both in the data modeling side, again with something like Canvas and dbt Enterprise, but also in the digital work and agentic side? Is the data purpose-built for their use case? Are they getting the right answers? Because if not, none of this matters. We need to make sure that they can trust what they're doing.

[![Thumbnail 1010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e98c1908314d9ed4/1010.jpg)](https://www.youtube.com/watch?v=VBwX-w8wnMc&t=1010)

So with that being said, I finished three minutes earlier so everyone can go get a beer or a cocktail. If you have any questions, feel free to ask, but I  appreciate everyone coming out today and I hope you enjoy the rest of re:Invent.


----

; This article is entirely auto-generated using Amazon Bedrock.
