---
title: 'AWS re:Invent 2025 - Optimizing generative AI workloads for sustainability and cost (AIM253)'
published: true
description: 'In this video, Isha Dua, Senior Solutions Architect at AWS, addresses the environmental crisis caused by AI''s growing energy demands. She highlights that data center electricity consumption has surged since 2021, with model training sizes growing 350,000 times over ten years. Goldman Sachs forecasts 60% of increased data center electricity will come from fossil fuels, producing 215-220 million tons of CO2. Dua recommends using AWS managed services like Bedrock, choosing appropriately-sized models through Bedrock Evaluations rather than the largest available, and following a customization hierarchy: prompt engineering, RAG, parameter efficient fine-tuning, then full fine-tuning. She emphasizes selecting energy-efficient silicon like Trainium (25% more efficient) and Inferentia instances, applying inference optimization techniques including pruning, distillation, and quantization, and implementing continuous monitoring throughout the ML lifecycle. Building on AWS can reduce carbon footprint by up to 99% compared to on-premises solutions.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/0.jpg'
series: ''
canonical_url: null
id: 3085654
date: '2025-12-05T07:20:22Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Optimizing generative AI workloads for sustainability and cost (AIM253)**

> In this video, Isha Dua, Senior Solutions Architect at AWS, addresses the environmental crisis caused by AI's growing energy demands. She highlights that data center electricity consumption has surged since 2021, with model training sizes growing 350,000 times over ten years. Goldman Sachs forecasts 60% of increased data center electricity will come from fossil fuels, producing 215-220 million tons of CO2. Dua recommends using AWS managed services like Bedrock, choosing appropriately-sized models through Bedrock Evaluations rather than the largest available, and following a customization hierarchy: prompt engineering, RAG, parameter efficient fine-tuning, then full fine-tuning. She emphasizes selecting energy-efficient silicon like Trainium (25% more efficient) and Inferentia instances, applying inference optimization techniques including pruning, distillation, and quantization, and implementing continuous monitoring throughout the ML lifecycle. Building on AWS can reduce carbon footprint by up to 99% compared to on-premises solutions.

{% youtube https://www.youtube.com/watch?v=9SV_rIFrFh8 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/0.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=0)

### The Environmental Crisis of AI: Understanding the Carbon Footprint Challenge

 Good afternoon. I know that over the past two days you have probably heard quite a bit about agentic AI and generative AI. We are going to talk about it as well. All of this artificial intelligence is promising to solve humanity's greatest challenges, but it is coming with one of our more severe environmental crises. In this talk today, we are going to be discussing optimizing generative AI workloads for sustainability and cost. I am Isha Dua, a Senior Solutions Architect at AWS. I have been here for about six years, and I am looking forward to this talk. Let's get right into it.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/50.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=50)

 Over the past decade, data center electricity demand and energy consumption remained relatively stable at around 100 terawatt hours. We were able to incorporate offsets to balance the growing demand for electricity and energy. However, in 2021, we saw the generative AI boom with foundation models flooding the market. Since then, energy consumption, resource consumption, and data center electricity consumption have increased at a drastic rate. The model training size itself has grown approximately 350,000 times over the last ten years, which is a huge number. There are also many studies indicating this trend.

[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/100.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=100)

 In August 2025, Goldman Sachs Research forecasted that about 60 percent of the increasing electricity demand from data centers will actually be met by burning fossil fuels, which will amount to around 215 to 220 million tons of carbon dioxide in the atmosphere. To give you some perspective, if you were to drive a gas-powered vehicle for 5,000 miles, it would produce a ton of carbon dioxide. So this is a very staggering number. There are other studies as well. If you look around, you will find many of them now talking about the environmental crisis that we are entering because of this increasing demand for electricity. A 2024 article from the World Economic Forum also notes that the computational power for these workloads is doubling every 100 days or so.

The statistics and numbers are staggering. We as developers, architects, and hyperscalers have to do our part to innovate more responsibly and build interventions into the system so that we are able to mitigate this ballooning carbon footprint. Not just developers, but even hyperscalers have to look into how to optimize the building of data centers, how to use efficient algorithms, and how to build efficient silicon for these model training and model inference workloads. That is where sustainability at AWS comes in.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/200.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=200)

 At AWS, back in 2019, we took the Climate Pledge, and one of our core tenets was to power our operations with 100 percent renewable energy. We did meet that goal in 2023. My recommendation here is that if you were to build a generative AI workload or an agentic AI workload, or if you were to consume these services, building it on AWS is automatically going to be a huge carbon reduction opportunity. AWS is about 4.1 times more energy efficient than doing this on premises, and this comes because of many efficiencies that we have built. For example, I was talking about powering our operations with 100 percent renewable energy. We have hardware efficiencies, and all of our managed services have service teams making sure that sustainability optimizations are built into the infrastructure for these services.

We also have cooling efficiency in the way we cool our data centers. We use evaporative cooling and lower carbon concrete in our data centers. That is another way we are helping in this cause. We have optimized silicon for model training and model inference, and we will talk about it later as well. What I want to say is that basically, optimizing these workloads on AWS and building them on AWS can automatically reduce your carbon footprint by up to 99 percent.

[![Thumbnail 300](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/300.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=300)

### Strategic Recommendations for Sustainable AI Development on AWS

 Let me jump into some of the recommendations we have when you are actually building these workloads out. The first recommendation I have here is using a managed service.

When you use a managed service, it lets you operate more efficiently because you've shifted the responsibility of high utilization and sustainability optimization to AWS. Our service teams are going to manage the capacity underneath. We're going to manage how we're going to optimize the infrastructure. We're going to manage the sustainability of the infrastructure, whereas you don't have to do all of that undifferentiated heavy lifting, and you can focus on just building your workloads.

Some of the services that you see here are managed services. I'm sure you guys have already heard a lot about Bedrock. It's a serverless, fully managed service that gives you access to over 200 foundational models through a single API. We also have other services here that you can see on the screen. If you're very comfortable with the SageMaker ecosystem, for example, then using SageMaker for model training is a good approach. If you come from the HPC world and you're very comfortable with SLURM orchestration and you want to do model training but you want to use SLURM as an orchestrator, then we also have SageMaker HyperPod which can help. But if you're more inclined toward Kubernetes orchestration, then we do have EKS as well, which is a managed service for Kubernetes orchestration.

[![Thumbnail 410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/410.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=410)

Similarly, we're building agents in genetic systems and multi-agent systems. We have a service called Bedrock Agents. The main takeaway here would be that using a managed service can help you make more sustainable choices because of the optimizations that we have done on these services in the background. 

Now, one could ask me, OK, if I choose Bedrock and Bedrock has 200 plus models, how am I supposed to make that model selection? How do I know which is the more sustainable model, or how do I know that this is the model that's the right fit for my use case? Here is where you would ask yourself certain questions. What is my use case? What's the business outcome I'm trying to achieve? Do I need an open source model or do I need a proprietary model? Do the outputs only need to be in English, or does the model need to be multilingual? Does the model need to be very general, or does it have to produce outputs that are very specific to, let's say, the healthcare domain or the financial services domain? So is it a general model or is it a domain-specific model? There are some questions that I would recommend that you ask yourself here.

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/470.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=470)

Bedrock does have a capability called Bedrock Evaluations.  This comes in handy. It lets you assess and compare different models. You have three techniques here. You can use an LLM as a judge, which means you can judge the output of these models based on correctness, completeness, and even potential harm. But if you're more used to traditional metrics like BLEU scores or F1 scores, you can do that as well. And you have the third method, which is using human-in-the-loop evaluation. You can use your own private workforce. You can use an Amazon-provided workforce. It's a fairly easy process. You define the task type, you give it your custom prompts, you define some evaluation criteria, and then you assess the results. Bedrock Evaluations is going to automatically give you the more sustainable model choice.

The key takeaway here is that you do not need the best and the brightest and the biggest model out there for your use case. Sometimes a smaller model can just do the trick. Remember, the bigger the model is, the more inference resources it's going to need during inference, the more it's going to cost you, and the more the carbon footprint is going to be. So just because it's the shiniest object out there, that's not the way or the approach to actually choose a model. One example here would be ChatGPT 3.5, which has 175 billion parameters, but there's an Alpaca model which is 7 billion parameters, and they both behave qualitatively very similar because Alpaca is a distilled version of ChatGPT 3.5. Again, consider the use case and the business outcome. Make sure that you select the model based on what you're trying to do.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/580.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=580)

Now, once you select a model, there are chances that you may have to customize the model a little bit. You may have to adapt it to your use case because it's not generating the output exactly the way you want it or exactly how it fits your use case or your scenarios. So here we have a sort of photographic representation, and if you look at this, it goes up in the order of increasing energy consumption and carbon emissions. 

The first recommendation here would be that whenever you're trying to customize a model, never think about training from scratch unless you've tried all of these other techniques first. If you have a base model to customize, the first thing that you can do is try prompt engineering with some simple prompts. Check if this is something that can actually help you and if it's producing adequate results. If you're happy with the accuracy, then this is the simplest approach with minimal investment, low cost, and a lower carbon footprint. It's a good technique which is on the bottom, labeled as PE.

Let's say there's a scenario where prompt engineering did not work, and you may have to augment the model with some proprietary information. You may want to add your own internal documentation that could provide the model more context to produce more tailored outputs. In that case, retrieval augmented generation can actually help. So that would be the second approach that we would recommend. Now there could be scenarios where the model is very domain specific. In those cases, you may have to resort to fine tuning, which could be parameter efficient fine tuning or, if that doesn't work, full fine tuning.

Fine tuning is another approach that you can try if retrieval augmented generation and prompt engineering don't work. Parameter efficient fine tuning is more sustainable because it only tunes a subset of parameters and not all the parameters. Techniques like LoRA, prefix tuning, or P-tuning are out there that can help. Finally, you have full fine tuning, continued pre-training, and training from scratch approaches, which we recommend only if nothing else worked out. That is the most costly approach with maximum resource consumption and the heaviest carbon emissions impact. Make sure that your use case actually justifies training from scratch for the scenario that you're trying to meet.

Another quick recommendation would be choosing the right silicon, whether you're doing model training or model inference. Choosing the right silicon really matters. There are EC2 instances and silicon that we produce in-house that are more efficient than comparable EC2 instances. For example, we have the Trainium family which you can use for model training, which is about 25% more energy efficient than comparable EC2 instances. This was the Trainium 1 family. Now we have Trainium 2, which is 3 times more energy efficient. We're also coming out with Trainium 3, which is likely to be even more energy efficient.

[![Thumbnail 730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/730.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=730)

Making sure that you're using the right silicon for training, coupled with the managed services that we spoke about previously, is actually going to give you better price performance, better cost, and obviously better resource consumption and energy consumption. For inference as well, we do have Inferentia instances which are more energy efficient and have 50% better performance per watt. You can use those for inference, but there are scenarios where you may not need GPU instances for inference. There are smaller models, for example, classifier models that you can actually run on non-GPU instances. Think about Graviton instances, which are 60% more energy efficient than comparable EC2 instances, and their performance has become 4 times better since Graviton was launched in 2018. So choosing the right silicon really matters here as well. 

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/840.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=840)

Now, another technique is related to when you're deploying the model or generating inference and prediction responses. Once you've trained the model, adapted it, and customized it, you're at the stage where you're producing inference responses. There are many techniques out there that can reduce the model size, compress the model, and optimize memory usage. I would definitely recommend that you think about these. There are libraries like DeepSpeed, Hugging Face Accelerate, and Faster Transformer, and these libraries offer capabilities wherein you can compress a model. 

For example, you have the ability to optimize the model for distributed processing. You can prune unnecessary weights. You can sometimes distill or transfer the knowledge of a larger model to a smaller model with very little loss in accuracy. Model distillation is also a very good process here. You can try different precision types to further optimize your models for efficiency and performance.

[![Thumbnail 930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/930.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=930)

Quantization is another approach that you can consider. You're optimizing the models for hardware and the silicon that you're deploying on, so there are techniques during the inference stage that you can implement to reduce the model size. If you reduce the model size, you're optimizing resource consumption, reducing cost, and reducing carbon emissions  in the process.

### Continuous Monitoring and Key Takeaways for Optimizing AI Workloads

Throughout this entire life cycle, from the moment you thought of a use case, built or customized a model, and deployed it, we recommend building in observability at every stage. You need a way to continuously monitor and optimize these models once they're deployed to production or wherever they're deployed. Monitoring is essential. You need to check for data drift and model drift, and verify that the outputs are actually relevant in real-world scenarios.

You also need to check for potential harm, such as bias in the outputs or harmful content being generated. Building responsible AI practices throughout the entire life cycle is critical. Many tools are available for monitoring and optimization. CloudWatch allows you to monitor your CPU, memory, disk, and other metrics. SageMaker Profiler and Neuron Monitor are well-suited for model training, providing insights into training metrics, data distribution, loss, and accuracy. These tools help you look into the black box of training and optimize parts that don't look quite right.

[![Thumbnail 1070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/1070.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=1070)

For those using Nvidia family instances, such as the P family for training or inference, you can use the Nvidia Systems Management Interface to examine GPU usage and other metrics that help you optimize further. The key takeaway is to build observability across the entire life cycle, as this will help you optimize different paths and phases. 

Let me wrap up with the key things we discussed. For user-managed services, ensure that the base model you select has been chosen with your business outcome and use case in mind. The biggest and brightest model is not always the best. Model customization techniques should be considered progressively, starting from prompt engineering to RAG to PEFT, then full fine-tuning, and finally training from scratch.

[![Thumbnail 1140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/07d4d88d5e69d740/1140.jpg)](https://www.youtube.com/watch?v=9SV_rIFrFh8&t=1140)

There are many inference optimization techniques we spoke about, including pruning, distillation, and quantization. Libraries are available to help you implement these. For silicon choice, there are options available for training and inference that are more energy efficient and offer better performance, price-performance, and performance per watt. Finally, make sure you're continuously improving this entire process by monitoring and building observability at every stage. 

These are some links I'm leaving you with that discuss optimizing different phases of the generative AI life cycle and guidance for optimizing the ML Ops ecosystem for sustainability on AWS. Thank you very much for your time. I'm available if anybody has any questions.


----

; This article is entirely auto-generated using Amazon Bedrock.
