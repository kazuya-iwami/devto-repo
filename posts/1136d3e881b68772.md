---
title: 'AWS re:Invent 2025 - Build Advanced Search with Vector, Hybrid, and AI Techniques (ANT314)'
published: true
description: 'In this video, AWS Solutions Architect Jon Handler, Product Manager Bobby Mohammed, and Recruit ML engineer Ryosuke Sudo explore the evolution from lexical to agentic search in OpenSearch. Handler explains how lexical search uses BM25 scoring for exact matches, semantic search leverages embeddings for meaning-based retrieval, and hybrid search combines both using Reciprocal Rank Fusion, achieving up to 13% improvement in NDCG scores. Sudo demonstrates Recruit''s HotPepperGourmet implementation, showing how custom hybrid search with separated lexical and vector scores delivered 10% more bookings and 90% fewer zero-result searches. Mohammed introduces OpenSearch 3.3''s agentic search capabilities, including multimodal embeddings with Amazon Nova, MCP integration with 20 ready-to-use tools, memory APIs for short and long-term storage, and specialized agents for RAG orchestration. The session highlights 11x performance improvements since version 1.3 and new cost optimization features like S3 vector integration for trillion-scale workloads.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/0.jpg'
series: ''
canonical_url: null
id: 3086801
date: '2025-12-05T15:22:53Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Build Advanced Search with Vector, Hybrid, and AI Techniques (ANT314)**

> In this video, AWS Solutions Architect Jon Handler, Product Manager Bobby Mohammed, and Recruit ML engineer Ryosuke Sudo explore the evolution from lexical to agentic search in OpenSearch. Handler explains how lexical search uses BM25 scoring for exact matches, semantic search leverages embeddings for meaning-based retrieval, and hybrid search combines both using Reciprocal Rank Fusion, achieving up to 13% improvement in NDCG scores. Sudo demonstrates Recruit's HotPepperGourmet implementation, showing how custom hybrid search with separated lexical and vector scores delivered 10% more bookings and 90% fewer zero-result searches. Mohammed introduces OpenSearch 3.3's agentic search capabilities, including multimodal embeddings with Amazon Nova, MCP integration with 20 ready-to-use tools, memory APIs for short and long-term storage, and specialized agents for RAG orchestration. The session highlights 11x performance improvements since version 1.3 and new cost optimization features like S3 vector integration for trillion-scale workloads.

{% youtube https://www.youtube.com/watch?v=U3xO5QbzYQY %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/0.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=0)

### Introduction: The Critical Role of Search in Modern Applications

 I am Jon Handler, a Senior Principal Solutions Architect with AWS. I'm joined by Bobby Mohammed, who is a Principal Product Manager for AWS, and Ryosuke Sudo, who is an ML engineer at Recruit. We'll be giving you a talk today in three parts. I'll take the first part, then Ryosuke will take the second part, and then Bobby will take the third part.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/50.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=50)

Who here has actually bought a product on Amazon.com? Everybody, right? But who here has done that without using the search engine? Yeah, like nobody, right? When you go to Amazon.com, you have an intention, you have a goal, you want to buy something, and the search engine is how you express to Amazon.com that you want it.  If you go to Google, you have something you want to know, you type some words in and Google hopefully comes back with some helpful information that solves the question that you have.

If you're building an application with all kinds of different use cases across things apart from e-commerce, document stores, and all of these things, we mediate between our intentions, our desires, and what the application knows through search. Traditionally, until recently, that has meant we've used words to communicate to the search engine what our intentions are. Now we have a huge step change in the world that has changed how we interact. We expect more from our information retrieval systems. We expect them to be able to interpret and understand what we're saying. So we're going to talk a lot about that today.

[![Thumbnail 110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/110.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=110)

### The Limitations of Traditional Word-Matching Search and OpenSearch Overview

 One problem with search engines, and you probably have had this experience too, is that you say something like socks for children and you get random stuff, some of it's socks, some of it's children's toys. There's a mismatch between what you expressed and what you got. Really what's happening is that until these new technologies, search engines have keyed on words and have done word-to-word matching. Here's an example: I said I want a good TV to watch sports. I know that means fast refresh, very bright, very sharp picture, but those are not things that I actually typed into the search box. Even if I did, perhaps the people who were describing their TVs would not have actually used those exact words. You can see here there's a bunch of search results, and mostly what it keys on is the word TVs. With the newer technologies, we're going to talk about how we get to a better way to do retrieval.

[![Thumbnail 180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/180.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=180)

 Who is familiar with OpenSearch? Great, a lot of familiarity in the audience. OpenSearch is a community-driven, open-source suite for AI-powered search and log analytics. The project itself has open governance under the Linux Foundation. We have some of our premier members: AWS, IBM, SAP, and Uber, as well as general members of the foundation. It is open governance, it is a vibrant community, and it is going gangbusters.

[![Thumbnail 210](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/210.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=210)

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/230.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=230)

[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/240.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=240)

 Of course we have our Amazon OpenSearch Service. If you want to run OpenSearch in the AWS cloud, Amazon OpenSearch Service first of all makes it easy for you to deploy and operate OpenSearch in the AWS Cloud.  Second of all, we have performance gains both in the open source project as well as in the managed service. We have 6X performance gains from version 1.3 to 2.19, and we have more performance gains in version 3, which is live now.  We also provide a number of different options for cost efficiency, knobs and ways you can tune either for vectors or for log analytics, and we're going to be talking about those later. We also have a number of integrations that make it easy to move your data from other AWS services to OpenSearch Service.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/260.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=260)

[![Thumbnail 290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/290.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=290)

### OpenSearch Architecture and the Evolution from Lexical to Hybrid Search

 OpenSearch itself is a distributed database system. It runs on a set of nodes. Data nodes provide the storage and request processing. We have our cluster manager nodes, they orchestrate the cluster. We have our coordinator nodes, they coordinate request processing, and we have ultra warm nodes that let you tier out older data, especially log data.  We also have a serverless offering. With the serverless offering, you just create a collection, and the collection is a collection of indices. You use REST APIs to send your data. In all cases, we front the cluster with some kind of load balancer and you send your requests.

[![Thumbnail 320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/320.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=320)

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/340.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=340)

OpenSearch is a REST API, so it's all JSON-based and very familiar. As I was alluding to, if you're building a search application,  chances are you started with lexical search. You put some data into OpenSearch and use the lexical query with BM25 scoring to get results. But with all the new technologies, we have all this demand and drive to include semantic search.  Semantic search leverages large language models and embedding models that are able to pull out meaning from strings of text.

[![Thumbnail 360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/360.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=360)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/380.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=380)

[![Thumbnail 390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/390.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=390)

You have lexical search that works pretty well and semantic search that works pretty well, but ultimately, we see the world coming to a hybrid model where you want to have the benefits of lexical and the benefits of semantic and blend them somehow.  I'm going to take the first part of the talk to discuss this, and then the newest thing is agentic search.  With agentic search, you have an even better way of building search results from queries. So let's talk about the journey from lexical to hybrid. 

[![Thumbnail 420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/420.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=420)

### Understanding Lexical Search: Word Matching and BM25 Scoring

Lexical search is very good if you know you want the Samsung 64 inch OLED and you type those words. You can match those words and get a search result that's a good result. So it's good for very specific queries, but it's bad for more abstract queries like "I want a TV for watching sports." When you use OpenSearch, we  start with the search document. The lexical journey begins with the search document. As a design point, when you're using OpenSearch, first you figure out what you want to retrieve, then that's what you put into OpenSearch.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/460.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=460)

This search document is a bunch of JSON. This is an example from the Amazon product Q&A public dataset. It's a question with some information about the product and answers for the question. All of this goes to OpenSearch. OpenSearch creates indices for each of these JSON fields and enables fielded matching for that content. When we do lexical search, the first thing we do is take  blocks of text and process them according to language-aware analyzers.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/520.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=520)

If you look at the number 22 over there, the text "98 inch Q65 QLED 4K," you can see there are some language processing happening with words like "larg" and "Dolby" with an I. The purpose of that is to make it easier for queries to match those words. Runs, run, running are all the same word. A query that says any of those should match that document. There's stemming that goes on. We remove common terms called stop words, and we add synonyms to enable better matching. The second one there is a TV frame, actually not a TV. This query "large screen televisions" ought to match the first one better than the second one. 

[![Thumbnail 560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/560.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=560)

[![Thumbnail 570](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/570.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=570)

We have to analyze the query as well. We changed the text that we indexed, so we analyze the query as well. With OpenSearch, the core data structure is an inverted index. An inverted index maps every term that's been seen in a field to the documents that contain that term. That's an efficient way to set up for retrieval. When we have the terms "large," "screen," and "television," we just look them up in the index, which is very fast, and we have the set of posting lists, which are the documents that contain them.  We apply the query logic, in this case an OR, so we take all of the documents with "large," "screen," and "television" and OR them together and we get 13, 12, 18, and so on. 

[![Thumbnail 590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/590.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=590)

The next thing we want to do is score them. One of the core features of a search engine is to provide a relevance value that helps sort the possible search results into the desired search results. This is the core of what search engines do.  So how do we score? Well, the first thing we do is look at all the words and how common those words are. Something like "television" is less common across all of the documents than, say, "large." Large is going to be a very common word. So "television" gets a higher score than "large."

[![Thumbnail 610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/610.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=610)

[![Thumbnail 620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/620.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=620)

We then multiply by how many times  it matched in each of these two example documents. We multiply together, and that gives us a result.  In this case, we see that document 22 matches a little bit better than document 38. This is the desired result. This is what lexical search is. This is all lexical search does. We are matching words and scoring based on the value of those words.

[![Thumbnail 640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/640.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=640)

### Semantic Search with Vector Embeddings: Capturing Meaning Beyond Keywords

With semantic search,  we are using models to get at the meaning of strings of words. A word itself is already carrying meaning, so matching word to word, we already have some small window of meaning. Now we have expanded that window, so that when I say large screen television for sports viewing, the fact that sports viewing and large screen television are close together enables us to bring in that large screenness to the sports television. It lets the model bring that together. It is not good at specific queries though. If I say something like this tickle 98 inch Q65, I do not want it to try to understand what I mean. I just want to put those words in and get an exact match. But it is really good for contextual information, and it is good for multiple modalities, for multiple languages, all of these cases where meaning is what you want to match.

[![Thumbnail 710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/710.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=710)

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/750.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=750)

When we start with our document,  our document has a journey. We have some portion of the document that is textually representing what that document is. In this case, the product description is tiny and hard to see. I take that out as a chunk. That chunk I then send to Bedrock, SageMaker, or some other model technology to generate a vector embedding. I add that vector embedding back into the document, which you can see there in pink. I then ingest that document into OpenSearch Service. In this case, I was using OpenSearch ingestion. 

[![Thumbnail 780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/780.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=780)

How do we decide which is a better document? Vector embeddings are just large arrays of numbers, they are vectors. They create a multi-dimensional space. When I ask a question like what TVs are good for watching sports, I create an embedding for that as well, and that will live in the same space as all of the products and documents that I have.  I project that into the space, and I then find the nearest neighbors. Closeness is the measure of similarity for vector search.

[![Thumbnail 790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/790.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=790)

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/810.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=810)

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/820.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=820)

[![Thumbnail 830](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/830.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=830)

[![Thumbnail 840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/840.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=840)

 Closeness is a nebulous concept, but we can put some definition around it. Let us say I have these three vectors. The first thing we can do is calculate the Euclidean distance. The darker one on the left top is the query vector, and the other two are document vectors.  I can take a Cartesian distance between those and say that one is three times farther away than that one, so that one is closer.  We also can use cosine, which are common similarity measures.  Here are my three vectors again, with the query vector in the middle. We can look at the angles between the vectors to decide which one is actually closer, and the one on the bottom is twice as far away in cosine world. 

[![Thumbnail 860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/860.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=860)

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/870.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=870)

[![Thumbnail 890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/890.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=890)

### OpenSearch Neural Plugin: Simplifying Semantic Search Implementation

I am going to introduce you to the neural plug-in, if you do not know about it. OpenSearch's neural plug-in provides a bunch of different things that make all of this semantic processing easier. The first of those is a connector library that enables you to connect to embedding models that are external, as well as large language generating models.  We have a connector framework, and then we can define pipelines in OpenSearch.  As you ingest your documents into OpenSearch through the neural plug-in, OpenSearch calls out to the embedding model, gets the embedding, and puts it on the document. This simplifies the ingestion. There are lots of ways to do ingestion, but this is a simple way if you want to get started quickly and test. 

Similarly, on the search side, we provide a number of different processors that enable you to create a query embedding with a neural query. You send a text and a model ID, and OpenSearch does the embedding for you.

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/910.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=910)

 To engage this, you use an ingest pipeline, which is as simple as a put ingest pipeline command. You can see there's a field map in there that says for this field, create an embedding and put it in that field. It's super simple to use. You pass a model ID, which is the reference to the connector that tells OpenSearch which external model host to call. You put that into your mapping.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/940.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=940)

[![Thumbnail 970](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/970.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=970)

Now OpenSearch behind the scenes is somewhat database-like in that it has a schema for every field.  The schema tells it how to process that field. If it's text, it cuts it up and applies language processing, and all of that. Once you've defined the pipeline, you just set that as the pipeline for your mapping, and OpenSearch automatically calls out and creates vector embeddings on the way in. Finally, we have our query, so our neural query.  Here, we just say which field is the embedding field and we give it the query text and the model ID. It does all the work to execute that query.

[![Thumbnail 980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/980.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=980)

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1000.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1000)

I'll mention really quickly that we do have a batch inference that we recently launched that enables you to use OpenSearch ingestion, which is a feature of the OpenSearch service, to call out to Bedrock batch inference and embed all the data as it's going in.  I'll also mention we have something called Automatic Semantic Enrichment.  This works with a different kind of embedding model, a sparse model, and it's completely automatic. You simply set the field type, and OpenSearch does the work of creating the sparse vectors for that field.

[![Thumbnail 1020](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1020.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1020)

### Hybrid Search: Combining Lexical and Semantic Approaches for Superior Results

All right, so coming to hybrid search.  We saw problems with semantic search, and we saw problems with lexical search. So it's natural to say, sometimes I have really exact queries, sometimes I have really semantic or meaning-based queries, and I want to do the best of both of them. It's not something you're going to use for everything, but it's really good for most search use cases. In some ways, this helps you maintain a stronger relationship between the text and the meaning of the text.

[![Thumbnail 1060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1060.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1060)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1090.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1090)

In OpenSearch, it's pretty simple to use a hybrid query like this.  You have the query itself, and then you have one to five query clauses. Each of those clauses runs in parallel. In this example, we have a neural query and a sparse query running both of those, though it could be a lexical query as well. Running both of those, we then need to blend the scores. So how do we blend the scores? Well, first we have to create a search pipeline.  The search pipeline tells OpenSearch how to blend those scores together.

[![Thumbnail 1110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1110.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1110)

This example is using a normalization processor that's doing a numeric normalization of the scores and then comparing or blending them that way. So how that works in practice is, let's say I have query one.  Those are the scores on the left. And query two, query one is a lexical query. So I got 412 and 396 and all of that. And then query two is a neural query, and I'm using cosine similarity, so my score range is zero to one. Clearly I can't just put those together.

[![Thumbnail 1150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1150.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1150)

So with numerical normalization, what I do is I put them in a normal distribution. I normalize them to the same scale, and then I average to get the result. That gives me a final ranking like that. The other way that you can do this is called Reciprocal Rank Fusion.  Reciprocal Rank Fusion looks at the slot that each result is in and then gives a weight for closer to the top, descending. It then combines based on the weights for the query for the result position. So with RRF you actually get some benefits if you have outliers in terms of the scores. They don't affect as much, and it's a much more stable way to go about this.

[![Thumbnail 1180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1180.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1180)

So kind of wrapping up this little section, we have some standardized corpora here.  So NF Corpus, ArguAna, FIQA, Trec Covid, Sci Docs, and Quora. You can see the lexical score. This is NDCG at 10, so this is the Normalized Discounted Cumulative Gain, and what that means is the higher up you are in the results.

Cumulative gain measures how well your results match the ideal set. The higher up a result appears compared with the ideal ranking, the more points you get. It's a scale of percentages where you improve as you more closely match the ideal search results. We have our lexical scores, hybrid with numeric combination, hybrid with Reciprocal Rank Fusion (RRF), and then a bunch of percent differences. The thing about these standard measures is that it's extremely difficult to move even 0.1%. When we look at something like the Quora or FIQA workloads, which are both question and answer workloads, we get up to 12 or 13% improvement with RRF. That's a huge result, showing significant benefit from doing hybrid search.

Most of the results are better, though some are a little bit worse. That's what I have for you, and I'm seeding the thought about lexical, semantic, and hybrid search, and how to use each of those and what the benefits are of picking hybrid. I'm going to ask Ryosuke Sudo to come up and talk about how they do that at Recruit. Thanks.

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1330.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1330)

### Recruit's Challenge: Building Search for HotPepperGourmet Restaurant Platform



Hi everyone. I'm Ross Castillo. I'm a machine learning engineer at Redwood. This is my first time at Green Band, so I'm very excited and a little nervous, but I'm okay. I will enjoy the sessions. Today, I'm going to talk about our hybrid search systems, why we need hybrid search, and how to build it. Let's get started.

This slide is one of the most important slides today. Our legal team asked me to include it. The key takeaway is that we are good guys. No personally identifiable information was collected, stored, or used for training. Our legal teams are happy now, so let's move on.

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1350.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1350)



This is our company, Recruit. We operate in many kinds of fields from jobs to travel and beauty. You might know our global brands like Indeed and Glassdoor, which are part of our company. Today, I'll focus on one of them: HotPepperGourmet, a restaurant search and booking platform in Japan.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1370.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1370)



HotPepperGourmet is one of Japan's largest restaurant search and booking platforms. We are number one in both the number of reservable restaurants and online reservation users. We handle hundreds of thousands of reservations every day and operate tens of millions of users monthly. Most users start from the search box at the top, so improving search quality is key to our success.

[![Thumbnail 1400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1400.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1400)



### The Fragility of Lexical Search and the Limitations of Pure Vector Search

Let me walk through our search challenges. Our original search systems relied heavily on lexical search, which you might call traditional keyword matching.

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1410.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1410)



For a long time, it had great demand, but it had obvious limits. For example, imagine a user wants sushi restaurants. If they spell it perfectly, lexical search matches correctly. But what if they make a typo, like sushi restaurants? Simple lexical search can't match sushi to sushi. So lexical search is fragile against typos.

[![Thumbnail 1430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1430.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1430)



These problems get way worse in languages like Japanese, which has no spaces. To help you imagine what that's like, I'll remove spaces from the query. In the query "sushi restaurants," our segmentation process correctly splits the query into sushi and restaurants. That's great. But with one typo, "sushi restaurants," our segmentation process breaks. The word is wrong and the number of words is also wrong. So lexical search is fragile against typos, especially in languages like Japanese without spaces.

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1450.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1450)



You might think, "Hey, let's use semantic search. It's modern and it handles typos better." You're right, partly. Let me share another scenario.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1480.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1480)



[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1490.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1490)



Here is a user query "Sushihiro," and there are two candidates: "Sushi Hero" on strip and "Sushi Magic."

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1520.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1520)

[![Thumbnail 1560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1560.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1560)

We are using state-of-the-art MNE model on Sushi Magic and other restaurants. You might know that starting with G, that's a very strong large language model. You might think this is a very easy problem and better to succeed.  Let's look at the result. Number one is Sushi Magic. Why, even with a state-of-the-art large language model? Because vector search focuses on meaning, not keywords. The word "magic" has context-less meaning, but the word "on the strip" has strong location context. So strong location context ironically pushes the correct answer further away rather than completely different restaurant names. That is the limitation of vector search. 

[![Thumbnail 1580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1580.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1580)

[![Thumbnail 1590](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1590.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1590)

This is a quick summary. Lexical search is precise but struggles with typos and segmentation. Vector search is semantic and context-aware, but it can miss exact matches. What we want is the best of both worlds. That is why we need hybrid search, which combines precision and semantic understanding.  So let's get into the technical details of how we build it. First, why did we choose OpenSearch? 

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1660.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1660)

### Why Recruit Chose OpenSearch and the Importance of Fine-Tuned Models

For three quick reasons. First, hybrid search capabilities for both lexical and vector search. It's easy to implement, so we can focus on improving the vector search model. Second, it's fully managed on AWS. Development and operations, scalability, blue-green deployments, backups, and security are all managed. It helped us tremendously. Third, it has a rich ecosystem. It has managed ETL like open source Ingestion, and it also has language-specific plugins that help for Japanese language support. 

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1660.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1660)

We also considered other options. Vendor B has rich features, but higher cost and less diverse integrations. Another vendor has powerful capabilities, but it's complex to manage. Other vector databases are great for similarity search, but they're limited for text search. For us, OpenSearch hit our requirements perfectly. So let's next talk about our vector model. 

[![Thumbnail 1710](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1710.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1710)

We use a two-tower model architecture, with one encoder for queries and one for restaurant information. It trained on our user logs and synthetic data we generated with large language models. Today, I'm not going to go deep into our architecture, but the key takeaway is that fine-tuning really matters. For our case, lightweight fine-tuned BERT actually outperforms large language models. So I recommend that if you're dealing with domain-specific vocabulary like restaurant names, don't just rely on off-the-shelf vector models. Fine-tune them. It makes a significant difference. 

[![Thumbnail 1750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1750.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1750)

After we got a better model, our first step was to try using the built-in hybrid search systems. It is very simple. We just write multiple queries in different fields. That's very easy. We don't need any architecture changes, and it's very easy to implement, so we can focus on improving vector models. It gave us many business impacts, but we realized that we could go even further. To understand this opportunity, let's look at how our hybrid search system works. 

[![Thumbnail 1770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1770.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1770)

### Custom Hybrid Search Architecture: Separating Scores for Smarter Ranking

In building hybrid search systems, OpenSearch first takes lexical and vector results in parallel and normalizes them, and after that, merges them into a single score before ranking. That is a problem.  When both scores are high and both scores are low, that is okay, we can carry on. But when the vector score is high and the lexical score is low, that means typos or paraphrase. When the lexical score is high and the vector score is low, it might mean rare words or domain-specific vocabulary. So they represent completely different things, and which should be better depends on the user context.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1820.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1820)

To handle this problem, we realized that our ranker needed vector scores and lexical scores separated. That is our key insight. To solve this, we built a custom hybrid search.  In our custom hybrid search, our executor runs lexical search and vector search in parallel and gets both scores separately, then feeds them into our ranker. Our ranker can use lexical scores and vector scores separately and also incorporate user logs, user intent, and other signals. This enables us to make much smarter decisions and adaptive ranking. It is a little more complex to implement, but it gave us up to twice the improvement in top one search accuracy, so it was totally worth it.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1860.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1860)

[![Thumbnail 1890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1890.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1890)

 This is our binary architecture. Our custom ranker moved out of OpenSearch and runs in an HS container, and our vector models also run in an HS container. This setup gave us more flexibility and made scalability much easier. Let's look at our business impacts. We tested several versions from the original, hybrid, and hybrid plus ranker approaches.  After launching custom hybrid search, we achieved a ten percent improvement in the number of bookings via search and a ninety percent reduction in their searches. Our key takeaway is that properly combining lexical and vector scores led to significant business impact.

[![Thumbnail 1920](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1920.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1920)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1940.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1940)

To wrap up, what worked well? First, managed blue-green deployment is great.  Zero-time downtime scaling gave us operational confidence, and open source accelerated our experimentation. Finally, hybrid search seamlessly integrated in our solutions is great. It enabled us to launch quickly our first steps.  What's next? First, I would love support for building hybrid structures with model-based fusion. That would improve our customer solutions entirely. We also need multi-vector search exploration. Multi-vector search improves not only the money but also accuracy, so that is a key feature for us. I was going to ask for support for OpenSearch 3.3, but it was released last week, so thanks for that.

[![Thumbnail 1990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/1990.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=1990)

### Agentic Search: Moving Beyond RAG with Reasoning-Driven Retrieval

 That concludes my presentation, and thank you for listening. I'll hand it over to Bobby. That was an excellent demonstration of the value of hybrid search. Thanks for sharing with us, coming all the way from Japan. Before I get started, I have a quick question. How many of you are doing agentic search or agentic retrieval in your applications now, experimenting or trying out? Can I see a quick show of hands? That's a good number. How about people who are deploying in production? Anybody deployed in production with agents or agentic workloads? That's good. That's more than I thought. Two people. That's good.

[![Thumbnail 2050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2050.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2050)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2070.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2070)

We are going to cover agentic search. Basically, John introduced traditional search and  advanced search and how they can be used to improve relevance. I am going to cover agentic search and how OpenSearch is implementing many features to help with this emerging use case in agentic AI applications. Before we talk about agentic  search, I want to quickly understand how agentic workloads differ from traditional workloads. If you look at traditional workloads, they are meant for humans with single queries or short queries expecting millisecond response times. On the other hand, agentic workloads are executing multiple dense queries expecting real-time responses. For that to happen, they have to use the agentic reasoning loop that is dynamic and iterative in nature.

The agentic reasoning loop has many components, as you see in the diagram. It uses Large Language Models, which serve as the brain behind this agentic reasoning loop. The LLM leverages tools and data sources to enrich the context, retrieves information, and brings results. This process keeps iterating until it meets the goals for that particular job. While iterating, it keeps updating the index so that the data remains fresh and all actions are updated accordingly.

For the agent to run these iterations and extract the right context, retrieval is the foundational layer to make that happen. If you don't have good retrieval, the amount of time you spend in iterations and the context you have will not be suitable, and the relevance will be very poor. Force-fitting traditional keyword search methods onto agentic workloads will lead to poor relevance, and you will waste a lot of money running many iterations and trying to retrieve unnecessary context.

[![Thumbnail 2190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2190.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2190)



The industry is approaching this challenge through multiple approaches to enrich context. The first approach is using Large Language Models. Why can't we just use LLMs to enrich this context and enhance the relevance that you can get with search results? Over the last three to five years, the explosion in use cases has been primarily driven by innovations happening in Large Language Models. They are getting smarter with each iteration, and the primary driver for making them smarter is in agentic applications where we want to make them model-driven. The models are smart enough to decompose tasks and execute all the steps you have seen in the agentic reasoning loop so that you don't depend on external tools.

However, there is a problem. LLM vendors are enhancing context windows. Over the last one to two years, context windows have exploded. They used to be a couple of hundred to a thousand tokens. Now you see context windows in the millions. Basically, you can hold a couple of pages of information with a few thousand or a few hundred tokens. Now you can hold thousands of pages, which is great. That means you can really enhance the context and hold a lot of information, but it has its own challenges.

If you really look at agentic workloads and run test cases, POCs, or experimentation phases, those are mostly lightweight agents. You try with a few agents, and the context window that you see with these LLMs is sufficient to handle those kinds of jobs. But when you get into production workloads, these agents are getting into hundreds, if not thousands of agents. The context window that you have, like millions, is not enough. That means you need additional mechanisms to hold this information. Basically, they cannot hold all the information.

The second challenge is retrieval quality and latency. Just like humans, we cannot hold infinite information. After a certain point, if I keep talking to you, you will stop listening to me. For LLMs with large context windows, it's the same problem. There is so much context that as you try to retrieve it, the quality of that retrieval doesn't meet the mark. Also, you will spend a lot of time retrieving that information. So just increasing the context window forever doesn't solve the problems.

The third challenge is context management. As you have context between the short-term context window and how you process this information, there has to be a mechanism to really move that information back and forth between this memory hierarchy. LLMs cannot handle that, and you need some external mechanism to do it. That's why we have seen popular design patterns like RAG and agentic search really evolve over the last few years.

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2390.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2390)



### OpenSearch 3.3 Agentic Search Capabilities: Multimodal Embeddings, MCP, Memory, and Specialized Agents

If you really look at RAG, it is the most popular use case that I have seen over the last twelve years at AWS. We get to work with hundreds of thousands of customers, and they really benefit if you have a single query in a static system.

We have built a rich set of capabilities within OpenSearch vector database to enable these RAG use cases, which is why we are the most recommended vector database for AWS and also the default vector database whenever you use Bedrock knowledge base. Because we have this rich set of capabilities and also provide the price performance to make context augmentation using the RAG design pattern work effectively.

[![Thumbnail 2450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2450.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2450)

As I said, the primary problem with RAG is it cannot reason and it is very static. If it is a single query system, it works really well. Given these limitations, we see all these search use cases slowly moving towards agentic search.  Agentic search is primarily retrieval driven by reasoning, which means it can leverage the agent reasoning loop. Whenever you have a dense or multiple vector queries, it has the intelligence to decompose that complex query and create subqueries or DSL queries if necessary so that you can find results that are very relevant for your search.

Another benefit with the agentic reasoning loop is it can really enrich the data by talking to a number of tools using the MCP protocol and also extract a lot of information from multiple data sources. You can retrieve enterprise data and if you need to retrieve external data, you can use the MCP to enrich this context. As you enrich this context, you need to have the ability to store this information because the volumes will grow and you need memory management and a memory hierarchy.

That is why for an agent reasoning loop, if you have the ability to manage the short-term or working memory and bounce back and forth between the long-term memory and make that judgment very smartly, you have that information when you need it. When you do not need it, you push it to the persistent long-term memory. That way you manage the valuable short-term memory to do the job at hand. Agentic search has tremendous value when you have a dense query system or when you are looking for a multi-turn system where you need to go after and think and solve these problems.

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2560.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2560)

That is why we invested in this future with OpenSearch 3.3 and also with OpenSearch Managed Service.  You have the out-of-the-box experience with agentic search. I want to highlight some of the key capabilities here, though there are more to it. The first one is query planning. As an agent receives a query, it could be a very complex query. We have built a query planning tool that can break down these complex queries and translate these natural language queries into DSL queries and do the heavy lifting for you.

The second one is AI-powered intelligence. The benefit of OpenSearch is it has a rich set of search capabilities and a rich set of quantization capabilities and others to save cost and performance for you. However, it also creates a burden for our customers because they have to think through which one makes sense for them and when to use what, which brings some complexity. With agentic search, we added this layer of AI-powered intelligence where based on the query type and user intent, the agentic search module can pick and choose which aspects of quantization and which aspects of search make sense so that they can apply them at the right time for the right problem.

This intelligence layer is really useful and includes some of the things that were mentioned regarding scoring and what kind of search technique you want to use, whether lexical, semantic, or hybrid. Underneath there are many filtering techniques and many things you can use with OpenSearch, but this layer does a lot of simplification for the user. The third one is flexible customizations. With the ability to work with the MCP both with internal data and also external data, you can customize your search index with what kind of information you want to keep within your index.

[![Thumbnail 2700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2700.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2700)

What I mean by that is you can add tools, you can add data sources, you can customize the context that you want to deal with, and you have that complete freedom to pick and choose. You can go to hundreds or whatever makes your application useful. The last one is accuracy, especially when you have multi-turn applications. Agentic search really makes total sense and gives you that additional benefit with relevance.  To make agentic search work, there are many components that go into it.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2740.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2740)

As you have seen, there is memory, there is how you work with MCP server, and there are capabilities like dealing with multimodal embeddings. So there are a bunch of capabilities we brought with OpenSearch 3.3 and also in the managed service that you can leverage to build agentic search, either out of the box experience or you can put together an agentic search solution by yourself. I'll quickly walk through these four capabilities here, and there are more, but I just want to quickly highlight them so that you know what's there and can leverage for your application. 

The first one is multimodal embeddings. Why multimodal? If you look at most organizational data, it's unstructured data. It's very fragmented and very siloed. If you take a medium-sized organization, you'll have hundreds of developers building applications. Each application brings data in different modalities, and the ability to tap into those modalities is critical. If the company or organization cannot use all these modalities, there is so much hidden information that's missing out. That's why if you see the LLM vendors, they invested quite a bit over the last year. Most of the models you see are multimodal generative models or multimodal embedding models.

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2800.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2800)

To make this happen, we worked with Amazon Nova model. If you look at OpenSearch, we already took this path of multimodal last year. We support text and image with the Titan embedding models. Now with Amazon Nova multimodal embeddings, we support six modalities that we integrated natively with OpenSearch. It's a very intuitive and easy-to-use experience with one click. The main thing with Nova multimodal is a unified embedding space. What I mean by that is when you have multiple modalities, you can create a vector embedding space for each modality and store them separately. But with Nova multimodal, it unifies all these modalities. You build one embedding space that's unified and bring together the semantic understanding of the relationships between these modalities. 

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/2890.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=2890)

What's happening is you have rich context. Like humans, we perceive in many ways through audio, video, and visually, and we make sense of that. Now the context you have is really enriched. The second benefit is because you're saving in one embedding space instead of multiple embedding spaces, your vector storage is also smaller, and you're saving cost on vector storage. That's the benefit with the Nova model and the integration. You can quickly access and build these next-generation applications that are multimodal. 

The next one is MCP. We heard about this over the last year, and the whole industry accepted or recognized this as the standard protocol to work with agentic systems to access external data or tools. To enable this, OpenSearch also built MCP client and server that you can use based on your needs. We built a lot of ready-to-use tools. We have about twenty tools that you can tap into based on your application needs, like query planning and search index, and so forth. We continue to build more tools so that you don't have to invest time building these tools and can work with MCP.

The third one is authentication and credentials. The moment MCP came into the industry, the biggest concern for many people was that they're giving access to their data to external systems. How is authentication handled? How secure is the data? So we wanted to address that head on. We provided configurable credentials based on the session or the user. You can give permissions and manage these security boundaries when you're using MCP server with OpenSearch. On top of that, in agentic applications, it's very early and there are many frameworks people are using. Everybody has their own preferences and the winner is yet to be declared. So just to play with the ecosystem, we integrated our OpenSearch MCP server pretty much with all the mainstream vendors you see here, like AWS Bedrock, Crew, and Langchain, and so forth. If you go to use those frameworks, you can just quickly use the OpenSearch MCP server.

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3000.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3000)

The third building block is memory. Memory is very important. It's the backbone of intelligence. 

This is what makes agents stateful. If I want to highlight a few key benefits of having memory for agencies, one is knowledge retention and retrieval. If you have a short-term and long-term memory hierarchy and the ability to store vectors that are immediately required and process them for future usage, that really gives you the flexibility to enhance the context.

The second benefit is personalization. Because you have this memory, you can store every user behavior and their reactions. You can store information per session or across sessions, and also over a period of time. That really gives you the ability to know what your preferences are when you're searching, and the system gives you more targeted results. You can really hyper-personalize once you have the memory module integrated.

The third benefit is adaptive intelligence. What I mean by that is because you know preferences and you're very context aware, the agentic reasoning group that we discussed can deploy the right set of tools and the right set of tasks to really solve the problem at hand. Based on the user information, it can change the tools as well. So you are highly context aware and can be adaptive with your actions and tasks.

We built agentic memory with OpenSearch as a memory API. You can access that if you're using within OpenSearch, or you can access that using MCP as a tool. The memory API has the ability to store short-term memory, long-term memory, and session memory. We also store all the event history so you're not missing out on any actions that were taken within the agentic system.

The way it works is once the raw agent passes the raw messages, it captures them and has the ability to run memory operations like add, delete, or others so that the information is up to date. Once you have the up-to-date information, you can decide to represent it either in dense or sparse representations because it's a trade-off between quality and cost. When you go to dense, you get a lot of quality. If you're trying to trade off to save storage and cost, you can go to sparse representation.

It's also very multi-tenant. You can configure the memory per user and really store all this information based on your enterprise and how many users are there. You can give them certain specific access to certain memory boundaries. I strongly recommend giving it a try. It really makes your agentic application smarter and wiser.

The fourth building block we have built is specialized agents. What it means is we are giving local agents within the OpenSearch environment. You don't need to go to some third-party agency platform to build any applications. The benefit of giving this is you have access to OpenSearch data. You'll be within the OpenSearch cluster environment, and whatever permissions you have, you inherit the security and the security boundaries for that user.

You will also have the observability that comes with it. You can look at logs and traces. You can understand what's really going on. It's really quick to use. All you have to do is a few lines of code. You pick your agent and you're good to go. These are production worthy as well. So basically within the OpenSearch environment you can pick the local agents and deploy them to meet your needs.

Just to give an example, a Flow agent is very good if you are trying to do some RAG pipeline orchestration. It can get you started really quickly. The conversational agent is something if you have a chatbot or conversational application, it can help you build that really quickly. The third one here is the Plan-Execute-Reflect agent. If you are trying to build something like a deep research agent, which is exploratory in nature, going after multiple resources, trying to think and explore and find the best answer, this can help you build that kind of solution.

We're going to build more of these specialized agents to make the user experience easier looking forward. So that concludes the agentic search of what we are doing there. There is a lot of interest. Some of our best customers already tried some of these features.

### Performance Improvements, Cost Optimization, and Resources for Getting Started

We're going to invest further as we're already working on some new features. We're looking forward to sharing those in the near future. Let me change gears really quickly. We talked about different kinds of search techniques and how OpenSearch is building these capabilities. Most of these capabilities will be running on the OpenSearch core engine. If you don't have the performance the core engine offers, all the things that you're applying may not benefit you as much. We always invest quite a bit of time in how we optimize performance and how we can get that extra boost to your performance.

[![Thumbnail 3310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3310.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3310)



If you look at this chart compared to version 1.3, which was released a few years ago, we've already boosted the core search engine performance by about 11x, which is significant. Vector search is a very dominant workload for us, and we've improved that performance by 2.5x. This is all with the latest release 3.3. We want to make sure the performance is what you're looking for.

[![Thumbnail 3380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3380.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3380)



On the cost side, if you really look at vector search workloads, the workloads are exploding in size. They used to be in the billions, and now they're transferring to trillions of vectors. The more we talk to customers every other week, we hear it's becoming common to enable these such large workloads. The memory footprint really becomes a challenge, and how you manage the memory footprint while giving the performance and cost benefits is a problem on the table.

We already have exact KNN and ANN techniques where it gives you most accuracy and also high performance. We added disk mode, which gives you that additional cost advantage when you have a large scale storage for these large workloads. What disk mode does is it has the ability to manage the vectors both in RAM and also on disk. You store the high precision vectors on the disk, and you apply quantization, and then you store them in the RAM. You use the combination of these and kind of save the cost while maintaining the quality of those results.

[![Thumbnail 3430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3430.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3430)



The last one we made in this tiered vector storage is S3 vector integration. As you all know, S3 vectors are the lowest cost way of storing information, and this year they introduced vector storage capability. We natively integrated S3 vectors with OpenSearch. You can work with managed service OpenSearch Managed Service or OpenSearch Serverless and store the information in S3 vectors while taking advantage of OpenSearch search capabilities. It's a good, powerful complementary strength to leverage and build applications that are at massive scale.

[![Thumbnail 3470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3470.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3470)



That concludes all the things that we wanted to discuss today. Just to summarize, OpenSearch search relevancy is front and center of search. To give you the best relevance, we've been investing quite a bit in all types of search techniques, and you have a choice to make. Each search technique is applicable for a specific use case. Lexical search is not going away, semantic search is not going away, and based on the complexity, you can apply agentic search. Like I shared with you, with agentic search, we built multiple capabilities. I strongly recommend giving it a try, and I'm sure you'll find it useful for your advanced applications.

[![Thumbnail 3520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3520.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3520)



The third one is the cost structure. Like I mentioned, we've been investing quite a bit to improve the performance and also to give you the best cost structure to deploy these large scale workloads. If you upgrade to 3.3, you get all these additional features and also the performance benefits. The last one is easy integrations. We strongly believe in working with all the services within AWS. We are natively integrated with services like Bedrock, SageMaker, and Agent Core if you're building AI or agentic applications. We're also doing a lot of third party integrations. If you're building any search applications, we want to make sure we are part of the ecosystem.

[![Thumbnail 3620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3620.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3620)

Either you build within OpenSearch and work with those partners, or you build applications with the partners and leverage OpenSearch to build those applications. We try to compress a lot  of information into one hour, so there's more to do. OpenSearch is a very broad service, and we continue to build and innovate.

[![Thumbnail 3650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3650.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3650)

If you want to learn more, get hands-on experience, or dive deep into OpenSearch, I strongly recommend reading this book written by our friend John and other teammates. It covers a lot of things and is a very dense book with many details. I strongly recommend you get a copy of it. If you want to get lucky, you can visit the booth and give it a try.  You can get a free copy, as we have a bunch of them at the booth.

[![Thumbnail 3670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/1136d3e881b68772/3670.jpg)](https://www.youtube.com/watch?v=U3xO5QbzYQY&t=3670)

We talked about a lot of techniques today. If you want to see them in action with live demos, and also see what our partners are doing, you can see all the things we discussed in action at the booth. Please go ahead and check out the booth so that you can see them in action.  On top of that, if you want to build skills, AWS has many services and too many choices, so there's so much to learn. I strongly recommend you check out AWS Skill Builder, where you can double-click on any particular area of your choice, any service like OpenSearch or others, to learn and develop your skills.

Last, please give us feedback. We work hard to bring together the right content and information that's useful for us. Your feedback will help us iterate and make sure we bring good content for you in future sessions. Thank you for your attention in joining us, and thanks. Have a good one.


----

; This article is entirely auto-generated using Amazon Bedrock.
