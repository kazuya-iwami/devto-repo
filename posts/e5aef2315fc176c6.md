---
title: 'AWS re:Invent 2025 - Intelligent Outage Management Using Automated Reasoning and AI (AIM275)'
published: true
description: 'In this video, PwC Directors Ambuj Gupta and Kevin, along with AWS''s Automated Reasoning team members Nafi and Wale, demonstrate how Amazon Bedrock Automated Reasoning addresses LLM hallucination issues by providing mathematically verifiable answers. They explain that automated reasoning works as a verification layer, checking if generative AI responses are logically sound and factually accurate, returning valid/invalid/unknown assessments. Two industry applications are showcased: a utility outage chatbot that ensures consistent customer responses across different state regulations using Amazon Lex, Bedrock, Lambda, and DynamoDB; and pharmaceutical content review that verifies promotional materials comply with FDA rules. The service, available through Bedrock Guardrails API, converts business rules into logical statements with 99% accuracy, enabling auditability and traceability crucial for regulated industries.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/0.jpg'
series: ''
canonical_url: null
id: 3086241
date: '2025-12-05T11:19:49Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project aims to enhances multilingual accessibility and discoverability while maintaining the integrity of original content. Detailed transcriptions and keyframes preserve the nuances and technical insights that make each session compelling.

# Overview


ðŸ“– **AWS re:Invent 2025 - Intelligent Outage Management Using Automated Reasoning and AI (AIM275)**

> In this video, PwC Directors Ambuj Gupta and Kevin, along with AWS's Automated Reasoning team members Nafi and Wale, demonstrate how Amazon Bedrock Automated Reasoning addresses LLM hallucination issues by providing mathematically verifiable answers. They explain that automated reasoning works as a verification layer, checking if generative AI responses are logically sound and factually accurate, returning valid/invalid/unknown assessments. Two industry applications are showcased: a utility outage chatbot that ensures consistent customer responses across different state regulations using Amazon Lex, Bedrock, Lambda, and DynamoDB; and pharmaceutical content review that verifies promotional materials comply with FDA rules. The service, available through Bedrock Guardrails API, converts business rules into logical statements with 99% accuracy, enabling auditability and traceability crucial for regulated industries.

{% youtube https://www.youtube.com/watch?v=DGD3cYBMGk0 %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
### Introduction: Addressing Hallucination and Accuracy Challenges in Large Language Models

Hello, everyone. I'm Ambuj Gupta from PwC, Director in AI Engineering. With me today we have Kevin, who is also from PwC as a Director in AI Engineering, and Nafi and Wale who are from the Automated Reasoning team and helped with this overall solution. Today we are going to talk about how we are utilizing automated reasoning in addition to Amazon Bedrock to bring more accurate and verifiable answers to different inquiries across any sector.

[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/0.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=0)



[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/50.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=50)



Let me start by discussing why we came up with automated reasoning. What is automated reasoning and why do we need it when we already have guardrails? We all know that over the last couple of years, we have seen tremendous progress with respect to Large Language Models and Generative AI, and we have seen it spanning across all dimensions and horizons at the greatest speed. However, one consistent problem we have seen all the time is the accuracy of the answers and hallucination. Hallucination is the toughest outcome of that, where it creates a lot of problems if we are dealing with the customer service industry or any industry where accurate answers are more important. It becomes a problem when we don't have a solution for that in a timely manner.

[![Thumbnail 60](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/60.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=60)



[![Thumbnail 100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/100.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=100)



To address this, we have Amazon Bedrock Automated Reasoning checks, which work automatically within the guardrail space to ensure that the answers are accurate. This means it identifies and suggests why an answer is correct and based on what factual knowledge. Secondly, it checks for soundness, which means whether the answer is derived logically based on verifiable information or something else. It always comes back with three types of responses: number one, whether the answer is valid or not; number two, if it's not valid, why it's not valid; and number three, it never pretends. It will say that it doesn't know if the answer is valid or invalid.

[![Thumbnail 120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/120.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=120)



In addition to that, it doesn't just come back with a couple of responses. It provides recommendations on whether we should ask more questions to the customer because information might be missing, or whether we should pass this information to another scenario where we can present different answers to the customers.

[![Thumbnail 170](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/170.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=170)



Lastly, with responsible AI implementation, we need to make sure that any answer provided is explainable and there is transparency in that, and it is consistent across any channel if a customer is reaching out to us. All of this happened because, as I was saying, guardrails have limitations. You can do content filtering and topic filtering, but after that, what we cannot do at scale is implement business rules. This is where automated reasoning comes into the picture to ensure that we are able to utilize these business rules or standard operating procedures, whether in the pharmaceutical industry, utility industry, financial industry, or anything where we can have these business rules that would vary with different subject matter. With automated reasoning, we are able to scale it without a lot of modifications.

[![Thumbnail 230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/230.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=230)

### Understanding Automated Reasoning: Mathematical Proofs and AWS Implementation



[![Thumbnail 240](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/240.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=240)



Automated reasoning is an algorithm that searches for new mathematical proofs. The way it works is that it derives the mathematical proof of the rules and applies them to the answers coming from Generative AI or Large Language Models, and then provides answers based on that. For that, it uses a logical model and software to explore the states because it is able to search across different or vast states of information.

[![Thumbnail 260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/260.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=260)



Let me also specify that automated reasoning is not a new service. It has been used already in Amazon for many years.

[![Thumbnail 270](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/270.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=270)



If you go to the next page, you would see that it has been used for VPC reachability to make sure that if a connection should be going out or going in, IMS analyzer, that's how it's able to quickly change the permission whenever we make changes in the policy or a role for a group or anything like that. It ensures that S3 block public access, making sure that in services where our data is not supposed to be exposed publicly, it is managed accordingly, and then utilizing the Inspector service as well, which is utilizing the same automated reasoning to identify different vulnerabilities and anomalies happening in EC2 instances.

[![Thumbnail 330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/330.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=330)

Let me present a diagram where you can see how we have been utilizing automated reasoning with generative AI.  On the top, you would see that generative AI is providing answers for all types of questions. However, as we all know, there would always be an answer, but accuracy is not guaranteed. That is where automated reasoning comes in like a lawyer who never pretends to know the information. It will say whether the information is valid or not valid, or it will say that we need to take one of these systems, maybe ask more questions to the customer, or perhaps some information is missing. It can provide additional annotations because utilizing those downstream processes, teams can take corrective actions, transfer the chat or call to a human agent, or receive recommendations on what needs to be done.

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/380.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=380)

### Policy Design and Verification: Bringing Mathematical Certainty to Probabilistic Systems

Hello everyone.  Just to quickly dial back to everything that Ambuj was mentioning here. Working across different use cases and building AI solutions in different industries, all regulated. These are probabilistic systems, so we are very aware that we are producing outcomes that need to be verified. We are not flying blind. We have metrics such as faithfulness. We build retrieval augmented generation and metrics that assess quality that falls into the evaluation dimension of the AI solution development life cycle. When AWS came to us and told us they had this specific service, it gave us an entirely different dimension which could be brought in, which could give us auditability and traceability of what we are building.

Primarily, what I am referring to is that there is a different branch of mathematics that could be applied and effectively verify the solutions that are being produced and the language model outcomes that are being derived. For any solution you are building, you are building all of these wrappers on top of language models, but there is a layer that actually mathematically verifies this. When PwC started doing this, we approached it from the lens of having two different concrete portions. Whatever Ambuj was describing in terms of that logical design and then the answer verification piece is what we did in these two parts that you see. We call it policy design and then verification and testing.

The policy design is a key concept which has not necessarily been approached in this way before while dealing with probabilistic systems. You have a bunch of business rules. Language models are good at understanding those business rules. You ask models to look at those business rules, follow them, and ensure they are producing a result in line with those business rules. The most we used to do was absolutely follow these business rules and do not spell something out if you are unsure about the solution. But how do you ensure that those results are consistent, mathematically verifiable, and accurate?

This was good news to the ears of all governance committees that review models and then pass them through to production. We came up with a solution which first and foremost lets us define those policies as logic statements. Logic statements are basically statements that use AND operations, OR operations, and all of your boolean logic that we traditionally learned in high school. Once we are able to derive them technically, what is supposed to happen, and this is where you need full certification from an actual logician or somebody who has a PhD in formal methods looking through those statements and ensuring that they actually reflect the business rules.

What Nafi Diallo and Wale Akinfaderin and their team have created is a system that is close to 99% accurate in terms of deriving those policies as well as verifying them. Verification is a step where you populate these logic statements with variables coming in from your language model outputs. Say you have X and Y and then R with a bunch of other variables, you fill them in and then you have a true or false outcome. That effectively is a standardized deterministic outcome for you to verify if the model is performing as expected based on some policy that you have laid out. This becomes paramount in regulated industries.

That was the key thing that we were able to derive from the service and we are starting to extrapolate and apply this to as many use cases as possible. The verify and test bucket that is being described here is effectively testing it at training time.

You derive a bunch of statements and try to see if that applies to your use case. If it works well enough, you're effectively progressing it to inference time, which is the deployment and monitoring solution. This is where Ambuj was referring to this being applied to security protocols already within AWS. In some sense, AWS is the leader in this field. They've already developed formal verification methods for security protocols, such as within S3 or judging IAM policies through converting network topology into these formal statements.

However, now given the fact that we have language models, this could broadly be applied to any given use case that you have. That's what we found value in, and we've tested this out in a few industry applications. One being pharma content review, where you're generating content for pharma copy and making sure that it is aligned with the label and a bunch of business rules that are set out by central organizations like the FDA. Then Ambuj will be talking about utilities outage, which is the other use case that we're focusing on here.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/700.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=700)

There are other use cases like the EU AI Compliance Act where we're broadly applying every single use case and linking it back to the Compliance Act itself. Then there's cloud security where we're ensuring that cloud infrastructure is compliant against a bunch of rules like HIPAA or FINRA. With that, moving back to utilities outage, I'll let Ambuj go ahead. 

[![Thumbnail 720](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/720.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=720)

### Real-World Applications: Utility Outage Management and Pharmaceutical Content Review

Thank you, Kevin. Let's talk about the use case of utility outage. We all know that during an outage situation or a storm situation, utility companies prepare really well for situations where a customer is losing power. They are able to respond to how soon power will be back and how soon customers will be getting services back. 

The way we have been working with other utilities is that they work across states, and every state has different state laws that they have to follow. So far, it has been determined that processes are designed such that for one state, a low priority ticket will be responded to or fixed in maybe 10 to 12 hours, but in other places, the ETAs are different. To manage all these scenarios, the system has been tightly coupled and built in a manner that it is doing the job. However, when it comes to scalability, we hit a roadblock.

[![Thumbnail 810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/810.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=810)

The problem that we are seeing here is that a customer experiencing a power outage demands that they are provided a response in a quick manner. There should be no inaccuracy or inconsistency at all, and there should be useful self-service options. Customers want to know when the power is coming back or if a truck rollout would happen and when there would be a crew at their location. To do that, utilizing Amazon services, we created a utility outage chatbot. 

What you would see on the top is that outage standard operating procedures, these documents which can change based on the states, are loaded to automated reasoning. Automated reasoning extracts all that information and creates a mathematical model, derives different variables, creates rules, and indexes those rules in an efficient manner. At the bottom, you would see that we have a chatbot which is created using Amazon services such as Amazon Lex, Amazon generative AI application Bedrock, Lambda, and DynamoDB, which stores the real-time information about the outage situation across different zip codes.

Many companies offer different versions of these databases, but DynamoDB is a NoSQL highly scalable information database that stores all the insights related to those locations. When a customer asks when their power will be back or what is the status of the outage, the generative AI provides an answer. However, that answer has to be verified, and that's where automated reasoning comes into the picture. It looks into the answer and before letting that answer go to the customer, it checks whether the answer is valid or not.

As I was saying before, a low priority ticket should not be answered in two hours versus a high priority ticket or critical situation where someone is dealing with a life and death situation, which should be handled quickly. We want to make sure that those scenarios are scalable, and you would see that on the subject matter experts box.

[![Thumbnail 940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/940.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=940)

They are able to make these changes live. Because during a storm situation, we also have to make validations and verification that what policy should or should not exist. Does anyone have questions at this point with respect to this diagram? This makes sense. It seems like everyone understands it. With that, let me hand it over to Kevin. He will speak about the design for  pharmaceutical content review.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/960.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=960)

So getting back to this particular use case, we were dealing with the pharmaceutical content review use case where we looked at pharmaceutical promotional content and ensured that they were actually sticking to the rules  that were laid out by the FDA. Companies have their own internal rules that we need to align with, purely for regulatory purposes first and foremost. The flow that you have on the top is what we developed by instructing the language model and giving it access to the rules so that it can produce judgments on what needs to be updated within a piece of content.

What we found with the flow below is that it is complementary to what is on top. Language models are great at language understanding, so they produce outcomes, whereas formal verification or symbolic logic gives us a means to take those statements, convert them into logical statements, and effectively serve as a second line of defense for the outcomes being produced. In this case, if you ask what rules we necessarily used, sometimes you do not have these rules documented. In that case, you could take a perfect example that you think is already aligned with all of these rules, and you are able to derive rules out of them as well.

### Moving Forward with Confidence: Automated Reasoning Through Bedrock Guardrails API

To close what we are talking about here, this is gaining a lot of mainstream momentum. As of last week, Deep Seek Math came out, and they were using a verifier to verify mathematical theorems, which was effectively using a theorem prover or a solver that is used within the automated reasoning service. It is only probably going to get more ubiquitous as we build more and more use cases. I am happy to take more questions, but I do want to pass it along to Nafi and Wale to give us a sense of the roadmap and what they are using this for.

Thank you, Kevin. I just wanted to reinforce what both Kevin and Ambuj just talked about to highlight that even though we are talking about automated reasoning and about math, we are really talking about applying this. The service is generally available through the Bedrock Guardrails API. What we want to do is work in terms of common sense. There are a lot of small actions or rules that we want to make sure apply, and the idea is that you can take these rules and turn them into automated reasoning policy.

[![Thumbnail 1130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/e5aef2315fc176c6/1130.jpg)](https://www.youtube.com/watch?v=DGD3cYBMGk0&t=1130)

What you are really gaining is that now you can move with confidence. You can take the advantages of generative AI and move with confidence. We are available to help. Feel free to reach out to your account teams if you have use cases that you would like to look into. The team is available to help. I will now pass it over to Wale. 

Thanks, Nafi. I think my colleagues already said everything. I am a scientist from AWS. We have been doing automated reasoning for about a decade, and the idea now is to bring deterministic guarantees to probabilistic systems because we know LLMs hallucinate by design. With automated reasoning, you can take policies, auto formalize them into logical statements, build models, and then use that as a verification layer for your generative AI applications. If anyone wants to talk to us about automated reasoning and how to use it as part of Bedrock Guardrails, we will probably wait behind. Thank you everyone.


----

; This article is entirely auto-generated using Amazon Bedrock.
