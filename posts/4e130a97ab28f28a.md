---
title: 'AWS re:Invent 2025 - Building and validating cloud controls with generative AI (COP350)'
published: true
description: 'In this video, Pal Patel and Rodolfo Brenes demonstrate how generative AI accelerates AWS cloud governance through practical demos using Kiro CLI and Amazon Q. They showcase three key scenarios: creating AWS Config organization rules with automatic troubleshooting, implementing shift-left security controls using Service Control Policies and CloudFormation Guard Hooks, and deploying SRA-aligned patching solutions through Control Tower''s CFCT pipeline. The session highlights recent launches in CloudTrail, AWS Config, and Control Tower, while demonstrating how MCP servers enable natural language interactions with AWS services. Using a fictional cloud ops engineer named Nita managing 300+ accounts, they illustrate how specialized AI agents with proper context and knowledge bases can automate complex governance tasksâ€”from detecting noncompliant resources to deploying account customizationsâ€”all within a single terminal session.'
tags: ''
cover_image: 'https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/0.jpg'
series: ''
canonical_url: null
id: 3093234
date: '2025-12-08T21:35:22Z'
---

**ðŸ¦„ Making great presentations more accessible.**
This project enhances multilingual accessibility and discoverability while preserving the original content. Detailed transcriptions and keyframes capture the nuances and technical insights that convey the full value of each session.

**Note**: A comprehensive list of re:Invent 2025 transcribed articles is available in this [Spreadsheet](https://docs.google.com/spreadsheets/d/13fihyGeDoSuheATs_lSmcluvnX3tnffdsh16NX0M2iA/edit?usp=sharing)!

# Overview


ðŸ“– **AWS re:Invent 2025 - Building and validating cloud controls with generative AI (COP350)**

> In this video, Pal Patel and Rodolfo Brenes demonstrate how generative AI accelerates AWS cloud governance through practical demos using Kiro CLI and Amazon Q. They showcase three key scenarios: creating AWS Config organization rules with automatic troubleshooting, implementing shift-left security controls using Service Control Policies and CloudFormation Guard Hooks, and deploying SRA-aligned patching solutions through Control Tower's CFCT pipeline. The session highlights recent launches in CloudTrail, AWS Config, and Control Tower, while demonstrating how MCP servers enable natural language interactions with AWS services. Using a fictional cloud ops engineer named Nita managing 300+ accounts, they illustrate how specialized AI agents with proper context and knowledge bases can automate complex governance tasksâ€”from detecting noncompliant resources to deploying account customizationsâ€”all within a single terminal session.

{% youtube https://www.youtube.com/watch?v=bRTSI-UKl0s %}
; This article is entirely auto-generated while preserving the original presentation content as much as possible. Please note that there may be typos or inaccuracies.

# Main Part
[![Thumbnail 0](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/0.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=0)

### Introduction: The Challenge of Balancing Innovation with Governance

 Good afternoon everyone. Welcome to COP350, Building and Validating Cloud Controls. Make sure all of you have your headsets on so you don't miss out on the fun that I'm about to share. Thank you for joining us today, late in the afternoon. My name is Pal Patel. I am a Senior Solutions Architect specializing in cloud governance on AWS, and I'm joined today by Rodolfo Brenes, who is a Principal Solutions Architect on the same team with me.

[![Thumbnail 50](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/50.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=50)

Every time I open my Amazon app, I recently bought a home, so I do a lot  of purchasing lately. I tell myself that I'm only going to buy one item, just one, a $20 item, but 10 minutes later my cart looks like I am going to open a new data center. I add one cable and somehow I end up with a standing desk, a label printer, another monitor arm, and a chair named after a Scandinavian forest. All this while Amazon is on the side going, "Yeah, people who bought this also bought a whole new lifestyle," and I'm thinking, "Yeah, I feel great about finding all these products," but my credit card not so much.

If only I had this intelligent guardrail for my shopping cart that would not stop me or say "Do not buy this," but more like, "Hey, maybe you don't need this third monitor arm," or "This is a duplicate of the item you panic bought last week." Imagine I had a guardrail for my shopping cart that would not just stop me but quietly nudge me to make the right decisions and safeguard my bank balance. Now thankfully I do have my husband for that. We share my Amazon account, so next morning most of the items in my cart do disappear and only the items that he thinks we need remain. I call it unauthorized access, but he calls it cost optimization.

[![Thumbnail 150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/150.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=150)

Now going from there,  just like my cart needs intelligent guardrails, so does your AWS environment. How do we do that? That's what we are going to explore through this session. I have a really simple agenda for today. I will briefly start exploring the pressure that people feel to innovate while still maintaining security and governance in their environment. Then Rodolfo here will deep dive into the foundations of AWS governance services and also talk about some latest launches for these services. Finally, I will demonstrate how generative AI can accelerate governance for you.

[![Thumbnail 200](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/200.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=200)

In my day-to-day  conversations with customers, when I talk about governance, it's not so much about how to innovate, but it's about how to accelerate and automate safely. Think about it. A single team today can launch hundreds of thousands of dollars worth of cloud resources within minutes. But if you have to track their ownership, if you have to maintain their governance and compliance, that can take time. People are building AI models within days, but getting them shipped and deployed in production takes time, and for good reasons.

Businesses expect speed, but governance expects assurance, and it takes time. Deployments and reviews often move at fundamentally different speeds. The opportunity here is clear. Governance shouldn't slow us down, but we have to find ways to accelerate it, validate it, without losing the assurance that it provides, so you don't scale mistakes instead of your environment. How do you do that? How do you accelerate governance? That is where generative AI can really help, which is what I'm going to explore through this session.

[![Thumbnail 280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/280.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=280)

### Generative AI as a Governance Enabler: Meet Nita and Her Team's Challenges

 It can really help shift your governance from being just a gatekeeper to an enabler. Imagine instead of manually reviewing compliance for multiple AWS accounts, you would just describe that in natural language and it would give you automated compliance checks. Or crafting a custom AWS Config rule that can take days for you, you would just describe in plain English and automatically deployed as config controls.

For audit reasons, generative AI can correlate findings from AWS Config, CloudTrail, Security Hub events, and then show you the gaps and suggest remediations before they even become compliance issues.

[![Thumbnail 340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/340.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=340)

Moving on from these possibilities, let's talk about real world implementation. I want to introduce you to Nita, who is our lead cloud ops engineer  at any company. She's a builder behind the controls, and her team is responsible for managing 300 plus AWS accounts to keep them compliant and operational. Like many of you, she's always looking for ways to deploy controls in a faster, safer, and smarter way, and that's where she turned to explore generative AI, not as a shortcut to governance, but more as a way to remove the friction from building controls.

[![Thumbnail 370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/370.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=370)

[![Thumbnail 380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/380.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=380)

 Now, as Nita explored opportunities to improve her team's governance, these were the challenges that stood out to her for her environment.  First, maintaining consistent controls at scale. As her environments and AWS accounts keep growing, it becomes increasingly important for her to have consistent controls. Then their governance today is very much reactive, so they have ways to detect noncompliant issues, but they have limited capability to prevent the issues or prevent the noncompliant actions from happening. And while they are using Control Tower Account Factory to seamlessly provision their accounts, applying account customizations takes significant time and pressure for the team. So this is where she brings in Bob and I'll hand over.

[![Thumbnail 440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/440.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=440)

### Bob Explores AWS CloudTrail: Foundation for Governance Strategy

Thank you, Pal. All right, so let's talk a little bit about Bob and his role. Bob, in this any company example, he's one of our AI leads, and he's  going to work with Nita's team, the operations team, to actually build a governance strategy. One of the key things that we're focusing on in this example is that we're going to start with this strategy, but it needs to be agile enough to actually keep the pace of the innovation that we want to achieve. But in order to get started, we need to actually start looking at the foundations.

[![Thumbnail 460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/460.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=460)

[![Thumbnail 470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/470.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=470)

 And Bob starts exploring all the different AWS services that can actually help us with this. And well, let's start with a very  basic one, one of the long standing services in AWS, which is CloudTrail. CloudTrail allows you to capture events from console logins, control plane events to data events to network activity events. What is the activity that is going through your VPC endpoints, for example? It allows you to capture all that activity and actually aggregate and store that, which could be in S3, could be in CloudWatch Logs, and allows you to take actions on it. What happens if somebody created a new EC2 instance? Who created it, and then you can run investigations on it. And you can review that from places where you can store that on S3. You can actually deploy your dashboards on it, or you can actually use managed services like CloudTrail Lake in order to do that.

[![Thumbnail 520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/520.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=520)

Well, he started looking at CloudTrail and said, okay, let's take a look at what has been launched in the past few months. So we're going to focus,  we're going to start actually looking at the two that are related here for data events. As I mentioned, data events is one of the types of CloudTrail activity that can be tracked using AWS as a service. These are normally high volume events. For example, it is tracking how many objects have been stored on S3, how many invocations to your Lambdas, and that could be a lot of logs. Data events insights allows you to detect anomalies, and this is something that has been available for management events. Now it is available for data events as well. So it allows you to actually realize that something abnormal is happening so you can actually review those.

[![Thumbnail 580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/580.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=580)

And event aggregation is for those high volumes, allows you to identify patterns. What it's doing is actually taking that information, aggregated on five minute windows, so you can actually perform an investigation to see if this is something that has been happening in the past few days. So it allows you to ease those investigations on those data events that you're getting from there.  And the last one that he realized that was launched is CloudTrail Lake event enrichment. So, as I mentioned, CloudTrail Lake is a managed data store that allows customers to aggregate data from their whole organization in one single place, and then on that you can actually provide business context into that event. And this is done through, for example, tags on the resources that are being involved or the principals that are actually taking that action. So this is a very powerful mechanism that Bob

starts realizing that they can definitely leverage, and also, he realized that the event size has been actually increased from 256 kilobytes to 1 megabyte. So now, all that enrichment can be done and can provide you context while you're doing investigations. So, this is one of the things that he starts doing and then say, well, definitely context is very important. What else is there?

[![Thumbnail 630](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/630.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=630)

### AWS Config: Configuration History and Compliance Evaluation

So he stumbles into  AWS Config. AWS Config is a service that actually aggregates and sits on top of CloudTrail data and provides that configuration history of that resource. So it gives you metadata on that particular resource, gives you the historical changes based on, let's say, compliance evaluation. So it allows you to actually assess that configuration within your resources. You can run evaluations as Paul was explaining regarding to, let's say if it is matching on a specific status, right, but it's a desired configuration that I want to have in there.

So this is basically when you start setting up your controls and making sure that Polly is not buying more stuff that she needs, right? And it allows you to remediate that on top of it. So let's say it could be natively done from AWS Config or you can actually use services like EventBridge to remediate a non-compliant service. And, of course, at the end, it provides you the capability for you to audit your environments in a certain way. Let's say following a specific regulation or even internal policies that you have, and says, well, this is very good. I have a couple of services that can help me now.

[![Thumbnail 700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/700.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=700)

Let's see what AWS Config actually has launched in the last few months. The first one is the service-linked recorder. This is that we started the year working with an integration  with what is called the service-linked recorder that allows other AWS services to take actions on your behalf using AWS Config. A great example of this is the launch with CloudWatch. CloudWatch now allows you to actually take audit on your telemetry data. Let's say if you want to make sure that all your EC2 instances are using detailed monitoring. CloudWatch uses AWS Config data to give you an evaluation of those resources, as well as things like VPC flow logs or even having traces enabled in your Lambda environments.

[![Thumbnail 750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/750.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=750)

Security Hub, we just launched the new experience yesterday. It went GA and this also is using this. So now you can just go to Security Hub, enable what you need, and it will be taken care of by AWS Config on your behalf using this integration. And in terms of coverage, we have been increasing  the automation around AWS Config, so now we just last week, we launched over, I think the couple of weeks before, we just launched over 50 services to the services that are covered, as well as over 50 new managed rules, and this is something that keeps improving.

[![Thumbnail 770](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/770.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=770)

### Control Tower and Amazon Q: Comprehensive Control Catalog Meets Conversational AI

But then, as you start seeing, there's a lot of changes in these services. So,  it's looking, OK. Is there something else that can help me actually build all this, right, in a managed way? And that's when we start looking at Control Tower. Control Tower allows me to deploy a landing zone where I'm going to store all my resources, and I can do that following best practices for multi-account, for example. It allows me to establish that foundation to eventually move forward to how much I can automate account provisioning, and this is also something that is already embedded in the service. And then I can start working with controls.

[![Thumbnail 820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/820.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=820)

There is a controls catalog within Control Tower that allows me not just to deploy detective mechanisms but also going to, let's say, preventive and proactive, and with preventive, I can actually go to AWS Organizations policies. I can deploy things like service control policies, resource control policies from the catalog using the service. And let's take on what is new in Control Tower as well. So Bob's realized  that there is a comprehensive control catalog that was launched actually last week, and now he, even though maybe he doesn't need to start with the landing zone right now, he just wants to go and start deploying those controls without actually taking that path into a multi-account just now.

[![Thumbnail 850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/850.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=850)

So the control catalog now can be accessed independently of the landing zone settings. So now you can explore those controls easily and deploy those if needed. And then again, auto-enrollment of accounts is something that was launched a few weeks back as  well. This allows you to actually migrate accounts easier into Control Tower. All those prerequisites that sometimes were a blocker for some customers, now they can actually do that easily because the service is actually preparing the account to be migrated into Control Tower.

[![Thumbnail 870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/870.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=870)

[![Thumbnail 880](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/880.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=880)

And then finally, the service-linked AWS Config rules, as I mentioned, those integrations, what this is doing is basically I'm now from Control Tower  I can deploy AWS Config rules and I'm totally confident that maybe somebody with access to my member accounts, even with permissions in AWS Config, they cannot touch those rules, so those become immutable rules.  And says, OK, all good. But then, now I have three services and a lot of our services maybe even underlying Control Tower, and we just saw only three, and we saw that there were a lot of updates on each one of them. So how can I actually keep the pace on these things?

[![Thumbnail 900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/900.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=900)

So this is when generative AI actually comes in handy. And the main service that we're going to explore today is Amazon Q.  Amazon Q is a conversational system powered by AI, of course, that is helping developers and IT professionals to write code, run tests on what they're doing,

[![Thumbnail 910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/910.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=910)

access AWS APIs, and this is when we start looking at how we can embed Amazon Q into AWS in my AWS environment. So I can install Amazon Q in my AWS CLI and I can actually take context and deploy commands and do those centrally from my terminal or even from CloudShell in AWS.  I can deploy code and I can connect to MCP servers and deploy agents using those MCP servers.

[![Thumbnail 960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/960.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=960)

So it comes with the capabilities of generative AI. You can have rationalization in those natural language conversations with the agent to come up with a strategy, and those are the sorts of things that it can definitely take a look at. And then you realize that AWS also provides managed MCP servers, right? So this is the capability for you to not be worried about the standardization or how many APIs are actually in the backend.  CloudTrail is a good example of this. They also launched a CloudTrail MCP server that allows you to even run queries without actually knowing how to run those queries, so you can actually just have a conversation to run investigations using these kinds of things. And then you can leverage knowledge MCP servers from AWS to actually start doing all the cool stuff, the real cool stuff, and that's when I'm going to move back to Pal. She's going to explain actually how to do all this and how she can automate her husband.

[![Thumbnail 1000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1000.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1000)

### Demo 1: Creating Organization Config Rules with Kiro CLI and MCP Servers

So it is demo time. Now, before we move on to the demo, I quickly want to share the current test environment that  Nita's team built for exploring these generative AI capabilities for accelerating governance. So on the top you have the management account. It is a standard Control Tower landing zone that I have built on behalf of Nita, and it has five organizational units that you can see on the right-hand side, and it has about 18 accounts. And for most of the demos that I have prepared, we will be deploying it over the Labs OU. And then on the bottom part you can see some of the shared services, standard accounts, log archive and audit accounts that come up when you build a landing zone with Control Tower.

[![Thumbnail 1050](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1050.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1050)

So with that, we are going to really start simple and with foundational rules, that is how to create an  organization Config rule. And for all that I have prepared three demos in total, and for all three, I'm going to use Kiro CLI that Bob introduced to my team. So give me a second here to quickly change to my demo environment.

[![Thumbnail 1080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1080.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1080)

[![Thumbnail 1090](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1090.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1090)

[![Thumbnail 1100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1100.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1100)

So I'm starting here with the Kiro CLI  chat. This is a default chat without passing any additional arguments, and that comes up when you do not pass any of the additional arguments to Kiro CLI. And I have loaded  a single MCP server here, and that server is called AWS MCP Core Server. This is a really good server  to get started on using AWS MCP servers because it utilizes dynamic proxy to import other groups of servers based on roles. Let's take a look at what I mean.

[![Thumbnail 1120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1120.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1120)

So this is the MCP config file that I've configured, and you can see that I have provided  two roles here, Solutions Architect and Foundation. Both of these roles are like logical groupings of MCP servers that are commonly used together for a specific use case. So Solutions Architect role here provides me access to MCP servers such as Knowledge, AWS APIs, Pricing, Cost Explorer, etc., while Foundation gives me access to Knowledge as well as AWS API MCP server.

[![Thumbnail 1160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1160.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1160)

Next, let's explore some of the tools that come up with some of these MCP servers. An interesting one here that  Core MCP server provides is Prompt Understanding. This tool really helps you when you are designing AWS solutions and using natural language to do it because it really helps translating your intent in terms of what it means and which services to utilize and features and such.

[![Thumbnail 1190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1190.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1190)

Moving on, I have also utilized knowledge base here, so I have loaded the knowledge about custom Config  rules from AWS GitHub into my knowledge base. So the Knowledge command here is like an experimental feature on Kiro CLI right now that helps you, once you enable it, that helps you store, search, and manage additional context across chat sessions. And I have used best indexing type here to index the knowledge base so that the agent can perform intelligent or semantic searches against my knowledge base whenever it requires that information.

[![Thumbnail 1230](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1230.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1230)

With that, I will move on to create a basic AWS Config organization rule that helps me detect EC2 instances launched with public IP addresses in my account.  I'm asking the agent to first find if a managed Config rule exists for the similar use case. If not, then go to building the custom Config rule for that. I'm also asking it to utilize the knowledge base in case it requires additional information and finally use infrastructure as code best practices so every action that it takes can be traced back and audited.

[![Thumbnail 1260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1260.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1260)

[![Thumbnail 1280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1280.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1280)

[![Thumbnail 1290](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1290.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1290)

Now let's look as the agent searches through the knowledge base.  It's using the MCP server to search through AWS documentation. For each of the tools here, it is actually asking me for permission because I have not trusted any of the tools. But with the letter T, I'm trusting all the tools that are coming up for the rest of the duration of this session.  So let's watch it. The agent also went through the knowledge base  just to load the additional context for custom rules in case it needs it. It makes API calls to my current environment to find that there are no similar rules already running in my account to avoid any duplication.

[![Thumbnail 1310](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1310.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1310)

[![Thumbnail 1320](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1320.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1320)

[![Thumbnail 1330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1330.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1330)

[![Thumbnail 1340](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1340.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1340)

[![Thumbnail 1350](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1350.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1350)

Good news,  the agent has found the managed Config rule for us, and it is called EC2_INSTANCE_NO_PUBLIC_IP.  Next, you will see that it will create the required CloudFormation templates and files. It will create them in  your current working directory if you have not provided any additional settings in the config file, so it is utilizing my current working directory.  Looking at the summary, it created a CloudFormation template for deploying the Config rule. It  created a shell script that will actually create the CloudFormation stack and then a README file. I reviewed the shell script, and now I am going to run that shell script from within the session. You can do so by preceding your command with an exclamation mark.

[![Thumbnail 1370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1370.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1370)

[![Thumbnail 1380](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1380.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1380)

[![Thumbnail 1390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1390.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1390)

[![Thumbnail 1410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1410.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1410)

 Looks like the deployment failed, and this is where the fun begins because now I can just ask Kiro to troubleshoot the issue for me  instead of me having to go through multiple events in CloudTrail and correlate them. It quickly went through the  CloudFormation stack events. It found the issue and it deleted the stack. The issue is regarding the resource type for the organization Config rule as it does not support rule ARN. Instead it returns rule name.  You can see it making changes. Whichever lines are in red, it removed that from the template versus green is the addition.

[![Thumbnail 1420](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1420.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1420)

[![Thumbnail 1440](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1440.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1440)

[![Thumbnail 1450](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1450.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1450)

[![Thumbnail 1460](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1460.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1460)

 Now it is waiting for the stack to delete before redeploying it. It redeployed again and this time you can notice that it didn't wait for me to explicitly tell it to troubleshoot. Instead it went ahead on its own. Again, the stack seems to have failed.  This time the issue is related to the frequency parameter. The EC2_INSTANCE_NO_PUBLIC_IP Config rule is triggered by change,  so frequency is not a valid parameter for this rule. It quickly identifies the issue and it follows the same sequence. It deletes the stack, waits for the deletion,  and deploys the stack again.

[![Thumbnail 1480](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1480.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1480)

[![Thumbnail 1490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1490.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1490)

[![Thumbnail 1500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1500.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1500)

Yay! The third time was the charm, and we have a successful stack deployment and hence a successful  deployment of the organization Config rule. It will also provide the summary of all the steps that it went through, so all the issues that it  found and the resolution that it applied. For the next part, I'm going to query. I have a Config aggregator which is like  you can aggregate all the Config events from all your AWS accounts in the organization into a single account.

[![Thumbnail 1510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1510.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1510)

[![Thumbnail 1520](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1520.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1520)

I already have that deployed by Control  Tower in my audit account that we previously saw, and I'm using that to query for the noncompliant  instances that were found in respect to the rule that we just deployed. We found three instances across two AWS accounts that we can see. This shows us how easy it was for Nita's team to do the research, do the deployment, and even run the query for noncompliance all from within a single session without having to leave the terminal.

[![Thumbnail 1550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1550.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1550)

### Demo 2: Implementing Shift Left Strategy with Preventive and Proactive Controls

With that, let me move on to my next demo.  With this, we are going to use the agent to implement shift left strategy for controls. Let's talk about shift left. Traditionally, compliance and security controls have been added late in the deployment lifecycle, and sometimes it becomes costly, causes delays, and also creates gaps in security. Shift left literally means shifting left your controls in the development lifecycle so you can detect the issues earlier on and fix them rather than having to fix them later.

What that means in terms of AWS infrastructure, or what that means for Nita's team, is she's going to implement two types of controls. First, preventive controls, which are powered by Service Control Policies and applied to an organization. Second is proactive controls, which are powered by CloudFormation Guard Hook that can actually detect noncompliance in your infrastructure templates and block it even before having to go through the deployment. In this demo, I will show you how Kiro helps implement both the preventive as well as proactive controls, making it easier to catch and stop issues before they reach production.

[![Thumbnail 1640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1640.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1640)

[![Thumbnail 1650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1650.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1650)

[![Thumbnail 1660](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1660.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1660)

Let me switch to my  demo environment again. This time, instead of using a default agent, I created a new agent called Shift  Left Specialist. In the agent config file, I defined such that the agent is a specialized security architect,  and its expertise is to define or create shift left equivalent controls of a given Config rule or a given detective rule in terms of both the preventive controls as well as the proactive controls. For this agent, I loaded two MCP servers, starting simple with Knowledge as well as API.

[![Thumbnail 1700](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1700.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1700)

We do have AWS CLI tool inside the Kiro CLI that comes in by default, so Kiro CLI can interact and  make AWS calls to your infrastructure. But I like to add the AWS MCP because I have noticed in all my tests that it quickly generates the accurate API or accurate CLI command without doing many retries with the syntax and the parameters that are supported.

[![Thumbnail 1730](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1730.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1730)

Moving on, let's take a look at context. This time, I've added an additional file called Shift Left Best  Practices Guide. This is a context file that provides some extra information to the agent about what is shift left, what is how to author the preventive as well as proactive controls on AWS. This is different from the knowledge base. It is simply increasing the context. It's not something that the agent would search. It's not indexed.

[![Thumbnail 1760](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1760.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1760)

This time, you would see that there are much fewer number of tools  than before because I have just two MCP servers loaded. On the permissions, you would also see that I have pre-trusted some of the tools that I knew were going to get used throughout this session, like reading the files, calling AWS APIs, and from Knowledge MCP we have read and search through the AWS documentation.

[![Thumbnail 1790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1790.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1790)

Now, the first part of shifting or implementing shift left is to identify  what noncompliant resources you have through detective controls, and that is what I'm asking it to do here. I'm asking it to query the top two noncompliant Config rules with the most number of noncompliant resources.

[![Thumbnail 1820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1820.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1820)

Let's watch as the agent knows to query the config aggregator in my account. It describes the rules within the config aggregator, thinks some more, and based on the data it collected, it gives me two rules.  The first one is KMS tagged key. This control detects which KMS keys are not tagged. The second control identifies which Lambda functions or production Lambda functions were not deployed with X-Ray tracing enabled.

[![Thumbnail 1860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1860.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1860)

[![Thumbnail 1870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1870.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1870)

Now that we have that information, I'm using the editor command here to provide a longer prompt that I already had prebuilt on the side. By default, it opens a VI editor, but you  can set the editor of your choice if you want. Here I'm asking it to develop the shift-left controls, and I'm specifically asking it  to deploy this through Customization for Control Tower pipeline, or CFCT solution. If you are familiar with that solution, it lets you customize your AWS accounts after they get provisioned through Control Tower. I'm saying that for the config rule, it should create a Service Control Policy and only do it for the keys with cost and environment tags and apply it on the overall organization, just for simplicity. For the Lambda config rule, I want to create a CloudFormation Guard hook that will prevent the noncompliant Lambdas from getting created in the first place, and I only want to deploy this control on a specific Organizational Unit of my organization.

[![Thumbnail 1930](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1930.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1930)

[![Thumbnail 1940](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1940.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1940)

[![Thumbnail 1950](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/1950.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=1950)

Now it has access to my CFCT manifest file through the current working directory,  and as the agent creates and updates the files, I'm going to skip through some of these and get us through the part where we can see the git summary  for what changes were implemented. I'll pause here, and here you can see  what the changes were that were made by the agent. Some files were changed while some were deleted and added. This is because we were not successful in deploying the controls in the first run, so I had some troubleshooting that I had to skip in the interest of time here, but I'll just give you a brief summary of what happened.

While it was very smooth in creating the right SCP for the KMS control, like detecting which KMS keys are not tagged, it had some challenges creating the CloudFormation Guard hook. One of the challenges with CloudFormation Guard hook is that it evaluates your entire stack and then decides whether it should let the stack go ahead with the deployment or not. Since I am creating the hook with the stack, I don't want that hook to run against the stack that created it. It's like a chicken and egg problem. We do have a parameter in the CloudFormation resource to skip the tag from running or from getting evaluated by the hook, and the agent required some back and forth for it to find the exact parameter. It's a niche skill or parameter that not everyone might know. But I did not just feed that information to the agent. Instead, I asked it to go through the public documentation through the Knowledge MCP server. It has access to AWS docs, blogs, as well as what's new information and what's new posts, so it was able to find that information through a blog post that we have.

[![Thumbnail 2060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2060.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2060)

[![Thumbnail 2070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2070.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2070)

 Now here is the summary of the commit that it will make. Here I'm just again running the command  to push the commit. I didn't want the agent to do it on this run. This in the backend has started our pipeline, the core pipeline for the Customization for Control Tower solution. Again, I'm going to check on the status of the CodePipeline that was triggered with this git push. Let's see. Now it knows the exact API calls to make. It lists the CodePipeline that I have available, lists the execution of the given pipeline, and also gets the details of the recent execution or the most latest execution, and it gives us the status of the pipeline which is currently in progress.

[![Thumbnail 2120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2120.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2120)

[![Thumbnail 2130](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2130.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2130)

[![Thumbnail 2140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2140.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2140)

 So I waited for some time for the pipeline deployment to complete on the side and checked the status of the pipeline  again. I ran the same command to list the pipeline, and it shows us that the pipeline has succeeded. It has succeeded in all the stages, with a  total of six stages.

[![Thumbnail 2150](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2150.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2150)

[![Thumbnail 2160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2160.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2160)

[![Thumbnail 2180](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2180.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2180)

Now, from the same session, I'm going to validate one of the controls, which is the CloudFormation Lambda  Hook or Guard Hook that it created for preventing noncompliant Lambda functions from being created.  Remember in the beginning that I asked to deploy these CloudFormation Guard Hooks only on the Labs OU, and the Labs OU has two accounts, so it chose one of the accounts to run that test. But when I started this session, I provided the credentials  for my management account of my organization, and so it quickly realizes that it doesn't have access to the member account.

[![Thumbnail 2220](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2220.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2220)

So at this stage, I will ask the agent to assume the Control Tower execution role in one of these member accounts and then test it. Now I had to skip through the part where it shows the entire test because it prints the AWS credentials on the screen, which I did not want to share for security reasons. So I'm just going to ask it to print the results of the validation test that it performed instead. And so that's the test environment.  One of the accounts in the Labs OU, you can see at the top, and it created two tests, one for the compliant Lambda function and one for the noncompliant.

[![Thumbnail 2250](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2250.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2250)

[![Thumbnail 2280](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2280.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2280)

In the compliant one, the Lambda function had X-Ray tracing enabled, as well as it was tagged with the production environment key that I passed in the original prompt, and that was successful. While for the noncompliant test, we have the  tag again, but it was missing the X-Ray tracing, so this deployment was blocked by the CloudFormation Hook. And that concludes my second demo. And again, even this shows how easy it was for Nita's team to test the shift-left strategy for implementing and how easily they can scale across 300+ accounts that they have in their environment. 

[![Thumbnail 2330](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2330.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2330)

### Demo 3: Deploying SRA-Aligned Patching Solution Through Account Customization

I'm going to shift to our next and final demo of the session, which is I'm going to use this same agent to create an AWS account and deploy an SRA-aligned patching solution. So for those of you who may not know, SRA is like a Security Reference Architecture. It's a holistic guideline that AWS provides to implement a complete set of security services that we have following AWS best practices. It has a whole GitHub repository for this. And so today for this demo I'll use Kiro CLI to do two tasks. One is to create an AWS account and then deploy the SRA-aligned patching solution on one of the existing accounts. Actually, I'm  doing it on the Labs OU again just for simplicity.

[![Thumbnail 2360](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2360.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2360)

[![Thumbnail 2370](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2370.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2370)

So this is the architecture of the SRA-aligned patching solution from the GitHub, and you can see that it uses Patch Manager from Systems Manager along with Maintenance Windows to run these appropriate baselines and patch the instances. So let's see how our agent is able to interpret this solution that I will feed and implement the design through CFCT CodePipeline.  Let me switch to my third and final demo. 

[![Thumbnail 2390](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2390.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2390)

[![Thumbnail 2400](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2400.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2400)

Again, I created a new agent called Create AWS Account which specializes in  provisioning an AWS account and also applying account customization. Here I've loaded three MCP servers. Now let's look at the knowledge base, 

[![Thumbnail 2410](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2410.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2410)

[![Thumbnail 2430](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2430.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2430)

I have added the entire GitHub repository of Security Reference Architecture in my knowledge base and  indexed it with the best type so that the agent can do an intelligent search against this. Let's look at the context. I've also added an additional file about what are the general foundational best practices for account customization  in my context, and you can also see the percentage of context window it used. It's not much.

[![Thumbnail 2470](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2470.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2470)

[![Thumbnail 2490](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2490.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2490)

[![Thumbnail 2500](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2500.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2500)

I'm going to use the editor command and I will provide all the required parameters for creating an AWS account. It's the usual one: email address, the Organizational Unit, account name, and so on. Let's watch as the agent exactly knew which method to create the account with, which  was to use the Service Catalog product that Control Tower already provides you once you create your landing zone with Control Tower. It searches my available products, it searches the product that Control Tower created. It also finds the provisioning  parameters that are required for deploying this product through Service Catalog and then verifies if I have provided all the parameters in my prompt. 

[![Thumbnail 2510](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2510.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2510)

[![Thumbnail 2530](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2530.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2530)

[![Thumbnail 2550](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2550.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2550)

This line is important. This is a heavy  change that will create an AWS account. How did the agent determine this? Because I've defined what a heavy change is in my context file that I initially fed to my agent. Before making any of the heavy changes, I want the agent to ask for my confirmation, and that is what it exactly did. Once I confirm,  it goes ahead to create the Service Catalog product that will provision the AWS account for us. I'm just going to check on the status of the latest account that was created, and here's the account we have. 

[![Thumbnail 2560](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2560.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2560)

[![Thumbnail 2580](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2580.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2580)

[![Thumbnail 2600](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2600.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2600)

[![Thumbnail 2610](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2610.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2610)

Really simple. For the next part, now we are going to apply the customization,  the Security Reference Architecture aligned patching solution that we previously saw. For that, I'm asking the agent to take, again, it has the context about my current working directory, my Customization for Control Tower solution, my pipeline, and the Security Reference Architecture.  The Security Reference Architecture documentation or the GitHub repository that are fed in through the knowledge base. You can see how it searches through different things. It goes through my current working directory to find the manifest file. It understands that, then it searches the knowledge base  for the reference architecture and finds relevant, tries to find the relevant components of the patching solution to design the patching solution that I asked  for.

[![Thumbnail 2620](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2620.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2620)

[![Thumbnail 2640](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2640.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2640)

[![Thumbnail 2650](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2650.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2650)

Looking at the deployment plan that it provides, it provides the current state architecture, the existing ones,  existing resources that I already have, and the future state architecture. For the future state, it provides me with two CloudFormation templates that is going to deploy my patching solution. One is the prerequisite template that has the prerequisite for  creating the baselines or the maintenance window and so on. The second is the main solution that will actually deploy the maintenance window. Here we have three maintenance  windows. One is going to update the Systems Manager agent that is required for the patching. The second is run the maintenance for, sorry, run the patching for Windows, and the third is run the patching for Linux instances.

[![Thumbnail 2670](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2670.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2670)

[![Thumbnail 2680](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2680.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2680)

[![Thumbnail 2690](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2690.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2690)

We can also take a look at some of the key  configuration parameters, like what is the schedule of my maintenance window. Do I want to run the patching in the install mode or the scan mode?  I have not provided any of this to the agent. All I've done is provided the prompt that we saw in the beginning.  It identified some of the key parameters that needs confirmation from me. One is I did not mention where to deploy the solution, so it is confirming if it should start with the Labs Organizational Unit because it has knowledge of my environment.

[![Thumbnail 2740](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2740.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2740)

It picked the Labs OU based on the name, I guess. It asks whether I want to patch in scan mode or directly go to install mode. It also asks if the schedules are okay and whether I have EC2 instances already tagged. I'm going to provide answers to all the questions here. Sorry for the text, it is at the bottom, but I couldn't scroll it at that point. 

[![Thumbnail 2750](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2750.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2750)

[![Thumbnail 2780](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2780.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2780)

Now that it has confirmation, it moves on to create the CloudFormation templates and all the other required files. Let me skip past some of this  and get us through the git summary so that we can look at the changes. We saw that it created two CloudFormation templates, one for the prerequisites and one for the main solution. This is the manifest file which controls where the CloudFormation StackSets would be deployed, which is Labs OU only. It also provides me with a markdown  file of what all the changes it did and how.

[![Thumbnail 2790](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2790.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2790)

[![Thumbnail 2800](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2800.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2800)

[![Thumbnail 2810](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2810.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2810)

[![Thumbnail 2820](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2820.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2820)

I'm going to ask it to commit the changes which will trigger our CodePipeline in the backend.  This time it already had the command to do a git push, so when I asked it to commit, it also pushed the commit  to my repository. We are going to check on the status of the CodePipeline again.  It'll go through the same sequence of events, describing the pipeline, the latest execution, and getting the details for us. Currently the pipeline is in  progress at the second stage, which is the build stage.

[![Thumbnail 2840](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2840.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2840)

[![Thumbnail 2850](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2850.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2850)

[![Thumbnail 2860](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2860.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2860)

[![Thumbnail 2870](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2870.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2870)

Now, for this one, while I was waiting for the demo, I decided to experiment with the tangent mode. Tangent mode kind of allows you to have a side conversation without spoiling the context of your current environment or  whatever current session you have. Here I'm just asking it, what are some of the top feature launches at 2025 re:Invent  specifically for AWS Organizations and AWS Control Tower services. You can see it searches through AWS documentation using different sets of keywords. It also searches  through the What's New posts and blog posts because we do announcements through all these different channels. Based on the search, it provided a list of a couple of things,  actually there were more in AWS Organizations like direct account transfers and transfer billing. You can see some of the announcements that Rodolfo also spoke about for AWS Control Tower, which is Controls Dedicated experience, more rules, more compliance frameworks, and so on.

[![Thumbnail 2890](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2890.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2890)

[![Thumbnail 2900](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2900.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2900)

[![Thumbnail 2910](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2910.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2910)

Now that we are done having fun, let's  get out of the tangent mode and head back to check on the status of our pipeline.  It makes the same calls again and looks like we have a successful pipeline deployment.  This was a really smooth experience. Again, this shows how easy it was for Nita's team to test. Sometimes these account customizations get really complex, and even for humans, it's kind of not every human would have all the necessary context. But it's easier to feed such large context and knowledge base to the agent and train it. We also saw that their team kind of utilized these specialized agents instead, so having the agents do very specific dedicated tasks, and that's why they got great results.

[![Thumbnail 2960](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2960.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2960)

[![Thumbnail 2980](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2980.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2980)

[![Thumbnail 2990](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/2990.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=2990)

[![Thumbnail 3000](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3000.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3000)

I also have one more demo here. Let me switch. I promise this is the last one.  It is just a validation of what the agent deployed. Now I am in the management account of my Control Tower in the console, and I'm going through CloudFormation just to show you the StackSets that it deployed. Remember how we  saw two stacks that the agent provided us for the patching solution. One was the prerequisites, the other was the main solution. These are those two StackSets  that were created through the pipeline. Let's go into one of these StackSets and take a look that this was deployed into two of our  accounts which are under Labs OU.

[![Thumbnail 3010](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3010.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3010)

I'm going to log into one of the accounts that I'm highlighting here. So this is that account again.  I am in the CloudFormation stacked window. This is the stack it created, and take a look at the timestamp. If you noticed it in my last demo, it should match with the execution time of our pipeline, just to confirm that these stacks are actually from that build.

[![Thumbnail 3030](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3030.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3030)

[![Thumbnail 3060](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3060.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3060)

[![Thumbnail 3070](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3070.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3070)

[![Thumbnail 3080](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3080.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3080)

 Now I am in the Systems Manager console, where I'm showing the three maintenance windows that we saw, each for doing Linux patching, Windows patching, and just updating the SSM agents. Let me get into the detail of one of those windows, which is for Linux. I'll head into the task to see  actually what it created, so it shows the default patch, default document to run the patch baseline.  Then, towards the bottom, we will see the operations type, which is scan. This is what I had prompted in my  earlier demo.

[![Thumbnail 3100](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3100.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3100)

[![Thumbnail 3110](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3110.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3110)

[![Thumbnail 3120](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3120.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3120)

 Next up, I just quickly go into the targets.  It's selected the instance tags, selected to specify or select instances based on the tags, and these were the tags that we had earlier confirmed in our prompt.  And finally, this is the execution. If you remember the schedule of the maintenance window, this is one of the last executions that ran successfully.

[![Thumbnail 3140](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3140.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3140)

[![Thumbnail 3160](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3160.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3160)

### Key Takeaways: Start Small, Think Big, Scale Fast with Generative AI

 That concludes the end of all of my demos. Here are some additional resources for you to review before I summarize our journey or Nita's journey. First, you have Cloud Governance recent launches and resources. Second is Kiro CLI docs.  These are great docs to get you started with how to install Kiro and utilize all the niche commands and tools that it supports. The last one is the AWS MCP Servers GitHub, which is where we publish servers for all the services, the CloudTrail one that Rodolfo mentioned, as well as all the other ones that I utilized in my demo.

[![Thumbnail 3190](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3190.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3190)

Now to summarize Nita's journey,  if you think back, why they were successful was, as I was talking earlier, they utilized the agents for niche use cases. They provided as much context that was available, they provided good examples for each of the tasks that they wanted to implement, and that is why it was successful for them. More importantly, they started small, really simple, from a simple Config rule to deploying a complex account customization, and with each test it kind of built trust with their team in how generative AI can help accelerate the governance as well as controls for them.

[![Thumbnail 3260](https://raw.githubusercontent.com/kazuya-iwami/devto-repo/main/posts/images/4e130a97ab28f28a/3260.jpg)](https://www.youtube.com/watch?v=bRTSI-UKl0s&t=3260)

So just start small, think big, and scale fast, and you can leverage Kiro CLI today to accelerate the governance as well as controls. GenAI isn't just a tool. It can really help you work smarter, not harder, or at least it has helped me a lot in my role. Before you leave, please drop by our Cloud Ops kiosk in AWS Expo hall, and then you can have more deeper  conversations around the stuff that you have seen today. There we have experts that can also give you one-on-one demos for all the cloud governance related services and recent launches. We also have swag and stickers for you if you like them. Finally, thank you for your time, and please don't forget to take a session survey on your mobile app. Thanks.


----

; This article is entirely auto-generated using Amazon Bedrock.
